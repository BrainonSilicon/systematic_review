Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
"Baceviciute S., Terkildsen T., Makransky G.","55441702500;57205338475;50361371800;","Remediating learning from non-immersive to immersive media: Using EEG to investigate the effects of environmental embeddedness on reading in Virtual Reality",2021,"Computers and Education","164",, 104122,"","",,,"10.1016/j.compedu.2020.104122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100105880&doi=10.1016%2fj.compedu.2020.104122&partnerID=40&md5=0e064f3f47964fe4caef507e98daa3ff","University of Copenhagen, Department of Psychology, Denmark","Baceviciute, S., University of Copenhagen, Department of Psychology, Denmark; Terkildsen, T., University of Copenhagen, Department of Psychology, Denmark; Makransky, G., University of Copenhagen, Department of Psychology, Denmark","Virtual Reality (VR) has the potential to enrich education but little is known about how unique affordances of immersive technology might influence leaning and cognition. This study investigates one particular affordance of VR, namely environmental embeddedness, which enables learners to be situated in simulated or imagined settings that contextualize their learning. A sample of 51 university students were administered written learning material in a between-subjects design study, wherein one group read text about sarcoma cancer on a physical pamphlet in the real world, and the other group read identical text on a virtual pamphlet embedded in an immersive VR environment which resembled a hospital room. The study combined advanced EEG measurement techniques, learning tests, and cognitive load measures to compare conditions. Results show that the VR group performed significantly better on a knowledge transfer post-test. However, reading in VR was found to be more cognitively effortful and less time-efficient. Findings suggest the significance of environmental embeddedness for learning, and provide important considerations for the design of educational VR environments, as we remediate learning content from non-immersive to immersive media. © 2021 Elsevier Ltd","EEG; Embeddedness; Learning; Remediation; Virtual reality environments","E-learning; Knowledge management; Remediation; Cognitive loads; Immersive media; Immersive technologies; Knowledge transfer; Learning contents; Learning materials; Measurement techniques; University students; Virtual reality",Article,"Final","",Scopus,2-s2.0-85100105880
"Araiza-Alba P., Keane T., Chen W.S., Kaufman J.","57191667278;55582336900;57188973057;7403022831;","Immersive virtual reality as a tool to learn problem-solving skills",2021,"Computers and Education","164",, 104121,"","",,,"10.1016/j.compedu.2020.104121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099353627&doi=10.1016%2fj.compedu.2020.104121&partnerID=40&md5=82079d081459ac2caf3cea01c4ad7087","Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia","Araiza-Alba, P., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Keane, T., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Chen, W.S., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Kaufman, J., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia","Immersive virtual reality (IVR) technology has demonstrated positive educational outcomes related to its use and is gaining traction in educational and training settings; IVR is expected to have widespread adoption within the classroom in the upcoming years. However, the educational potential of IVR with children has not been thoroughly investigated, especially as a tool for problem-solving skills. Therefore, this study was designed to answer the following questions: (1) Is IVR a useful tool to learn and practice problem-solving skills? More specifically, do children using IVR solve a game better than those using a tablet application or a board game? (2) Does IVR provide a more engaging experience for children to practice problem-solving skills than on a tablet or a board game? (3) Do problem-solving skills learned with IVR technology transfer to real-life (physical game)? Children (n = 120) aged 7–9.9 years were randomly assigned to a problem-solving game in one of three conditions: board game, tablet, or IVR. The results showed that, overall, the percentage of children who completed the problem-solving game was higher in the IVR condition (77.5%), compared with those in the tablet (32.5%) or board game (30%) conditions. We also found that the interest and enjoyment scores of participants using IVR were significantly higher than participants in the other two conditions, and that the children in the IVR condition were able to learn how to solve the problem and transfer their learning to the physical game. IVR is a technology capable of engaging interest and motivating the user, as well as having the potential to assist in cognitive processing and knowledge transfer. © 2021 Elsevier Ltd","21st century abilities; Elementary education; Games; Problem-solving skills; Virtual reality","Knowledge management; Technology transfer; Transfer learning; Board games; Cognitive processing; Educational potential; Immersive virtual reality; Knowledge transfer; Physical games; Problem solving skills; Tablet applications; Virtual reality",Article,"Final","",Scopus,2-s2.0-85099353627
"Kourtesis P., Collina S., Doumas L.A.A., MacPherson S.E.","57210959726;35309731500;12345301900;57212061035;","Validation of the Virtual Reality Everyday Assessment Lab (VR-EAL): An Immersive Virtual Reality Neuropsychological Battery with Enhanced Ecological Validity",2021,"Journal of the International Neuropsychological Society","27","2",,"181","196",,2,"10.1017/S1355617720000764","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095727298&doi=10.1017%2fS1355617720000764&partnerID=40&md5=682a1271572f8544a09deaa59f15dad1","Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy; Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy","Kourtesis, P., Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy, Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy; Collina, S., Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy, Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy; Doumas, L.A.A., Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; MacPherson, S.E., Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom","The assessment of cognitive functions such as prospective memory, episodic memory, attention, and executive functions benefits from an ecologically valid approach to better understand how performance outcomes generalize to everyday life. Immersive virtual reality (VR) is considered capable of simulating real-life situations to enhance ecological validity. The present study attempted to validate the Virtual Reality Everyday Assessment Lab (VR-EAL), an immersive VR neuropsychological battery, against an extensive paper-and-pencil neuropsychological battery. Methods: Forty-one participants (21 females) were recruited: 18 gamers and 23 non-gamers who attended both an immersive VR and a paper-and-pencil testing session. Bayesian Pearson's correlation analyses were conducted to assess construct and convergent validity of the VR-EAL. Bayesian t-tests were performed to compare VR and paper-and-pencil testing in terms of administration time, similarity to real-life tasks (i.e., ecological validity), and pleasantness. Results: VR-EAL scores were significantly correlated with their equivalent scores on the paper-and-pencil tests. The participants' reports indicated that the VR-EAL tasks were significantly more ecologically valid and pleasant than the paper-and-pencil neuropsychological battery. The VR-EAL battery also had a shorter administration time. Conclusion: The VR-EAL appears as an effective neuropsychological tool for the assessment of everyday cognitive functions, which has enhanced ecological validity, a highly pleasant testing experience, and does not induce cybersickness. © 2020 INS. Published by Cambridge University Press.","Attention; Episodic memory; Everyday functioning; Executive function; Prospective memory; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85095727298
"Wilf M., Cerra Cheraka M., Jeanneret M., Ott R., Perrin H., Crottaz-Herbette S., Serino A.","36515832500;57222264361;57221090466;57221094468;57221091485;57202925776;23390163100;","Combined virtual reality and haptic robotics induce space and movement invariant sensorimotor adaptation",2021,"Neuropsychologia","150",, 107692,"","",,,"10.1016/j.neuropsychologia.2020.107692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098145308&doi=10.1016%2fj.neuropsychologia.2020.107692&partnerID=40&md5=1d5ecc00fd2c0fe69faadad48c343573","MySpace Lab, Department of Clinical Neurosciences, Lausanne University Hospital (CHUV), Lausanne, Switzerland; MindMaze SA, Chemin de Roseneck 5, Lausanne, 1006, Switzerland; Neuropsychology and Neurorehabilitation Service, Lausanne University Hospital (CHUV), Lausanne, Switzerland","Wilf, M., MySpace Lab, Department of Clinical Neurosciences, Lausanne University Hospital (CHUV), Lausanne, Switzerland; Cerra Cheraka, M., MindMaze SA, Chemin de Roseneck 5, Lausanne, 1006, Switzerland; Jeanneret, M., MindMaze SA, Chemin de Roseneck 5, Lausanne, 1006, Switzerland; Ott, R., MindMaze SA, Chemin de Roseneck 5, Lausanne, 1006, Switzerland; Perrin, H., MySpace Lab, Department of Clinical Neurosciences, Lausanne University Hospital (CHUV), Lausanne, Switzerland; Crottaz-Herbette, S., Neuropsychology and Neurorehabilitation Service, Lausanne University Hospital (CHUV), Lausanne, Switzerland; Serino, A., MySpace Lab, Department of Clinical Neurosciences, Lausanne University Hospital (CHUV), Lausanne, Switzerland, MindMaze SA, Chemin de Roseneck 5, Lausanne, 1006, Switzerland","Prism adaptation is a method for studying visuomotor plasticity in healthy individuals, as well as for rehabilitating patients suffering spatial neglect. We developed a new set-up based on virtual-reality (VR) and haptic-robotics allowing us to induce sensorimotor adaptation and to reproduce the effect of prism adaptation in a more ecologically valid, yet experimentally controlled context. Participants were exposed to an immersive VR environment while controlling a virtual hand via a robotic-haptic device to reach virtual objects. During training, a rotational shift was induced between the position of the participant's real hand and that of the virtual hand in order to trigger sensorimotor recalibration. The use of VR and haptic-robotics allowed us to simulate and test multiple components of sensorimotor adaptation: training either peripersonal or extrapersonal space and testing generalization for the non-trained sector of space, and using active versus robot-guided reaching movements. Results from 60 neurologically intact participants show that participants exposed to the virtual shift were able to quickly adapt their reaching movements to aim correctly at the target objects. When the shift was removed, participants showed a systematic deviation of their movements during open-loop tasks in the direction opposite to that of the shift, which generalized to un-trained portions of space and occurred also when their movements were robotically-guided during the adaptation. Interestingly, follow-up questionnaires revealed that when the adaptation training was robotically-guided, participants were largely unaware of the mismatch between their hand and the virtual hand's position. The stability of the aftereffects, despite the changing experimental parameters, suggests that the induced sensory-motor adaptation does not rely on low-level processing of sensory stimuli during the training, but taps into high-level representations of space. Importantly, the flexibility of the trained space and the option of robotically-guided movements open novel possibilities of fine-tuning the training to patients’ level of spatial and motor impairment, thus possibly resulting in a better outcome. © 2020 The Author(s)","Far space; Guided movement; Haptic robot; Hemispatial neglect; Passive movement; Prism adaptation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85098145308
"Haar S., Sundar G., Faisal A.A.","56497714500;57221842054;6602900233;","Embodied virtual reality for the study of real-world motor learning",2021,"PLoS ONE","16","1 January", e0245717,"","",,,"10.1371/journal.pone.0245717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100402457&doi=10.1371%2fjournal.pone.0245717&partnerID=40&md5=ea815d1c3bc8770fcd8183182d0d35ba","Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, London, United Kingdom; Dept. of Computing, Imperial College London, London, United Kingdom; UKRI Centre for Doctoral Training in AI for Healthcare, Imperial College London, London, United Kingdom; MRC London Institute of Medical Sciences, Imperial College London, London, United Kingdom","Haar, S., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, London, United Kingdom; Sundar, G., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, London, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, London, United Kingdom, Dept. of Computing, Imperial College London, London, United Kingdom, UKRI Centre for Doctoral Training in AI for Healthcare, Imperial College London, London, United Kingdom, MRC London Institute of Medical Sciences, Imperial College London, London, United Kingdom","Motor-learning literature focuses on simple laboratory-tasks due to their controlled manner and the ease to apply manipulations to induce learning and adaptation. Recently, we introduced a billiards paradigm and demonstrated the feasibility of real-world-neuroscience using wearables for naturalistic full-body motion-tracking and mobile-brain-imaging. Here we developed an embodied virtual-reality (VR) environment to our real-world billiards paradigm, which allows to control the visual feedback for this complex real-world task, while maintaining sense of embodiment. The setup was validated by comparing real-world ball trajectories with the trajectories of the virtual balls, calculated by the physics engine. We then ran our short-term motor learning protocol in the embodied VR. Subjects played billiard shots when they held the physical cue and hit a physical ball on the table while seeing it all in VR. We found comparable short-term motor learning trends in the embodied VR to those we previously reported in the physical real-world task. Embodied VR can be used for learning real-world tasks in a highly controlled environment which enables applying visual manipulations, common in laboratory-tasks and rehabilitation, to a real-world full-body task. Embodied VR enables to manipulate feedback and apply perturbations to isolate and assess interactions between specific motor-learning components, thus enabling addressing the current questions of motor-learning in real-world tasks. Such a setup can potentially be used for rehabilitation, where VR is gaining popularity but the transfer to the real-world is currently limited, presumably, due to the lack of embodiment. Copyright: © 2021 Haar et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; article; motor learning; physics; rehabilitation; virtual reality; visual feedback",Article,"Final","",Scopus,2-s2.0-85100402457
"Harris D.J., Hardcastle K.J., Wilson M.R., Vine S.J.","57192429891;57221862684;55574207642;36811509000;","Assessing the learning and transfer of gaze behaviours in immersive virtual reality",2021,"Virtual Reality",,,,"","",,,"10.1007/s10055-021-00501-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100476971&doi=10.1007%2fs10055-021-00501-w&partnerID=40&md5=ad4779acdc2d1563f331327866d64c86","School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Counter Terrorism Protective Security Operations, Metropolitan Police Service, Lambeth HQ, London, SE1 7LP, United Kingdom","Harris, D.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Hardcastle, K.J., Counter Terrorism Protective Security Operations, Metropolitan Police Service, Lambeth HQ, London, SE1 7LP, United Kingdom; Wilson, M.R., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Vine, S.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom","Virtual reality (VR) has clear potential for improving simulation training in many industries. Yet, methods for testing the fidelity, validity and training efficacy of VR environments are, in general, lagging behind their adoption. There is limited understanding of how readily skills learned in VR will transfer, and what features of training design will facilitate effective transfer. Two potentially important elements are the psychological fidelity of the environment, and the stimulus correspondence with the transfer context. In this study, we examined the effectiveness of VR for training police room searching procedures, and assessed the corresponding development of perceptual-cognitive skill through eye-tracking indices of search efficiency. Participants (n = 54) were assigned to a VR rule-learning and search training task (FTG), a search only training task (SG) or a no-practice control group (CG). Both FTG and SG developed more efficient search behaviours during the training task, as indexed by increases in saccade size and reductions in search rate. The FTG performed marginally better than the CG on a novel VR transfer test, but no better than the SG. More efficient gaze behaviours learned during training were not, however, evident during the transfer test. These findings demonstrate how VR can be used to develop perceptual-cognitive skills, but also highlight the challenges of achieving transfer of training. © 2021, The Author(s).","Fidelity; Police; Policing; Training; Validity; VR","E-learning; Eye tracking; Cognitive skill; Gaze behaviours; Immersive virtual reality; Search behaviours; Search efficiency; Simulation training; Training design; Transfer of trainings; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85100476971
"Rojas-Muñoz E., Lin C., Sanchez-Tamayo N., Cabrera M.E., Andersen D., Popescu V., Barragan J.A., Zarzaur B., Murphy P., Anderson K., Douglas T., Griffis C., McKee J., Kirkpatrick A.W., Wachs J.P.","57205650789;57193616240;57195214444;56661822000;56661805900;7103266698;57218000158;57207592471;57219322088;57212093286;57210912536;57210919543;55233918200;7103368482;9241519000;","Evaluation of an augmented reality platform for austere surgical telementoring: a randomized controlled crossover study in cricothyroidotomies",2020,"npj Digital Medicine","3","1", 75,"","",,3,"10.1038/s41746-020-0284-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087848610&doi=10.1038%2fs41746-020-0284-9&partnerID=40&md5=68d255c42100215022ef3ba3cf678424","School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Department of Computer Science, Purdue University, West Lafayette, IN, United States; Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Naval Medical Center Portsmouth, Portsmouth, VA, United States; Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada; Department of Critical Care Medicine, University of Calgary, Calgary, AB, Canada; Canadian Forces Medical Services, Ottawa, ON, Canada","Rojas-Muñoz, E., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Lin, C., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Sanchez-Tamayo, N., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Cabrera, M.E., Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Andersen, D., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Popescu, V., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Barragan, J.A., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Zarzaur, B., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Murphy, P., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Anderson, K., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Douglas, T., Naval Medical Center Portsmouth, Portsmouth, VA, United States; Griffis, C., Naval Medical Center Portsmouth, Portsmouth, VA, United States; McKee, J., Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada; Kirkpatrick, A.W., Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada, Department of Critical Care Medicine, University of Calgary, Calgary, AB, Canada, Canadian Forces Medical Services, Ottawa, ON, Canada; Wachs, J.P., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States, Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States","Telementoring platforms can help transfer surgical expertise remotely. However, most telementoring platforms are not designed to assist in austere, pre-hospital settings. This paper evaluates the system for telementoring with augmented reality (STAR), a portable and self-contained telementoring platform based on an augmented reality head-mounted display (ARHMD). The system is designed to assist in austere scenarios: a stabilized first-person view of the operating field is sent to a remote expert, who creates surgical instructions that a local first responder wearing the ARHMD can visualize as three-dimensional models projected onto the patient’s body. Our hypothesis evaluated whether remote guidance with STAR could lead to performing a surgical procedure better, as opposed to remote audio-only guidance. Remote expert surgeons guided first responders through training cricothyroidotomies in a simulated austere scenario, and on-site surgeons evaluated the participants using standardized evaluation tools. The evaluation comprehended completion time and technique performance of specific cricothyroidotomy steps. The analyses were also performed considering the participants’ years of experience as first responders, and their experience performing cricothyroidotomies. A linear mixed model analysis showed that using STAR was associated with higher procedural and non-procedural scores, and overall better performance. Additionally, a binary logistic regression analysis showed that using STAR was associated to safer and more successful executions of cricothyroidotomies. This work demonstrates that remote mentors can use STAR to provide first responders with guidance and surgical knowledge, and represents a first step towards the adoption of ARHMDs to convey clinical expertise remotely in austere scenarios. © 2020, The Author(s).",,"adult; Article; augmented reality; controlled study; female; human; male; mentoring; priority journal; rating scale; surgeon; telementoring; tracheotomy; work experience",Article,"Final","",Scopus,2-s2.0-85087848610
"Brickler D., Teather R.J., Duchowski A.T., Babu S.V.","57063224300;24588246800;6701824388;9039004700;","A fitts’ law evaluation of visuo-haptic fidelity and sensory mismatch on user performance in a near-field disc transfer task in virtual reality",2020,"ACM Transactions on Applied Perception","17","4", 15,"","",,,"10.1145/3419986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098327645&doi=10.1145%2f3419986&partnerID=40&md5=405cfd1df7d08411fef108bce167fcb8","Clemson University, School of Computing, 100 McAdams Hall, Clemson, SC  29634, United States; Carleton University, School of Information Technology, 230 Azrieli Pavilion, 1125 Colonel By Drive, Ottawa, ON  K1S 5B6, Canada","Brickler, D., Clemson University, School of Computing, 100 McAdams Hall, Clemson, SC  29634, United States; Teather, R.J., Carleton University, School of Information Technology, 230 Azrieli Pavilion, 1125 Colonel By Drive, Ottawa, ON  K1S 5B6, Canada; Duchowski, A.T., Clemson University, School of Computing, 100 McAdams Hall, Clemson, SC  29634, United States; Babu, S.V., Clemson University, School of Computing, 100 McAdams Hall, Clemson, SC  29634, United States","The trade-off between speed and accuracy in precision tasks is important to evaluate during user interaction with input devices. When different sensory cues are added or altered in such interactions, those cues have an effect on this trade-off, and thus, they affect overall user performance. For instance, adding cues like haptic feedback and stereoscopic viewing will result in more realistic user interaction, thus improving performance in these tasks. Also, adding a noticeable disparity between physical and virtual movements creates a mismatch between visual and proprioceptive systems, which generally has a negative effect on performance. In this study, we investigate the effects of haptic feedback, stereoscopic viewing, and visuo-proprioceptive mismatch on how quickly and accurately users complete a virtual pick-and-place task using the PHANToM OMNI. Through this experiment, we find that in the movement phase of a ring transfer, movement time and user performance are affected by haptic feedback and visuo-proprioceptive mismatch, and the main effects of stereoscopic viewing appears to be limited to the more precise step when the ring is around the target peg. © 2020 Association for Computing Machinery.","Fitts’ law; Haptics; Near-field virtual reality; Stereo","Economic and social effects; Stereo image processing; Haptic feedbacks; Improving performance; Law evaluation; Movement time; Pick and place; Stereoscopic viewing; User interaction; User performance; Virtual reality",Article,"Final","",Scopus,2-s2.0-85098327645
"Saghafian M., Laumann K., Akhtar R.S., Skogstad M.R.","57220037198;6602490074;57220035991;57215534124;","The Evaluation of Virtual Reality Fire Extinguisher Training",2020,"Frontiers in Psychology","11",, 593466,"","",,,"10.3389/fpsyg.2020.593466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096573932&doi=10.3389%2ffpsyg.2020.593466&partnerID=40&md5=fca9427d1ab4d858a5e268f33309576e","Department of Psychology, Faculty of Social and Educational Sciences, Norwegian University of Science and Technology, Trondheim, Norway","Saghafian, M., Department of Psychology, Faculty of Social and Educational Sciences, Norwegian University of Science and Technology, Trondheim, Norway; Laumann, K., Department of Psychology, Faculty of Social and Educational Sciences, Norwegian University of Science and Technology, Trondheim, Norway; Akhtar, R.S., Department of Psychology, Faculty of Social and Educational Sciences, Norwegian University of Science and Technology, Trondheim, Norway; Skogstad, M.R., Department of Psychology, Faculty of Social and Educational Sciences, Norwegian University of Science and Technology, Trondheim, Norway","The aim of this research was to explore trainees’ perceptions and evaluation of Virtual Reality fire extinguisher training. Virtual Reality technology is being adopted by many industries for various purposes including safety training for safety critical industries. The future direction of Virtual Reality training requires an understanding of trainees’ evaluation of it; this fact motivated this research. Data were collected from 85 participants using a questionnaire after the training. Observation notes were taken to provide a better understanding of the context. Qualitative research with a thematic analysis was used to analyze the data. The results of this analysis revealed that the most salient themes reflect on issues surrounding the realism of the Virtual Reality simulation, namely different emotional and bodily experiences during the training, while the benefits of the training (health, safety, environmental advantages, efficiency and convenience, repeatability and variety of scenarios) make it a good supplement. Nevertheless, improved realism is needed to make it more effective and enhance transfer and acceptance. This study encourages the consideration of important matters (such as realism and emotions) when using Virtual Reality for fire training. It also describes the positive perceptions of this type of training (repeatability of training, safety and environmental concerns). © Copyright © 2020 Saghafian, Laumann, Akhtar and Skogstad.","convenience; fire extinguisher training; realism; safety; virtual reality",,Article,"Final","",Scopus,2-s2.0-85096573932
"Monteiro D., Liang H.-N., Wang J., Chen H., Baghaei N.","57144011000;8636386200;57207048907;57221155309;14020983900;","An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284718,"713","724",,,"10.1109/ISMAR50242.2020.00102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099294395&doi=10.1109%2fISMAR50242.2020.00102&partnerID=40&md5=9a9ee4dfccc97f04f5650a4b63a6eea1","Xi'an Jiaotong-Liverpool University, China; Massey University, New Zealand","Monteiro, D., Xi'an Jiaotong-Liverpool University, China; Liang, H.-N., Xi'an Jiaotong-Liverpool University, China; Wang, J., Xi'an Jiaotong-Liverpool University, China; Chen, H., Xi'an Jiaotong-Liverpool University, China; Baghaei, N., Massey University, New Zealand","The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness. © 2020 IEEE.","2D/3D Views; Controller types; First Person Shooter; Gaming; Head-Mounted Displays; Virtual Reality","Augmented reality; Controllers; Diseases; Helmet mounted displays; Contributing factor; First person shooter games; Head mounted displays; Input methods; Input techniques; Simulator sickness; Visual techniques; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099294395
"Klingenberg S., Jørgensen M.L.M., Dandanell G., Skriver K., Mottelson A., Makransky G.","57214077924;56015706700;6603030047;6701521625;57191531178;50361371800;","Investigating the effect of teaching as a generative learning strategy when learning through desktop and immersive VR: A media and methods experiment",2020,"British Journal of Educational Technology","51","6",,"2115","2138",,3,"10.1111/bjet.13029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091609462&doi=10.1111%2fbjet.13029&partnerID=40&md5=c050635d4c8cd7c89671a2651c5559af","Virtual Learning Lab at the Department of Psychology at the University of Copenhagen, United States; Department of Biology at the University of Copenhagen, United States; Department of Psychology at the University of Copenhagen, United States","Klingenberg, S., Virtual Learning Lab at the Department of Psychology at the University of Copenhagen, United States; Jørgensen, M.L.M., Department of Biology at the University of Copenhagen, United States; Dandanell, G., Department of Biology at the University of Copenhagen, United States; Skriver, K., Department of Biology at the University of Copenhagen, United States; Mottelson, A., Virtual Learning Lab at the Department of Psychology at the University of Copenhagen, United States; Makransky, G., Department of Psychology at the University of Copenhagen, United States","Immersive virtual reality (IVR) simulations for education have been found to increase affective outcomes compared to traditional media, but the effects on learning are mixed. As reflection has previously shown to enhance learning in traditional media, we investigated the efficacy of appropriate reflection exercises for IVR. In a 2 × 2 mixed-methods experiment, 89 (61 female) undergraduate biochemistry students learned about the electron transport chain through desktop virtual reality (DVR) and IVR (media conditions). Approximately, half of each group engaged in a subsequent generative learning strategy (GLS) of teaching in pairs (method conditions). A significant interaction between media and methods illustrated that the GLS of teaching significantly improved transfer (d = 1.26), retention (d = 0.60) and self-efficacy (d = 0.82) when learning through IVR, but not DVR. In the second part of the study, students switched media conditions and the experiment was repeated. This time, significant main effects favoring the IVR group on the outcomes of intrinsic motivation (d = 0.16), perceived enjoyment (d = 0.94) and presence (d = 1.29) were observed, indicating that students preferred IVR after having experienced both media conditions. The results support the view that methods enable media that affect learning and that the GLS of teaching is specifically relevant for IVR. © 2020 British Educational Research Association","biochemistry education; generative learning strategies; head-mounted displays; immersive virtual reality; learning; media versus methods","Electron transport properties; Students; Virtual reality; Desktop virtual reality; Electron transport chain; Enhance learning; Immersive virtual reality; Intrinsic motivation; Learning strategy; Perceived enjoyment; Self efficacy; Learning systems",Article,"Final","",Scopus,2-s2.0-85091609462
"Martin N., Mathieu N., Pallamin N., Ragot M., Diverrez J.-M.","57188745225;57221495248;13405769400;57073515600;56928312300;","Virtual reality sickness detection: An approach based on physiological signals and machine learning",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284654,"387","399",,,"10.1109/ISMAR50242.2020.00065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099321186&doi=10.1109%2fISMAR50242.2020.00065&partnerID=40&md5=eef96cc31598451773f458ec1c3fa5c9","Irt B<com, Cesson-Sevigne, France; Ubisoft, Montreuil, France","Martin, N., Irt B<com, Cesson-Sevigne, France; Mathieu, N., Ubisoft, Montreuil, France; Pallamin, N., Irt B<com, Cesson-Sevigne, France; Ragot, M., Irt B<com, Cesson-Sevigne, France; Diverrez, J.-M., Irt B<com, Cesson-Sevigne, France","Virtual Reality (VR) is spreading to the general public but still has a major issue: VR sickness. To take it into consideration and minimize its occurrence, evaluation methods are required. The current methods are mainly based on subjective measurements and therefore have several drawbacks (e.g., non-continuous, intrusive). Physiological signals combined with Machine Learning (ML) methods seem an interesting approach to go beyond these limits. In this paper, we present a large-scale experimentation (103 participants) where physiological data (cardiac and electrodermal activities) and subjective data (perceived VR sickness) were gathered during 30-minute VR video game sessions. Using ML methods, models were trained to predict VR sickness level (based on the physiological data labeled with the subjective data). Results showed an explained variance up to 75% (in a regression approach) and an accuracy up to 91% (in a classification approach). Despite generalization issues, this method seems promising and valuable for a real time, automatic and continuous evaluation of VR sickness, based on physiological signals and ML models. © 2020 IEEE.","Ergonomics; H.1.2 [Models and principles]: User/Machine Systems; Human factors; I.3.6 [Computer graphics]: Methodology and Techniques","Augmented reality; Diseases; E-learning; Machine learning; Physiology; Virtual reality; Classification approach; Electrodermal activity; Evaluation methods; General publics; Large-scale experimentations; Physiological data; Physiological signals; Subjective measurements; Physiological models",Conference Paper,"Final","",Scopus,2-s2.0-85099321186
"David E., Beitner J., Võ M.L.-H.","57202111778;57203870149;9634349400;","Effects of transient loss of vision on head and eye movements during visual search in a virtual environment",2020,"Brain Sciences","10","11", 841,"1","26",,,"10.3390/brainsci10110841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095995234&doi=10.3390%2fbrainsci10110841&partnerID=40&md5=2ba85748a2dc9beeeb7a622d24f7ff47","Scene Grammar Lab, Department of Psychology, Johann Wolfgang-Goethe-Universität, Theodor-W.-Adorno-Platz 6, Frankfurt, 60323, Germany","David, E., Scene Grammar Lab, Department of Psychology, Johann Wolfgang-Goethe-Universität, Theodor-W.-Adorno-Platz 6, Frankfurt, 60323, Germany; Beitner, J., Scene Grammar Lab, Department of Psychology, Johann Wolfgang-Goethe-Universität, Theodor-W.-Adorno-Platz 6, Frankfurt, 60323, Germany; Võ, M.L.-H., Scene Grammar Lab, Department of Psychology, Johann Wolfgang-Goethe-Universität, Theodor-W.-Adorno-Platz 6, Frankfurt, 60323, Germany","Central and peripheral fields of view extract information of different quality and serve different roles during visual tasks. Past research has studied this dichotomy on-screen in conditions remote from natural situations where the scene would be omnidirectional and the entire field of view could be of use. In this study, we had participants looking for objects in simulated everyday rooms in virtual reality. By implementing a gaze-contingent protocol we masked central or peripheral vision (masks of 6 deg. of radius) during trials. We analyzed the impact of vision loss on visuo-motor variables related to fixation (duration) and saccades (amplitude and relative directions). An important novelty is that we segregated eye, head and the general gaze movements in our analyses. Additionally, we studied these measures after separating trials into two search phases (scanning and verification). Our results generally replicate past on-screen literature and teach about the role of eye and head movements. We showed that the scanning phase is dominated by short fixations and long saccades to explore, and the verification phase by long fixations and short saccades to analyze. One finding indicates that eye movements are strongly driven by visual stimulation, while head movements serve a higher behavioral goal of exploring omnidirectional scenes. Moreover, losing central vision has a smaller impact than reported on-screen, hinting at the importance of peripheral scene processing for visual search with an extended field of view. Our findings provide more information concerning how knowledge gathered on-screen may transfer to more natural conditions, and attest to the experimental usefulness of eye tracking in virtual reality. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Gaze-contingent protocol; Virtual reality; Visual attention; Visual field loss; Visual search",,Article,"Final","",Scopus,2-s2.0-85095995234
"Lampen E., Lehwald J., Pfeiffer T.","57209683199;57202847650;14027435500;","Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095834964&doi=10.1145%2f3385956.3418974&partnerID=40&md5=6990b67981dda33559c0b31456034299","EvoBus GmbH, Neu-Ulm, Germany; Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Lampen, E., EvoBus GmbH, Neu-Ulm, Germany; Lehwald, J., EvoBus GmbH, Neu-Ulm, Germany; Pfeiffer, T., Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al. [21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks. © 2020 ACM.","Augmented Reality; Expert-Based Learning; Virtual Human","E-learning; Educational settings; Implementation cost; Industrial use case; Learning by observation; Learning scenarios; Sports and rehabilitations; State of the art; Threedimensional (3-d); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095834964
"Armstrong M., Tsuchiya K., Liang F., Kunze K., Pai Y.S.","57219866273;57194083287;57191504624;21743317500;56267209600;","Multiplex Vision: Understanding Information Transfer and F-Formation with Extended 2-Way FOV",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095814394&doi=10.1145%2f3385956.3418954&partnerID=40&md5=89c416380a230628f82185e756df2444","Keio University, Graduate School of Media Design, Tokyo, Japan; INNOCC Ignition Point Inc., Tokyo, Japan; University of Auckland, Auckland, New Zealand","Armstrong, M., Keio University, Graduate School of Media Design, Tokyo, Japan; Tsuchiya, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Liang, F., INNOCC Ignition Point Inc., Tokyo, Japan; Kunze, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Pai, Y.S., University of Auckland, Auckland, New Zealand","Research in sociology shows that effective conversation relates to people's spatial and orientational relationship, namely the proxemics (distance, eye contact, synchrony) and the F-formation (orientation and arrangement). In this work, we introduce novel conversational paradigms that effects conventional F-formation by introducing the concept of multi-directional conversation. Multiplex Vision is a head-mounted device capable of providing a 360° field-of-view (FOV) and facilitating multi-user interaction multi-directionally, thereby providing novel methods on how people can interact with each other. We propose 3 possible new forms of interactions from our prototype: one-to-one, one-to-many, and many-to-many. To facilitate them, we manipulate 2 key variables, which are the viewing parameter and the display parameter. To gather feedback for our system, we conducted a study to understand information transfer between various modes, as well as a user study on how different proposed paradigms effect conversation. Finally, we discuss present and future use cases that can benefit from our system. © 2020 ACM.","360 field-of-view; conversation; F-formation; vision augmentation","Sociology; Display parameters; Field of views; Information transfers; Key variables; Multi-user interaction; Novel methods; Orientational relationship; Viewing parameters; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095814394
"Pletz C., Zinn B.","57218546432;36770118100;","Evaluation of an immersive virtual learning environment for operator training in mechanical and plant engineering using video analysis",2020,"British Journal of Educational Technology","51","6",,"2159","2179",,1,"10.1111/bjet.13024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089538860&doi=10.1111%2fbjet.13024&partnerID=40&md5=f0549fc6714395b5221f74594500ae9d","Department of Vocational Education focused on Teaching Technology (BPT) at the, University of Stuttgart, Germany; Department of Vocational Education focused on Teaching Technology (BPT), University of Stuttgart, Germany","Pletz, C., Department of Vocational Education focused on Teaching Technology (BPT) at the, University of Stuttgart, Germany; Zinn, B., Department of Vocational Education focused on Teaching Technology (BPT), University of Stuttgart, Germany","A structural evaluation is imperative for developing an effective virtual learning environment. Understanding the extent to which content that has been learned virtually can be applied practically holds particular importance. A group of persons from the technical field of mechanical and plant engineering (N = 13) participated in a virtual operator training for a case application of additive manufacturing. To evaluate the virtual learning environment the participants answered quantitative questionnaires and were asked to apply what they had learned virtually to the real machine. Both the virtual training and testing phase on the real machine were recorded by video (800 minutes in total). The category system resulting from a structured qualitative video analysis with a total of 568 codes contains design-, instruction- and interaction-related optimisation potentials for further development of the virtual learning sequence. Mistakes, difficulties and other anomalies during the application on the real machine provide further revision options. The study uses video data for the first time to derive optimisation potentials and to investigate the learning transfer of virtually learned action knowledge to the real-world activity. © 2020 The Authors. British Journal of Educational Technology published by John Wiley & Sons Ltd on behalf of British Educational Research Association",,"Computer aided instruction; Engineering education; Learning systems; Personnel training; Surveys; Virtual reality; Learning Transfer; Operator training; Plant engineering; Real-world activities; Structural evaluation; Virtual learning; Virtual learning environments; Virtual operators; E-learning",Article,"Final","",Scopus,2-s2.0-85089538860
"Du J., Yu F.R., Lu G., Wang J., Jiang J., Chu X.","57188706756;57213980384;7403460635;13613533800;57198571713;8536386700;","MEC-Assisted Immersive VR Video Streaming over Terahertz Wireless Networks: A Deep Reinforcement Learning Approach",2020,"IEEE Internet of Things Journal","7","10", 9120235,"9517","9529",,10,"10.1109/JIOT.2020.3003449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709939&doi=10.1109%2fJIOT.2020.3003449&partnerID=40&md5=9d415f1ee32084295aa918f217856727","Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON  K1S 5B6, Canada; Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom","Du, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Yu, F.R., Department of Systems and Computer Engineering, Carleton University, Ottawa, ON  K1S 5B6, Canada; Lu, G., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Wang, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Jiang, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Chu, X., Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom","Immersive virtual reality (VR) video is becoming increasingly popular owing to its enhanced immersive experience. To enjoy ultrahigh resolution immersive VR video with wireless user equipments, such as head-mounted displays (HMDs), ultralow-latency viewport rendering, and data transmission are the core prerequisites, which could not be achieved without a huge bandwidth and superior processing capabilities. Besides, potentially very high energy consumption at the HMD may impede the rapid development of wireless panoramic VR video. Multiaccess edge computing (MEC) has emerged as a promising technology to reduce both the task processing latency and the energy consumption for HMD, while bandwidth-rich terahertz (THz) communication is expected to enable ultrahigh-speed wireless data transmission. In this article, we propose to minimize the long-term energy consumption of a THz wireless access-based MEC system for high quality immersive VR video services support by jointly optimizing the viewport rendering offloading and downlink transmit power control. Considering the time-varying nature of wireless channel conditions, we propose a deep reinforcement learning-based approach to learn the optimal viewport rendering offloading and transmit power control policies and an asynchronous advantage actor-critic (A3C)-based joint optimization algorithm is proposed. The simulation results demonstrate that the proposed algorithm converges fast under different learning rates, and outperforms existing algorithms in terms of minimized energy consumption and maximized reward. © 2014 IEEE.","Asynchronous advantage actor-critic (A3C); computation offloading; deep reinforcement learning (DRL); terahertz (THz) communication; virtual reality (VR)","Bandwidth; Data communication equipment; Data transfer; Deep learning; Energy utilization; Green computing; Helmet mounted displays; Learning algorithms; Power control; Quality control; Reinforcement learning; Rendering (computer graphics); Video streaming; Wave transmission; Wireless networks; Head mounted displays; Immersive virtual reality; Processing capability; Reinforcement learning approach; Terahertz(THz) communications; Transmit power control; Wireless channel condition; Wireless data transmission; Virtual reality",Article,"Final","",Scopus,2-s2.0-85092709939
"Zhou T., Zhu Q., Du J.","57218949339;57209806243;57219889677;","Intuitive robot teleoperation for civil engineering operations with virtual reality and deep learning scene reconstruction",2020,"Advanced Engineering Informatics","46",, 101170,"","",,,"10.1016/j.aei.2020.101170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090898690&doi=10.1016%2fj.aei.2020.101170&partnerID=40&md5=908bb4c201303c8747d6ecb275b0542a","Engineering School of Sustainable Infrastructure & Environment, University of Florida, 1949 Stadium Road 454A Weil Hall, Gainesville, FL  32611, United States; Engineering School of Sustainable Infrastructure & Environment, University of Florida, 1949 Stadium Road 460F Weil Hall, Gainesville, FL  32611, United States","Zhou, T., Engineering School of Sustainable Infrastructure & Environment, University of Florida, 1949 Stadium Road 454A Weil Hall, Gainesville, FL  32611, United States; Zhu, Q., Engineering School of Sustainable Infrastructure & Environment, University of Florida, 1949 Stadium Road 454A Weil Hall, Gainesville, FL  32611, United States; Du, J., Engineering School of Sustainable Infrastructure & Environment, University of Florida, 1949 Stadium Road 460F Weil Hall, Gainesville, FL  32611, United States","Robotic teleoperation, i.e., manipulating remote robotic systems at a distance, has gained its popularity in various industrial applications, including construction operations. The key to a successful teleoperation robot system is the delicate design of the human-robot interface that helps strengthen the human operator's situational awareness. Traditional human-robot interface for robotic teleoperation is usually based on imagery data (e.g., video streaming), causing the limited field of view (FOV) and increased cognitive burden for processing additional spatial information. As a result, 3D scene reconstruction methods based on point cloud models captured by scanning technologies (e.g., depth camera and LiDAR) have been explored to provide immersive and intuitive feedback to the human operator. Despite the added benefits of applying reconstructed 3D scenes in telerobotic systems, challenges still present. Most 3D reconstruction methods utilize raw point cloud data due to the difficulty of real-time model rendering. The significant size of point cloud data makes the processing and transfer between robots and human operators difficult and slow. In addition, most reconstructed point cloud models do not contain physical properties such as weight and colliders. A more enriched control mechanism based on physics engine simulations is impossible. This paper presents an intelligent robot teleoperation interface that collects, processes, transfers, and reconstructs the immersive scene model of the workspace in Virtual Reality (VR) and enables intuitive robot controls accordingly. The proposed system, Telerobotic Operation based on Auto-reconstructed Remote Scene (TOARS), utilizes a deep learning algorithm to automatically detect objects in the captured scene, along with their physical properties, based on the point cloud data. The processed information is then transferred to the game engine where rendered virtual objects replace the original point cloud models in the VR environment. TOARS is expected to significantly improve the efficiency of 3D scene reconstruction and situational awareness of human operators in robotic teleoperation. © 2020 Elsevier Ltd","Deep learning; Robot; Scene reconstruction; Teleoperation; Virtual reality","Biofeedback; Cloud computing; Data handling; Deep learning; E-learning; Image reconstruction; Industrial robots; Intelligent robots; Learning algorithms; Machine design; Object detection; Personnel; Physical properties; Remote control; Rendering (computer graphics); Robotics; Social robots; Virtual reality; 3D scene reconstruction; Construction operations; Engineering operation; Human-Robot Interface; Processed information; Robot teleoperation interfaces; Robotic teleoperation; Situational awareness; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85090898690
"Wang H., Nie K., Chang J., Kuang Y.","57216290996;25634070200;57218387242;16245272900;","A Monte Carlo study to investigate the feasibility of an on-board SPECT/spectral-CT/CBCT imager for medical linear accelerator",2020,"Medical Physics","47","10",,"5112","5122",,,"10.1002/mp.14398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089034487&doi=10.1002%2fmp.14398&partnerID=40&md5=9ddbdf3b0ac49456f1b1f2b8c93c7b95","Medical Physics Program, University of Nevada, Las Vegas, NV  89154, United States; Department of Radiation Oncology, Rutgers—Cancer Institute of New Jersey, Rutgers—Robert Wood Johnson Medical School, Rutgers—The State University of New Jersey, New Brunswick, NJ  08901, United States","Wang, H., Medical Physics Program, University of Nevada, Las Vegas, NV  89154, United States; Nie, K., Department of Radiation Oncology, Rutgers—Cancer Institute of New Jersey, Rutgers—Robert Wood Johnson Medical School, Rutgers—The State University of New Jersey, New Brunswick, NJ  08901, United States; Chang, J., Medical Physics Program, University of Nevada, Las Vegas, NV  89154, United States; Kuang, Y., Medical Physics Program, University of Nevada, Las Vegas, NV  89154, United States","Purpose: The on-board flat-panel cone-beam computed tomography (CBCT) lacks molecular/functional information for current online image-guided radiation therapy (IGRT). It might not be adequate for adaptive radiation therapy (ART), particularly for biologically guided tumor delineation and targeting which might be shifted and/or distorted during the course of RT. A linear accelerator (Linac) gantry-mounted on-board imager (OBI) was proposed using a single photon counting detector (PCD) panel to achieve single photon emission computed tomography (SPECT), energy-resolved spectral CT, and conventional CBCT triple on-board imaging, which might facilitate online ART with an addition of volumetric molecular/functional imaging information. Methods: The system was designed and evaluated in the GATE Monte Carlo platform. The OBI system including a kV-beam source and a pixelated cadmium zinc telluride (CZT) detector panel mounted on a medical Linac orthogonally to the MV beam direction was designed to obtain online CBCT, spectral CT, and SPECT tri-modal imaging of patients in the treatment room. The spatial resolutions of the OBI system were determined by imaging simulated phantoms. The CBCT imaging was evaluated by a simulated contrast phantom. A PMMA phantom containing gadolinium was imaged to demonstrate quantitative imaging of spectral-CT/CBCT of the system. The capability of tri-modal imaging of the OBI was demonstrated using three different spectral CT imaging methods to differentiate gadolinium, gold, calcium within simulated PMMA and the SPECT to image radioactive 99mTc distribution. The dual-isotope SPECT imaging of the system was also evaluated by imaging a phantom containing 99mTc and 123I. The radiotherapy-related parameters of iodine contrast fraction and virtual non-contrast (VNC) tissue electron density in the Kidney1 inserts of a simulated phantom were decomposed using the Bayesian eigentissue decomposition method for contrast-enhanced CBCT/spectral-CT of the OBI in a single scan. Results: The spatial resolutions of CBCT and SPECT of the OBI were determined to be 15.1 lp/cm at 10% MTF and 4.8–12 mm for radii of rotation of 10–40 cm, respectively. In CBCT image of the contrast phantom, most of the soft-tissue inserts were visible with sufficient spatial structure details. As compared to the CBCT image of gadolinium, the spectral CT image provided higher image contrasts. Calcium, gadolinium, and gold were separated well by using the spectral CT material imaging methods. The reconstructed distribution of 99mTc agreed with the spatial position within the phantom. The two isotopes were separated from each other in dual-isotope SPECT imaging of the OBI. The iodine fractions and the VNC electron densities were estimated in the iodine-enhanced Kidney1 tissue inserts with reasonable RMS errors. The main procedures of the tri-modal imaging guided online ART workflow were presented with new functional features included. Conclusions: Using a single photon counting CZT detector panel, an on-board SPECT, spectral CT, and CBCT tri-modal imaging could be realized in Linacs. With the added online molecular/functional imaging obtained from the new OBI for the online ART proposed, the accuracy of radiation treatment delivery could be further improved. © 2020 American Association of Physicists in Medicine","cone beam CT; molecular imaging; on-board imager; SPECT; spectral CT; tri-modality","Arts computing; Cadmium compounds; Cadmium telluride; Calcium; Electron density measurement; Gadolinium; Gold; II-VI semiconductors; Iodine; Isotopes; Linear accelerators; Monte Carlo methods; Particle beams; Phantoms; Photons; Radioactivity; Radiotherapy; Single photon emission computed tomography; Tissue; Tissue engineering; Z transforms; Zinc compounds; Adaptive radiation therapies; Cadmium zinc telluride detectors; Cone-beam computed tomography; Decomposition methods; Quantitative imaging; Radiation treatments; Single photon counting; Single photon counting detectors; Molecular imaging; calcium; gadolinium; gold; iodine 123; technetium 99m; Article; computer assisted tomography; cone beam computed tomography; contrast enhancement; decomposition; electron beam; feasibility study; human; image display; modulation transfer function; molecular imaging; Monte Carlo method; quantitative analysis; radiation beam; radiation energy; radioactivity; single photon emission computed tomography; soft tissue; spectral imaging",Article,"Final","",Scopus,2-s2.0-85089034487
"Pinardi D., Ebri L., Belicchi C., Farina A., Binelli M.","57195467991;56770178100;57219547851;7202992441;55354577100;","Direction Specific Analysis of Psychoacoustics Parameters inside Car Cockpit: A Novel Tool for NVH and Sound Quality",2020,"SAE Technical Papers",,"2020",,"","",,,"10.4271/2020-01-1547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093822780&doi=10.4271%2f2020-01-1547&partnerID=40&md5=c96cd017a73014350d0726971591bd1f","University of Parma, Italy; University of Parma, Ask Industries SpA, Italy","Pinardi, D., University of Parma, Italy; Ebri, L., University of Parma, Ask Industries SpA, Italy; Belicchi, C., University of Parma, Italy; Farina, A., University of Parma, Italy; Binelli, M., University of Parma, Italy","Psychoacoustics parameters are widely employed in automotive field for objective evaluation of Sound Quality (SQ) of vehicle cabins and their components. The standard approach relies on binaural recordings from which numerical values and curves are calculated. In addition, head-locked binaural listening playback can be performed. The Virtual Reality (VR) technology recently started to diffuse also in automotive field, bringing new possibilities for enhanced and immersive listening sessions, thanks to the usage of massive microphone arrays instead of binaural microphones. In this paper, we combine both solutions: the principal SQ parameters are derived from multichannel recordings. This allows computing a map of direction-dependent values of SQ parameters. The acquisition system consists in a spherical microphone array with 32 capsules and a multiple-lens camera for capturing a panoramic equirectangular background image. The audio recording is encoded into High Order Ambisonics (HOA) format for being compared with a classic omnidirectional microphone and into Spatial PCM Sampling (SPS) format for producing 360° equirectangular color maps. The SPS encoding is used to plot over the background image the distribution of SPL values in dB (A) and of the SQ parameters: by adding to them the directional information, it results into a novel 360° diagnostic tool for localizing the most annoying sources. Furthermore, the playback of the HOA soundtrack can be performed both on a loudspeaker rig inside an Ambisonics listening room or on binaural headphones attached to a Head Mounted Display (HMD), benefiting from head-tracking and personalized Head Related Transfer Functions (HRTFs), allowing to make quick subjective evaluations with a degree of realism unattainable with the older static binaural approach. © 2020SAE International. All Rights Reserved.",,"Acoustic noise; Acoustic variables measurement; Helmet mounted displays; Loudspeakers; Microphones; Parameter estimation; Quality control; Virtual reality; Binaural recordings; Directional information; Head mounted displays; Head related transfer function; Multi-channel recording; Objective evaluation; Spherical microphone array; Subjective evaluations; Audio recordings",Conference Paper,"Final","",Scopus,2-s2.0-85093822780
"Delamarre A., Lisetti C., Buche C.","57195671754;6602670860;8349259000;","A Cross-Platform Classroom Training Simulator: Interaction Design and EvaluationA Cross-Platform Classroom Training Simulator: Interaction Design and Evaluation",2020,"Proceedings - 2020 International Conference on Cyberworlds, CW 2020",,, 9240533,"86","93",,,"10.1109/CW49994.2020.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099535199&doi=10.1109%2fCW49994.2020.00020&partnerID=40&md5=0afab14bfbc31f31c90b991bafa5ee8d","Florida International University, Visage Lab, Scis, Miami, United States; LAB-STICC Cnrs, Enib, Brest, France","Delamarre, A., Florida International University, Visage Lab, Scis, Miami, United States; Lisetti, C., Florida International University, Visage Lab, Scis, Miami, United States; Buche, C., LAB-STICC Cnrs, Enib, Brest, France","Virtual training environments experienced with different immersive technologies can accommodate users' preferences, proficiency, and platform availability. Whereas research comparing the effects of immersive technologies can provide important insights about their impact on users' experience (e.g. engagement, transfer of learning), current studies do not address how to design the user interface (UI) to ensure sound comparisons across platforms. For effective comparisons, however, the UI designs must be adapted for the platform used to provide comparable usability. In this article we describe our UI design methodology for the development of an effective and usable virtual classroom training simulator built for three technologies: (1) desktop; (2) Head-Mounted Display (HMD); and (3) Cave Automatic Virtual Environment (CAVE). Usability and other user experience factors were evaluated for each platform with concurrent think-aloud protocol and semi-structured interviews indicating that all three UIs were easy to use and to learn. We discuss insights for future development of cross-platform VTEs. © 2020 IEEE.","Design; Human Computer Interaction; Immersive Virtual Environment; User Study; Virtual Reality","Availability; Caves; Computer aided instruction; Design; E-learning; Helmet mounted displays; Simulators; Transfer learning; User experience; User interfaces; Cave automatic virtual environments; Head mounted displays; Immersive technologies; Platform availabilities; Semi structured interviews; Think-aloud protocol; Transfer of learning; Virtual training environments; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099535199
"Houshmand B., Khan N.M.","57219792826;38361516100;","Facial Expression Recognition under Partial Occlusion from Virtual Reality Headsets based on Transfer Learning",2020,"Proceedings - 2020 IEEE 6th International Conference on Multimedia Big Data, BigMM 2020",,, 9232653,"70","75",,,"10.1109/BigMM50055.2020.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241474&doi=10.1109%2fBigMM50055.2020.00020&partnerID=40&md5=b69f6f1256f605345e4e4028c5b57250","Ryerson University Data Science, Toronto, Canada; Ryerson University Electrical and Computer Engineering, Toronto, Canada","Houshmand, B., Ryerson University Data Science, Toronto, Canada; Khan, N.M., Ryerson University Electrical and Computer Engineering, Toronto, Canada","Facial expressions of emotion are a major channel in our daily communications, and it has been subject of intense research in recent years. To automatically infer facial expressions, convolutional neural network based approaches has become widely adopted due to their proven applicability to Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR) has gained popularity as an immersive multimedia platform, where FER can provide enriched media experiences. However, recognizing facial expression while wearing a head-mounted VR headset is a challenging task due to the upper half of the face being completely occluded. In this paper we attempt to overcome these issues and focus on facial expression recognition in presence of a severe occlusion where the user is wearing a head-mounted display in a VR setting. We propose a geometric model to simulate occlusion resulting from a Samsung Gear VR headset that can be applied to existing FER datasets. Then, we adopt a transfer learning approach, starting from two pretrained networks, namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB datasets. Experimental results show that our approach achieves comparable results to existing methods while training on three modified benchmark datasets that adhere to realistic occlusion resulting from wearing a commodity VR headset. Code for this paper is available at: https://github.com/bita-github/MRP-FER © 2020 IEEE.","Facial expression recognition; Facial occlusion; Transfer learning; VR","Big data; Convolutional neural networks; E-learning; Face recognition; Helmet mounted displays; Transfer learning; Wear of materials; Benchmark datasets; Facial expression recognition; Facial Expressions; Geometric modeling; Head mounted displays; Multimedia platforms; Partial occlusions; Virtual-reality headsets; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85097241474
"Harris D.J., Buckingham G., Wilson M.R., Brookes J., Mushtaq F., Mon-Williams M., Vine S.J.","57192429891;14069958400;55574207642;57197801653;56999078200;7006287402;36811509000;","The effect of a virtual reality environment on gaze behaviour and motor skill learning",2020,"Psychology of Sport and Exercise","50",, 101721,"","",,3,"10.1016/j.psychsport.2020.101721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086586041&doi=10.1016%2fj.psychsport.2020.101721&partnerID=40&md5=608f65aed21033b6e58a9a8feac30da8","School of Sport and Health Sciences, University of Exeter, Exeter, EX1 2LU, United Kingdom; School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Bradford Teaching Hospitals NHS Foundation Trust, Bradford, West Yorkshire, United Kingdom; National Centre for Optics, Vision and Eye Care, University of South-Eastern Norway, Kongsberg, Hasbergs Vei 363616, Norway","Harris, D.J., School of Sport and Health Sciences, University of Exeter, Exeter, EX1 2LU, United Kingdom; Buckingham, G., School of Sport and Health Sciences, University of Exeter, Exeter, EX1 2LU, United Kingdom; Wilson, M.R., School of Sport and Health Sciences, University of Exeter, Exeter, EX1 2LU, United Kingdom; Brookes, J., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Mushtaq, F., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Mon-Williams, M., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Bradford Teaching Hospitals NHS Foundation Trust, Bradford, West Yorkshire, United Kingdom, National Centre for Optics, Vision and Eye Care, University of South-Eastern Norway, Kongsberg, Hasbergs Vei 363616, Norway; Vine, S.J., School of Sport and Health Sciences, University of Exeter, Exeter, EX1 2LU, United Kingdom","Objective: Virtual reality (VR) systems hold significant potential for training skilled behaviours and are currently receiving intense interest in the sporting domain. They offer both practical and pedagogical benefits, but there are concerns about the effect that perceptual deficiencies in VR systems (e.g. reduced haptic information, and stereoscopic display distortions) may have on learning and performance. ‘Specificity of learning’ theories suggest that VR could be ineffective (or even detrimental) if important differences (e.g. perceptual deficiencies) exist between practice and real task performance conditions. Nevertheless, ‘structural learning’ theories suggest VR could be a useful training tool, despite these deficiencies, because a trainee can still learn the underlying structure of the behaviour. We explored these theoretical predictions using golf putting as an exemplar skill. Method: In Experiment 1 we used a repeated measures design to assess putting accuracy (radial error) and quiet eye duration of expert golfers (n = 18) on real putts before and after 40 VR ‘warm up’ putts. In Experiment 2, novice golfers (n = 40) were assigned to either VR or real-world putting training. Putting accuracy and quiet eye durations were then assessed on a real-world retention test. Results: Both visual guidance (quiet eye) and putting accuracy were disrupted temporarily when moving from VR to real putting (Experiment 1). However, real-world and VR practice produced comparable improvements in putting accuracy in novice golfers (Experiment 2). Conclusion: Overall, the results suggest that: (i) underlying skill structures can be learned in VR and transferred to the real-world; (ii) perceptual deficiencies will place limits on the use of VR. These findings demonstrate the challenges and opportunities for VR as a training tool, and emphasise the need to empirically test the costs and benefits of specific systems before deploying VR training. © 2020 Elsevier Ltd","Quiet eye; Skill acquisition; Sport; Stereoscopic; Transfer; VR","article; clinical article; gaze; golf; human; human experiment; learning; motor performance; prediction; virtual reality; warm up",Article,"Final","",Scopus,2-s2.0-85086586041
"Catal C., Akbulut A., Tunali B., Ulug E., Ozturk E.","22633325800;25960607500;57211942369;57211949025;57211945039;","Evaluation of augmented reality technology for the design of an evacuation training game",2020,"Virtual Reality","24","3",,"359","368",,2,"10.1007/s10055-019-00410-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075481004&doi=10.1007%2fs10055-019-00410-z&partnerID=40&md5=1f2c3d6562a0c05b9c1b4538db9f7821","Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey","Catal, C., Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; Akbulut, A., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Tunali, B., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Ulug, E., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Ozturk, E., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey","Building evacuation training systems and training employees in an organization have a vital role in emergency cases in which people need to know what to do exactly. In every building, procedures, rules, and actions are attractively shown on the walls, but most of the people living in that building are not aware of these procedures and do not have any experience what to do in these dangerous situations. In order to be able to apply these procedures properly in an emergency situation, community members should be trained with the state-of-the-art equipment and technologies, but to do so, up-front investment and development of such a system are necessary. In this study, augmented reality (AR) technology was applied to realize a game-based evacuation training system that implements gamification practices. The architectural plans of a university were used to model the floors and the relevant environment. Employees are trained to learn how to reach the nearest exit location in the event of a fire or earthquake, and also, the system provides the shortest path for the evacuation. In addition to these features, our training game has educational animations about the fire, chemical attack, and earthquake events. A mobile application was implemented to train employees working in the building and inform them to know how to escape in an emergency situation. The technology acceptance model and the related questionnaire form were applied, and the response of 36 participants was analyzed. It was demonstrated that AR and relevant tools provide a flexible environment to develop evacuation systems in a university, our mobile application enabled participants to be trained in a realistic environment, and trainees were highly satisfied with the system. Educational animations were also another benefit for the trainees. © 2019, The Author(s).","Animation; ARKit framework; Augmented reality; Evacuation training system; Game engine; Software; Training; Unity3D","Animation; Augmented reality; Chemical attack; Computer software; Earthquakes; Investments; Mobile computing; Technology transfer; ARKit framework; Augmented reality technology; Game Engine; Realistic environments; State-of-the-art equipments; Technology acceptance model; Training Systems; Unity3d; Personnel training",Article,"Final","",Scopus,2-s2.0-85075481004
"Hensen B., Klamma R.","57203715350;6603333022;","Comparing authoring workflows and learning paths in web mixed reality environments",2020,"Proceedings - IEEE 20th International Conference on Advanced Learning Technologies, ICALT 2020",,, 9155647,"319","321",,,"10.1109/ICALT49669.2020.00102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091165216&doi=10.1109%2fICALT49669.2020.00102&partnerID=40&md5=40e1befd400cbb1c1d7357c1b5345d55","Rwth Aachen University, Informatik 5, Aachen, Germany","Hensen, B., Rwth Aachen University, Informatik 5, Aachen, Germany; Klamma, R., Rwth Aachen University, Informatik 5, Aachen, Germany","With recent developments in information technology, we can realize learning experiences on the Web and in mixed reality. Many disciplines have profited from this, e.g. anatomy with the possibility to learn with 3D models in both environments. However, designing authoring workflows and learning paths can be different in these environments as the transfer of design ideas can affect the quality of the learning experiences. In this paper, we juxtapose a Web application and a mixed reality application of the same anatomy course. We highlight where differences need to be considered and which modules can be used in both environments, e.g. authentication and gamification. With the practical knowledge presented here, the design of learning experiences on the Web and in mixed reality can be enhanced. To support this, all code is available as open-source software on GitHub. © 2020 IEEE.","Augmented Reality; Gamification; Mixed Reality; Virtual Reality; Web-based Learning; WebXR","Open source software; Open systems; Design ideas; Learning experiences; Learning paths; Mixed-reality environment; WEB application; Work-flows; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85091165216
"Nie J., Wu B.","57219057682;57213495940;","Investigating the effect of immersive virtual reality and planning on the outcomes of simulation-based learning: A media and method experiment",2020,"Proceedings - IEEE 20th International Conference on Advanced Learning Technologies, ICALT 2020",,, 9155971,"329","332",,,"10.1109/ICALT49669.2020.00106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091147966&doi=10.1109%2fICALT49669.2020.00106&partnerID=40&md5=0cfc6e5a1bd46be168e53bd114e7a60d","East China Normal University, Department of Educational Informational Technology, Shanghai, China","Nie, J., East China Normal University, Department of Educational Informational Technology, Shanghai, China; Wu, B., East China Normal University, Department of Educational Informational Technology, Shanghai, China","The application of the immersive virtual reality (VR) has injected new vitality into educational innovation, but there are also some voices of doubt on its practical learning effects. Considering two aspects of media and instructional methods, the study investigated the effect of immersive virtual reality and planning strategy on simulation-based learning by a 2×2 experimental cross-panel design. The results showed that both of them had a significant main effect, indicating that the immersive VR and planning led to better behavioral transfer performance and immersive VR increased sense of presence and self-efficacy as well. No interaction effect between media and method was found. © 2020 IEEE.","Immersive virtual reality; Planning Strategy; Simulation-based learning; Virtual simulation","E-learning; Learning systems; Educational innovations; Immersive virtual reality; Instructional methods; Interaction effect; Planning strategies; Sense of presences; Simulation-based learning; Transfer performance; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85091147966
"Rout S.P.","57220038960;","6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",2020,"2020 11th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2020",,, 9225680,"","",,1,"10.1109/ICCCNT49239.2020.9225680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096544829&doi=10.1109%2fICCCNT49239.2020.9225680&partnerID=40&md5=58c77867b3e94be4b5a2d9b5edf945cb","Electronics and Communication Engineering, TempleCity Institute of Technology Engineering, Bhubaneswar, India","Rout, S.P., Electronics and Communication Engineering, TempleCity Institute of Technology Engineering, Bhubaneswar, India","The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented. © 2020 IEEE.","AR; BCI; BY5G; E-IoT; eMBB; Io-BNT; IoE; M2M; mMTC; MR; OAM; QoE; SM-MIMO; UAV; URLLC; VR","Augmented reality; Data transfer; Deep learning; Intelligent computing; Internet of things; MIMO systems; Optical data processing; Queueing networks; Security of data; Telecommunication equipment; Three dimensional computer graphics; Virtual corporation; Virtual reality; Cellular network; Mobile Technology; Network connectivity; Propagation models; Real-time transfer; Reliable data transmission; Triple Play services; Wireless communications; 5G mobile communication systems",Conference Paper,"Final","",Scopus,2-s2.0-85096544829
"Lopez C.E., Ashour O., Cunningham J.D., Tucker C., Lynch P.C.","57193163382;36080456300;57194829435;15833577900;56300591400;","The CLICK approach and its impact on learning introductory probability concepts in an industrial engineering course",2020,"ASEE Annual Conference and Exposition, Conference Proceedings","2020-June",, 1339,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095792991&partnerID=40&md5=2b11cbd2f228cb0f4a897f4e2b19ce26","Lafayette College, United States; Penn State Erie, Behrend College, United States; Carnegie Mellon University, United States; Pennsylvania State University, Behrend College, United States","Lopez, C.E., Lafayette College, United States; Ashour, O., Penn State Erie, Behrend College, United States; Cunningham, J.D., Carnegie Mellon University, United States; Tucker, C., Carnegie Mellon University, United States; Lynch, P.C., Pennsylvania State University, Behrend College, United States","The objective of this work is to present an initial investigation of the impact the Connected Learning and Integrated Course Knowledge (CLICK) approach has had on students' motivation, engineering identity, and learning outcomes. CLICK is an approach that leverages Virtual Reality (VR) technology to provide an integrative learning experience in the Industrial Engineering (IE) curriculum. To achieve this integration, the approach aims to leverage VR learning modules to simulate a variety of systems. The VR learning modules offer an immersive experience and provide the context for real-life applications. The virtual simulated system represents a theme to transfer the system concepts and knowledge across multiple IE courses as well as connect the experience with real-world applications. The CLICK approach has the combined effect of immersion and learning-by-doing benefits. In this work, VR learning modules are developed for a simulated manufacturing system. The modules teach the concepts of measures of location and dispersion, which are used in an introductory probability course within the IE curriculum. This work presents the initial results of comparing the motivation, engineering identity, and knowledge gain between a control and an intervention group (i.e., traditional vs. CLICK teaching groups). The CLICK approach group showed greater motivation compared to a traditional teaching group. However, there were no effects on engineering identity and knowledge gain. Nevertheless, it is hypothesized that the VR learning modules will have a positive impact on the students' motivation, engineering identity, and knowledge gain over the long run and when used across the curriculum. Moreover, IE instructors interested in providing an immersive and integrative learning experience to their students could leverage the VR learning modules developed for this project. © American Society for Engineering Education 2020.",,"Curricula; Learning systems; Manufacture; Motivation; Students; Teaching; Virtual reality; Industrial engineering course; Integrated course; Integrative learning; Learning by doing; Learning modules; Learning outcome; Probability concepts; Real-life applications; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-85095792991
"Johnson D., Damian D., Tzanetakis G.","57214167710;57192297254;6602262192;","Evaluating the effectiveness of mixed reality music instrument learning with the theremin",2020,"Virtual Reality","24","2",,"303","317",,5,"10.1007/s10055-019-00388-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068827721&doi=10.1007%2fs10055-019-00388-8&partnerID=40&md5=7e681b8efa8420e7e54ec8a41f3173ea","University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada","Johnson, D., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada; Damian, D., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada; Tzanetakis, G., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada","Learning music is a challenging process that requires years of practice to master, either with lessons from a professional teacher or through self-teaching. While practicing, students are expected to self-evaluate their performance which may be difficult without timely feedback from a professional. Research into computer-assisted music instrument tutoring (CAMIT) attempts to address this through the use of emerging technologies. In this paper, we study CAMIT for mixed reality (MR) by developing MR:emin, an immersive MR music learning environment for the theremin, an electronic music instrument that is controlled without physical contact. MR:emin integrates a physical theremin with the immersive learning environment. To better understand the effectiveness of such environments, we perform a user study with MR:emin comparing traditional music learning with two virtual learning environments, an immersive one and a non-immersive one. In a between-groups study, 30 participants were trained to play a sequence of notes on the theremin using one of the three training environments. Results of our statistical analysis show that performance error during training is significantly smaller in the immersive MR environment. This does not necessarily lead to improved performance after training; analysis of post-training improvement indicates that immersive training results in the smallest amount of improvement. Participants, however, indicate that the MR:emin environment is more engaging and increases confidence during practice. We discuss potential factors leading to the decrease in learning and provide some environment guidelines to aid in the design of engaging immersive music learning environments. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Immersive learning environments; Learning transfer; Mixed reality; Music pedagogy; Training","Computer aided instruction; Computer music; Mixed reality; Personnel training; Emerging technologies; Environment guidelines; Immersive learning; Learning environments; Learning Transfer; Music pedagogy; Performance error; Virtual learning environments; E-learning",Article,"Final","",Scopus,2-s2.0-85068827721
"Brun D., George S., Gouin-Vallerand C.","57193572581;56911617500;24464753700;","Keycube: Text entry evaluation with a cubic device",2020,"Conference on Human Factors in Computing Systems - Proceedings",,, 3382837,"","",,,"10.1145/3334480.3382837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090240646&doi=10.1145%2f3334480.3382837&partnerID=40&md5=9f979c43b59c70ab8ef51b249597d339","Université TÉLUQ, Montreal, PQ, Canada; Le Mans University, Le Mans, France; Université de Sherbrooke, Sherbrooke, Canada","Brun, D., Université TÉLUQ, Montreal, PQ, Canada; George, S., Le Mans University, Le Mans, France; Gouin-Vallerand, C., Université de Sherbrooke, Sherbrooke, Canada","The keycube is a tangible cubic device including a text entry interface for different apparatuses such as augmented, mixed or virtual reality headsets, as well as smart TVs, desktop computers, laptops, tablets. The keycube comprises 80 keys equally disposed on 5 faces. In this paper we investigate keycube text entry performances and the potential typing skill transfer from traditional keyboard. Using prototype implementations, we conducted a user study comparing different cubic layouts and included a baseline from traditional keyboards. Experiments show that users are able to attain about 19 words per minute within one hundred minutes of practice with a QWERTY-based cubic layout, more than twice the speed of an unknown-based cubic layout with similar error rate, and about 30% of their speed with a traditional keyboard. © 2020 Owner/Author.","Cube; Device; Evaluation; Input speed; Keyboard; Text entry","Human engineering; Personal computers; Error rate; Prototype implementations; Skill transfer; Text entry; User study; Virtual-reality headsets; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85090240646
"Shah S.H.H., Han K., Lee J.W.","57210145879;56739751100;8948633800;","Real-time application for generating multiple experiences from 360° panoramic video by tracking arbitrary objects and viewer's orientations",2020,"Applied Sciences (Switzerland)","10","7", 2248,"","",,,"10.3390/app10072248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083571542&doi=10.3390%2fapp10072248&partnerID=40&md5=1e365e082a2f584e3c6a957dc533296e","Department of Software Convergence, Sejong University, 209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, South Korea","Shah, S.H.H., Department of Software Convergence, Sejong University, 209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, South Korea; Han, K., Department of Software Convergence, Sejong University, 209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, South Korea; Lee, J.W., Department of Software Convergence, Sejong University, 209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, South Korea","We propose a novel authoring and viewing system for generating multiple experiences with a single 360° video and eciently transferring these experiences to the user. An immersive video contains much more interesting information within the 360° environment than normal videos. There can be multiple interesting areas within a 360° frame at the same time. Due to the narrow field of view in virtual reality head-mounted displays, a user can only view a limited area of a 360° video. Hence, our system is aimed at generating multiple experiences based on interesting information in different regions of a 360° video and ecient transferring of these experiences to prospective users. The proposed system generates experiences by using two approaches: (1) Recording of the user's experience when the user watches a panoramic video using a virtual reality head-mounted display, and (2) tracking of an arbitrary interesting object in a 360° video selected by the user. For tracking of an arbitrary interesting object, we have developed a pipeline around an existing simple object tracker to adapt it for 360° videos. This tracking algorithm was performed in real time on a CPU with high precision. Moreover, to the best of our knowledge, there is no such existing system that can generate a variety of different experiences from a single 360° video and enable the viewer to watch one 360° visual content from various interesting perspectives in immersive virtual reality. Furthermore, we have provided an adaptive focus assistance technique for ecient transferring of the generated experiences to other users in virtual reality. In this study, technical evaluation of the system along with a detailed user study has been performed to assess the system's application. Findings from evaluation of the system showed that a single 360° multimedia content has the capability of generating multiple experiences and transfers among users. Moreover, sharing of the 360° experiences enabled viewers to watch multiple interesting contents with less effort. © 2020 by the authors.","360° video experiences; object tracking; Authoring system; Experience transfer; Focus assistance; Human-computer interaction (HCI); Virtual reality; Visualization",,Article,"Final","",Scopus,2-s2.0-85083571542
"Pallavicini F., Pepe A.","6701879031;55744410200;","Virtual reality games and the role of body involvement in enhancing positive emotions and decreasing anxiety: Within-subjects pilot study",2020,"JMIR Serious Games","8","2", e15635,"","",,1,"10.2196/15635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097499071&doi=10.2196%2f15635&partnerID=40&md5=c7fbb876de190177fb7d8f4c6dfacc7b","Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy","Pallavicini, F., Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy; Pepe, A., Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy","Background: In the last few years, the introduction of immersive technologies, especially virtual reality, into the gaming market has dramatically altered the traditional concept of video games. Given the unique features of virtual reality in terms of interaction and its ability to completely immerse the individual into the game, this technology should increase the propensity for video games to effectively elicit positive emotions and decrease negative emotions and anxiety in the players. However, to date, few studies have investigated the ability of virtual reality games to induce positive emotions, and the possible effect of this new type of video game in diminishing negative emotions and anxiety has not yet been tested. Furthermore, given the critical role of body movement in individuals’ well-being and in emotional responses to video games, it seems critical to investigate how body involvement can be exploited to modulate the psychological benefits of virtual reality games in terms of enhancing players’ positive emotions and decreasing negative emotions and anxiety. Objective: This within-subjects study aimed to explore the ability of commercial virtual reality games to induce positive emotions and diminish negative emotions and state anxiety of the players, investigating the effects of the level of body involvement requested by the game (ie, high vs low). Methods: A total of 36 young adults played a low body-involvement (ie, Fruit Ninja VR) and a high body-involvement (ie, Audioshield) video game in virtual reality. The Visual Analogue Scale (VAS) and the State-Trait Anxiety Inventory, Form-Y1 (STAI-Y1) were used to assess positive and negative emotions and state anxiety. Results: Results of the generalized linear model (GLM) for repeated-measures multivariate analysis of variance (MANOVA) revealed a statistically significant increase in the intensity of happiness (P<.001) and surprise (P=.003) and, in parallel, a significant decrease in fear (P=.01) and sadness (P<.001) reported by the users. Regarding the ability to improve anxiety in the players, the results showed a significant decrease in perceived state anxiety after game play, assessed with both the STAI-Y1 (P=.003) and the VAS-anxiety (P=.002). Finally, the results of the GLM MANOVA showed a greater efficacy of the high body-involvement game (ie, Audioshield) compared to the low body-involvement game (ie, Fruit Ninja VR), both for eliciting positive emotions (happiness, P<.001; and surprise, P=.01) and in reducing negative emotions (fear, P=.05; and sadness, P=.05) and state anxiety, as measured by the STAI-Y1 (P=.05). Conclusions: The two main principal findings of this study are as follows: (1) virtual reality video games appear to be effective tools to elicit positive emotions and to decrease negative emotions and state anxiety in individuals and (2) the level of body involvement of the virtual video game has an important effect in determining the ability of the game to improve positive emotions and decrease negative emotions and state anxiety of the players. © 2020 JMIR Publications. All Rights Reserved.","Anxiety; Emotions; Positive emotions; State anxiety; Video games; Virtual reality; Virtual reality gaming",,Article,"Final","",Scopus,2-s2.0-85097499071
"Frøland T.H., Heldal I., Sjøholt G., Ersvær E.","57215113864;6506576998;8668775300;15019082500;","Games on mobiles via web or virtual reality technologies: How to support learning for biomedical laboratory science education",2020,"Information (Switzerland)","11","4", 195,"","",,2,"10.3390/info11040195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084759701&doi=10.3390%2finfo11040195&partnerID=40&md5=8830595e0bd0b1b9cae5becc371e2b19","Department of Safety, Chemistry and Biomedical Laboratory Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway; Department of Computer Science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway","Frøland, T.H., Department of Safety, Chemistry and Biomedical Laboratory Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway; Heldal, I., Department of Computer Science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway; Sjøholt, G., Department of Safety, Chemistry and Biomedical Laboratory Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway; Ersvær, E., Department of Safety, Chemistry and Biomedical Laboratory Sciences, Western Norway University of Applied Sciences, Bergen, 5063, Norway","Simulations, serious games, and virtual reality (SSG) applications represent promising support for achieving practical proficiency, but it is difficult to know how to introduce them into a new environment. This paper aims to contribute to a better understanding of introducing new SSGs to a non-computer related educational environment-biomedical laboratory science (BLS) education. By following the choice, construction, and evaluation of a gamified app for practicing phlebotomy (StikkApp), not only the usefulness of the application, but also the general needs and possibilities for supporting SSG applications, are discussed. This paper presents the evaluation of StikkApp through an experimental study examining its use on mobile devices, as a web app and by discussing challenges for a corresponding virtual reality app by BLS students and their teachers. This evaluation focused on questions concerning usage scenarios, technologies, and how the design of the app can be aligned to learning goals necessary for education. By discussing these requirements and possibilities for apps and technology support for using SSG apps for BLS students, this paper contributes to a better understanding of using digital support for sustainable education. © 2020 by the authors.","Biomedical engineering; Education; Evaluation of educational games; Game-based learning; Phlebotomy","Serious games; Students; Technology transfer; Virtual reality; Biomedical laboratories; Educational environment; Science education; Support learning; Sustainable educations; Technology support; Usage scenarios; Virtual reality technology; E-learning",Article,"Final","",Scopus,2-s2.0-85084759701
"Jakl A., Lienhart A.-M., Baumann C., Jalaeefar A., Schlager A., Schoffer L., Bruckner F.","35242591600;57216955058;57216949185;57216948882;57216950067;57205459948;56562723100;","Enlightening Patients with Augmented Reality",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,, 9089476,"195","203",,1,"10.1109/VR46266.2020.1581532258804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526448&doi=10.1109%2fVR46266.2020.1581532258804&partnerID=40&md5=4538f04908530fbf0229b5588000429e","University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria","Jakl, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Lienhart, A.-M., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Baumann, C., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Jalaeefar, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Schlager, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Schoffer, L., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Bruckner, F., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria","Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures. © 2020 IEEE.","concepts and paradigms Human-centered computing; Human-centered computing; Interaction design theory; Interface design prototyping Human-centered computing; Mixed / augmented reality Human-centered computing; Usability testing","Augmented reality; User interfaces; Complementary tools; Information sheets; Information transfers; Interactive storytelling; Medical procedures; Patient education; System usability; Target group; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085526448
"Chirico A., Giovannetti T., Neroni P., Simone S., Gallo L., Galli F., Giancamilli F., Predazzi M., Lucidi F., De Pietro G., Giordano A.","56736363600;6603442366;56449399500;57208625839;24072878600;57216654873;57190075356;57204580043;6603819858;6508247659;57211929456;","Virtual Reality for the Assessment of Everyday Cognitive Functions in Older Adults: An Evaluation of the Virtual Reality Action Test and Two Interaction Devices in a 91-Year-Old Woman",2020,"Frontiers in Psychology","11",, 123,"","",,1,"10.3389/fpsyg.2020.00123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086524265&doi=10.3389%2ffpsyg.2020.00123&partnerID=40&md5=33521c197f288a9b3c08c5978e8e3e06","Department of Psychology of Developmental and Socialization Processes, Sapienza University of Rome, Rome, Italy; Psychology Department, Temple University, Philadelphia, PA, United States; Institute for High Performance Computing and Networking, National Research Council, Naples, Italy; Department of Engineering, Parthenope University of Naples, Naples, Italy; Fondazione Il Melo Onlus, Gallarate, Italy; Sbarro Institute for Cancer Research and Molecular Medicine, Center for Biotechnology, College of Science and Technology, Temple University, Philadelphia, PA, United States; Department of Medical Biotechnologies, University of Siena, Siena, Italy","Chirico, A., Department of Psychology of Developmental and Socialization Processes, Sapienza University of Rome, Rome, Italy; Giovannetti, T., Psychology Department, Temple University, Philadelphia, PA, United States; Neroni, P., Institute for High Performance Computing and Networking, National Research Council, Naples, Italy, Department of Engineering, Parthenope University of Naples, Naples, Italy; Simone, S., Psychology Department, Temple University, Philadelphia, PA, United States; Gallo, L., Institute for High Performance Computing and Networking, National Research Council, Naples, Italy; Galli, F., Department of Psychology of Developmental and Socialization Processes, Sapienza University of Rome, Rome, Italy; Giancamilli, F., Department of Psychology of Developmental and Socialization Processes, Sapienza University of Rome, Rome, Italy; Predazzi, M., Fondazione Il Melo Onlus, Gallarate, Italy; Lucidi, F., Department of Psychology of Developmental and Socialization Processes, Sapienza University of Rome, Rome, Italy; De Pietro, G., Institute for High Performance Computing and Networking, National Research Council, Naples, Italy; Giordano, A., Sbarro Institute for Cancer Research and Molecular Medicine, Center for Biotechnology, College of Science and Technology, Temple University, Philadelphia, PA, United States, Department of Medical Biotechnologies, University of Siena, Siena, Italy","Performance-based functional tests for the evaluation of daily living activities demonstrate strong psychometric properties and solve many of the limitations associated with self- and informant-report questionnaires. Virtual reality (VR) technology, which has gained interest as an effective medium for administering interventions in the context of healthcare, has the potential to minimize the time-demands associated with the administration and scoring of performance-based assessments. To date, efforts to develop VR systems for assessment of everyday function in older adults generally have relied on non-immersive systems. The aim of the present study was to evaluate the feasibility of an immersive VR environment for the assessment of everyday function in older adults. We present a detailed case report of an elderly woman who performed an everyday activity in an immersive VR context (Virtual Reality Action Test) with two different types of interaction devices (controller vs. sensor). VR performance was compared to performance of the same task with real objects outside of the VR system (Real Action Test). Comparisons were made on several dimensions, including (1) quality of task performance (e.g., order of task steps, errors, use and speed of hand movements); (2) subjective impression (e.g., attitudes), and (3) physiological markers of stress. Subjective impressions of performance with the different controllers also were compared for presence, cybersickness, and usability. Results showed that the participant was capable of using controllers and sensors to manipulate objects in a purposeful and goal-directed manner in the immersive VR paradigm. She performed the everyday task similarly across all conditions. She reported no cybersickness and even indicated that interactions in the VR environment were pleasant and relaxing. Thus, immersive VR is a feasible approach for function assessment even with older adults who might have very limited computer experience, no prior VR exposure, average educational experiences, and mild cognitive difficulties. Because of inherent limitations of single case reports (e.g., unknown generalizability, potential practice effects, etc.), group studies are needed to establish the full psychometric properties of the Virtual Reality Action Test. © Copyright © 2020 Chirico, Giovannetti, Neroni, Simone, Gallo, Galli, Giancamilli, Predazzi, Lucidi, De Pietro and Giordano.","activities of daily living; cognitive aging; everyday action; psychometric assessment; virtual reality",,Article,"Final","",Scopus,2-s2.0-85086524265
"Hejtmanek L., Starrett M., Ferrer E., Ekstrom A.D.","14008461600;56442554600;7101784591;7005951533;","How much of what we learn in virtual reality transfers to real-world navigation?",2020,"Multisensory Research","33","4-5",,"479","503",,1,"10.1163/22134808-20201445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082813726&doi=10.1163%2f22134808-20201445&partnerID=40&md5=a5f02f4195dd7bc0560e2209286c8f3e","Third Faculty of Medicine, Charles University, Ruská 87, Prague 10, 100 00, Czech Republic; Center for Neuroscience, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States; Department of Psychology, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States; Department of Psychology, University of Arizona, 1503 E. University Blvd., Tucson, AZ  85719, United States","Hejtmanek, L., Third Faculty of Medicine, Charles University, Ruská 87, Prague 10, 100 00, Czech Republic, Center for Neuroscience, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States; Starrett, M., Center for Neuroscience, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States, Department of Psychology, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States, Department of Psychology, University of Arizona, 1503 E. University Blvd., Tucson, AZ  85719, United States; Ferrer, E., Department of Psychology, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States; Ekstrom, A.D., Center for Neuroscience, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States, Department of Psychology, University of California, Davis, 1 Shields Ave, Davis, CA  95618, United States, Department of Psychology, University of Arizona, 1503 E. University Blvd., Tucson, AZ  85719, United States","Past studies suggest that learning a spatial environment by navigating on a desktop computer can lead to significant acquisition of spatial knowledge, although typically less than navigating in the real world. Exactly how this might differ when learning in immersive virtual interfaces that offer a rich set of multisensory cues remains to be fully explored. In this study, participants learned a campus building environment by navigating (1) the real-world version, (2) an immersive version involving an omnidirectional treadmill and head-mounted display, or (3) a version navigated on a desktop computer with a mouse and a keyboard. Participants first navigated the building in one of the three different interfaces and, afterward, navigated the real-world building to assess information transfer. To determine how well they learned the spatial layout, we measured path length, visitation errors, and pointing errors. Both virtual conditions resulted in significant learning and transfer to the real world, suggesting their efficacy in mimicking some aspects of real-world navigation. Overall, real-world navigation outperformed both immersive and desktop navigation, effects particularly pronounced early in learning. This was also suggested in a second experiment involving transfer from the real world to immersive virtual reality (VR). Analysis of effect sizes of going from virtual conditions to the real world suggested a slight advantage for immersive VR compared to desktop in terms of transfer, although at the cost of increased likelihood of dropout. Our findings suggest that virtual navigation results in significant learning, regardless of the interface, with immersive VR providing some advantage when transferring to the real world. Copyright © 2020 by Koninklijke Brill NV, Leiden, The Netherlands.","Navigation; Proprioception; Spatial cognition; Transfer; Virtual reality","Helmet mounted displays; Mammals; Navigation; Sensory perception; Transfer learning; Head mounted displays; Immersive virtual reality; Information transfers; Spatial cognition; Spatial environments; Transfer; Virtual interfaces; Virtual navigation; Virtual reality",Article,"Final","",Scopus,2-s2.0-85082813726
"Carreon A., Smith S.J., Mosher M., Rao K., Rowland A.","57200799882;8063955600;57219305897;57221021182;56821362700;","A Review of Virtual Reality Intervention Research for Students With Disabilities in K–12 Settings",2020,"Journal of Special Education Technology",,,,"","",,,"10.1177/0162643420962011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092143583&doi=10.1177%2f0162643420962011&partnerID=40&md5=24117c6927194ddf8e5de88897d87620","The University of Kansas, Lawrence, KS, United States; University of Hawaii, Honolulu, HI, United States","Carreon, A., The University of Kansas, Lawrence, KS, United States; Smith, S.J., The University of Kansas, Lawrence, KS, United States; Mosher, M., The University of Kansas, Lawrence, KS, United States; Rao, K., University of Hawaii, Honolulu, HI, United States; Rowland, A., The University of Kansas, Lawrence, KS, United States","Virtual reality (VR) technology has improved in access and availability in the area of K–12 instruction, increasingly being cited for its promise to meet the varied learning needs of individuals with disabilities. This descriptive review of 25 research studies conducted in K–12 settings examined the defining characteristics of immersion levels associated with VR, the purpose and application of the augmented reality intervention, the outcomes associated with the current use of VR, and the possibility of generalization beyond VR. The results of the review reveal that a majority of studies are utilizing nonimmersive screen-based simulations. While still considered under the VR domain, these technologies do not take advantage of the features of semi- and fully immersive VR which make it an appealing intervention for students with disabilities. Based on the results of this review, we provide recommendations to establish a strong research base on emerging VR technology and its use for students with disabilities in the K–12 classroom. © The Author(s) 2020.","assistive technology; instructional technology; literature review; methodologies; mixed reality; mobile devices; technology perspectives; virtual reality",,Article,"Article in Press","",Scopus,2-s2.0-85092143583
"Ke Y., Liu P., An X., Song X., Ming D.","55760984800;57213603004;57214860474;55814078100;9745824400;","An online SSVEP-BCI system in an optical see-through augmented reality environment",2020,"Journal of Neural Engineering","17","1", 016066,"","",,5,"10.1088/1741-2552/ab4dc6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080070514&doi=10.1088%2f1741-2552%2fab4dc6&partnerID=40&md5=bd56e2fce8c2131522bac0a349d1c469","Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Department of Biomedical Engineering, College of Precision Instrument and Optoelectronics Engineering, Tianjin University, Tianjin, 300072, China","Ke, Y., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Liu, P., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; An, X., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Song, X., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Ming, D., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China, Department of Biomedical Engineering, College of Precision Instrument and Optoelectronics Engineering, Tianjin University, Tianjin, 300072, China","Objective. This study aimed to design and evaluate a high-speed online steady-state visually evoked potential (SSVEP)-based brain-computer interface (BCI) in an optical see-through (OST) augmented reality (AR) environment. Approach. An eight-class BCI was designed in an OST-AR headset which is wearable and allows users to see the user interface of the BCI and the device to be controlled in the same view field via the OST head-mounted display. The accuracies, information transfer rates (ITRs), and SSVEP signal characteristics of the AR-BCI were evaluated and compared with a computer screen-based BCI implemented with a laptop in offline and online cue-guided tasks. Then, the performance of the AR-BCI was evaluated in an online robotic arm control task. Main results. The offline results obtained during the cue-guided task performed with the AR-BCI showed maximum averaged ITRs of 65.50 ± 9.86 bits min-1 according to the extended canonical correlation analysis-based target identification method. The online cue-guided task achieved averaged ITRs of 65.03 ± 11.40 bits min-1. The online robotic arm control task achieved averaged ITRs of 45.57 ± 7.40 bits min-1. Compared with the screen-based BCI, some limitations of the AR environment impaired BCI performance and the quality of SSVEP signals. Significance. The results showed the potential for providing a high-performance brain-control interaction method by combining AR and BCI. This study could provide methodological guidelines for developing more wearable BCIs in OST-AR environments and will also encourage more interesting applications involving BCIs and AR techniques. © 2020 IOP Publishing Ltd.","augmented reality; brain-computer interface; electroencephalogram; optical see-through; steady-state visual evoked potential","Augmented reality; Electroencephalography; Helmet mounted displays; Interface states; Robotic arms; Robotics; User interfaces; Wearable computers; Canonical correlation analysis; Head mounted displays; Information transfer rate; Methodological guidelines; Optical see-through; Steady state visual evoked potentials; Steady state visually evoked potentials; Target identification; Brain computer interface; adult; augmented reality; Conference Paper; controlled study; correlation analysis; female; human; human experiment; male; normal human; online system; priority journal; robotics; signal transduction; task performance; visual evoked potential",Conference Paper,"Final","",Scopus,2-s2.0-85080070514
"Balzerkiewitz H.-P., Stechert C.","57219207718;24448259400;","Use of Virtual Reality in Product Development by Distributed Teams",2020,"Procedia CIRP","91",,,"577","582",,,"10.1016/j.procir.2020.02.216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091709372&doi=10.1016%2fj.procir.2020.02.216&partnerID=40&md5=b9c2cf61ceefb8d3f8b8d602c11ebcad","Ostfalia University of Applied Science, Wolfenbüttel, Germany","Balzerkiewitz, H.-P., Ostfalia University of Applied Science, Wolfenbüttel, Germany; Stechert, C., Ostfalia University of Applied Science, Wolfenbüttel, Germany","This paper shows how future VR software improvements can support work in distributed teams. The comprehensive summary to the topics ""VR-technology"" and ""working in distributed teams"" lead to the following recommendations. In order to improve distributed teamwork in product development in the future the VR-technology can be a suitable tool. Therefor the data transfer between CAD and VR must be smooth. Moreover, existing or new VR-Software must be adapted in a way that allows the creation of 3D objects directly in the virtual environment. © 2017 The Authors. Published by Elsevier B.V.","Distributed Teams; Product Development; Virtual Reality","Computer aided design; Data transfer; Product development; 3D object; Distributed teams; VR technology; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85091709372
"Matsangidou M., Otkhmezuri B., Ang C.S., Avraamides M., Riva G., Gaggioli A., Iosif D., Karekla M.","57196007400;57196009850;15831174100;6506947139;56962750600;6603138127;57219454910;7801543407;","“Now i can see me” designing a multi-user virtual reality remote psychotherapy for body weight and shape concerns",2020,"Human-Computer Interaction",,,,"","",,3,"10.1080/07370024.2020.1788945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092795269&doi=10.1080%2f07370024.2020.1788945&partnerID=40&md5=67a8f0e1d96c2ded63dded4a70672b50","School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus; School of Psychology, University of Cyprus, Nicosia, Cyprus; School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy; Applied Technology for Neuro-Psychology Lab, Milan, Italy","Matsangidou, M., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom, Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus; Otkhmezuri, B., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Ang, C.S., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Avraamides, M., Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus, School of Psychology, University of Cyprus, Nicosia, Cyprus; Riva, G., School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab, Milan, Italy; Gaggioli, A., School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab, Milan, Italy; Iosif, D., School of Psychology, University of Cyprus, Nicosia, Cyprus; Karekla, M., School of Psychology, University of Cyprus, Nicosia, Cyprus","Recent years have seen a growing research interest towards designing computer-assisted health interventions aiming to improve mental health services. Digital technologies are becoming common methods for diagnosis, therapy, and training. With the advent of lower-cost VR head-mounted-displays (HMDs) and high internet data transfer capacity, there is a new opportunity for applying immersive VR tools to augment existing interventions. This study is among the first to explore the use of a Multi-User Virtual Reality (MUVR) system as a therapeutic medium for participants at high-risk for developing Eating Disorders. This paper demonstrates the positive effect of using MUVR remote psychotherapy to enhance traditional therapeutic practices. The study capitalises on the opportunities which are offered by a MUVR remote psychotherapeutic session to enhance the outcome of Acceptance and Commitment Therapy, Play Therapy and Exposure Therapy for sufferers with body shape and weight concerns. Moreover, the study presents the design opportunities and challenges of such technology, while strengths on the feasibility, and the positive user acceptability of introducing MUVR to facilitate remote psychotherapy. Finally, the appeal of using VR for remote psychotherapy and its observed positive impact on both therapists and participants is discussed. © 2020 The Author(s). Published with license by Taylor & Francis Group, LLC.","Acceptance and Commitment Therapy (ACT); high-risk for eating disorders; Multi-User virtual reality; Play Therapy; Exposure Therapy; remote psychotherapy","Data transfer; Helmet mounted displays; Computer assisted; Digital technologies; Head mounted displays; Health interventions; Mental health services; Research interests; Therapeutic practices; Transfer capacities; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85092795269
"Rothe S., Schmidt A., Montagud M., Buschek D., Hußmann H.","57199996760;57219418508;35868074700;55850134500;23389275800;","Social viewing in cinematic virtual reality: a design space for social movie applications",2020,"Virtual Reality",,,,"","",,1,"10.1007/s10055-020-00472-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092676924&doi=10.1007%2fs10055-020-00472-4&partnerID=40&md5=def8d44ae1a903490a6dd20957473a4d","Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Universitat de València & i2CAT Foundation, Valencia, Spain; Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany","Rothe, S., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Schmidt, A., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Montagud, M., Universitat de València & i2CAT Foundation, Valencia, Spain; Buschek, D., Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany; Hußmann, H., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany","Since watching movies is a social experience for most people, it is important to know how an application should be designed for enabling shared cinematic virtual reality (CVR) experiences via head-mounted displays (HMDs). Viewers can feel isolated when watching omnidirectional movies with HMDs. Even if they are watching the movie simultaneously, they do not automatically see the same field of view, since they can freely choose their viewing direction. Our goal is to explore interaction techniques to efficiently support social viewing and to improve social movie experiences in CVR. Based on the literature review and insights from earlier work, we identify seven challenges that need to be addressed: communication, field-of-view (FoV) awareness, togetherness, accessibility, interaction techniques, synchronization, and multiuser environments. We investigate four aspects (voice chat, sending emotion states, FoV indication, and video chat) to address some of the challenges and report the results of four user studies. Finally, we present and discuss a design space for CVR social movie applications and highlight directions for future work. © 2020, The Author(s).","360° video; Cinematic virtual reality; Interactive TV; Omnidirectional video; Social viewing","Helmet mounted displays; Motion pictures; Technology transfer; Design spaces; Field of views; Head mounted displays; Interaction techniques; Literature reviews; Multiuser environments; User study; Viewing directions; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85092676924
"Hoppe A.H., Marek F., De Camp F.V., Stiefelhagen R.","57195069885;57210910641;57194787505;6602180348;","Extending movable surfaces with touch interaction using the virtualtablet: An extended view",2020,"Advances in Science, Technology and Engineering Systems","5","2",,"328","337",,,"10.25046/AJ050243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087525968&doi=10.25046%2fAJ050243&partnerID=40&md5=df0c63da1836cd541fada857bd9f6d7e","Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany; Fraunhofer IOSB, Interactive Analysis and Diagnosis (IAD), Karlsruhe, 76131, Germany; Fraunhoferstr. 1, Karlsruhe, 76131, Germany","Hoppe, A.H., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany, Fraunhoferstr. 1, Karlsruhe, 76131, Germany; Marek, F., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany; De Camp, F.V., Fraunhofer IOSB, Interactive Analysis and Diagnosis (IAD), Karlsruhe, 76131, Germany; Stiefelhagen, R., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany","Immersive output and natural input are two core aspects of a virtual reality experience. Current systems are frequently operated by a controller or gesture-based approach. However, these techniques are either very accurate but require an effort to learn, or very natural but miss haptic feedback for optimal precision. We transfer ubiquitous touch interaction with haptic feedback into a virtual environment. To validate the performance of our implementation, we performed a user study with 28 participants. As the results show, the movable and cheap real world object supplies an accurate touch detection that is equal to a laserpointer-based interaction with a controller. Moreover, the virtual tablet can extend the functionality of a real world tablet. Additional information can be displayed in mid-air around the touchable area and the tablet can be turned over to interact with both sides. Therefore, touch interaction in virtual environments allows easy to learn and precise system interaction and can even augment the established touch metaphor with new paradigms. © 2020 ASTES Publishers. All rights reserved.","Haptic feedback; Touch interaction; Virtual environment; Virtual reality; VirtualTablet",,Article,"Final","",Scopus,2-s2.0-85087525968
"Zhao X., Liu C., Xu Z., Zhang L., Zhang R.","17436661300;57214105967;57214126904;57201266084;56898266500;","SSVEP Stimulus Layout Effect on Accuracy of Brain-Computer Interfaces in Augmented Reality Glasses",2020,"IEEE Access","8",, 8947980,"5990","5998",,,"10.1109/ACCESS.2019.2963442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078237308&doi=10.1109%2fACCESS.2019.2963442&partnerID=40&md5=149148c8923287a093bfa864726e505a","School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China","Zhao, X., School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Liu, C., School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Xu, Z., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China; Zhang, L., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China; Zhang, R., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China","Steady-state visual evoked potentials-based brain-computer interfaces (SSVEP-BCI) has the advantage of high information transfer rate (ITR) and little user training, and it has a high application value in the field of disability assistance and human-computer interaction. Generally SSVEP-BCI requires a personal computer screen (PC) to display several repetitive visual stimuli for inducing the SSVEP response, which reduces its portability and flexibility. Using augmented reality (AR) glasses worn on the head to display the repetitive visual stimuli could solve the above drawbacks, but whether it could achieve the same accuracy as PC screen in the case of reduced brightness and increased interference is unknown. In current study, we firstly designed 4 stimulus layouts and displayed them with Microsoft HoloLens (AR-SSVEP) glasses, comparison analysis showed that the classification accuracies are influenced by the stimulus layout when the stimulus duration is less than 3s. When the stimulus duration exceeds 3s, there is no significant accuracy difference between the 4 layouts. Then we designed a similar experimental paradigm on PC screen (PC-SSVEP) based on the best layout of AR. Classification results showed that AR-SSVEP achieved similar accuracy with PC-SSVEP when the stimulus duration is more than 3s, but when the stimulus duration is less than 2s, the accuracy of AR-SSVEP is lower than PC-SSVEP. Brain topological analysis indicated that the spatial distribution of SSVEP responses is similar, both of which are strongest in the occipital region. Current study indicated that stimulus layout is a key factor when building SSVEP-BCI with AR glasses, especially when the stimulation time is short. © 2013 IEEE.","augmented reality (AR); brain-computer interfaces (BCI); human-computer interaction; optical see-through (OST); Steady-state visual evoked potentials (SSVEP)","Augmented reality; Electrophysiology; Glass; Human computer interaction; Interface states; Personal computers; Topology; Classification accuracy; Classification results; Comparison analysis; High application value; Information transfer rate; Optical see-through; Steady state visual evoked potentials; Topological analysis; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85078237308
"Nikouline A., Jimenez M.C., Okrainec A.","47461465000;57198326439;6508182372;","Feasibility of remote administration of the fundamentals of laparoscopic surgery (FLS) skills test using Google wearable device",2020,"Surgical Endoscopy","34","1",,"443","449",,,"10.1007/s00464-019-06788-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065202277&doi=10.1007%2fs00464-019-06788-w&partnerID=40&md5=f4d93a9dae247ee244cc935b13172ff4","Division of Emergency Medicine, Department of Medicine, University of Toronto, Toronto, Canada; Division of General Surgery, University Health Network (UHN), Toronto, Canada; Department of Surgery, University of Toronto, Toronto, Canada; Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada; University Health Network (UHN), 399 Bathurst Street, Toronto, ON  M5T 2S8, Canada","Nikouline, A., Division of Emergency Medicine, Department of Medicine, University of Toronto, Toronto, Canada; Jimenez, M.C., Division of General Surgery, University Health Network (UHN), Toronto, Canada, Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada; Okrainec, A., Division of General Surgery, University Health Network (UHN), Toronto, Canada, Department of Surgery, University of Toronto, Toronto, Canada, Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada, University Health Network (UHN), 399 Bathurst Street, Toronto, ON  M5T 2S8, Canada","Background: The fundamentals of laparoscopic surgery (FLS) program is a simulation-based training program designed to teach and assess the basic skills necessary for laparoscopic surgery. Preliminary work has demonstrated the feasibility of using Skype™ as a telesimulation modality in reliably scoring the exam for remote centers. Google Glass (GG) (Mountain View, California) is referred to as a wearable computer containing a heads-up display and front-facing camera allowing point-of-view video transmission. The objective of this study was to evaluate the feasibility of GG in scoring the technical skills component of the FLS exam. Methods: Twenty-eight participants were asked to complete the peg transfer and intracorporeal knot tasks of FLS using both GG and Skype™ setups. GG employed a third-party HIPAA-compliant video software (Pristine; Austin, TX) for video transmission. Participants were alternated between setups and evaluated by onsite and remote proctors. Times and errors were recorded by both proctors. Interrater reliability of their FLS scores was compared using Intraclass Correlation Coefficients (ICCs). GG experience was evaluated based on participant survey responses using a 5-point Likert scale. Results: Interrater reliability for GG demonstrated a statistically significant correlation between onsite (OP) and remote (RP) proctors with ICCs of 0.985 (95% Confidence Interval [CI], 0.969–0.993) and 0.997 (95% CI 0.993–0.998), respectively, for peg and suture tasks. Skype™ demonstrated ICCs of 1.0 (95% CI 1.0–1.0). Average Likert scale responses found GG to be distracting (2.71), obstructive of the view (2.79), and a limitation to task execution (2.75). Overall, there was no statistical difference in scores between GG and Skype™ setups for either the peg (t = 1.446, p = 0.154) or suture task (t = − 0.710, p = 0.480), only 1 participant found the use of GG superior to Skype™. Conclusions: Our findings suggest that although GG are feasible in remote assessment of FLS with strong interrater reliability (ICC > 0.95), Skype™ was the preferred modality. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Fundamentals of laparoscopic surgery (FLS); Google Glass; Telesimulation","adult; Article; computer simulation; error; female; human; interrater reliability; laparoscopic surgery; male; operating room personnel; pilot study; priority journal; surgical training; telehealth; time; videoconferencing; Canada; clinical competence; education; electronic device; feasibility study; laparoscopy; procedures; reproducibility; simulation training; Adult; Canada; Clinical Competence; Education, Distance; Feasibility Studies; Female; Humans; Laparoscopy; Male; Pilot Projects; Reproducibility of Results; Simulation Training; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85065202277
"Dhiman H., Buttner S., Rocker C., Reisch R.","57202890221;56954752800;55919922900;57214365751;","Handling work complexity with ar/deep learning",2019,"ACM International Conference Proceeding Series",,,,"518","522",,1,"10.1145/3369457.3370919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078696444&doi=10.1145%2f3369457.3370919&partnerID=40&md5=197e2b1fe56134c8ada58b36cf5d8ccf","Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Fraunhofer IOSB-INA, Lemgo, Germany; Resolto Informatik GmbH, Herford, Germany","Dhiman, H., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Buttner, S., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Rocker, C., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Fraunhofer IOSB-INA, Lemgo, Germany; Reisch, R., Resolto Informatik GmbH, Herford, Germany","Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair. © 2019 Association for Computing Machinery.","Augmented Reality; Deep Learning","Augmented reality; Human computer interaction; Object recognition; Product design; Repair; Assistance system; Digital design techniques; Hardware/software; Learning approach; Product assembly; Work complexity; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85078696444
"Meena Y.K., Cecotti H., Wong-Lin K.F., Prasad G.","57212351705;8838569700;26532134800;55812562500;","Design and evaluation of a time adaptive multimodal virtual keyboard",2019,"Journal on Multimodal User Interfaces","13","4",,"343","361",,3,"10.1007/s12193-019-00293-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061282696&doi=10.1007%2fs12193-019-00293-z&partnerID=40&md5=4b3d90c23ab7b84d8cb09f3fbd862063","Future Interaction Technology Lab, Swansea University, Swansea, United Kingdom; Department of Computer Science, College of Science and Mathematics, Fresno State University, Fresno, CA, United States; School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom","Meena, Y.K., Future Interaction Technology Lab, Swansea University, Swansea, United Kingdom; Cecotti, H., Department of Computer Science, College of Science and Mathematics, Fresno State University, Fresno, CA, United States; Wong-Lin, K.F., School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom; Prasad, G., School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom","The usability of virtual keyboard based eye-typing systems is currently limited due to the lack of adaptive and user-centered approaches leading to low text entry rate and the need for frequent recalibration. In this work, we propose a set of methods for the dwell time adaptation in asynchronous mode and trial period in synchronous mode for gaze based virtual keyboards. The rules take into account commands that allow corrections in the application, and it has been tested on a newly developed virtual keyboard for a structurally complex language by using a two-stage tree-based character selection arrangement. We propose several dwell-based and dwell-free mechanisms with the multimodal access facility wherein the search of a target item is achieved through gaze detection and the selection can happen via the use of a dwell time, soft-switch, or gesture detection using surface electromyography in asynchronous mode; while in the synchronous mode, both the search and selection may be performed with just the eye-tracker. The system performance is evaluated in terms of text entry rate and information transfer rate with 20 different experimental conditions. The proposed strategy for adapting the parameters over time has shown a significant improvement (more than 40%) over non-adaptive approaches for new users. The multimodal dwell-free mechanism using a combination of eye-tracking and soft-switch provides better performance than adaptive methods with eye-tracking only. The overall system receives an excellent grade on adjective rating scale using the system usability scale and a low weighted rating on the NASA task load index, demonstrating the user-centered focus of the system. © 2019, The Author(s).","Adaptive control; Eye-typing; Gaze-based access control; Graphical user interface; Human-computer interaction; Multimodal dwell-free control; Virtual keyboard","Access control; Computer keyboards; Graphical user interfaces; Human computer interaction; NASA; Scales (weighing instruments); Virtual reality; Adaptive Control; Design and evaluations; Experimental conditions; Eye-typing; Free control; Information transfer rate; Surface electromyography; Virtual Keyboards; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061282696
"Moriyama T., Asazu H., Takahashi A., Kajimoto H.","57202339848;6506297693;57195231000;6602141526;","Simple is vest: High-density tactile vest that realizes tactile transfer of fingers",2019,"SIGGRAPH Asia 2019 Emerging Technologies, SA 2019",,,,"42","43",,3,"10.1145/3355049.3360532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076672766&doi=10.1145%2f3355049.3360532&partnerID=40&md5=4fb9058399b8282d250c118fdd67b102","University of Electro-Communications, Japan","Moriyama, T., University of Electro-Communications, Japan; Asazu, H., University of Electro-Communications, Japan; Takahashi, A., University of Electro-Communications, Japan; Kajimoto, H., University of Electro-Communications, Japan","We developed a high-density tactile vest that presents the haptic sensation of the five fingertips to the back rather than to the fingertip as a new haptic presentation method for objects in a virtual reality (VR) environment. The device adopts 144 eccentric mass vibration motors actuated individually and five Peltier elements. The voice coils present the contact points on the finger to the back and abdomen. When holding an object with fingers in VR scene, the sense of touch is presented to the wide area of the abdomen and back, so an accurate sense of which part of the finger is contacted can be comprehended. Compared with a fingertip-mounted display, it is possible to address issues of weight and size that hinder the free movement of fingers, while high-density distributed information can be presented. Furthermore, abdomen and back are totally free space in typical VR scenarios. © 2019 Copyright held by the owner/author(s).","Haptic devices; Vibrators","Vibrators; Virtual reality; Contact points; Distributed information; Eccentric mass; Haptic devices; Haptic sensation; Peltier elements; Sense of touch; Vibration motor; Interactive computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85076672766
"Zou Y., Wang P., Tang Q., Sun Y.","57217006652;57217006341;57217400762;57217004882;","Implement Multi-Character Display in Virtual Reality Environment Based on Unet and Tracker",2019,"Proceedings - 2019 2nd International Conference on Safety Produce Informatization, IICSPI 2019",,, 9096025,"530","532",,,"10.1109/IICSPI48186.2019.9096025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085683784&doi=10.1109%2fIICSPI48186.2019.9096025&partnerID=40&md5=c77d19ebaa5c757cccf48ac7163524b2","Zunyi Power Supply Bureau, Safety Supervision Department, Zunyi, Guizhou, China","Zou, Y., Zunyi Power Supply Bureau, Safety Supervision Department, Zunyi, Guizhou, China; Wang, P., Zunyi Power Supply Bureau, Safety Supervision Department, Zunyi, Guizhou, China; Tang, Q., Zunyi Power Supply Bureau, Safety Supervision Department, Zunyi, Guizhou, China; Sun, Y., Zunyi Power Supply Bureau, Safety Supervision Department, Zunyi, Guizhou, China","Power grid operation is a very safe operation site, it is very important to be able to carry out related operations in accordance with the safe operation process, the existing traditional safe operation process, mainly in the traditional teaching mode or simulated real scene operation mode. In the links of management and training, operators lack intuitive experience and interaction, and they are not enough to show the serious consequences of illegal operation; As the head-mounted display becomes the most important interactive virtual reality display device, the 3d interactive display and the expressive force of the game engine are gradually improved, significantly improving participants' sense of immersion. Virtual reality technology can be used to build a simulation teaching platform for authentic visual experience, authentic representation of work flow and convincing interactive feedback. Virtual reality technology is used to establish a practical environment for grid operation in the virtual three-dimensional space of the computer that integrates class a illegal operation events. Users participating in the learning practice can operate in the virtual space according to the process. In this paper, based on unity built-in HAPI unet and third-party plug-in VRTK, the complex interaction of large-scale training business process is realized. Through VRTK, steam VR interface, the handle button state is obtained and information is transmitted on multiple instances, so as to determine the transfer and pickup state of interactive objects. © 2019 IEEE.","Inverse dynamics; Multiplayer networking technology; Multiplayer online interaction; Virtual reality technology","Crime; Electric power transmission networks; Helmet mounted displays; Interface states; Personnel training; Simulation platform; Head mounted displays; Interactive feedback; Interactive virtual reality; Power grid operations; Simulation teachings; Three dimensional space; Virtual reality technology; Virtual-reality environment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085683784
"Piccione J., Collett J., De Foe A.","57211538030;57206464821;55555885000;","Virtual skills training: the role of presence and agency",2019,"Heliyon","5","11", e02583,"","",,6,"10.1016/j.heliyon.2019.e02583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074370179&doi=10.1016%2fj.heliyon.2019.e02583&partnerID=40&md5=41c139affc0f49a955d0d2674602e6aa","RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Piccione, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; Collett, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; De Foe, A., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Virtual reality (VR) simulations provide increased feelings of presence and agency that could allow increased skill improvement during VR training. Direct relationships between active agency in VR and skill improvement have previously not been investigated. This study examined the relationship between (a) presence and agency, and (b) presence and skills improvement, via active and passive VR simulations and through measuring real-world golf-putting skill. Participants (n = 23) completed baseline putting skill assessment before using an Oculus Rift VR head-mounted display to complete active (putting with a virtual golf club) and passive (watching a game of golf) VR simulations. Measures of presence and agency were administered after each simulation, followed by a final putting skill assessment. The active simulation induced higher feelings of general presence and agency. However, no relationship was identified between presence and either agency or skill improvement. No skill improvement was evident in either the active or passive simulations, potentially due to the short training period applied, as well as a lack of realism in the VR simulations inhibiting a transfer of skills to a real environment. These findings reinforce previous literature that shows active VR to increase feelings of presence and agency. This study generates a number of fruitful research questions about the relationship between presence and skills training. © 2019 The AuthorsPsychology; Virtual reality; Presence; Human factors; Sport psychology © 2019 The Authors","Human factors; Presence; Psychology; Sport psychology; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85074370179
"Chew J.Y., Okayama K., Okuma T., Kawamoto M., Onda H., Kato N.","57189996822;57204778207;7004950282;56257346500;7006391806;57211832917;","Development of A Virtual Environment to Realize Human-Machine Interaction of Forklift Operation",2019,"2019 7th International Conference on Robot Intelligence Technology and Applications, RiTA 2019",,, 8932837,"112","118",,,"10.1109/RITAPP.2019.8932837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078030425&doi=10.1109%2fRITAPP.2019.8932837&partnerID=40&md5=5c2a1a0ec9614f4b07ba4b3c1567ae88","National Institute of Advanced Industrial Science and Technology, Japan","Chew, J.Y., National Institute of Advanced Industrial Science and Technology, Japan; Okayama, K., National Institute of Advanced Industrial Science and Technology, Japan; Okuma, T., National Institute of Advanced Industrial Science and Technology, Japan; Kawamoto, M., National Institute of Advanced Industrial Science and Technology, Japan; Onda, H., National Institute of Advanced Industrial Science and Technology, Japan; Kato, N., National Institute of Advanced Industrial Science and Technology, Japan","This study presents an experimental concept to develop realistic Human-Machine Interaction (HMI) for a Virtual Environment (VE) and a novel evaluation methodology of such system. Such evaluation is motivated by the need to facilitate transfer of model/knowledge from VE to the Real Environment (RE), where it is crucial for the VE to trigger similar user behavior as in the RE. This paper discusses the application of such concept to evaluate interactions of forklift operation in the VE. First, a Virtual Reality (VR) forklift simulator is developed using motion capture and 3D reconstruction methods to mimic HMI of the real forklift operation. Then, the Dynamic Time Warping (DTW) algorithm is used for temporal evaluation of operation behaviors in VE and RE. Results of DTW (i.e. distance and correlation) are used as objective measures to evaluate fidelity of VE during forklift operations on the simulator. Results suggest the proposed forklift simulator triggers operation behavior which is similar (highly correlated) to that of real forklift operation. The contributions of this paper are (a) the novel VR forklift simulator system to realize interactions of real forklift in the VE, and (b) the proposed objective measures for temporal evaluation of the fidelity of VE. © 2019 IEEE.",,"Behavioral research; Intelligent robots; Man machine systems; Materials handling equipment; Simulators; Dynamic time warping algorithms; Evaluation methodologies; Highly-correlated; Human-machine interaction; Objective measure; Real environments; Simulator systems; Temporal evaluation; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85078030425
"Narciso D., Bessa M., Melo M., Vasconcelos-Raposo J.","57188767021;14031038800;7102354924;36070012200;","Virtual reality for training-the impact of smell on presence, cybersickness, fatigue, stress and knowledge transfer",2019,"ICGI 2019 - Proceedings of the International Conference on Graphics and Interaction",,, 8955071,"115","121",,1,"10.1109/ICGI47575.2019.8955071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078949972&doi=10.1109%2fICGI47575.2019.8955071&partnerID=40&md5=eea844c1f887a3d220806d274c29acf7","UTAD, Engineering Department, Vila Real, Portugal; INESC TEC, Porto, Portugal; UTAD, Departament of Education and Psychology, Vila Real, Portugal","Narciso, D., UTAD, Engineering Department, Vila Real, Portugal; Bessa, M., UTAD, Engineering Department, Vila Real, Portugal; Melo, M., INESC TEC, Porto, Portugal; Vasconcelos-Raposo, J., UTAD, Departament of Education and Psychology, Vila Real, Portugal","The area of professional training using virtual reality technologies has received considerable investment due to the advantages that virtual reality provides over traditional training. In this paper, we present an experiment whose goal was to analyse the impact that an additional stimulus has on the effectiveness of a virtual environment designed to train firefighters. The additional stimulus is a smell, more specifically the smell of burnt wood, which is consistent with the audiovisual content presented, and the effectiveness of the VE is measured through participant's feeling of presence, cybersickness, fatigue, stress and transfer of knowledge. The results indicate that, although the VE was successful in transferring knowledge, the addition of smell did not influence any of the measured variables. In the discussion section, we present the various factors that we believe have influenced this result. As future work, more experiments will be performed, with other stimuli, to understand better which stimuli increase participant's feeling of presence in the VE. © 2019 IEEE.","Multisensory Stimulation; Olfactory Sense; Virtual Reality","Knowledge management; Virtual reality; Audio-visual content; Feeling of presences; Knowledge transfer; Multisensory stimulations; Olfactory sense; Professional training; Transfer of knowledge; Virtual reality technology; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85078949972
"Yu D., Liang H.-N., Lu X., Fan K., Ens B.","57203763734;8636386200;57210919536;57203767958;53867874600;","Modeling endpoint distribution of pointing selection tasks in virtual reality environments",2019,"ACM Transactions on Graphics","38","6", 3356544,"","",,7,"10.1145/3355089.3356544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078889626&doi=10.1145%2f3355089.3356544&partnerID=40&md5=0e7c4cf924b43bed65cb3efc6fe21c3d","University of Melbourne, Melbourne, Australia; Xi'an Jiaotong-Liverpool University, China; Monash University, Australia","Yu, D., University of Melbourne, Melbourne, Australia, Xi'an Jiaotong-Liverpool University, China; Liang, H.-N., Xi'an Jiaotong-Liverpool University, China; Lu, X., Xi'an Jiaotong-Liverpool University, China; Fan, K., Xi'an Jiaotong-Liverpool University, China; Ens, B., Monash University, Australia","Understanding the endpoint distribution of pointing selection tasks can reveal the underlying patterns on how users tend to acquire a target, which is one of the most essential and pervasive tasks in interactive systems. It could further aid designers to create new graphical user interfaces and interaction techniques that are optimized for accuracy, efficiency, and ease of use. Previous research has explored the modeling of endpoint distribution outside of virtual reality (VR) systems that have shown to be useful in predicting selection accuracy and guide the design of new interactive techniques. This work aims at developing an endpoint distribution of selection tasks for VR systems which has resulted in EDModel, a novel model that can be used to predict endpoint distribution of pointing selection tasks in VR environments. The development of EDModel is based on two users studies that have explored how factors such as target size, movement amplitude, and target depth affect the endpoint distribution. The model is built from the collected data and its generalizability is subsequently tested in complex scenarios with more relaxed conditions. Three applications of EDModel inspired by previous research are evaluated to show the broad applicability and usefulness of the model: Correcting the bias in Fitts's law, predicting selection accuracy, and enhancing pointing selection techniques. Overall, EDModel can achieve high prediction accuracy and can be adapted to different types of applications in VR. © 2019 Association for Computing Machinery.","Endpoint distribution; Error prediction; Fitts's Law; Selection modeling; Target selection","Graphical user interfaces; Virtual reality; Endpoint distribution; Error prediction; Fitts's law; Selection model; Target selection; Forecasting",Article,"Final","",Scopus,2-s2.0-85078889626
"Braly A.M., Nuernberger B., Kim S.Y.","57195557852;56155189000;57205624514;","Augmented Reality Improves Procedural Work on an International Space Station Science Instrument",2019,"Human Factors","61","6",,"866","878",,4,"10.1177/0018720818824464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060878478&doi=10.1177%2f0018720818824464&partnerID=40&md5=d2c29980abe0ea26b6a5af91df1ed688","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States","Braly, A.M., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Nuernberger, B., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Kim, S.Y., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States","Objective: The purpose of the current study was to determine whether an augmented reality instruction method would result in faster task completion times, lower mental workload, and fewer errors for simple tasks in an operational setting. Background: Prior research on procedural work that directly compared augmented reality instructions to traditional instruction methods (e.g., paper) showed that augmented reality instructions can enhance procedural work, but this was not true for simple tasks in an operational setting. Method: Participants completed simple procedural tasks on spaceflight hardware using an augmented reality instruction method and a paper instruction method. Results: Our results showed that the augmented reality instruction method resulted in faster task completion times and lower levels of mental and temporal demand compared with paper instructions. When participants used the augmented reality instruction method before the paper instruction method, there was a transfer of training that improved a subsequent procedure using the paper instruction method. Conclusion: An off-the-shelf augmented reality head-mounted display (HoloLens) can enhance procedural work for simple tasks in an operational setting. Application: The ability of augmented reality to enhance procedural work for simple tasks in an operational setting can help in reducing costs and mitigating risks that could ultimately lead to accidents and critical failures. © 2019, Human Factors and Ergonomics Society.","augmented reality; procedural work; spaceflight","Helmet mounted displays; Space flight; Space stations; Critical failures; Head mounted displays; Instruction methods; International Space stations; procedural work; Space-flight hardware; Traditional instruction; Transfer of trainings; Augmented reality; adult; devices; female; human; male; middle aged; space flight; task performance; workload; young adult; Adult; Augmented Reality; Female; Humans; Male; Middle Aged; Spacecraft; Task Performance and Analysis; Workload; Young Adult",Article,"Final","",Scopus,2-s2.0-85060878478
"He Q., McNamara T.P., Bodenheimer B., Klippel A.","56675107000;7004768772;56133948600;8438953000;","Acquisition and transfer of spatial knowledge during wayfinding",2019,"Journal of Experimental Psychology: Learning Memory and Cognition","45","8",,"1364","1386",,11,"10.1037/xlm0000654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051931514&doi=10.1037%2fxlm0000654&partnerID=40&md5=a0330f54db114282d221d66a78e37eb9","School of Psychology, Georgia Institute of Technology, United States; Department of Psychology, Vanderbilt University, United States; Department of Electrical Engineering and Computer Science, Vanderbilt University, United States; Department of Geography, The Pennsylvania State University, United States","He, Q., School of Psychology, Georgia Institute of Technology, United States; McNamara, T.P., Department of Psychology, Vanderbilt University, United States; Bodenheimer, B., Department of Electrical Engineering and Computer Science, Vanderbilt University, United States; Klippel, A., Department of Geography, The Pennsylvania State University, United States","In the current study, we investigated the ways in which the acquisition and transfer of spatial knowledge were affected by (a) the type of spatial relations predominately experienced during learning (routes determined by walkways vs. straight-line paths between locations); (b) environmental complexity; and (c) the availability of rotational body-based information. Participants learned the layout of a virtual shopping mall by repeatedly searching for target storefronts located in 1 of the buildings. We created 2 novel learning conditions to encourage participants to use either route knowledge (paths on walkways between buildings) or survey knowledge (straight-line distances and directions from storefront to storefront) to find the target, and measured the development of route and survey knowledge in both learning conditions. Environmental complexity was manipulated by varying the alignment of the buildings with the enclosure, and the visibility within space. Body-based information was manipulated by having participants perform the experiment in front of a computer monitor or using a head-mounted display. After navigation, participants pointed to various storefronts from a fixed position and orientation. Results showed that the frequently used spatial knowledge could be developed similarly across environments with different complexities, but the infrequently used spatial knowledge was less developed in the complex environment. Furthermore, rotational body-based information facilitated spatial learning under certain conditions. Our results suggest that path integration may play an important role in spatial knowledge transfer, both from route to survey knowledge (cognitive map construction), and from survey to route knowledge (using cognitive map to guide wayfinding). © 2018 American Psychological Association.","Body-based information; Environmental complexity; Path integration; Spatial knowledge; Spatial navigation","adult; association; attention; distance perception; female; human; male; orientation; problem solving; social environment; spatial orientation; virtual reality; young adult; Adult; Attention; Cues; Distance Perception; Female; Humans; Male; Orientation; Problem Solving; Social Environment; Spatial Navigation; Transfer, Psychology; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85051931514
"Steffen J.H., Gaskin J.E., Meservy T.O., Jenkins J.L., Wolman I.","57200579874;36006215900;8965919200;36713489900;57210317891;","Framework of Affordances for Virtual Reality and Augmented Reality",2019,"Journal of Management Information Systems","36","3",,"683","729",,10,"10.1080/07421222.2019.1628877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070323987&doi=10.1080%2f07421222.2019.1628877&partnerID=40&md5=4cc7e578379bb372fe5452b489db6237","Terry College of Business, University of Georgia, United States; Management Information Systems, Brigham Young University, United States; Information Systems, Brigham Young University, United States; Information Systems, Brigham Young University, United States; Master of Information Systems Management program, Brigham Young University, United States","Steffen, J.H., Terry College of Business, University of Georgia, United States; Gaskin, J.E., Management Information Systems, Brigham Young University, United States; Meservy, T.O., Information Systems, Brigham Young University, United States; Jenkins, J.L., Information Systems, Brigham Young University, United States; Wolman, I., Master of Information Systems Management program, Brigham Young University, United States","Virtual reality (VR) and augmented reality (AR) technologies continue to grow and present possibilities to change the ways we learn, accomplish tasks, and interact with the world. However, widespread adoption has continually languished below purported potential. We suggest that a more complete understanding of the underlying motives driving users to take advantage of VR and AR would aid researchers by consolidating fragmented knowledge across domains and by identifying paths for additional inquiry. Additionally, practitioners could identify areas of unmet motives for using VR and AR. To examine the motives for virtualization, we draw upon Gibson’s seminal work on affordances to create a framework of generalized affordances for virtually assisted activities relative to the affordances of physical reality. This framework facilitates comparison of virtualized activities to non-virtualized activities, comparison of similar activities across VR and AR, and delineates areas of inquiry for future research. The validity of the framework was explored through two quantitative studies and one qualitative study of a wide variety of professionals. We found that participants perceive a significant difference between physical reality and both VR and AR for all proposed affordances, and that for many affordances, users perceive a difference in the ability of AR and VR to enact them. The qualitative study confirmed the general structure of the framework, while also revealing additional sub-affordances to explore. Theoretically, this suggests that examining the affordances that differentiate these technologies from physical reality may be a valid approach to understanding why users adopt these technologies. Practitioners may find success by focusing development on the specific affordances that VR or AR is best equipped to enact. ©, Copyright © Taylor & Francis Group, LLC.","adoption motivations; augmented reality; technology adoption; technology affordances; theoretical framework; virtual reality; virtually assisted activities","Augmented reality; Virtual reality; Adoption motivations; Technology adoption; Technology affordances; Theoretical framework; virtually assisted activities; Engineering education",Article,"Final","",Scopus,2-s2.0-85070323987
"Juliano J.M., Saldana D., Schmiesing A., Liew S.-L.","57209396794;57194044260;57194041188;36992162200;","Experience with head-mounted virtual reality (HMD-VR) predicts transfer of HMD-VR motor skills",2019,"International Conference on Virtual Rehabilitation, ICVR","2019-July",, 8994345,"","",,,"10.1109/ICVR46560.2019.8994345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080146106&doi=10.1109%2fICVR46560.2019.8994345&partnerID=40&md5=e52cf9a653b976ff6d3a6891f629dc26","University of Southern California (USC), Neuroscience Graduate Program, Los Angeles, CA, United States; University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States","Juliano, J.M., University of Southern California (USC), Neuroscience Graduate Program, Los Angeles, CA, United States; Saldana, D., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States; Schmiesing, A., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States; Liew, S.-L., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States","Immersive, head-mounted virtual reality (HMD-VR) has the potential to be a useful tool for motor rehabilitation. However, when developing tools for rehabilitation, it is essential to design interventions that will be most effective for generalizing to the real world. Therefore, it is important to understand what factors facilitate transfer from HMD-VR to non-HMD-VR environments. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train healthy individuals in an HMD-VR environment. We examined whether learned motor skills transferred to a more conventional (non-HMD-VR) environment and what factors facilitated transfer. Our results suggest that on average, learned motor skills from this task transfer from an immersive virtual environment to a conventional environment; however, some individuals did not transfer the learned motor skills. We then examined individual differences between those that did show transfer and those that did not. We found that individuals who had previous exposure to HMD-VR were more likely to transfer their learned motor skills than those who did not. Individual differences in previous exposure to HMD-VR environments prior to training may serve as a predictor to whether learned motor skills will transfer out of HMD-VR. © 2019 IEEE.","head-mounted virtual reality; skilled motor learning; transfer","Helmet mounted displays; Virtual reality; Head mounted virtual reality; Healthy individuals; Immersive virtual environments; Individual Differences; Motor learning; Motor rehabilitation; Task transfer; transfer; Transfer learning",Conference Paper,"Final","",Scopus,2-s2.0-85080146106
"Maidenbaum S., Patel A., Stein E., Jacobs J.","54585718400;57202138700;57216239090;35558503500;","Spatial Memory Rehabilitation in Virtual Reality - Extending findings from Epilepsy Patients to the General Population",2019,"International Conference on Virtual Rehabilitation, ICVR","2019-July",, 8994573,"","",,3,"10.1109/ICVR46560.2019.8994573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077928726&doi=10.1109%2fICVR46560.2019.8994573&partnerID=40&md5=44af684832ef69c1fb3ec696f673b77f","Columbia University, Biomedical EngineeringNY, United States","Maidenbaum, S., Columbia University, Biomedical EngineeringNY, United States; Patel, A., Columbia University, Biomedical EngineeringNY, United States; Stein, E., Columbia University, Biomedical EngineeringNY, United States; Jacobs, J., Columbia University, Biomedical EngineeringNY, United States","Spatial memory is a critical function. Without it, we cannot understand our environment, situate ourselves within it, or remember where items are located. Most research on the neural basis of spatial memory is conducted either with invasive brain recordings from animals or with non-invasive imaging in humans. An emerging way to link these areas is by studying rare invasive recordings from the human brain, which can be obtained from epilepsy patients who have electrodes surgically implanted for seizure mapping. In recent years this invasive method has expanded our understanding of how the human brain represents space and has also suggested methods for modulating and potentially rehabilitating memory. However, it is unclear whether these results from epilepsy patients generalize to the non-epileptic population, and from testing in hospital rooms to more immersive and comfortable setups. Here, groups of epilepsy patients (n=69) and healthy participants (n=17) performed the same virtual spatial memory task, enabling us to compare their spatial memory performance. Moreover, we compared spatial memory performance between a standard computer screen versus a head-mounted display. We found that the spatial memory performance of epilepsy patients performing our task in a hospital was similar to that of matched healthy participants performing the task in the lab. Furthermore, actual spatial memory performance was similar on the group level irrespective of the interface used, despite the fact that subjects reported higher immersion with the head mounted display. By showing consistent spatial memory performance with a single paradigm across epilepsy patients and healthy participants, as well as with the use of different display modalities, our results provide a baseline for evaluating findings regarding the neural basis of spatial memory and neuromodulation for rehabilitation. More broadly, these results demonstrate that findings from neurosurgical patients are comparable to the wider population. © 2019 IEEE.","Epilepsy; Immersive; Memory Rehabilitation; Spatial memory; Virtual Reality","Brain; Brain mapping; Helmet mounted displays; Hospitals; Neurology; Virtual reality; Critical functions; Epilepsy; General population; Head mounted displays; Immersive; Neurosurgical patients; Non-invasive imaging; Spatial memory; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-85077928726
"Wei S.-E., Saragih J., Simon T., Harley A.W., Lombardi S., Perdoch M., Hypes A., Wang D., Badino H., Sheikh Y.","55485702800;16178291100;36444916900;56115289800;57204687314;24822136000;57189798081;57211427209;8981371600;9437184000;","VR facial animation via multiview image translation",2019,"ACM Transactions on Graphics","38","4", 67,"","",,16,"10.1145/3306346.3323030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073888466&doi=10.1145%2f3306346.3323030&partnerID=40&md5=f6eae33b4d0ae00f03aadbb5bba01c86","Carnegie Mellon University, Facebook Reality Labs, United States; Facebook Reality Labs, Pittsburgh, PA, United States","Wei, S.-E., Facebook Reality Labs, Pittsburgh, PA, United States; Saragih, J., Facebook Reality Labs, Pittsburgh, PA, United States; Simon, T., Facebook Reality Labs, Pittsburgh, PA, United States; Harley, A.W., Carnegie Mellon University, Facebook Reality Labs, United States, Facebook Reality Labs, Pittsburgh, PA, United States; Lombardi, S., Facebook Reality Labs, Pittsburgh, PA, United States; Perdoch, M., Facebook Reality Labs, Pittsburgh, PA, United States; Hypes, A., Facebook Reality Labs, Pittsburgh, PA, United States; Wang, D., Facebook Reality Labs, Pittsburgh, PA, United States; Badino, H., Facebook Reality Labs, Pittsburgh, PA, United States; Sheikh, Y., Facebook Reality Labs, Pittsburgh, PA, United States","A key promise of Virtual Reality (VR) is the possibility of remote social interaction that is more immersive than any prior telecommunication media. However, existing social VR experiences are mediated by inauthentic digital representations of the user (i.e., stylized avatars). These stylized representations have limited the adoption of social VR applications in precisely those cases where immersion is most necessary (e.g., professional interactions and intimate conversations). In this work, we present a bidirectional system that can animate avatar heads of both users’ full likeness using consumer-friendly headset mounted cameras (HMC). There are two main challenges in doing this: unaccommodating camera views and the image-to-avatar domain gap. We address both challenges by leveraging constraints imposed by multiview geometry to establish precise image-to-avatar correspondence, which are then used to learn an end-to-end model for real-time tracking. We present designs for a training HMC, aimed at data-collection and model building, and a tracking HMC for use during interactions in VR. Correspondence between the avatar and the HMC-acquired images are automatically found through self-supervised multiview image translation, which does not require manual annotation or one-to-one correspondence between domains. We evaluate the system on a variety of users and demonstrate significant improvements over prior work. © 2019 Copyright held by the owner/author(s).","And Phrases: Face Tracking; Differentiable Rendering; Unsupervised Image Style Transfer","Virtual reality; Bidirectional system; Differentiable Rendering; Digital representations; Face Tracking; Multi-view geometry; Professional interactions; Social interactions; Unsupervised Image Style Transfer; Cameras",Article,"Final","",Scopus,2-s2.0-85073888466
"Vertemati M., Cassin S., Rizzetto F., Vanzulli A., Elli M., Sampogna G., Gallieni M.","6602085318;57205576019;57205575796;16026541900;7003507564;57193081915;7004243711;","A Virtual Reality Environment to Visualize Three-Dimensional Patient-Specific Models by a Mobile Head-Mounted Display",2019,"Surgical Innovation","26","3",,"359","370",,7,"10.1177/1553350618822860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060629868&doi=10.1177%2f1553350618822860&partnerID=40&md5=e7b6cc98e1f592ddf43393f6e8f6d519","Università degli Studi di Milano, Milan, Italy","Vertemati, M., Università degli Studi di Milano, Milan, Italy; Cassin, S., Università degli Studi di Milano, Milan, Italy; Rizzetto, F., Università degli Studi di Milano, Milan, Italy; Vanzulli, A., Università degli Studi di Milano, Milan, Italy; Elli, M., Università degli Studi di Milano, Milan, Italy; Sampogna, G., Università degli Studi di Milano, Milan, Italy; Gallieni, M., Università degli Studi di Milano, Milan, Italy","Introduction. With the availability of low-cost head-mounted displays (HMDs), virtual reality environments (VREs) are increasingly being used in medicine for teaching and clinical purposes. Our aim was to develop an interactive, user-friendly VRE for tridimensional visualization of patient-specific organs, establishing a workflow to transfer 3-dimensional (3D) models from imaging datasets to our immersive VRE. Materials and Methods. This original VRE model was built using open-source software and a mobile HMD, Samsung Gear VR. For its validation, we enrolled 33 volunteers: morphologists (n = 11), trainee surgeons (n = 15), and expert surgeons (n = 7). They tried our VRE and then filled in an original 5-point Likert-type scale 6-item questionnaire, considering the following parameters: ease of use, anatomy comprehension compared with 2D radiological imaging, explanation of anatomical variations, explanation of surgical procedures, preoperative planning, and experience of gastrointestinal/neurological disorders. Results in the 3 groups were statistically compared using analysis of variance. Results. Using cross-sectional medical imaging, the developed VRE allowed to visualize a 3D patient-specific abdominal scene in 1 hour. Overall, the 6 items were evaluated positively by all groups; only anatomy comprehension was statistically significant different among the 3 groups. Conclusions. Our approach, based on open-source software and mobile hardware, proved to be a valid and well-appreciated system to visualize 3D patient-specific models, paving the way for a potential new tool for teaching and preoperative planning. © The Author(s) 2019.","anatomy; head-mounted display; mobile; training; virtual reality","anatomy; Article; clinical practice; computer assisted tomography; human; image segmentation; medical education; nuclear magnetic resonance imaging; partial nephrectomy; questionnaire; radiography; surgeon; surgical technique; surgical training; three-dimensional imaging; training; treatment planning; virtual reality; computer assisted surgery; computer interface; devices; equipment design; software; three dimensional imaging; x-ray computed tomography; Equipment Design; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Software; Surgery, Computer-Assisted; Surveys and Questionnaires; Tomography, X-Ray Computed; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85060629868
"Makransky G., Terkildsen T.S., Mayer R.E.","50361371800;57205338475;7403065717;","Adding immersive virtual reality to a science lab simulation causes more presence but less learning",2019,"Learning and Instruction","60",,,"225","236",,159,"10.1016/j.learninstruc.2017.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039065813&doi=10.1016%2fj.learninstruc.2017.12.007&partnerID=40&md5=bec8f76c5a5c407cea29ee47829f155c","Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Psychological and Brain Sciences, University of California Santa BarbaraCA, United States","Makransky, G., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Terkildsen, T.S., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Mayer, R.E., Psychological and Brain Sciences, University of California Santa BarbaraCA, United States","Virtual reality (VR) is predicted to create a paradigm shift in education and training, but there is little empirical evidence of its educational value. The main objectives of this study were to determine the consequences of adding immersive VR to virtual learning simulations, and to investigate whether the principles of multimedia learning generalize to immersive VR. Furthermore, electroencephalogram (EEG) was used to obtain a direct measure of cognitive processing during learning. A sample of 52 university students participated in a 2 × 2 experimental cross-panel design wherein students learned from a science simulation via a desktop display (PC) or a head-mounted display (VR); and the simulations contained on-screen text or on-screen text with narration. Across both text versions, students reported being more present in the VR condition (d = 1.30); but they learned less (d = 0.80), and had significantly higher cognitive load based on the EEG measure (d = 0.59). In spite of its motivating properties (as reflected in presence ratings), learning science in VR may overload and distract the learner (as reflected in EEG measures of cognitive load), resulting in less opportunity to build learning outcomes (as reflected in poorer learning outcome test performance). © 2017 Elsevier Ltd","Cognitive load; EEG; Presence; Redundancy principle; Simulation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85039065813
"Jones J.A., Luckett E., Key T., Newsome N.","57196738064;57210912806;57210910865;57063160100;","Latency measurement in head-mounted virtual environments",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8798361,"1000","1001",,4,"10.1109/VR.2019.8798361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071842820&doi=10.1109%2fVR.2019.8798361&partnerID=40&md5=36a303749d045ab3bbdd3f0e38908c11","University of Mississippi, United States; Rust College, United States; Clemson University, United States","Jones, J.A., University of Mississippi, United States; Luckett, E., University of Mississippi, United States; Key, T., Rust College, United States; Newsome, N., Clemson University, United States","In this paper, we discuss a generalizable method to measure end-to-end latency. This is the length of time that elapses between when a real-world movement occurs and when the pixels within a head-mounted display are updated to reflect this movement. The method described here utilizes components commonly available at electronics and hobby shops. We demonstrate this measurement method using an HTC Vive and discuss the influence of its low-persistence display on latency measurement. © 2019 IEEE.","Centered computing; Centered computing; Human; Interaction paradigms; Virtual reality Human; VisualizationVisualization design and evaluation methods","Helmet mounted displays; User interfaces; Centered computing; Design and evaluation methods; End to end latencies; Head mounted displays; Human; Interaction paradigm; Latency measurements; Measurement methods; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85071842820
"Shen S., Chen H.-T., Leong T.W.","56157229100;7501615750;55234419400;","Training transfer of bimanual assembly tasks in cost-differentiated virtual reality systems",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8797917,"1152","1153",,1,"10.1109/VR.2019.8797917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071837898&doi=10.1109%2fVR.2019.8797917&partnerID=40&md5=13e8c93f7df6294c0c8067b89ed781b6","School of Software University of Technology, Sydney, Australia","Shen, S., School of Software University of Technology, Sydney, Australia; Chen, H.-T., School of Software University of Technology, Sydney, Australia; Leong, T.W., School of Software University of Technology, Sydney, Australia","Recent advances of the affordable virtual reality headsets make virtual reality training an economical choice when compared to traditional training. However, these virtual reality devices present a range of different levels of virtual reality fidelity and interactions. Few works have evaluated their validity against the traditional training formats. This paper presents a study that compares the learning efficiency of a bimanual gearbox assembly task among traditional training, virtual reality training with direct 3D inputs (HTC VIVE), and virtual reality training without 3D inputs (Google Cardboard). A pilot study was conducted and the result shows that HTC VIVE brings the best learning outcomes. © 2019 IEEE.","Assistive systems; Head; Learning transfer; Mounted display; Virtual reality","User interfaces; Virtual reality; Assistive system; Head; Learning efficiency; Learning Transfer; Virtual reality devices; Virtual reality system; Virtual reality training; Virtual-reality headsets; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85071837898
"Bai J., Bao M., Zhang T., Jiang Y.","57193438414;8570352300;56137669700;55733649000;","A virtual reality approach identifies flexible inhibition of motion aftereffects induced by head rotation",2019,"Behavior Research Methods","51","1",,"96","107",,4,"10.3758/s13428-018-1116-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052942037&doi=10.3758%2fs13428-018-1116-6&partnerID=40&md5=30efa1f078fda8d87237deab26bd500c","CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, 100101, China; Xinjiang Astronomical Observatory, Chinese Academy of Sciences, Urumqi, 830011, China; University of Chinese Academy of Sciences, Beijing, 100049, China; State Key Laboratory of Brain and Cognitive Science, Beijing, 100101, China; Department of Psychology, University of Chinese Academy of Sciences, Beijing, 100049, China; CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China","Bai, J., CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, 100101, China, Xinjiang Astronomical Observatory, Chinese Academy of Sciences, Urumqi, 830011, China, University of Chinese Academy of Sciences, Beijing, 100049, China; Bao, M., CAS Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, 100101, China, State Key Laboratory of Brain and Cognitive Science, Beijing, 100101, China, Department of Psychology, University of Chinese Academy of Sciences, Beijing, 100049, China; Zhang, T., State Key Laboratory of Brain and Cognitive Science, Beijing, 100101, China, Department of Psychology, University of Chinese Academy of Sciences, Beijing, 100049, China; Jiang, Y., State Key Laboratory of Brain and Cognitive Science, Beijing, 100101, China, Department of Psychology, University of Chinese Academy of Sciences, Beijing, 100049, China, CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China","As we move in space, our retinae receive motion signals from two causes: those resulting from motion in the world and those resulting from self-motion. Mounting evidence has shown that vestibular self-motion signals interact with visual motion processing profoundly. However, most contemporary methods arguably lack portability and generality and are incapable of providing measurements during locomotion. Here we developed a virtual reality approach, combining a three-space sensor with a head-mounted display, to quantitatively manipulate the causality between retinal motion and head rotations in the yaw plane. Using this system, we explored how self-motion affected visual motion perception, particularly the motion aftereffect (MAE). Subjects watched gratings presented on a head-mounted display. The gratings drifted at the same velocity as head rotations, with the drifting direction being identical, opposite, or perpendicular to the direction of head rotations. We found that MAE lasted a significantly shorter time when subjects’ heads rotated than when their heads were kept still. This effect was present regardless of the drifting direction of the gratings, and was also observed during passive head rotations. These findings suggest that the adaptation to retinal motion is suppressed by head rotations. Because the suppression was also found during passive head movements, it should result from visual–vestibular interaction rather than from efference copy signals. Such visual–vestibular interaction is more flexible than has previously been thought, since the suppression could be observed even when the retinal motion direction was perpendicular to head rotations. Our work suggests that a virtual reality approach can be applied to various studies of multisensory integration and interaction. © 2018, Psychonomic Society, Inc.","Adaptation; Head movement; Motion aftereffect; Multisensory; Virtual reality","adolescent; adult; coping behavior; female; head movement; human; illusion; inhibition (psychology); male; movement perception; physiology; rotation; virtual reality; young adult; Adaptation, Psychological; Adolescent; Adult; Female; Head Movements; Humans; Illusions; Inhibition (Psychology); Male; Motion Perception; Rotation; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85052942037
"Geronazzo M., Sikstrom E., Kleimola J., Avanzini F., De Gotzen A., Serafin S.","36720522500;55354784700;24829233900;7005300654;24724148200;6603367536;","The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of Immersive Virtual Reality Scenarios",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613754,"90","97",,8,"10.1109/ISMAR.2018.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062179431&doi=10.1109%2fISMAR.2018.00034&partnerID=40&md5=5a0b7c6c5270986682096e1aad3ed439","Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Virsabi ApS, Denmark; Hefio Ltd, Denmark; Dept. of Computer Science, University of Milano, Italy","Geronazzo, M., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Sikstrom, E., Virsabi ApS, Denmark; Kleimola, J., Hefio Ltd, Denmark; Avanzini, F., Dept. of Computer Science, University of Milano, Italy; De Gotzen, A., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Serafin, S., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark","Achieving a full 3D auditory experience with head-related transfer functions (HRTFs) is still one of the main challenges of spatial audio rendering. HRTFs capture the listener's acoustic effects and personal perception, allowing immersion in virtual reality (VR) applications. This paper aims to investigate the connection between listener sensitivity in vertical localization cues and experienced presence, spatial audio quality, and attention. Two VR experiments with head-mounted display (HMD) and animated visual avatar are proposed: (i) a screening test aiming to evaluate the participants' localization performance with HRTFs for a non-visible spatialized audio source, and (ii) a 2 minute free exploration of a VR scene with five audiovisual sources in a both non-spatialized (2D stereo panning) and spatialized (free-field HRTF rendering) listening conditions. The screening test allows a distinction between good and bad localizers. The second one shows that no biases are introduced in the quality of the experience (QoE) due to different audio rendering methods; more interestingly, good localizers perceive a lower audio latency and they are less involved in the visual aspects. © 2018 IEEE.","Auditory feedback; Human-centered computing; Interaction devices; Interaction paradigms; Interaction techniques; Sound-based input / output Human-centered computing; Virtual reality; Human-centered computing","Augmented reality; Helmet mounted displays; Three dimensional computer graphics; Transfer functions; Virtual reality; Auditory feedback; Human-centered computing; Input/output; Interaction devices; Interaction paradigm; Interaction techniques; Sound reproduction",Conference Paper,"Final","",Scopus,2-s2.0-85062179431
"Werrlich S., Daniel A., Ginger A., Nguyen P.-A., Notni G.","57195069564;57201618829;57207031366;57209915720;7004204934;","Comparing HMD-Based and Paper-Based Training",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613759,"134","142",,22,"10.1109/ISMAR.2018.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062171885&doi=10.1109%2fISMAR.2018.00046&partnerID=40&md5=c206a2dd977504ba7c080ff7a8072a51","BMW Group, Germany; TU Ilmenau, Germany","Werrlich, S., BMW Group, Germany; Daniel, A., BMW Group, Germany; Ginger, A., BMW Group, Germany; Nguyen, P.-A., BMW Group, Germany; Notni, G., TU Ilmenau, Germany","Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ. © 2018 IEEE.","Augmented Reality; Evaluation; Head Mounted Displays; Training","Augmented reality; NASA; Personnel training; Surveys; Assembly trainings; Collaborative systems; Evaluation; Head mounted displays; Laboratory studies; Learning approach; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85062171885
"Kramberger I., Kacic Z., Donaj G.","6602310875;7004382027;55874344700;","Binocular Phase-Coded Visual Stimuli for SSVEP-Based BCI",2019,"IEEE Access","7",, 8688386,"48912","48922",,2,"10.1109/ACCESS.2019.2910737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065069612&doi=10.1109%2fACCESS.2019.2910737&partnerID=40&md5=a0d4afa389cdff1a61934e32668abebb","Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia","Kramberger, I., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia; Kacic, Z., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia; Donaj, G., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia","This paper presents a method of binocular visual stimulation for brain-computer interfaces (BCIs) based on steady-state visual evoked potentials (SSVEPs) using phase-coded symbols. The proposed method's emphasis is on a binocular phase-coded visual stimulus, which is based on the phase differences between the left- and right-eye stimuli, and a symbol detection and recognition procedure based on SSVEP response of the left and right occipital lobes of the user's scalp, where the SSVEP response is obtained as electroencephalography (EEG) signaling. The symbols are coded as phase differences and maintain the same frequency of the sine wave-modulated light provided to the user's left and right eyes as a binocular visual stimulation. Based on this method, a basic system setup is presented to explore the possibilities of binocular phase-coded visual stimuli for virtual or augmented reality applications, where the binocular visual stimulation was achieved by the specially designed head-mounted displays. Multiple visually coded targets are realized as eight different phase-coded binocular symbols and further evaluated as a random sequence of single targets, thus representing the situations in virtual or augmented reality, where multiple visually coded targets are present but not visualized to the user simultaneously within the same field of view. The offline results obtained from ten healthy subjects revealed that an average symbol recognition accuracy of 90.63% and an information transfer rate (ITR) of 70.55 bits/min were achieved for a symbol stimulation time of 2 s. The results of this paper demonstrate the feasibility of using binocular visual stimuli for SSVEP-based BCIs, where reasonable ITR is achieved using single-frequency binocular phase-coded symbols. The proposed method indicates the possibility of combining it with 3D wearable visualization technologies, such as binocular head-mounted displays (HMDs), in order to improve the intuitiveness of the interaction with more immersive user experience using BCI modalities. © 2013 IEEE.","Brain computer interfaces; Brain stimulation; Data acquisition; Electroencephalography; Human computer interaction; Phase measurement; Steady state visually evoked potential; Three-dimensional displays; Virtual reality; Visualization; Wearable sensors","Augmented reality; Binoculars; Data acquisition; Data visualization; Electroencephalography; Electrophysiology; Flow visualization; Helmet mounted displays; Human computer interaction; Interface states; Pattern recognition; Phase measurement; Three dimensional computer graphics; Three dimensional displays; Virtual reality; Visualization; Wearable computers; Wearable sensors; Augmented reality applications; Brain computer interfaces (BCIs); Brain stimulation; Head mounted displays; Information transfer rate; Steady state visual evoked potential (SSVEPs); Steady state visually evoked potentials; Visualization technologies; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85065069612
"Zundel S., Lehnick D., Heyne-Pietschmann M., Trück M., Szavay P.","35238982000;6602292585;57205387250;57205394681;6603029197;","A suggestion on how to compare 2d and 3d laparoscopy: A qualitative analysis of the literature and randomized pilot study",2019,"Journal of Laparoendoscopic and Advanced Surgical Techniques","29","1",,"114","120",,6,"10.1089/lap.2018.0164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059868105&doi=10.1089%2flap.2018.0164&partnerID=40&md5=11dba7c2f9f18951c5ac9187f41be594","Department of Pediatric Surgery, Children's Hospital, Spitalstrasse, Lucerne, 6000, Switzerland; Faculty of Humanities and Social Sciences, Department of Health Sciences and Health Policy, University of Lucerne, Luzern, Switzerland","Zundel, S., Department of Pediatric Surgery, Children's Hospital, Spitalstrasse, Lucerne, 6000, Switzerland; Lehnick, D., Faculty of Humanities and Social Sciences, Department of Health Sciences and Health Policy, University of Lucerne, Luzern, Switzerland; Heyne-Pietschmann, M., Department of Pediatric Surgery, Children's Hospital, Spitalstrasse, Lucerne, 6000, Switzerland; Trück, M., Department of Pediatric Surgery, Children's Hospital, Spitalstrasse, Lucerne, 6000, Switzerland; Szavay, P., Department of Pediatric Surgery, Children's Hospital, Spitalstrasse, Lucerne, 6000, Switzerland","Background and Aims: The results of studies comparing two-dimensional (2D) and three-dimensional (3D) laparoscopy have shown variable results. We aimed to review the literature and develop an appropriate instrument to compare 2D and 3D laparoscopy. We further aimed to use the data extracted to perform a pilot study. Methods: Sixty-seven recent articles on 3D laparoscopy were reviewed and data extracted on factors influencing outcome variables. These variables were used to design a pilot study of 28 novices using a randomized crossover design. The results were analyzed using descriptive statistics and the Wilcoxon signed-rank tests. Results: Seven themes were identified to influence the outcome of 3D studies: applied technique (1), experience of subjects (2), study design (3), learning curve (4), subjective qualitative reports (5), laparoscopic tasks (6), and chosen outcome variables (7). The consecutively developed five laparoscopic simulation tasks contained placing a rubber band over hooks, ring and pearl transfer, threading a pipe cleaner through loops, and placing a suturise. The pilot study showed a primary benefit of 3D laparoscopy that was unrelated to repetition. Two tasks served well to assess first-time performance, and two tasks promise to serve well to assess a learning curve if performed repeatedly. Conclusion: We were able to identify important issues influencing the outcome of studies analyzing 3D laparoscopy. These may help evaluate future studies. The developed tasks resulted in meaningful data in favor of 3D visualization, but further studies are necessary to confirm the pilot test results. © 2019, Mary Ann Liebert, Inc., publishers 2019.","3D; Laparoscopy; Novice; Simulation; Three-dimensional; Training","Article; controlled study; human; laparoscopy; learning curve; pilot study; priority journal; qualitative analysis; randomized controlled trial; simulation; three dimensional imaging; two-dimensional imaging; Wilcoxon signed ranks test; clinical competence; comparative study; crossover procedure; devices; education; laparoscopy; learning curve; medical student; methodology; procedures; randomization; Clinical Competence; Cross-Over Studies; Humans; Imaging, Three-Dimensional; Laparoscopy; Learning Curve; Pilot Projects; Random Allocation; Research Design; Students, Medical",Article,"Final","",Scopus,2-s2.0-85059868105
"Babaians E., Tamiz M., Sarti Y., Mogoei A., Mehrabi E.","57188707391;55746759800;57211075801;57211074600;55569573500;","ROS2Unity3D; High-performance plugin to interface ROS with unity3d engine",2018,"2018 9th Conference on Artificial Intelligence and Robotics and 2nd Asia-Pacific International Symposium, AIAR 2018",,, 8769798,"59","64",,3,"10.1109/AIAR.2018.8769798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072559001&doi=10.1109%2fAIAR.2018.8769798&partnerID=40&md5=f2cda416efc765297cb2be81c4c277ae","Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran","Babaians, E., Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran; Tamiz, M., Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran; Sarti, Y., Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran; Mogoei, A., Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran; Mehrabi, E., Tehran Polytechnic, Arsamrobotics Co. LTD., Amirkabir University of Technology, No.424, Hafez AVE, Tehran, Iran","In this paper, we propose a novel highperformance method to interface ROS (Robot Operating System) from the Unity3d engine. In this regard, we introduce a message passing middleware to perform decentralized and efficient data transfer. We utilize the ZeroMQ; Google Protobuf and GStreamer libraries to achieve these aims. For evaluation, we compare our approach with state of the art Siemens ROS# U nity3D plugin. On the other hand, we simulate various essential robotic sensors such as LIDAR, RGBD, and Monocular cameras in Unity3D to experiment our solution in complex enough robotic scenarios. As Unity3D support a variety of devices and VR (Virtual Reality) capabilities, this solution may help researchers to perform better human-computer interaction using ROS and Unity3d engine. Besides, for developers who are not familiar with URDF and programming side of ROS Gazebo Sim, this will be a more natural way to exporting their 3D meshes to the Unity3D engine and simulate robotic experiments with ROS. © 2018 IEEE.","Google Protobuf; GStreamer; Robotic Simulation; ROS; Unity3d; ZeroMQ","Data transfer; Engines; Human computer interaction; Message passing; Middleware; Robotics; Virtual reality; Google Protobuf; Gstreamer; Robotic simulation; Unity3d; ZeroMQ; Robot programming",Conference Paper,"Final","",Scopus,2-s2.0-85072559001
"Szczurowski K., Smith M.","57202899868;57196078717;","'Woodlands'-A Virtual Reality Serious Game Supporting Learning of Practical Road Safety Skills",2018,"2018 IEEE Games, Entertainment, Media Conference, GEM 2018",,, 8516493,"427","435",,6,"10.1109/GEM.2018.8516493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057030782&doi=10.1109%2fGEM.2018.8516493&partnerID=40&md5=ad57ac1f8e24833e90e1f57db49254f1","Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland","Szczurowski, K., Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland; Smith, M., Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland","In developed societies road safety skills are taught early and often practiced under the supervision of a parent, providing children with a combination of theoretical and practical knowledge. At some point children will attempt to cross a road unsupervised, at that point in time their safety depends on the effectiveness of their road safety education. To date, various attempts to supplement road safety education with technology were made. Most common approach focus on addressing declarative knowledge, by delivering road safety theory in an engaging fashion. Apart from expanding on text based resources to include instructional videos and animations, some stakeholders (e.g.: Irish Road Safety Authority) attempt to take advantage of game-based learning [1]. However, despite the high capacity for interaction being common in Virtual Environments, available game-based solutions to road safety education are currently limited to delivering and assessing declarative knowledge. With recent advancements in the field of Virtual Reality (VR) Head Mounted Displays, procedural knowledge might also be addressed in Virtual Environments. This paper describes the design and development process of a computer-supported learning system that attempts to address psycho-motor skills involved in crossing a road safely, changing learners' attitude towards road safety best practices, and enabling independent practice of transferable skills. By implementing game-based learning principles and following best practice for serious game design (such as making educational components essential to successful game-play, or instructional scaffolding) we hope to make it not only more effective, but also engaging, allowing us to rely on learners' intrinsic motivation [2], to increase their independent practice time and provide them with feedback that will help to condition safe behaviour and increase retention. Presence in Virtual Reality might evoke responses to Virtual Environment as if it was real (RAIR) [3] and enable learners to truly experience learning scenarios. In consequence leading to formation of autobiographical memories constructed from multisensory input, which should result in an increased knowledge retention and transfer [4]. © 2018 IEEE.","Experiential Learning; Game-Based Learning; Road Safety; Serious Game; Virtual Environment; Virtual Reality; VR","Accident prevention; Animation; E-learning; Education computing; Helmet mounted displays; Learning systems; Motor transportation; Roads and streets; Scaffolds; Virtual reality; Autobiographical memory; Computer supported learning systems; Declarative knowledge; Design and development process; Experiential learning; Game-based Learning; Head mounted displays; Road safety; Serious games",Conference Paper,"Final","",Scopus,2-s2.0-85057030782
"Brouwer A.-M., van der Waa J., Stokking H.","7102575587;57193858656;25032071700;","BCI to potentially enhance streaming images to a VR headset by predicting head rotation",2018,"Frontiers in Human Neuroscience","12",, 420,"","",,1,"10.3389/fnhum.2018.00420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056811876&doi=10.3389%2ffnhum.2018.00420&partnerID=40&md5=51b5275784df9bd4315f22469dfaa8f4","Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; Department of Media Networking, Netherlands Organization for Applied Scientific Research (TNO), Den Haag, Netherlands","Brouwer, A.-M., Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; van der Waa, J., Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; Stokking, H., Department of Media Networking, Netherlands Organization for Applied Scientific Research (TNO), Den Haag, Netherlands","While numerous studies show that brain signals contain information about an individual’s current state that are potentially valuable for smoothing man–machine interfaces, this has not yet lead to the use of brain computer interfaces (BCI) in daily life. One of the main challenges is the common requirement of personal data that is correctly labeled concerning the state of interest in order to train a model, where this trained model is not guaranteed to generalize across time and context. Another challenge is the requirement to wear electrodes on the head. We here propose a BCI that can tackle these issues and may be a promising case for BCI research and application in everyday life. The BCI uses EEG signals to predict head rotation in order to improve images presented in a virtual reality (VR) headset. When presenting a 360° video to a headset, field-of-view approaches only stream the content that is in the current field of view and leave out the rest. When the user rotates the head, other content parts need to be made available soon enough to go unnoticed by the user, which is problematic given the available bandwidth. By predicting head rotation, the content parts adjacent to the currently viewed part could be retrieved in time for display when the rotation actually takes place. We here studied whether head rotations can be predicted on the basis of EEG sensor data and if so, whether application of such predictions could be applied to improve display of streaming images. Eleven participants generated left- and rightward head rotations while head movements were recorded using the headsets motion sensing system and EEG. We trained neural network models to distinguish EEG epochs preceding rightward, leftward, and no rotation. Applying these models to streaming EEG data that was withheld from the training showed that 400 ms before rotation onset, the probability “no rotation” started to decrease and the probabilities of an upcoming right- or leftward rotation started to diverge in the correct direction. In the proposed BCI scenario, users already wear a device on their head allowing for integrated EEG sensors. Moreover, it is possible to acquire accurately labeled training data on the fly, and continuously monitor and improve the model’s performance. The BCI can be harnessed if it will improve imagery and therewith enhance immersive experience. © 2018 Brouwer, van der Waa and Stokking.","Applied neuroscience; Brain computer interface; EEG; Head mounted display; Head rotation; Movement prediction; Neuroadaptive technology; Virtual reality","adult; Article; artificial neural network; brain computer interface; electroencephalography; head movement; human; human experiment; image display; image processing; imagery; normal human; prediction; probability; signal transduction; videorecording; virtual reality",Article,"Final","",Scopus,2-s2.0-85056811876
"Peterson S.M., Rios E., Ferris D.P.","57194682956;57203844599;35236545200;","Transient visual perturbations boost short-term balance learning in virtual reality by modulating electrocortical activity",2018,"Journal of Neurophysiology","120","4",,"1998","2010",,12,"10.1152/jn.00292.2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054775411&doi=10.1152%2fjn.00292.2018&partnerID=40&md5=6bb8b6acedd0dbcf8cee246dcfe15e9e","Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville, FL, United States","Peterson, S.M., Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; Rios, E., Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; Ferris, D.P., J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville, FL, United States","Immersive virtual reality can expose humans to novel training and sensory environments, but motor training with virtual reality has not been able to improve motor performance as much as motor training in real-world conditions. An advantage of immersive virtual reality that has not been fully leveraged is that it can introduce transient visual perturbations on top of the visual environment being displayed. The goal of this study was to determine whether transient visual perturbations introduced in immersive virtual reality modify electrocortical activity and behavioral outcomes in human subjects practicing a novel balancing task during walking. We studied three groups of healthy young adults (5 male and 5 female for each) while they learned a balance beam walking task for 30 min under different conditions. Two groups trained while wearing a virtual reality headset, and one of those groups also had half-second visual rotation perturbations lasting ~10% of the training time. The third group trained without virtual reality. We recorded high-density electroencephalography (EEG) and movement kinematics. We hypothesized that virtual reality training with perturbations would increase electrocortical activity and improve balance performance compared with virtual reality training without perturbations. Our results confirmed the hypothesis. Brief visual perturbations induced increased theta spectral power and decreased alpha spectral power in parietal and occipital regions and improved balance performance in posttesting. Our findings indicate that transient visual perturbations during immersive virtual reality training can boost short-term motor learning by inducing a cognitive change, minimizing the negative effects of virtual reality on motor training. NEW & NOTEWORTHY We found that transient visual perturbations in virtual reality during balance training can boost short-term motor learning by inducing a cognitive change, overcoming the negative effects of immersive virtual reality. As a result, subjects training in immersive virtual reality with visual perturbations have equivalent performance improvement as training in real-world conditions. Visual perturbations elicited cortical responses in occipital and parietal regions and may have improved the brain’s ability to adapt to variations in sensory input. © 2018 American Physiological Society. All rights reserved.","Adaptive generalization; Balance training; Electroencephalography; Motor learning; Virtual reality","adult; article; case report; clinical article; electroencephalography; female; human; human experiment; kinematics; male; motor learning; occipital cortex; rotation; sensory stimulation; virtual reality; walking; young adult",Article,"Final","",Scopus,2-s2.0-85054775411
"Grubert J., Witzani L., Ofek E., Pahud M., Kranz M., Kristensson P.O.","35114754100;56024891600;10139546600;6602493330;12239425600;6507412583;","Text Entry in Immersive Head-Mounted Display-Based Virtual Reality Using Standard Keyboards",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,, 8446059,"159","166",,41,"10.1109/VR.2018.8446059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053830754&doi=10.1109%2fVR.2018.8446059&partnerID=40&md5=751e12a3f39023107cd2837e1c6effd0","Coburg University of Applied Sciences and Arts, Germany; University of Passau, Germany; Microsoft Research, Germany; University of Cambridge, United Kingdom","Grubert, J., Coburg University of Applied Sciences and Arts, Germany; Witzani, L., University of Passau, Germany; Ofek, E., Microsoft Research, Germany; Pahud, M., Coburg University of Applied Sciences and Arts, Germany; Kranz, M., Coburg University of Applied Sciences and Arts, Germany; Kristensson, P.O., University of Cambridge, United Kingdom","We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing. © 2018 IEEE.","H.5.2: [User Interfaces-Input devices and strategies.]","Economic and social effects; Helmet mounted displays; Typewriter keyboards; Virtual reality; Visual communication; Base-line performance; Head mounted displays; Input devices and strategies; Learning effects; Performance trade-off; Touch-screen keyboards; User experience; Visual feedback; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85053830754
"McFadyen B.J., Fiset F., Charette C.","7004877191;57201912628;56480445300;","Substituting anticipatory locomotor adjustments online is time constrained",2018,"Experimental Brain Research","236","7",,"1985","1996",,2,"10.1007/s00221-018-5277-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046492734&doi=10.1007%2fs00221-018-5277-4&partnerID=40&md5=74e19b1833539eff69206aeccf389cfb","Centre for Interdisciplinary Research in Rehabilitation and Social Integration, CIUSSS-CN, IRDPQ, 525 HamelQC  G1M 2S8, Canada; Department of Rehabilitation, Faculty of Medicine, Université LavalQC, Canada","McFadyen, B.J., Centre for Interdisciplinary Research in Rehabilitation and Social Integration, CIUSSS-CN, IRDPQ, 525 HamelQC  G1M 2S8, Canada, Department of Rehabilitation, Faculty of Medicine, Université LavalQC, Canada; Fiset, F., Centre for Interdisciplinary Research in Rehabilitation and Social Integration, CIUSSS-CN, IRDPQ, 525 HamelQC  G1M 2S8, Canada, Department of Rehabilitation, Faculty of Medicine, Université LavalQC, Canada; Charette, C., Centre for Interdisciplinary Research in Rehabilitation and Social Integration, CIUSSS-CN, IRDPQ, 525 HamelQC  G1M 2S8, Canada, Department of Rehabilitation, Faculty of Medicine, Université LavalQC, Canada","Two crucial, multi-articular strategies for anticipatory locomotor adjustments (ALA) are knee flexor generation to step over obstacles and hip flexor generation to step up. While lower limb control can be adapted online to modify an already planned obstacle avoidance, or to avoid the sudden appearance of an obstacle, it is not known whether a planned ALA can be substituted by different one online. The present objective was to study such ALA substitutions at two specific timepoints: the final planning stage and the initiation of ALA execution. Ten healthy, young adults (22.0 ± 1.7 years; 5 males) walked in a Virtual Environment (VE) representing the laboratory within a head mounted display. Two blocks of trials, one involving an initial VE with an obstacle (OB) and the other an initial VE with a platform (PL) (heights of 15% of lower limb length for both), were presented, where the initial VE could remain unchanged or be randomly switched between them at one of the two timepoints. The final VE always corresponded to the real environment. Lead limb kinematics, joint kinetics and energetics, as well as electromyography were measured. Repeated measures ANOVAs were used to compare across conditions. Foot clearance, knee flexor generation, and hip flexor generation all changed in the expected directions for the final VEs when requiring early substitution, but not when switched late. These findings show that volitional, locomotor strategies may be substituted at the end of the ALA planning phase, but not once execution is initiated. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.","Anticipatory control; Gait; Locomotion; Obstacle avoidance","adjustment; adult; analysis of variance; anticipatory locomotor adjustment; Article; controlled study; electromyography; energy transfer; female; foot; hip; human; human experiment; kinematics; kinetics; knee function; locomotion; male; normal human; online system; priority journal; repeat procedure; virtual reality; biomechanics; escape behavior; locomotion; lower limb; physiology; skeletal muscle; social adaptation; young adult; Adult; Biomechanical Phenomena; Electromyography; Escape Reaction; Female; Humans; Kinetics; Locomotion; Lower Extremity; Male; Muscle, Skeletal; Online Systems; Social Adjustment; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85046492734
"Wei C.-S., Lin Y.-P., Wang Y.-T., Lin C.-T., Jung T.-P.","36814917100;16318996800;36018650200;8942403600;7201389395;","A subject-transfer framework for obviating inter- and intra-subject variability in EEG-based drowsiness detection",2018,"NeuroImage","174",,,"407","419",,29,"10.1016/j.neuroimage.2018.03.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044567738&doi=10.1016%2fj.neuroimage.2018.03.032&partnerID=40&md5=9d24ead687026fa423d2b2a0a01301e1","Department of Bioengineering, University of California San Diego, La Jolla, CA, United States; Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California San Diego, La Jolla, CA, United States; Center for Advanced Neurological Engineering, Institute of Engineering in Medicine, University of California San Diego, La Jolla, CA, United States; Institute of Medical Science and Technology, National Sun Yat-Sen University, Kaohsiung, Taiwan; Centre for Artificial Intelligence, FEIT, University of Technology, Sydney, Australia","Wei, C.-S., Department of Bioengineering, University of California San Diego, La Jolla, CA, United States, Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California San Diego, La Jolla, CA, United States, Center for Advanced Neurological Engineering, Institute of Engineering in Medicine, University of California San Diego, La Jolla, CA, United States; Lin, Y.-P., Institute of Medical Science and Technology, National Sun Yat-Sen University, Kaohsiung, Taiwan; Wang, Y.-T., Department of Bioengineering, University of California San Diego, La Jolla, CA, United States, Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California San Diego, La Jolla, CA, United States; Lin, C.-T., Centre for Artificial Intelligence, FEIT, University of Technology, Sydney, Australia; Jung, T.-P., Department of Bioengineering, University of California San Diego, La Jolla, CA, United States, Swartz Center for Computational Neuroscience, Institute for Neural Computation, University of California San Diego, La Jolla, CA, United States, Center for Advanced Neurological Engineering, Institute of Engineering in Medicine, University of California San Diego, La Jolla, CA, United States","Inter- and intra-subject variability pose a major challenge to decoding human brain activity in brain-computer interfaces (BCIs) based on non-invasive electroencephalogram (EEG). Conventionally, a time-consuming and laborious training procedure is performed on each new user to collect sufficient individualized data, hindering the applications of BCIs on monitoring brain states (e.g. drowsiness) in real-world settings. This study proposes applying hierarchical clustering to assess the inter- and intra-subject variability within a large-scale dataset of EEG collected in a simulated driving task, and validates the feasibility of transferring EEG-based drowsiness-detection models across subjects. A subject-transfer framework is thus developed for detecting drowsiness based on a large-scale model pool from other subjects and a small amount of alert baseline calibration data from a new user. The model pool ensures the availability of positive model transferring, whereas the alert baseline data serve as a selector of decoding models in the pool. Compared with the conventional within-subject approach, the proposed framework remarkably reduced the required calibration time for a new user by 90% (18.00 min–1.72 ± 0.36 min) without compromising performance (p = 0.0910) when sufficient existing data are available. These findings suggest a practical pathway toward plug-and-play drowsiness detection and can ignite numerous real-world BCI applications. © 2018","Brain-computer interface (BCI); Drowsiness; EEG baseline; Electroencephalogram (EEG); Hierarchical cluster analysis (HCA); Subject-transfer decoding","Article; attention test; brain computer interface; car driving; cluster analysis; drowsiness; electroencephalography; hierarchical cluster analysis; human; human experiment; normal human; priority journal; simulation; virtual reality; brain; calibration; electroencephalogram; electroencephalography; physiology; procedures; psychomotor performance; reproducibility; signal processing; wakefulness; Brain; Brain Waves; Brain-Computer Interfaces; Calibration; Cluster Analysis; Electroencephalography; Humans; Psychomotor Performance; Reproducibility of Results; Signal Processing, Computer-Assisted; Wakefulness",Article,"Final","",Scopus,2-s2.0-85044567738
"Werrlich S., Nguyen P.-A., Notni G.","57195069564;57209915720;7004204934;","Evaluating the training transfer of Head-Mounted Display based training for assembly tasks",2018,"ACM International Conference Proceeding Series",,,,"297","302",,10,"10.1145/3197768.3201564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049917107&doi=10.1145%2f3197768.3201564&partnerID=40&md5=cf6d53b805f90786790e6809e4a4b063","BMW AG, Taunusstraße 41, München, 80807, Germany; Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","Werrlich, S., BMW AG, Taunusstraße 41, München, 80807, Germany; Nguyen, P.-A., BMW AG, Taunusstraße 41, München, 80807, Germany; Notni, G., Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","The automotive industry is growing constantly and more and more assembly workers are needed to negotiate the production volume. The training of new employees is essential to ensure premium quality products and processes. New technologies for training such as head-mounted displays (HMDs) receive a growing amount of attention by the scientific community, especially in the industrial domain. Due to its possibility to work hands-free while providing users with necessary augmented information, HMDs can enhance the quality and efficiency of assembly training tasks. However, comprehensive evaluations in industrial environments regarding the training transfer using augmented reality (AR) technologies are still very limited. In this paper, we aim to close this gap by conducting a user study with two groups and 30 participants, measuring the training transfer. We compare the effects of two slightly different HMD-based training applications. The first group complete a tutorial, beginner, intermediate and expert training level, while the second group received an additional quiz level. Results show that group two needed 17% more time to complete the training but made 79% less sequence mistakes compared to the first group. Additionally, we compare the user satisfaction by using the system usability scale (SUS) and the perceived workload by measuring the NASA-TLX. © 2018 Association for Computing Machinery.","Assembly; Augmented reality; Evaluation; Head-mounted display; Training","Assembly; Augmented reality; Automotive industry; NASA; Personnel training; Sensory perception; Street traffic control; Comprehensive evaluation; Evaluation; Head mounted displays; Industrial environments; Premium-quality products; Scientific community; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85049917107
"Huber T., Paschold M., Hansen C., Lang H., Kneist W.","18535462800;50361876100;55890379200;7402486188;7005632003;","Artificial Versus Video-Based Immersive Virtual Surroundings: Analysis of Performance and User’s Preference",2018,"Surgical Innovation","25","3",,"280","285",,5,"10.1177/1553350618761756","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047495181&doi=10.1177%2f1553350618761756&partnerID=40&md5=0ad928be9ec543737a9d0024a2671f68","University Medicine of the Johannes Gutenberg-University, Mainz, Germany; Otto-von-Guericke University, Magdeburg, Germany","Huber, T., University Medicine of the Johannes Gutenberg-University, Mainz, Germany; Paschold, M., University Medicine of the Johannes Gutenberg-University, Mainz, Germany; Hansen, C., Otto-von-Guericke University, Magdeburg, Germany; Lang, H., University Medicine of the Johannes Gutenberg-University, Mainz, Germany; Kneist, W., University Medicine of the Johannes Gutenberg-University, Mainz, Germany","Introduction. Immersive virtual reality (VR) laparoscopy simulation connects VR simulation with head-mounted displays to increase presence during VR training. The goal of the present study was the comparison of 2 different surroundings according to performance and users’ preference. Methods. With a custom immersive virtual reality laparoscopy simulator, an artificially created VR operating room (AVR) and a highly immersive VR operating room (IVR) were compared. Participants (n = 30) performed 3 tasks (peg transfer, fine dissection, and cholecystectomy) in AVR and IVR in a crossover study design. Results. No overall difference in virtual laparoscopic performance was obtained when comparing results from AVR with IVR. Most participants preferred the IVR surrounding (n = 24). Experienced participants (n = 10) performed significantly better than novices (n = 10) in all tasks regardless of the surrounding (P <.05). Participants with limited experience (n = 10) showed differing results. Presence, immersion, and exhilaration were significantly higher in IVR. Two thirds assumed that IVR would have a positive influence on their laparoscopic simulator use. Conclusion. This first study comparing AVR and IVR did not reveal differences in virtual laparoscopic performance. IVR is considered the more realistic surrounding and is therefore preferred by the participants. © 2018, © The Author(s) 2018.","abdominal surgery; immersive virtual reality; laparoscopy; simulation; training; virtual surgery","adult; Article; cholecystectomy; clinical assessment; crossover procedure; cyber sickness; diseases; female; fine dissection; human; immersive virtual reality; laparoscopy; male; medical procedures; medical student; middle aged; peg transfer; performance; questionnaire; task performance; virtual reality; young adult; clinical competence; education; medical education; procedures; surgeon; Adult; Clinical Competence; Education, Medical; Female; Humans; Laparoscopy; Male; Middle Aged; Surgeons; Surveys and Questionnaires; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85047495181
"Jung J., Ahn Y.J.","57220995238;57201853639;","Effects of interface on procedural skill transfer in virtual training: Lifeboat launching operation study",2018,"Computer Animation and Virtual Worlds","29","3-4", e1812,"","",,7,"10.1002/cav.1812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046267906&doi=10.1002%2fcav.1812&partnerID=40&md5=0f4881ff1662eaf041a1adb3184611b7","Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","Jung, J., Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Ahn, Y.J., Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","A comparative study assessing the effect of interface type on procedural skill transfer during virtual training is presented. The aim of this research is to evaluate the transferability of two aspects of procedural skills, that is, procedural knowledge and technical skills. We established one group with a lecture and three virtual training groups with a combination of output and input devices: a monitor and keyboard/mouse, a head-mounted display (HMD) and joypad, and an HMD and wearable sensors. The task for assessment was a lifeboat launching operation that requires a participant to memorize a 10-step procedure utilizing 14 different pieces of equipment that should be manipulated in each step. Before and after training, we evaluated the participants' procedural knowledge and technical skill on a real lifeboat. The monitor and keyboard/mouse group showed the best performance in a procedural knowledge assessment that addressed visually induced recollections from the real lifeboat. Alternatively, in the assessment of technical skills that determined manipulation ability that requires word-based mnemonics, the HMD and wearable sensors group outperformed the other groups. Moreover, the results showed that the virtual training was a more efficient training format for short-term training than a lecture due to the freedom of observation viewpoint, despite simulator sickness. Copyright © 2018 John Wiley & Sons, Ltd.","interface; maritime safety; procedural skill transfer; virtual reality; virtual training","Helmet mounted displays; Interfaces (materials); Lifeboats; Typewriter keyboards; Virtual reality; Wearable sensors; Comparative studies; Head mounted displays; Lifeboat launching; Maritime safety; Procedural knowledge; Simulator sickness; Skill transfer; Virtual training; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85046267906
"Rousset T., Bourdin C., Goulon C., Monnoyer J., Vercher J.-L.","57063135900;6603566648;26535970800;57063170900;7004221343;","Misperception of egocentric distances in virtual environments: More a question of training than a technological issue?",2018,"Displays","52",,,"8","20",,2,"10.1016/j.displa.2018.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042939701&doi=10.1016%2fj.displa.2018.02.004&partnerID=40&md5=bb33373e8b990b9c57e6879e0a9e61d9","Aix-Marseille Univ, CNRS, ISM, Marseille, France; Groupe PSA, Velizy Villacoublay, France","Rousset, T., Aix-Marseille Univ, CNRS, ISM, Marseille, France; Bourdin, C., Aix-Marseille Univ, CNRS, ISM, Marseille, France; Goulon, C., Aix-Marseille Univ, CNRS, ISM, Marseille, France; Monnoyer, J., Aix-Marseille Univ, CNRS, ISM, Marseille, France, Groupe PSA, Velizy Villacoublay, France; Vercher, J.-L., Aix-Marseille Univ, CNRS, ISM, Marseille, France","Findings from virtual reality applications in general and driving or flight simulators in particular, are frequently generalized to the study of human behavior. Thus, it is crucial to ensure that the virtuality of the experimental setup has little or no effect on perception of space and motion. Most studies show that observers immersed in virtual environments (VE) perceive virtual space as compressed relative to the real world, resulting in systematic underestimations of egocentric distance. Parallax and stereopsis, known to be important depth cues for distance perception, at least for short distances, are rarely used together in driving simulators, so their interactive role during driving tasks is still not clear. Inter-individual differences in misperception are also referred to, though few studies have explored this. The aim of this study was, first, to determine whether egocentric distance perception in driving simulation depends on two depth cues, binocular disparity and motion parallax, and, second, to examine the effect of inter-individual differences. Several conditions were tested, both with and without stereoscopic vision of the scene and/or motion parallax of the head. We focused first on a range of long distances, 40–80 m (Experiment 1) and subsequently widened the range to distances from 5 to 80 m, thereby including short distances where stereopsis should be more relevant (Experiment 2). The study reveals great inter-individual variability, clearly distinguishing two participant profiles. However, results suggest that such differences do not depend on the availability of motion parallax and stereoscopic vision. The findings also show that an initial familiarization phase, under conditions similar to those of the experiments, can be predictive of participants’ perceptual behavior. © 2018 The Authors","Distance perception; Inter-individual variability; Parallax; Stereoscopy","Behavioral research; Depth perception; E-learning; Flight simulators; Geometrical optics; Stereo image processing; Binocular disparity; Distance perception; Driving simulation; Driving simulator; Individual variability; Inter-individual differences; Parallax; Stereoscopic vision; Virtual reality",Article,"Final","",Scopus,2-s2.0-85042939701
"Ma X., Yao Z., Wang Y., Pei W., Chen H.","57201641690;56911568900;7601519371;8365780300;10041330500;","Combining brain-computer interface and eye tracking for high-speed text entry in virtual reality",2018,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"263","267",,5,"10.1145/3172944.3172988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045452448&doi=10.1145%2f3172944.3172988&partnerID=40&md5=fff13c6fae5b3a42372fadd56cf28d65","Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; University of Chinese, Academy of Sciences, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China","Ma, X., Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China, University of Chinese, Academy of Sciences, Beijing, China, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China; Yao, Z., Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China, University of Chinese, Academy of Sciences, Beijing, China, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China; Wang, Y., Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China, University of Chinese, Academy of Sciences, Beijing, China, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China; Pei, W., Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China, University of Chinese, Academy of Sciences, Beijing, China, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China; Chen, H., Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China, University of Chinese, Academy of Sciences, Beijing, China, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Shanghai, China","Gaze interaction provides an efficient way for users to communicate and control in virtual reality (VR) presented by head-mounted displays. In gaze-based text-entry systems, eye tracking and brain-computer interface (BCI) are the two most commonly used approaches. This paper presents a hybrid BCI system for text entry in VR by combining steady-state visual evoked potentials (SSVEP) and eye tracking. The user interface in VR designed a 40-target virtual keyboard using a joint frequency-phase modulation method for SSVEP. Eye position was measured by an eyetracking accessory in the VR headset. Target-related gaze direction was detected by combining simultaneously recorded SSVEP and eye position data. Offline and online experiments indicate that the proposed system can type at a speed around 10 words per minute, leading to an information transfer rate (ITR) of 270 bits per minute. The results further demonstrate the superiority of the hybrid method over single-modality methods for VR applications. © 2018 Association for Computing Machinery.","Brain-computer interface; Eye tracking; Steady-state visual evoked potentials; Text entry; Virtual reality","Electrophysiology; Eye tracking; Helmet mounted displays; Interface states; User interfaces; Virtual reality; Head mounted displays; Information transfer rate; Modulation methods; On-line experiments; Steady state visual evoked potentials; Text entry; Text entry systems; Virtual Keyboards; Brain computer interface",Conference Paper,"Final","",Scopus,2-s2.0-85045452448
"Brito P.Q., Stoyanova J., Coelho A.","16678683600;55992151400;23089899600;","Augmented reality versus conventional interface: Is there any difference in effectiveness?",2018,"Multimedia Tools and Applications","77","6",,"7487","7516",,3,"10.1007/s11042-017-4658-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018835569&doi=10.1007%2fs11042-017-4658-1&partnerID=40&md5=41cff006e4398a3d761d6a60896d3c3a","Faculdade de Economia, LIAAD/INESC-Tec, Universidade do Porto, Rua Dr. Roberto Frias, Porto, 4200-464, Portugal; Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal","Brito, P.Q., Faculdade de Economia, LIAAD/INESC-Tec, Universidade do Porto, Rua Dr. Roberto Frias, Porto, 4200-464, Portugal; Stoyanova, J., Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal; Coelho, A., Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal","The moment immediately before the “add to cart” decision is very critical in online shopping. Drawing on theories of transfer, spreading activation and human-computer interaction, the superiority of markerless Augmented Reality (AR) and Marker-based augmented reality (M) over Conventional Interactive (CI) is hypothesized. Although those multimedia tools are not part of the product/brand motivating the consumer interest they interfere in the interactive performance of the ecommerce. 150 consumers in a lab experiment showed higher emotional response, interactive response and brand evaluation in M and AR than CI. Contrary to what was expected the usability results were the inverse. That is, usability of CI outperforms M and AR. Considering only AR and M interfaces their effect on psychological variables was not statistically significant. A sophisticated or a simple interface had no impact on intention to buy the target brand, but the brand recommendation improved from M to AR. The differing effect of those three interface systems was mediated by brand familiarity, perceived risk, opinion leadership and positive emotional traits. © 2017, Springer Science+Business Media New York.","Augmented reality; Brand evaluation; Effectiveness; Emotion; Experimental design; Online shopping","Augmented reality; Behavioral research; Design of experiments; Electronic commerce; Brand evaluation; Effectiveness; Emotion; Emotional response; Interactive performance; Online shopping; Psychological variables; Spreading activations; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85018835569
"Ip H.H.S., Wong S.W.L., Chan D.F.Y., Byrne J., Li C., Yuan V.S.N., Lau K.S.Y., Wong J.Y.W.","7005395690;35095800900;7402216810;55327177600;56180363500;57190301256;56014513500;57190292326;","Enhance emotional and social adaptation skills for children with autism spectrum disorder: A virtual reality enabled approach",2018,"Computers and Education","117",,,"1","15",,52,"10.1016/j.compedu.2017.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034031316&doi=10.1016%2fj.compedu.2017.09.010&partnerID=40&md5=5461287673669e8bfbf4e12a366653f0","Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Education Studies, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Paediatrics, Faculty of Medicine, The Chinese University of Hong Kong, Shatin, Hong Kong","Ip, H.H.S., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Wong, S.W.L., Department of Education Studies, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Chan, D.F.Y., Department of Paediatrics, Faculty of Medicine, The Chinese University of Hong Kong, Shatin, Hong Kong; Byrne, J., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Li, C., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Yuan, V.S.N., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Lau, K.S.Y., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Wong, J.Y.W., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong","Deficits in social-emotional reciprocity, one of the diagnostic criteria of Autism Spectrum Disorder (ASD), greatly hinders children with ASD from responding appropriately and adapting themselves in various social situations. Although evidences have shown that virtual reality environment is a promising tool for emotional and social adaptation skills training on ASD population, there is a lack of large-scale trials with intensive evaluations to support such findings. This paper presents a virtual reality enabled program for enhancing emotional and social adaptation skills for children with ASD. Six unique learning scenarios, of which one focuses on emotion control and relaxation strategies, four that simulate various social situations, and one that facilitates consolidation and generalization, are designed and developed with corresponding psychoeducation procedures and protocols. The learning scenarios are presented to the children via a 4-side immersive virtual reality environment (a.k.a., half-CAVE) with non-intrusive motion tracking. A total number of 94 children between the ages of 6–12 with clinical diagnosis of ASD participated in the 28-session program that lasted for 14 weeks. By comparing pre- and post-assessments, results reported in this paper show significant improvements in the project's primary measures on children's emotion expression and regulation and social-emotional reciprocity but not on other secondary measures. © 2017 Elsevier Ltd","Autism spectrum disorders; Emotional skills; Situated learning; Social adaptation; Virtual reality","Diagnosis; Diseases; Motion tracking; Program diagnostics; Social aspects; Autism spectrum disorders; Children with autisms; Emotional skills; Immersive virtual reality; Relaxation strategies; Situated learning; Social adaptation; Virtual-reality environment; Virtual reality",Article,"Final","",Scopus,2-s2.0-85034031316
"Dehn L.B., Kater L., Piefke M., Botsch M., Driessen M., Beblo T.","55893825700;57192807227;6507173176;6602523499;56056374100;55978325200;","Training in a comprehensive everyday-like virtual reality environment compared to computerized cognitive training for patients with depression",2018,"Computers in Human Behavior","79",,,"40","52",,15,"10.1016/j.chb.2017.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007894&doi=10.1016%2fj.chb.2017.10.019&partnerID=40&md5=2353908903251aac94bfb95d545d8767","Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany; Clinic of Internal and Geriatric Medicine, Schildische Str. 99, Bielefeld, D-33611, Germany; Department of Psychology and Psychotherapy, Witten/Herdecke University, Alfred-Herrhausen-Str. 50, Witten, D-58448, Germany; Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany","Dehn, L.B., Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany; Kater, L., Clinic of Internal and Geriatric Medicine, Schildische Str. 99, Bielefeld, D-33611, Germany; Piefke, M., Department of Psychology and Psychotherapy, Witten/Herdecke University, Alfred-Herrhausen-Str. 50, Witten, D-58448, Germany; Botsch, M., Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany; Driessen, M., Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany; Beblo, T., Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany","Neurocognitive impairments in patients with depression compromise everyday functioning. Thus, should neuropsychological therapy be designed as real-life-like as possible to maximize transfer effects? We investigated whether ecological validity of computerized cognitive training could be increased by a comprehensive everyday-life-simulating training device combining virtual reality, 360°-all-around visibility and autonomous navigation motions. In an eight days training program, patients exercised the learning and purchasing of shopping list products in a virtual supermarket using either the novel training device (n = 21) or a corresponding desktop application (n = 17). In a pre-post-design, effects of the two training conditions were compared regarding several outcome measures. Altogether, results did not prove a benefit of the more naturalistic training setting regarding different training performances (recognition, performance speed, spatial orientation), self-perceived daily cognitive impairments, a real-life shopping task as well as various neuropsychological capabilities. Findings are discussed in the context of general challenges in striving after ecological validity in neuropsychology. © 2017 Elsevier Ltd","Cognitive impairment; Cognitive remediation; Computerized training; Depression; Ecological validity; Virtual reality","Application programs; Ecology; Virtual reality; Autonomous navigation; Cognitive impairment; Depression; Desktop applications; Ecological validity; Spatial orientations; Training conditions; Virtual-reality environment; E-learning",Article,"Final","",Scopus,2-s2.0-85032007894
"Harada H., Kanaji S., Nishi M., Otake Y., Hasegawa H., Yamamoto M., Matsuda Y., Yamashita K., Matsuda T., Oshikiri T., Sumi Y., Nakamura T., Suzuki S., Sato Y., Kakeji Y.","57201492908;6602447690;56410427800;57204280185;55793187513;57207791395;35603877700;37008161100;55825472500;6603679030;36928186800;56183829400;55732211900;56579458400;55403089300;","The learning effect of using stereoscopic vision in the early phase of laparoscopic surgical training for novices",2018,"Surgical Endoscopy","32","2",,"582","588",,10,"10.1007/s00464-017-5654-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021093563&doi=10.1007%2fs00464-017-5654-2&partnerID=40&md5=73cb7807f5ec4f2a91383fe50a7e6b08","Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan","Harada, H., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Kanaji, S., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Nishi, M., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Otake, Y., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Hasegawa, H., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Yamamoto, M., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Matsuda, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Yamashita, K., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Matsuda, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Oshikiri, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Sumi, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Nakamura, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Suzuki, S., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Sato, Y., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Kakeji, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan","Background: Recently to improve depth perception, the performance of three-dimensional (3D) laparoscopic surgeries has increased. However, the effects of laparoscopic training using 3D are still unclear. This study aimed to clarify the effects of using a 3D monitor among novices in the early phase of training. Methods: Participants were 40 novices who had never performed laparoscopic surgery (20 medical students and 20 junior residents). Three laparoscopic phantom tasks (task 1: touching markers on a flat disk with a rod; task 2: straight rod transfer through a single loop; and task 3: curved rod transfer through two loops) in the training box were performed ten times, respectively. Performances were recorded by an optical position tracker. The participants were randomly divided into two groups: one group performed each task five times initially under a 2D system (2D start group), and the other group performed each task five times under a 3D system (3D start group). Both groups then performed the same task five times. After the trial, we evaluated the performance scores (operative time, path length of forceps, and technical errors) and the learning curves for both groups. Results: Scores for all tasks performed under the 3D system were significantly better than scores for tasks using the 2D system. Scores for each task in the 2D start group improved after switching to the 3D system. However, scores for each task in the 3D start group were worse after switching to the 2D system, especially scores related to technical errors. Conclusions: The stereoscopic vision improved laparoscopic surgical techniques of novices from the early phase of training. However, the performance of novices trained only by 3D worsened by changing to the 2D environment. © 2017, The Author(s).","2D laparoscopy; 3D laparoscopy; Learning effect; Novice; Task performance; Training","Article; depth perception; eye tracking; human; laparoscopic surgery; learning curve; medical student; operation duration; outcome assessment; priority journal; resident; stereoscopic vision; surgical training; three dimensional imaging; clinical competence; education; laparoscopy; procedures; simulation training; task performance; Clinical Competence; Depth Perception; Humans; Imaging, Three-Dimensional; Laparoscopy; Learning Curve; Simulation Training; Students, Medical; Task Performance and Analysis",Article,"Final","",Scopus,2-s2.0-85021093563
"Ankomah P., Vangorp P.","57217224949;18435394900;","Virtual reality: A literature review and metrics-based classification",2018,"Computer Graphics and Visual Computing, CGVC 2018",,,,"173","181",,,"10.2312/cgvc.20181222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086820445&doi=10.2312%2fcgvc.20181222&partnerID=40&md5=67913b689e62c8521ffe6266f4e15c46","Edge Hill University, United Kingdom","Ankomah, P., Edge Hill University, United Kingdom; Vangorp, P., Edge Hill University, United Kingdom","This paper presents a multi-disciplinary overview of research evaluating virtual reality (VR). The main aim is to review and classify VR research based on several metrics: presence and immersion, navigation and interaction, knowledge improvement, performance and usability. With the continuous development and consumerisation of VR, several application domains have studied the impact of VR as an enhanced alternative environment for performing tasks. However, VR experiment results often cannot be generalised but require specific datasets and tasks suited to each domain. This review and classification of VR metrics presents an alternative metrics-based view of VR experiments and research. © 2018 The Author(s) Eurographics Proceedings © 2018 The Eurographics Association.",,"Consumerisation; Continuous development; Literature reviews; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85086820445
"Siegel Z.D., Kelly J.W., Cherep L.A.","56094810300;55346243800;57193440989;","Rescaling of perceived space transfers across virtual environments",2017,"Journal of Experimental Psychology: Human Perception and Performance","43","10",,"1805","1814",,8,"10.1037/xhp0000401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027048373&doi=10.1037%2fxhp0000401&partnerID=40&md5=f4e76dd94dd38e9b80bcfb4f93c94273","Department of Psychology, Iowa State University, United States","Siegel, Z.D., Department of Psychology, Iowa State University, United States; Kelly, J.W., Department of Psychology, Iowa State University, United States; Cherep, L.A., Department of Psychology, Iowa State University, United States","Research over the past 20 years has consistently shown that egocentric distance is underperceived in virtual environments (VEs) compared with real environments. In 2 experiments, judgments of object distance (Experiment 1) and object size (Experiment 2) improved after a brief period of walking through the VE with continuous visual feedback. Whereas improvement of blind-walking distance judgments could be attributable to recalibration of walking, improvement in perceived size is considered evidence for rescaling of perceived space, whereby perceived size and distance increased after walking interaction. Furthermore, improvements in judged distance and size transferred to a new VE. Distance judgments, but not size judgments, continued to improve after additional walking interaction in the new VE. These results have theoretical implications regarding the effects of walking interaction on perceived space, and practical implications regarding methods of improving perceived distance in VEs. © 2017 American Psychological Association.",,"depth perception; human; psychological feedback; psychology; virtual reality; vision; walking; Feedback, Psychological; Humans; Space Perception; Virtual Reality; Visual Perception; Walking",Article,"Final","",Scopus,2-s2.0-85027048373
"Tidoni E., Abu-Alqumsan M., Leonardis D., Kapeller C., Fusco G., Guger C., Hintermuller C., Peer A., Frisoli A., Tecchia F., Bergamasco M., Aglioti S.M.","46261708700;41261045900;54389543000;37083868500;56388423300;55903211100;8440419500;15823160300;6603070041;6603477008;7003907071;7007006465;","Local and Remote Cooperation with Virtual and Robotic Agents: A P300 BCI Study in Healthy and People Living with Spinal Cord Injury",2017,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","25","9", 7797151,"1622","1632",,19,"10.1109/TNSRE.2016.2626391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029600422&doi=10.1109%2fTNSRE.2016.2626391&partnerID=40&md5=b17bb08935767322e6f8bed36b476792","Department of Psychology, University of Rome la Sapienza, Rome, 00185, Italy; Fondazione Santa Lucia IRCCS, Rome, Italy; Chair of Automatic Control Engineering, Technical University of Munich, TUM, Munich, Germany; Percro Laboratory, Scuola Superiore sant'Anna, Pisa, Italy; Guger Technologies OG, Graz, 8020, Austria; G.tec Medical Engineering GmbH, Schiedlberg, Austria; Bristol Robotics Laboratory, University of the West of England, Bristol, Bristol, United Kingdom","Tidoni, E., Department of Psychology, University of Rome la Sapienza, Rome, 00185, Italy, Fondazione Santa Lucia IRCCS, Rome, Italy; Abu-Alqumsan, M., Chair of Automatic Control Engineering, Technical University of Munich, TUM, Munich, Germany; Leonardis, D., Percro Laboratory, Scuola Superiore sant'Anna, Pisa, Italy; Kapeller, C., Guger Technologies OG, Graz, 8020, Austria, G.tec Medical Engineering GmbH, Schiedlberg, Austria; Fusco, G., Department of Psychology, University of Rome la Sapienza, Rome, 00185, Italy, Fondazione Santa Lucia IRCCS, Rome, Italy; Guger, C., Guger Technologies OG, Graz, 8020, Austria, G.tec Medical Engineering GmbH, Schiedlberg, Austria; Hintermuller, C., Guger Technologies OG, Graz, 8020, Austria, G.tec Medical Engineering GmbH, Schiedlberg, Austria; Peer, A., Bristol Robotics Laboratory, University of the West of England, Bristol, Bristol, United Kingdom; Frisoli, A., Percro Laboratory, Scuola Superiore sant'Anna, Pisa, Italy; Tecchia, F., Percro Laboratory, Scuola Superiore sant'Anna, Pisa, Italy; Bergamasco, M., Percro Laboratory, Scuola Superiore sant'Anna, Pisa, Italy; Aglioti, S.M., Department of Psychology, University of Rome la Sapienza, Rome, 00185, Italy, Fondazione Santa Lucia IRCCS, Rome, Italy","The development of technological applications that allow people to control and embody external devices within social interaction settings represents a major goal for current and future brain-computer interface (BCI) systems. Prior research has suggested that embodied systems may ameliorate BCI end-user's experience and accuracy in controlling external devices. Along these lines, we developed an immersive P300-based BCI application with a head-mounted display for virtual-local and robotic-remote social interactions and explored in a group of healthy participants the role of proprioceptive feedback in the control of a virtual surrogate (Study 1). Moreover, we compared the performance of a small group of people with spinal cord injury (SCI) to a control group of healthy subjects during virtual and robotic social interactions (Study 2), where both groups received a proprioceptive stimulation. Our attempt to combine immersive environments, BCI technologies and neuroscience of body ownership suggests that providing realistic multisensory feedback still represents a challenge. Results have shown that healthy and people living with SCI used the BCI within the immersive scenarios with good levels of performance (as indexed by task accuracy, optimizations calls and Information Transfer Rate) and perceived control of the surrogates. Proprioceptive feedback did not contribute to alter performance measures and body ownership sensations. Further studies are necessary to test whether sensorimotor experience represents an opportunity to improve the use of future embodied BCI applications. © 2001-2011 IEEE.","Body illusions - tendon vibration; brain-computer interface (BCI) P300; spinal cord injury; teleoperation; virtual reality","Computer control systems; Feedback; Helmet mounted displays; Interfaces (computer); Patient rehabilitation; Remote control; Robotics; Sensory perception; Social sciences; Virtual reality; Body illusions - tendon vibration; Head mounted displays; Immersive environment; Information transfer rate; Multi-sensory feedback; Performance measure; Spinal cord injuries (SCI); Technological applications; Brain computer interface; adult; Article; biceps brachii muscle; brain computer interface; clinical article; computer graphics; controlled study; discriminant analysis; female; human; human computer interaction; kinematics; male; mathematical analysis; middle aged; neuroscience nursing; proprioceptive feedback; questionnaire; real time tracking system; remote sensing; sensorimotor cortex; social interaction; spinal cord injury; telemonitoring; tendon; virtual reality; visual acuity; visual stimulation; computer interface; event related potential; imagination; man machine interaction; movement (physiology); pathophysiology; procedures; randomized controlled trial; reproducibility; robotics; sensitivity and specificity; Spinal Cord Injuries; task performance; young adult; Adult; Brain-Computer Interfaces; Event-Related Potentials, P300; Female; Humans; Imagination; Male; Man-Machine Systems; Movement; Reproducibility of Results; Robotics; Sensitivity and Specificity; Spinal Cord Injuries; Task Performance and Analysis; User-Computer Interface; Young Adult",Article,"Final","",Scopus,2-s2.0-85029600422
"Ragan E.D., Scerbo S., Bacim F., Bowman D.A.","26667185300;55210765300;16174310700;57203231782;","Amplified Head Rotation in Virtual Reality and the Effects on 3D Search, Training Transfer, and Spatial Orientation",2017,"IEEE Transactions on Visualization and Computer Graphics","23","8", 7547900,"1880","1895",,22,"10.1109/TVCG.2016.2601607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028407406&doi=10.1109%2fTVCG.2016.2601607&partnerID=40&md5=36f5ebc4f3b8d70469432fd11b448a6f","Texas A and M University, College Station, TX  77843, United States; Virginia Tech, Blacksburg, VA  24061, United States","Ragan, E.D., Texas A and M University, College Station, TX  77843, United States; Scerbo, S., Virginia Tech, Blacksburg, VA  24061, United States; Bacim, F., Virginia Tech, Blacksburg, VA  24061, United States; Bowman, D.A., Virginia Tech, Blacksburg, VA  24061, United States","Many types of virtual reality (VR) systems allow users to use natural, physical head movements to view a 3D environment. In some situations, such as when using systems that lack a fully surrounding display or when opting for convenient low-effort interaction, view control can be enabled through a combination of physical and virtual turns to view the environment, but the reduced realism could potentially interfere with the ability to maintain spatial orientation. One solution to this problem is to amplify head rotations such that smaller physical turns are mapped to larger virtual turns, allowing trainees to view the entire surrounding environment with small head movements. This solution is attractive because it allows semi-natural physical view control rather than requiring complete physical rotations or a fully-surrounding display. However, the effects of amplified head rotations on spatial orientation and many practical tasks are not well understood. In this paper, we present an experiment that evaluates the influence of amplified head rotation on 3D search, spatial orientation, and cybersickness. In the study, we varied the amount of amplification and also varied the type of display used (head-mounted display or surround-screen CAVE) for the VR search task. By evaluating participants first with amplification and then without, we were also able to study training transfer effects. The findings demonstrate the feasibility of using amplified head rotation to view 360 degrees of virtual space, but noticeable problems were identified when using high amplification with a head-mounted display. In addition, participants were able to more easily maintain a sense of spatial orientation when using the CAVE version of the application, which suggests that visibility of the user's body and awareness of the CAVE's physical environment may have contributed to the ability to use the amplification technique while keeping track of orientation. © 1995-2012 IEEE.","3D interaction; cybersickness; Rotation amplification; Search; Spatial orientation; Virtual reality","Caves; E-learning; Helmet mounted displays; Virtual reality; 3D interactions; Amplification technique; Cybersickness; Head mounted displays; Physical environments; Search; Spatial orientations; Surrounding environment; Rotation",Article,"Final","",Scopus,2-s2.0-85028407406
"Sakata S., Grove P.M., Hill A., Watson M.O., Stevenson A.R.L.","24399877200;6701491981;55722558500;7403102665;7201609363;","Impact of simulated three-dimensional perception on precision of depth judgements, technical performance and perceived workload in laparoscopy",2017,"British Journal of Surgery","104","8",,"1097","1106",,20,"10.1002/bjs.10528","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018572944&doi=10.1002%2fbjs.10528&partnerID=40&md5=92b576c82c95f36b057829ec6676c039","Schools of Medicine, The University of Queensland, Brisbane, QLD, Australia; Schools of Psychology, The University of Queensland, Brisbane, QLD, Australia; Department of Colon and Rectal Surgery, Royal Brisbane and Women's Hospital, Brisbane, QLD, Australia; Clinical Skills Development Service, Metro North Hospital and Health Service, Brisbane, QLD, Australia","Sakata, S., Schools of Medicine, The University of Queensland, Brisbane, QLD, Australia, Department of Colon and Rectal Surgery, Royal Brisbane and Women's Hospital, Brisbane, QLD, Australia, Clinical Skills Development Service, Metro North Hospital and Health Service, Brisbane, QLD, Australia; Grove, P.M., Schools of Psychology, The University of Queensland, Brisbane, QLD, Australia; Hill, A., Schools of Psychology, The University of Queensland, Brisbane, QLD, Australia, Clinical Skills Development Service, Metro North Hospital and Health Service, Brisbane, QLD, Australia; Watson, M.O., Schools of Medicine, The University of Queensland, Brisbane, QLD, Australia, Schools of Psychology, The University of Queensland, Brisbane, QLD, Australia, Clinical Skills Development Service, Metro North Hospital and Health Service, Brisbane, QLD, Australia; Stevenson, A.R.L., Schools of Medicine, The University of Queensland, Brisbane, QLD, Australia, Department of Colon and Rectal Surgery, Royal Brisbane and Women's Hospital, Brisbane, QLD, Australia","Background: This study compared precision of depth judgements, technical performance and workload using two-dimensional (2D) and three-dimensional (3D) laparoscopic displays across different viewing distances. It also compared the accuracy of 3D displays with natural viewing, along with the relationship between stereoacuity and 3D laparoscopic performance. Methods: A counterbalanced within-subjects design with random assignment to testing sequences was used. The system could display 2D or 3D images with the same set-up. A Howard–Dolman apparatus assessed precision of depth judgements, and three laparoscopic tasks (peg transfer, navigation in space and suturing) assessed performance (time to completion). Participants completed tasks in all combinations of two viewing modes (2D, 3D) and two viewing distances (1 m, 3 m). Other measures administered included the National Aeronautics and Space Administration Task Load Index (perceived workload) and the Randot® Stereotest (stereoacuity). Results: Depth judgements were 6·2 times as precise at 1 m and 3·0 times as precise at 3 m using 3D versus 2D displays (P &lt; 0·001). Participants performed all laparoscopic tasks faster in 3D at both 1 and 3 m (P &lt; 0.001), with mean completion times up to 64 per cent shorter for 3D versus 2D displays. Workload was lower for 3D displays (up to 34 per cent) than for 2D displays at both viewing distances (P &lt; 0·001). Greater viewing distance inhibited performance for two laparoscopic tasks, and increased perceived workload for all three (P &lt; 0·001). Higher stereoacuity was associated with shorter completion times for the navigating in space task performed in 3D at 1 m (r = − 0·40, P = 0·001). Conclusion: 3D displays offer large improvements over 2D displays in precision of depth judgements, technical performance and perceived workload. © 2017 The Authors. BJS published by John Wiley & Sons Ltd on behalf of BJS Society Ltd.",,"adult; clinical competence; controlled study; decision making; depth perception; education; human; laparoscopy; medical education; medical staff; perception; physiology; procedures; psychology; psychomotor performance; Queensland; randomized controlled trial; simulation training; standards; statistics and numerical data; surgeon; three dimensional imaging; workload; Adult; Clinical Competence; Depth Perception; Education, Medical; Humans; Imaging, Three-Dimensional; Judgment; Laparoscopy; Medical Staff, Hospital; Perception; Psychomotor Performance; Queensland; Simulation Training; Surgeons; Workload",Article,"Final","",Scopus,2-s2.0-85018572944
"Lanzer P., Al-Naser M., Bukhari S.S., Dengel A., Krupinski E.A.","57204261399;53979297700;25924725100;6603764314;26643320200;","Eye tracking in catheter-based cardiovascular interventions: Early results",2017,"Journal of Medical Imaging","4","3", 035502,"","",,2,"10.1117/1.JMI.4.3.035502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027077024&doi=10.1117%2f1.JMI.4.3.035502&partnerID=40&md5=aa13847c84bdcb25e2f17f2fc696888b","Middle German Heart Center, Division of Cardiovascular Disease, Health Care Center Bitterfeld-Wolfen gGmbH, Bitterfeld-Wolfen, Germany; University of Kaiserslautern, Department of Computer Science, Kaiserslautern, Germany; German Research Center for Artificial Intelligence, Smart Data and Knowledge Services Department, Kaiserslautern, Germany; Emory University, Department of Radiology and Imaging Sciences, Atlanta, GA, United States","Lanzer, P., Middle German Heart Center, Division of Cardiovascular Disease, Health Care Center Bitterfeld-Wolfen gGmbH, Bitterfeld-Wolfen, Germany; Al-Naser, M., University of Kaiserslautern, Department of Computer Science, Kaiserslautern, Germany, German Research Center for Artificial Intelligence, Smart Data and Knowledge Services Department, Kaiserslautern, Germany; Bukhari, S.S., German Research Center for Artificial Intelligence, Smart Data and Knowledge Services Department, Kaiserslautern, Germany; Dengel, A., University of Kaiserslautern, Department of Computer Science, Kaiserslautern, Germany, German Research Center for Artificial Intelligence, Smart Data and Knowledge Services Department, Kaiserslautern, Germany; Krupinski, E.A., Emory University, Department of Radiology and Imaging Sciences, Atlanta, GA, United States","Visual x-ray image processing (XRIP) represents a fundamental component of catheter-based cardiovascular interventions (CBCVIs). To date, no data are available to define XRIP in this setting. To characterize CBCVI XRIP, we developed a computer-based method allowing continuous temporal-spatial analysis of data recorded by a head-mounted eye-tracking device. Quantitative analysis of gaze duration of an expert operator (EO) revealed that the average time in minutes spent viewing the images on the display screen was 39.5% ± 13.6% and 41.5% ± 18.3% of the total recorded time in coronary angiography (CA) and in CA followed by CBCVI, respectively. Qualitative analysis of gaze data of the EO revealed consistent focus on the center point of the screen. Only if suspicious findings were detected did gaze move toward the target. In contrast, a novice operator (NO) observing a subset of cases viewed coronary artery segments separately and sequentially. The developed methodology allows continuous registration and analysis of gaze data for analysis of XRIP strategies of EOs in live-cases scenarios and may assist in the transfer of experts reading skills to novices. © 2017 The Authors.","catheter-based cardiovascular interventions; eye tracking; perception; teaching","acute coronary syndrome; adult; aged; Article; brain angiography; brain ischemia; carotid artery stenting; clinical article; coronary angiography; coronary artery; coronary artery circumflex branch; coronary artery disease; eye tracking; female; gaze; heart catheterization; human; image processing; internal carotid artery; left anterior descending coronary artery; male; middle aged; non ST segment elevation myocardial infarction; percutaneous coronary intervention; percutaneous transluminal angioplasty; pilot study; qualitative analysis; quantitative analysis; superficial femoral artery",Article,"Final","",Scopus,2-s2.0-85027077024
"Bertrand J., Bhargava A., Madathil K.C., Gramopadhye A., Babu S.V.","55858839700;57194158382;37075253800;7005569103;9039004700;","The effects of presentation method and simulation fidelity on psychomotor education in a bimanual metrology training simulation",2017,"2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings",,, 7893318,"59","68",,7,"10.1109/3DUI.2017.7893318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019003187&doi=10.1109%2f3DUI.2017.7893318&partnerID=40&md5=01f06159d13212f502fbea389f779a5b","Clemson University, United States","Bertrand, J., Clemson University, United States; Bhargava, A., Clemson University, United States; Madathil, K.C., Clemson University, United States; Gramopadhye, A., Clemson University, United States; Babu, S.V., Clemson University, United States","In this study, we empirically evaluated the effects of presentation method and simulation fidelity on task performance and psychomotor skills acquisition in an immersive bimanual simulation towards precision metrology education. In a 2 × 2 experiment design, we investigated a large-screen immersive display (LSID) with a head-mounted display (HMD), and the presence versus absence of gravity. Advantages of the HMD include interacting with the simulation in a more natural manner as compared to using a large-screen immersive display due to the similarities between the interactions afforded in the virtual compared to the real-world task. Suspending the laws of physics may have an effect on usability and in turn could affect learning outcomes. Our dependent variables consisted of a pre and post cognition questionnaire, quantitative performance measures, perceived workload and system usefulness, and a psychomotor assessment to measure to what extent transfer of learning took place from the virtual to the real world. Results indicate that the HMD condition was preferable to the immersive display in several metrics while the no-gravity condition resulted in users adopting strategies that were not advantageous for task performance. © 2017 IEEE.","and virtual realities; augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial","Personnel training; Units of measurement; User interfaces; Virtual reality; augmented; Dependent variables; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; Head mounted displays; Large-screen immersive displays; Simulation fidelity; Training simulation; Transfer of learning; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85019003187
"Anglin J., Saldana D., Schmiesing A., Liew S.-L.","57193856710;57194044260;57194041188;36992162200;","Transfer of a skilled motor learning task between virtual and conventional environments",2017,"Proceedings - IEEE Virtual Reality",,, 7892346,"401","402",,10,"10.1109/VR.2017.7892346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018412711&doi=10.1109%2fVR.2017.7892346&partnerID=40&md5=65da755f34034bf37533b59e030ddbca","University of Southern California, Los Angeles, CA, United States","Anglin, J., University of Southern California, Los Angeles, CA, United States; Saldana, D., University of Southern California, Los Angeles, CA, United States; Schmiesing, A., University of Southern California, Los Angeles, CA, United States; Liew, S.-L., University of Southern California, Los Angeles, CA, United States","Immersive, head-mounted virtual reality (HMD-VR) can be a potentially useful tool for motor rehabilitation. However, it is unclear whether the motor skills learned in HMD-VR transfer to the non-virtual world and vice-versa. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train individuals in either an HMD-VR or conventional training (CT) environment. Participants were then tested in both environments. Our results show that participants who train in the CT environment have an improvement in motor performance when they transfer to the HMD-VR environment. In contrast, participants who train in the HMD-VR environment show a decrease in skill level when transferring to the CT environment. This has implications for how training in HMD-VR and CT may affect performance in different environments. © 2017 IEEE.","Skilled motor learning; Transfer; Virtual reality","E-learning; Helmet mounted displays; Virtual reality; Head mounted virtual reality; Motor learning; Motor performance; Motor rehabilitation; Motor skills; Skill levels; Transfer; Virtual worlds; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-85018412711
"Suefusa K., Tanaka T.","56648195700;55727208700;","A comparison study of visually stimulated brain-computer and eye-tracking interfaces",2017,"Journal of Neural Engineering","14","3", 036009,"","",,12,"10.1088/1741-2552/aa6086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020401545&doi=10.1088%2f1741-2552%2faa6086&partnerID=40&md5=fa70134efd003d2e284d81690b3adcc1","Department of Electronic and Information Engineering, Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo, 184-8588, Japan","Suefusa, K., Department of Electronic and Information Engineering, Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo, 184-8588, Japan; Tanaka, T., Department of Electronic and Information Engineering, Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo, 184-8588, Japan","Objective. Brain-computer interfacing (BCI) based on visual stimuli detects the target on a screen on which a user is focusing. The detection of the gazing target can be achieved by tracking gaze positions with a video camera, which is called eye-tracking or eye-tracking interfaces (ETIs). The two types of interface have been developed in different communities. Thus, little work on a comprehensive comparison between these two types of interface has been reported. This paper quantitatively compares the performance of these two interfaces on the same experimental platform. Specifically, our study is focused on two major paradigms of BCI and ETI: steady-state visual evoked potential-based BCIs and dwelling-based ETIs. Approach. Recognition accuracy and the information transfer rate were measured by giving subjects the task of selecting one of four targets by gazing at it. The targets were displayed in three different sizes (with sides 20, 40 and 60 mm long) to evaluate performance with respect to the target size. Main results. The experimental results showed that the BCI was comparable to the ETI in terms of accuracy and the information transfer rate. In particular, when the size of a target was relatively small, the BCI had significantly better performance than the ETI. Significance. The results on which of the two interfaces works better in different situations would not only enable us to improve the design of the interfaces but would also allow for the appropriate choice of interface based on the situation. Specifically, one can choose an interface based on the size of the screen that displays the targets. © 2017 IOP Publishing Ltd.","brain-computer interfaces; eye-tracking interfaces; information transfer rate; steady-state visual evoked potentials","Bioelectric potentials; Electrophysiology; Human computer interaction; Interface states; Interfaces (computer); Video cameras; Brain-computer interfacing; Comparison study; Comprehensive comparisons; Experimental platform; Eye-tracking; Information transfer rate; Recognition accuracy; Steady state visual evoked potentials; Brain computer interface; adult; Article; brain computer interface; comparative study; eye tracking; female; gaze; human; human experiment; male; measurement accuracy; normal human; priority journal; quantitative analysis; steady state; visual evoked potential; visual stimulation; computer interface; device failure analysis; devices; electroencephalography; equipment design; evaluation study; eye movement; photostimulation; physiology; procedures; reproducibility; sensitivity and specificity; task performance; vision; Adult; Brain-Computer Interfaces; Electroencephalography; Equipment Design; Equipment Failure Analysis; Evoked Potentials, Visual; Eye Movements; Humans; Photic Stimulation; Reproducibility of Results; Sensitivity and Specificity; Task Performance and Analysis; User-Computer Interface; Visual Perception",Article,"Final","",Scopus,2-s2.0-85020401545
"Halabi O., El-Seoud S.A., Aljaam J.M., Alpona H., Al-Hemadi M., Al-Hassan D.","57203219290;6507058670;24528095900;56407207400;57194434607;57194429167;","Design of immersive virtual reality system to improve communication skills in individuals with autism",2017,"International Journal of Emerging Technologies in Learning","12","5",,"50","64",,10,"10.3991/ijet.v12i05.6766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020066717&doi=10.3991%2fijet.v12i05.6766&partnerID=40&md5=e2208a321c0b786aa1272f0ea478869e","Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt","Halabi, O., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; El-Seoud, S.A., Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt; Aljaam, J.M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Alpona, H., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hemadi, M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hassan, D., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar","Individuals with autism spectrum disorder (ASD) regularly experience situations in which they need to give answers but do not know how to respond; for example, questions related to everyday life activities that are asked by strangers. Research geared at utilizing technology to mend social and communication impairments in children with autism is actively underway. Immersive virtual reality (VR) is a relatively recent technology that has the potential of being an effective therapeutic tool for developing various skills in autistic children. This paper presents an interactive scenario-based VR system developed to improve the communications skills of autistic children. The system utilizes speech recognition to provide natural interaction and role-play and turntaking to evaluate and verify the effectiveness of the immersive environment on the social performance of autistic children. In experiments conducted, participants showed more improved performance with a computer augmented virtual environment (CAVE) than with a head mounted display (HMD) or a normal desktop. The results indicate that immersive VR could be more satisfactory and motivational than desktop for children with ASD.","Autism spectrum disorder; Communication skill; Immersion; Social performance; Virtual reality","Diseases; Helmet mounted displays; Speech recognition; Technology transfer; Virtual reality; Autism spectrum disorders; Children with autisms; Communication skills; Head mounted displays; Immersion; Immersive environment; Immersive virtual reality; Social performance; Education",Article,"Final","",Scopus,2-s2.0-85020066717
"Kaul O.B., Rohs M.","57191506409;17346497700;","HapticHead: A spherical vibrotactile grid around the head for 3D guidance in virtual and augmented reality",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-January",,,"3729","3740",,37,"10.1145/3025453.3025684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029123271&doi=10.1145%2f3025453.3025684&partnerID=40&md5=a5e7f4386fd76112d0a37da34a90f577","University of Hannover, Hannover, Germany","Kaul, O.B., University of Hannover, Hannover, Germany; Rohs, M., University of Hannover, Hannover, Germany","Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4 % vs. 54.2 % success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is - as expected - more precise (99.7 % success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a headmounted display. © 2017 ACM.","3D output; Augmented reality; Guidance; Haptic feedback; Navigation; Spatial interaction; Vibrotactile; Virtual reality","Augmented reality; Electronic guidance systems; Helmet mounted displays; Navigation; Virtual reality; Visual communication; Human engineering; Sensory perception; Street traffic control; 3D output; Haptic feedbacks; Head mounted displays; Head related transfer function; Spatial interaction; Vibro-tactile feedbacks; Vibrotactile; Virtual and augmented reality; Feedback",Conference Paper,"Final","",Scopus,2-s2.0-85029123271
"Pundlik S., Yi H., Liu R., Peli E., Luo G.","8728311600;57193212126;57193214918;7005237694;36499782000;","Magnifying smartphone screen using google glass for low-vision Users",2017,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","25","1", 7439852,"49","58",,7,"10.1109/TNSRE.2016.2546062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011664764&doi=10.1109%2fTNSRE.2016.2546062&partnerID=40&md5=e2f14ea3908eeb8f7782abb76b22ef69","Schepens Eye Research Institute, Mass Eye and Ear, Harvard Medical School, Boston, MA  02114, United States; Computer Science Department, Northeastern University, Boston, MA  77005, United States","Pundlik, S., Schepens Eye Research Institute, Mass Eye and Ear, Harvard Medical School, Boston, MA  02114, United States; Yi, H., Computer Science Department, Northeastern University, Boston, MA  77005, United States; Liu, R., Schepens Eye Research Institute, Mass Eye and Ear, Harvard Medical School, Boston, MA  02114, United States; Peli, E., Schepens Eye Research Institute, Mass Eye and Ear, Harvard Medical School, Boston, MA  02114, United States; Luo, G., Schepens Eye Research Institute, Mass Eye and Ear, Harvard Medical School, Boston, MA  02114, United States","Magnification is a key accessibility feature used by low-vision smartphone users. However, small screen size can lead to loss of context and make interaction with magnified displays challenging. We hypothesize that controlling the viewport with head motion can be natural and help in gaining access to magnified displays. We implement this idea using a Google Glass that displays the magnified smartphone screenshots received in real time via Bluetooth. Instead of navigating with touch gestures on the magnified smartphone display, the users can view different screen locations by rotating their head, and remotely interacting with the smartphone. It is equivalent to looking at a large virtual image through a head contingent viewing port, in this case, the Glass display with ∼ 15° field of view. The system can transfer seven screenshots per second at 8× magnification, sufficient for tasks where the display content does not change rapidly. A pilot evaluation of this approach was conducted with eight normally sighted and four visually impaired subjects performing assigned tasks using calculator and music player apps. Results showed that performance in the calculation task was faster with the Glass than with the phone's built-in screen zoom. We conclude that head contingent scanning control can be beneficial in navigating magnified small smartphone displays, at least for tasks involving familiar content layout. © 2001-2011 IEEE.","Google Glass; low-vision aid; screen magnification; smartphone app","Glass; Smartphones; Touch screens; Wearable computers; Field of views; Gaining access; Head motion; Low vision; Music players; Small screens; Virtual images; Visually impaired; Signal encoding; Article; client server application; clinical trial (topic); human; image display; information processing; low vision; low vision magnifier; macular degeneration; mobile application; smartphone; visual acuity; visual orientation; adult; aged; case report; computer assisted diagnosis; computer interface; computer terminal; device failure analysis; devices; equipment design; female; image enhancement; male; middle aged; procedures; reproducibility; sensitivity and specificity; treatment outcome; Vision, Low; Adult; Aged; Computer Terminals; Data Display; Equipment Design; Equipment Failure Analysis; Female; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Male; Middle Aged; Reproducibility of Results; Sensitivity and Specificity; Smartphone; Treatment Outcome; User-Computer Interface; Vision, Low",Article,"Final","",Scopus,2-s2.0-85011664764
"Dawidek M.T., Roach V.A., Ott M.C., Wilson T.D.","56849383200;54914133700;57213967830;55777429000;","Changing the Learning Curve in Novice Laparoscopists: Incorporating Direct Visualization into the Simulation Training Program",2017,"Journal of Surgical Education","74","1",,"30","36",,3,"10.1016/j.jsurg.2016.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000399389&doi=10.1016%2fj.jsurg.2016.07.012&partnerID=40&md5=e13274c5db9aa4fc5236be02c9f7c4a0","Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada; Department of Anatomy and Cell Biology, Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada; Department of Biomedical Sciences, Oakland University William Beaumont School of Medicine, Rochester, Michigan, United States; Department of Surgery, Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada","Dawidek, M.T., Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada, Department of Anatomy and Cell Biology, Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada; Roach, V.A., Department of Biomedical Sciences, Oakland University William Beaumont School of Medicine, Rochester, Michigan, United States; Ott, M.C., Department of Surgery, Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada; Wilson, T.D., Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada, Department of Anatomy and Cell Biology, Schulich School of Medicine and Dentistry, The University of Western Ontario, London, Ontario, Canada","Objective A major challenge in laparoscopic surgery is the lack of depth perception. With the development and continued improvement of 3D video technology, the potential benefit of restoring 3D vision to laparoscopy has received substantial attention from the surgical community. Despite this, procedures conducted under 2D vision remain the standard of care, and trainees must become proficient in 2D laparoscopy. This study aims to determine whether incorporating 3D vision into a 2D laparoscopic simulation curriculum accelerates skill acquisition in novices. Design Postgraduate year-1 surgical specialty residents (n = 15) at the Schulich School of Medicine and Dentistry, at Western University were randomized into 1 of 2 groups. The control group practiced the Fundamentals of Laparoscopic Surgery peg-transfer task to proficiency exclusively under standard 2D laparoscopy conditions. The experimental group first practiced peg transfer under 3D direct visualization, with direct visualization of the working field. Upon reaching proficiency, this group underwent a perceptual switch, changing to standard 2D laparoscopy conditions, and once again trained to proficiency. Results Incorporating 3D direct visualization before training under standard 2D conditions significantly (p < 0.0.5) reduced the total training time to proficiency by 10.9 minutes or 32.4%. There was no difference in total number of repetitions to proficiency. Data were also used to generate learning curves for each respective training protocol. Conclusions An adaptive learning approach, which incorporates 3D direct visualization into a 2D laparoscopic simulation curriculum, accelerates skill acquisition. This is in contrast to previous work, possibly owing to the proficiency-based methodology employed, and has implications for resource savings in surgical training. © 2016","3D visualization; adaptive training; FLS; laparoscopy; skill acquisition","adult; Article; controlled study; curriculum; female; human; laparoscopic surgery; learning curve; male; postgraduate education; priority journal; resident; simulation training; surgical training; three dimensional imaging; clinical competence; comparative study; depth perception; education; laparoscopy; medical education; Ontario; procedures; randomized controlled trial; simulation training; three dimensional imaging; three dimensional printing; young adult; Adult; Clinical Competence; Curriculum; Depth Perception; Education, Medical, Graduate; Female; Humans; Imaging, Three-Dimensional; Internship and Residency; Laparoscopy; Learning Curve; Male; Ontario; Printing, Three-Dimensional; Simulation Training; Young Adult",Article,"Final","",Scopus,2-s2.0-85000399389
"Yu J., Park J.","56131024300;57192642325;","Real-time facial tracking in virtual reality",2016,"SA 2016 - SIGGRAPH ASIA 2016 VR Showcase",,, a3,"","",,5,"10.1145/2996376.2996390","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006968903&doi=10.1145%2f2996376.2996390&partnerID=40&md5=162a2b3d033461b479bad62831e3db98","BinaryVR. Inc., United States","Yu, J., BinaryVR. Inc., United States; Park, J., BinaryVR. Inc., United States","Virtual reality (VR) emerges as the next social computing platform. For realizing immersive social interactions, projecting facial expressions onto the virtual avatar a crucial component. This is a challenge in VR as it requires capturing the facial motions behind the VR head mounted displays (HMDs). In this paper, we present a real-time facial expression tracking system in VR HMDs. The core of the system is a 3D camera attached to the HMDs, capturing motions on the lower half of the face, which enables users to track and retarget their facial animations in realtime onto CG avatars. The system is capable of capturing 20 facial expression parameters and transfer it onto the 3D character in real-time. © 2016. ACM.","Facial capture; Virtual reality","Cameras; Helmet mounted displays; Interactive computer graphics; Three dimensional computer graphics; Virtual reality; Facial animation; Facial capture; Facial expression parameters; Facial Expressions; Head mounted displays; Social computing; Social interactions; Tracking system; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85006968903
"Ariza O., Freiwald J., Laage N., Feist M., Salloum M., Bruder G., Steinicke F.","57197830700;57191980479;57191982482;57191977364;35753484300;23391698600;8883314100;","Inducing body-transfer illusions in VR by providing brief phases of visual-tactile stimulation",2016,"SUI 2016 - Proceedings of the 2016 Symposium on Spatial User Interaction",,,,"61","68",,4,"10.1145/2983310.2985760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995687176&doi=10.1145%2f2983310.2985760&partnerID=40&md5=6963e453917408cd9526505c632cd9aa","Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany","Ariza, O., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Freiwald, J., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Laage, N., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Feist, M., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Salloum, M., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Bruder, G., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany; Steinicke, F., Human-Computer Interaction, Department of Informatics, Universität Hamburg, Germany","Current developments in the area of virtual reality (VR) allow numerous users to experience immersive virtual environments (VEs) in a broad range of application fields. In the same way, some research has shown novel advances in wearable devices to provide vibrotactile feedback which can be combined with low-cost technology for hand tracking and gestures recognition. The combination of these technologies can be used to investigate interesting psychological illusions. For instance, body-transfer illusions, such as the rubber-hand illusion or elongated-arm illusion, have shown that it is possible to give a person the persistent illusion of body transfer after only brief phases of synchronized visual-haptic stimulation. The motivation of this paper is to induce such perceptual illusions by combining VR, vibrotactile and tracking technologies, offering an interesting way to create new spatial interaction experiences centered on the senses of sight and touch. We present a technology framework that includes a pair of self-made gloves featuring vibrotactile feedback that can be synchronized with audio-visual stimulation in order to reproduce body-transfer illusions in VR. We present in detail the implementation of the framework and show that the proposed technology setup is able to induce the elongatedarm illusion providing automatic tactile stimuli, instead of the traditional approach based on manually synchronized stimulation. © 2016 ACM.","3D touch interaction; Body-transfer illusions; Head-mounted display; Vibrotactile feedback; Virtual environments","Helmet mounted displays; Sensory perception; Synchronization; Virtual reality; Audio-visual stimulation; Body-transfer illusions; Gestures recognition; Head mounted displays; Immersive virtual environments; Touch interaction; Traditional approaches; Vibro-tactile feedbacks; Wearable technology",Conference Paper,"Final","",Scopus,2-s2.0-84995687176
"Reddy S., Rao G.S.S.S., Hegde R.M.","57206759296;57189239489;9632693100;","On the development of a dynamic virtual reality system using audio and visual scenes",2016,"2016 22nd National Conference on Communication, NCC 2016",,, 7561204,"","",,,"10.1109/NCC.2016.7561204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988909542&doi=10.1109%2fNCC.2016.7561204&partnerID=40&md5=9798724ec6c9da4d18947af74cd5fb08","Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","Reddy, S., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Rao, G.S.S.S., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Hegde, R.M., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","Virtual reality systems have been widely used in many popular and diverse applications including education and gaming. However, development of a dynamic virtual reality system which combines both audio and visual scenes has hitherto not been investigated. In this work a dynamic virtual reality system which synchronizes both audio and visual information is developed. Realtime audio and visual information is obtained from a spherical audio visual camera with 64 microphones and 5 cameras. Subsequently, a head mounted display application is designed to render spherical video. A three dimensional sound rendering algorithm using head related transfer functions is developed. Finally, a virtual reality system that combines both spherical audio and video is realized. The head position of the user is also integrated into this system adaptively to make the system dynamic. Both subjective and objective evaluations of the proposed virtual reality system indicate its significance. © 2016 IEEE.",,"Cameras; Helmet mounted displays; Sound reproduction; Spheres; Three dimensional computer graphics; Audio and visual information; Diverse applications; Head mounted displays; Head related transfer function; Subjective and objective evaluations; System Dynamics; Three-dimensional sounds; Virtual reality system; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84988909542
"Viriyasiripong S., Lopez A., Mandava S.H., Lai W.R., Mitchell G.C., Boonjindasup A., Powers M.K., Silberstein J.L., Lee B.R.","55734807700;57022836100;55094137900;57189582430;54976410800;50960965300;55569319600;57199015431;7405438725;","Accelerometer measurement of head movement during laparoscopic surgery as a tool to evaluate skill development of surgeons",2016,"Journal of Surgical Education","73","4",,"589","594",,7,"10.1016/j.jsurg.2016.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975727055&doi=10.1016%2fj.jsurg.2016.01.008&partnerID=40&md5=4a3dc460a972dc944c6cb014102189ae","Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Interdisciplinary PhD Program, Tulane University, New Orleans, LA, United States; Division of Urology, University of Arizona College of Medicine, B. 245077, 1501 N. Campbell Ave, Tucson, AZ  85724-5077, United States","Viriyasiripong, S., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Lopez, A., Interdisciplinary PhD Program, Tulane University, New Orleans, LA, United States; Mandava, S.H., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Lai, W.R., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Mitchell, G.C., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Boonjindasup, A., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Powers, M.K., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Silberstein, J.L., Department of Urology, Tulane University School of Medicine, New Orleans, LA, United States; Lee, B.R., Division of Urology, University of Arizona College of Medicine, B. 245077, 1501 N. Campbell Ave, Tucson, AZ  85724-5077, United States","Objective To detect and measure surgeons' head movement during laparoscopic simulator performance to determine whether expert surgeons have economy of motion in their head movement, including change of direction, compared with intermediate and novice surgeons. We investigated head movement as an objective tool for assessment of laparoscopic surgical skill and its potential use for assessing novice surgeons' progress on the learning curve. Design After obtaining institutional review board approval, medical students, urology residents, and attending staff surgeons from an academic institution were recruited. Participants were grouped by level of experience and performed tasks on the Electronic Data Generation for Evaluation laparoscopic simulator. Surgeons wore a commercially available wireless electroencephalogram monitor as a flexible, adjustable, and lightweight headband with 7 sensors - 2 forehead sensors, 2 ear sensors, and 3 reference sensors. The headband incorporates a 3-axis accelerometer enabling head movement quantification. A variance analysis was used to compare the average head movement acceleration data between each group. Setting Tulane University Medical Center, New Orleans, LA, an academic medical center and the principal teaching hospital for Tulane University School of Medicine. Participants A total of following 19 participants were recruited for the study and stratified by surgical experience into novice (n = 6), intermediate (n = 9), and expert (n = 4) laparoscopy groups: 6 medical students, 9 urology residents (postgraduate years 1 to5), and 4 attending urologists, respectively. Results Analysis of the average acceleration rate of head movement showed statistically significant differences among groups on both the vertical axis (p = 0.006) and horizontal axis (p = 0.018) in the laparoscopic suturing task. This demonstrated the ability to distinguish between experts and novice laparoscopic surgeons. The average acceleration among groups did not demonstrate statistical significance on the vertical axis (p = 0.078) and horizontal axis (p = 0.077) in the peg transfer task. This may be in response to the ease of the task. The analysis of the forward-backward axis or depth perception also showed no significant differences between groups. Conclusion Accelerometer-based motion analysis of head movement appears to be a useful tool to evaluate laparoscopic skill development of surgeons in terms of their economy of motion, and it could potentially be used for ergonomic assessment of training in the future, and progression on the learning curve. © 2016 Association of Program Directors in Surgery. Published by Elsevier Inc. All rights reserved.","accelerometer; ergonomic; G-force; laparoscopy; motion analysis; simulation; surgical skill","acceleration; accelerometer; analysis of variance; Article; depth perception; electroencephalogram; head movement; human; laparoscopic surgery; learning curve; medical student; priority journal; resident; simulation training; skill; surgeon; urologist; accelerometry; clinical competence; education; laparoscopy; Louisiana; medical education; procedures; questionnaire; task performance; Accelerometry; Clinical Competence; Education, Medical; Head Movements; Humans; Laparoscopy; Louisiana; Surveys and Questionnaires; Task Performance and Analysis",Article,"Final","",Scopus,2-s2.0-84975727055
"Sankaranarayanan G., Li B., Manser K., Jones S.B., Jones D.B., Schwaitzberg S., Cao C.G.L., De S.","15623319200;57169023100;55996844500;15739783000;55387240300;7007036892;25957557800;7202304567;","Face and construct validation of a next generation virtual reality (Gen2-VR©) surgical simulator",2016,"Surgical Endoscopy","30","3",,"979","985",,13,"10.1007/s00464-015-4278-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959176014&doi=10.1007%2fs00464-015-4278-7&partnerID=40&md5=a5508327f4021456b0383555601182e6","Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; Cambridge Health Alliance, Cambridge, MA, United States; Beth Israel Deaconess Medical Center, Boston, MA, United States; Wright State University, Dayton, OH, United States; Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States","Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States; Li, B., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States, School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; Manser, K., Cambridge Health Alliance, Cambridge, MA, United States; Jones, S.B., Beth Israel Deaconess Medical Center, Boston, MA, United States; Jones, D.B., Beth Israel Deaconess Medical Center, Boston, MA, United States; Schwaitzberg, S., Cambridge Health Alliance, Cambridge, MA, United States; Cao, C.G.L., Wright State University, Dayton, OH, United States; De, S., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States, Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States","Introduction: Surgical performance is affected by distractors and interruptions to surgical workflow that exist in the operating room. However, traditional surgical simulators are used to train surgeons in a skills laboratory that does not recreate these conditions. To overcome this limitation, we have developed a novel, immersive virtual reality (Gen2-VR©) system to train surgeons in these environments. This study was to establish face and construct validity of our system. Methods and procedures: The study was a within-subjects design, with subjects repeating a virtual peg transfer task under three different conditions: Case I: traditional VR; Case II: Gen2-VR© with no distractions and Case III: Gen2-VR© with distractions and interruptions. In Case III, to simulate the effects of distractions and interruptions, music was played intermittently, the camera lens was fogged for 10 s and tools malfunctioned for 15 s at random points in time during the simulation. At the completion of the study subjects filled in a 5-point Likert scale feedback questionnaire. A total of sixteen subjects participated in this study. Results: Friedman test showed significant difference in scores between the three conditions (p &lt; 0.0001). Post hoc analysis using Wilcoxon signed-rank tests with Bonferroni correction further showed that all the three conditions were significantly different from each other (Case I, Case II, p &lt; 0.0001), (Case I, Case III, p &lt; 0.0001) and (Case II, Case III, p = 0.009). Subjects rated that fog (mean 4.18) and tool malfunction (median 4.56) significantly hindered their performance. Conclusion: The results showed that Gen2-VR© simulator has both face and construct validity and that it can accurately and realistically present distractions and interruptions in a simulated OR, in spite of limitations of the current HMD hardware technology. © 2015, Springer Science+Business Media New York.","Cognitive simulator; Face and construct validation; Gen2-VR©; Head-mounted display; Immersive virtual reality; Surgery simulator","accuracy; Article; Bonferroni correction; camera; computer program; construct validity; face validity; female; Friedman test; human; human experiment; Likert scale; male; monitor; music; operating room; post hoc analysis; priority journal; simulator; statistical model; surgeon; surgery simulator; surgical training; virtual reality; Wilcoxon signed ranks test; attention; computer interface; education; feedback system; laparoscopy; procedures; simulation training; validation study; Attention; Feedback; Female; Humans; Laparoscopy; Male; Simulation Training; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84959176014
"Alghamdi M., Regenbrecht H., Hoermann S., Langlotz T., Aldridge C.","36459710000;6603333462;54787745300;8250843500;57062294500;","Social presence and mode of video communication in a Collaborative Virtual Environment",2016,"Pacific Asia Conference on Information Systems, PACIS 2016 - Proceedings",,,,"","",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011117105&partnerID=40&md5=0235f7bf3ef9f4bdceecd4b42a98b91e","Department of Information Science, University of Otago, New Zealand; School of Electrical and Information Engineering, University of Sydney, Sydney, Australia","Alghamdi, M., Department of Information Science, University of Otago, New Zealand; Regenbrecht, H., Department of Information Science, University of Otago, New Zealand; Hoermann, S., School of Electrical and Information Engineering, University of Sydney, Sydney, Australia; Langlotz, T., Department of Information Science, University of Otago, New Zealand; Aldridge, C., Department of Information Science, University of Otago, New Zealand","Collaborative Virtual Environments (CVE) with co-located or remote video communication functionality require a continuous experience of social presence. If, at any stage during the experience the communication interrupts presence then the CVE experience as a whole is affected - spatial presence is then decoupled from social presence. We present a solution to this problem by introducing the concept of a virtualized version of Google Glass™ called Virtual Glass. Virtual Glass is integrated into the CVE as a real-world metaphor for a communication device, one particularly suited for collaborative instructor-performer systems. Together with domain experts we developed a prototype system based on an instructor-performer architecture. In two studies with a total number of 115 participants we showed that the concept of Virtual Glass is effective, that it supports a high level of social presence and that the social presence for the performers is rated significantly higher than a standard picture-in-picture videoconferencing approach used for the performers. We present our experimental system, our studies, and the generalizability of our approach towards future uses.","Human-computer interface; Videoconferencing; Virtual reality","Display devices; Glass; Human computer interaction; Information systems; Video conferencing; Collaborative virtual environment; Communication device; Domain experts; Experimental system; Human computer interfaces; Prototype system; Social presence; Video communications; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85011117105
"Lok B.","57203616548;","Training with virtual operating room teammates to influence team behaviors",2016,"Proceedings - 2016 International Conference on Collaboration Technologies and Systems, CTS 2016",,, 7871053,"615","616",,,"10.1109/CTS.2016.115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016990802&doi=10.1109%2fCTS.2016.115&partnerID=40&md5=43b8f2a891be99dc9798e92fe575cfde","University of Florida, Gainesville, FL, United States","Lok, B., University of Florida, Gainesville, FL, United States","Imagine you are an operating room nurse. Could training with virtual human teammates empower you to speak up to a bullying teammate? Could virtual teammates change the way you speak as to reduce errors? How about learn new patient safety policies or efficiently transfer care? In this talk, we will explore the emerging area of using virtual humans to subtly influence healthcare teams' teamwork and communication skills. This application of virtual humans could have significant patient safety impact as teamwork and communication is the top reason for adverse events in critical care areas, such as the emergency room, intensive care unit, and operating room. We will examine the latest research into simulating healthcare teams with mixed reality humans. Mixed reality humans are virtual humans that can share the same physical space as the user. These virtual humans combine interactive graphics, natural language processing, artificial intelligence, human-computer interaction, and data mining to create in situ learning experiences. In these learning experiences, critical care personnel can work to improve teamwork with life-sized interactive virtual team-mates [1]. These learning experiences can also help implement best-practices to address address difficult teamwork concepts such as authority gradients, conflict negotiation, empathy and critical thinking [2][3]. Our research team (Samsun Lampotang, Anesthesia Department, University of Florida, Adam Wendling, Anesthesia Department, University of Florida, and Casey White, College of Medicine, University of Virginia) has developed VR hardware and software platforms to create compelling experiences for users to work on teams with mixed reality humans (MRHs). MRHs are virtual humans that can inhabit the user's physical space [4]. The MRH virtual team members can respond to the user's speech and actions and respond with natural speech and gestures. The virtual team members cannot physically interact with the environment. However, they can present realistic personalities and role-play the roles of operating room teammates, such as surgeons, anesthesiologists, nurses, and surgical technicians. The virtual team members combine the benefits of dynamic visuals of virtual humans with the physicality of mannequins (Figure 1). (Figure Presented) The virtual teammates are composed of comprise a minitower desktop for computation, networking, and rendering, a 40′ TV for display, and a Microsoft Kinect® (version 2) for tracking. All of these components are mounted onto a TV stand. Additionally, a Sennheiser DW-Pro 1 wireless headset is used for speech capture. ANDI's torso, arms, and head are rendered using a virtual human model from Autodesk's Character Generator. The virtual teammate's legs were physical and were composed of shoes and pants filled with stuffing. The physical props were used to integrate the virtual teammate into the user's space. A series of studies evaluated the social presence impact of ANDI design decisions and the current system configuration was shown to provide a virtual teammate with which participants reported a high sense of presence [5]. The virtual teammates' audio responses are pre-recorded by voice talent, and gestures are generated using motion capture and professionally key-framed animations. The virtual teammates can gaze at whoever is speaking, and intermittently glance at the other team members. They also blink and mimic idle motions when not speaking. We will examine results from studies evaluating the perception of virtual teammates, lessons learned in integrating such systems into hospital training, and areas for future research. © 2016 IEEE.","Healthcare; Team training; Virtual humans","Anesthesiology; Computer graphics; Data mining; E-learning; Health care; Human computer interaction; In situ processing; Intensive care units; Natural language processing systems; Nursing; Operating rooms; Personnel training; Rendering (computer graphics); Communication skills; Hardware and software; NAtural language processing; System configurations; Team training; University of Florida; University of Virginia; Virtual humans; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85016990802
"Antoniou P.E., Dafli E., Bamidis P.D.","56237199800;35753237600;6603398831;","Design of novel teaching episodes in medical education using emerging experiential digital assets; technology enabled medical education beyond the Gimmicky",2015,"Proceedings - 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015",,, 7363280,"1560","1565",,2,"10.1109/CIT/IUCC/DASC/PICOM.2015.360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964308542&doi=10.1109%2fCIT%2fIUCC%2fDASC%2fPICOM.2015.360&partnerID=40&md5=bb402e284e22108407dcce7ed5955125","Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece","Antoniou, P.E., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece; Dafli, E., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece; Bamidis, P.D., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece","Medical education has always been about experiential hands on training to prepare future doctors to create the necessary skillset to deal with the sensitive and immediate nature of their work. Contemporary experiential technologies such as Virtual and Augmented reality offer a realistic but consequence free test-bed in which to interact safely and cope with emotional challenges pertaining to the realistic tasks simulated through these media. This work describes the design guidelines and workflow for incorporating these novel virtual assets in medical education through serious role play learning episodes. This approach consists of the case selection, the identification of roles, information flow and narrative requirements, implementation of technological narrative tools and implementation of the learning episode. Using a specific example of case transfer through this model we describe the process, outline implementation, assessment and technological considerations. Finally rationale for this work as a counterweight to technological hype fluctuations and integration of these media into the mainstream curricula is discussed. © 2015 IEEE.","Autgmented reality; Medical education; Serious role play; Virtual reality; Virtual worlds","Augmented reality; Curricula; Education; Medical education; Personnel training; Ubiquitous computing; Virtual reality; Autgmented reality; Case selections; Digital assets; Hands-on-trainings; Information flows; Role play; Virtual and augmented reality; Virtual worlds; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-84964308542
"Rousset T., Bourdin C., Goulon C., Monnoyer J., Vercher J.-L.","57063135900;6603566648;26535970800;57063170900;7004221343;","Does virtual reality affect visual perception of egocentric distance?",2015,"2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings",,, 7223403,"277","278",,3,"10.1109/VR.2015.7223403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954502515&doi=10.1109%2fVR.2015.7223403&partnerID=40&md5=7108f93a962da5ba619a2614530680f4","Aix Marseille Université, CNRS, ISM UMR 7287, Marseille, 13288, France; PSA Peugeot Citroën, Velizy Villacoublay, France","Rousset, T., Aix Marseille Université, CNRS, ISM UMR 7287, Marseille, 13288, France; Bourdin, C., Aix Marseille Université, CNRS, ISM UMR 7287, Marseille, 13288, France; Goulon, C., Aix Marseille Université, CNRS, ISM UMR 7287, Marseille, 13288, France; Monnoyer, J., PSA Peugeot Citroën, Velizy Villacoublay, France; Vercher, J.-L., Aix Marseille Université, CNRS, ISM UMR 7287, Marseille, 13288, France","Virtual reality (driving simulators) tends to generalize for the study of human behavior in mobility. It is thus crucial to ensure that perception of space and motion is little or not affected by the virtual environment (VE). The aim of this study was to determine a metrics of distance perception in VEs and whether this metrics depends on interactive factors: stereoscopy and motion parallax. After a training session, participants were asked, while driving, to estimate the relative location (5 to 80 m) of a car on the same road. The overall results suggest that distance perception in this range does not depend on interactive factors. In average, as generally reported, subjects underestimated the distances whatever the vision conditions. However, the study revealed a large interpersonal variability: two profiles of participants were defined, those who quite accurately perceived distances in VR and those who underestimated distances as usually reported. Overall, this classification was correlated to the level of performance of participants during the training phase. Furthermore, learning performance is predictive of the behavior of participants. © 2015 IEEE.","distance perception; Driving simulation; parallax; stereoscopy; variability","Behavioral research; Depth perception; Geometrical optics; Stereo image processing; Distance perception; Driving simulation; Driving simulator; Learning performance; parallax; Perceived distances; Relative location; variability; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84954502515
"Bleser G., Damen D., Behera A., Hendeby G., Mura K., Miezal M., Gee A., Petersen N., Maçães G., Domingues H., Gorecky D., Almeida L., Mayol-Cuevas W., Calway A., Cohn A.G., Hogg D.C., Stricker D.","22950167800;25654582700;8684934900;15925569800;55523862900;54788071500;24821358900;35318283600;50961471900;56765803700;36095668000;57206479891;6507899137;6602753391;7005699215;7103188000;6701489212;","Cognitive learning, monitoring and assistance of industrial workflows using egocentric sensor networks",2015,"PLoS ONE","10","6", e0127769,"","",,29,"10.1371/journal.pone.0127769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938704361&doi=10.1371%2fjournal.pone.0127769&partnerID=40&md5=e5314c0fab73a82e8d2900fcea2fd628","Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Department of Computer Science, University of Bristol, Bristol, United Kingdom; School of Computing, University of Leeds, Leeds, United Kingdom; Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden; Department of Electrical Engineering, Linköping University, Linköping, Sweden; SmartFactory KL e.V., Kaiserslautern, Germany; Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal","Bleser, G., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany, Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Damen, D., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Behera, A., School of Computing, University of Leeds, Leeds, United Kingdom, Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Hendeby, G., Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden; Mura, K., SmartFactory KL e.V., Kaiserslautern, Germany; Miezal, M., Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Gee, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Petersen, N., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Maçães, G., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Domingues, H., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Gorecky, D., SmartFactory KL e.V., Kaiserslautern, Germany; Almeida, L., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Mayol-Cuevas, W., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Calway, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Cohn, A.G., School of Computing, University of Leeds, Leeds, United Kingdom; Hogg, D.C., School of Computing, University of Leeds, Leeds, United Kingdom; Stricker, D., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany","Today, the workflows that are involved in industrial assembly and production activities are becoming increasingly complex. To efficiently and safely perform these workflows is demanding on the workers, in particular when it comes to infrequent or repetitive tasks. This burden on the workers can be eased by introducing smart assistance systems. This article presents a scalable concept and an integrated system demonstrator designed for this purpose. The basic idea is to learn workflows from observing multiple expert operators and then transfer the learnt workflow models to novice users. Being entirely learning-based, the proposed system can be applied to various tasks and domains. The above idea has been realized in a prototype, which combines components pushing the state of the art of hardware and software designed with interoperability in mind. The emphasis of this article is on the algorithms developed for the prototype: 1) fusion of inertial and visual sensor information from an on-body sensor network (BSN) to robustly track the user's pose in magnetically polluted environments; 2) learning-based computer vision algorithms to map the workspace, localize the sensor with respect to the workspace and capture objects, even as they are carried; 3) domain-independent and robust workflow recovery and monitoring algorithms based on spatiotemporal pairwise relations deduced from object and user movement with respect to the scene; and 4) context-sensitive augmented reality (AR) user feedback using a head-mounted display (HMD). A distinguishing key feature of the developed algorithms is that they all operate solely on data from the on-body sensor network and that no external instrumentation is needed. The feasibility of the chosen approach for the complete action-perception-feedback loop is demonstrated on three increasingly complex datasets representing manual industrial tasks. These limited size datasets indicate and highlight the potential of the chosen technology as a combined entity as well as point out limitations of the system. © 2015 Bleser et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; cognition; computer interface; computer program; egocentric sensor networks; feedback system; female; head mounted display; human; image display; industrial production; information system; information technology; learning algorithm; male; on body sensor network; online monitoring; operator; sensor; spatiotemporal analysis; virtual reality; visual information; workflow; algorithm; learning; occupational health; occupational medicine; system analysis; three dimensional imaging; workflow; Algorithms; Cognition; Humans; Imaging, Three-Dimensional; Learning; Occupational Health; Occupational Medicine; Systems Integration; User-Computer Interface; Workflow",Article,"Final","",Scopus,2-s2.0-84938704361
"Gentile G., Björnsdotter M., Petkova V.I., Abdulkarim Z., Ehrsson H.H.","36989536700;27567506800;25825450600;56509475300;7007134305;","Patterns of neural activity in the human ventral premotor cortex reflect a whole-body multisensory percept",2015,"NeuroImage","109",,,"328","340",,38,"10.1016/j.neuroimage.2015.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922463779&doi=10.1016%2fj.neuroimage.2015.01.008&partnerID=40&md5=4c65474e56f43dbb2462d0237dc2ddb6","Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden; Linköping University, Linköping, Sweden","Gentile, G., Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden; Björnsdotter, M., Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden, Linköping University, Linköping, Sweden; Petkova, V.I., Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden; Abdulkarim, Z., Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden; Ehrsson, H.H., Brain, Body, and Self Laboratory, Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden","Previous research has shown that the integration of multisensory signals from the body in fronto-parietal association areas underlies the perception of a body part as belonging to one's physical self. What are the neural mechanisms that enable the perception of one's entire body as a unified entity? In one behavioral and one fMRI multivoxel pattern analysis experiment, we used a full-body illusion to investigate how congruent visuo-tactile signals from a single body part facilitate the emergence of the sense of ownership of the entire body. To elicit this illusion, participants viewed the body of a mannequin from the first-person perspective via head-mounted displays while synchronous touches were applied to the hand, abdomen, or leg of the bodies of the participant and the mannequin; asynchronous visuo-tactile stimuli served as controls. The psychometric data indicated that the participants perceived ownership of the entire artificial body regardless of the body segment that received the synchronous visuo-tactile stimuli. Based on multivoxel pattern analysis, we found that the neural responses in the left ventral premotor cortex displayed illusion-specific activity patterns that generalized across all tested pairs of body parts. Crucially, a tripartite generalization analysis revealed the whole-body specificity of these premotor activity patterns. Finally, we also identified multivoxel patterns in the premotor, intraparietal, and lateral occipital cortices and in the putamen that reflected multisensory responses specific to individual body parts. Based on these results, we propose that the dynamic formation of a whole-body percept may be mediated by neuronal populations in the ventral premotor cortex that contain visuo-tactile receptive fields encompassing multiple body segments. © 2014.",,"abdomen; adult; Article; controlled study; depth perception; female; frontal lobe; functional magnetic resonance imaging; hand; human; illusion; leg; male; nerve potential; occipital cortex; parietal lobe; perception; premotor cortex; psychometry; putamen; spatiotemporal analysis; stimulus generalization; body image; brain; brain mapping; motor cortex; nuclear magnetic resonance imaging; physiology; touch; vision; young adult; Adult; Body Image; Brain; Brain Mapping; Female; Humans; Illusions; Magnetic Resonance Imaging; Male; Motor Cortex; Touch Perception; Visual Perception; Young Adult",Article,"Final","",Scopus,2-s2.0-84922463779
"Freina L., Canessa A.","26665429300;57210766595;","Immersive vs desktop virtual reality in game based learning",2015,"Proceedings of the European Conference on Games-based Learning","2015-January",,,"195","202",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955151400&partnerID=40&md5=f62727021ff0a306eef5ab32c8a4bce7","CNR-ITD, Genova, Italy; BioLab - DIBRIS, Università degli Studi di Genova, Italy","Freina, L., CNR-ITD, Genova, Italy; Canessa, A., BioLab - DIBRIS, Università degli Studi di Genova, Italy","Virtual environments are recognized as more effective than other digital approaches for the acquisition of several abilities. This is because the brain recognizes the virtual world as real and this facilitates the transfer of the newly acquired skills to the real world. In this paper, we present a game that has been designed and developed with the aim of teaching spatial orientation abilities to teenagers with mild intellectual impairments. In particular, the game focuses on the training of two basic skills: Perspective taking and mental rotation. Perspective taking refers to the ability of imagining how the world looks like from another person's point of view, while mental rotation is the ability to mentally represent and manipulate physical objects in one's mind. The game, which takes place in a virtual environment, shows the player a scene with some objects on the table. The player has to choose among four provided alternatives, the one that shows how the scene would look like from a different side of the table. The game was first developed to be used with either a desktop pc monitor or an interactive touch table. In this case, a virtual world is represented, but the player is not completely immersed in it, he just looks at the scene from outside. A second version of the same game has then been developed using a Head Mounted Display (HMD), which makes the player feel immersed in the virtual environment, where he can freely move around just as if it was real. In this paper, we discuss both advantages and disadvantages of the immersive Virtual Reality (VR) compared to the desktop VR. In fact, on the one hand, having the possibility to ""dive"" into the virtual world allows the player to: Better build a mental model of the scene and the involved objects by freely moving around the table and examining the objects from all the possible perspectives; Manage by himself the amount of help needed: It is always possible, at any time of the game, to move to the other side of the table and see what the scene looks like. Increase his involvement in the game by exploring the virtual world as he pleases. Have a better learning transfer thanks to the similarities between the virtual and the real worlds. On the other hand, using a HMD can be tiring and cause sickness to some players. Furthermore, the presence of a complete environment in which to move and explore, can draw the attention away from the main task of the game and therefore influence learning negatively. Experiments are planned to verify the foreseen advantages and disadvantages involving young adults with mild intellective disabilities.","Innovative games-based learning; Mental rotation; Perspective taking; Virtual worlds","Helmet mounted displays; Interactive computer graphics; Personnel training; Sensory perception; Desktop virtual reality; Head mounted displays; Immersive virtual reality; Innovative games-based learning; Mental rotation; Perspective taking; Spatial orientations; Virtual worlds; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84955151400
"Abed H., Pernelle P., Carron T., Benamar C., Kechiche M., Baert P.","57069934400;36138922300;14631830600;25959856600;56108613500;55177962500;","Serious game framework focusing on industrial traing: Application to steel industry",2015,"Proceedings of the European Conference on Games-based Learning","2015-January",,,"1","9",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955134969&partnerID=40&md5=34cec6144ed65704c9435f52ce269e5f","LIP6, UPMC, Paris, France; DISP, University of Lyon 1, Villeurbanne, France; REGIM, ENI, Sfax, Tunisia; ENISE, Saint-etienne, France","Abed, H., LIP6, UPMC, Paris, France; Pernelle, P., DISP, University of Lyon 1, Villeurbanne, France; Carron, T., LIP6, UPMC, Paris, France; Benamar, C., REGIM, ENI, Sfax, Tunisia; Kechiche, M., ENISE, Saint-etienne, France; Baert, P., ENISE, Saint-etienne, France","Serious games are growing more and more in the context of lifelong training and initial education. They cover several areas (human science, engineering science, life science, ...) that are used for industrial or academic purposes. However, some fields induce specific issues. Thus, in the industrial area, the constraints inherent to the activity impact the development of a scenario and implementation of a serious gaming environment. Indeed, the objective of the industry training must both lead to the acquisition of knowledge and the transfer of skills. Moreover, the actual validation of these skills is paramount especially if their uses are located on an industrial site, where there are often risks associated with the security of persons and equipment. In many industries, some regulatory constraints impose an obligation of means for the training of staff. In the sector of production, the proliferation of interim requires inevitably targeted training. Finally, it should be noted that even for permanent staff, alternation and fragmentation of training periods seriously complicate the deep learning task. Finally, we can wonder if the serious game can bring relevant answers to specific problems of training in an industrial context? This article offers some answers to this question. Thus, in this work, we propose a serious game scripting framework adapted to the industrial context. This scripting framework is structured around two approaches: The first defines a global framework for scheduling a fun or playful scenario. Moreover, this framework allows to take into account the phases of availability of learners while maintaining motivation. The second approach defines an immersive framework for validating acquired and security compliance that is based on two complementary purposes: Use of alternate observation activities (games / real) and an immersive simulation (Virtual Reality) for security. In partnership with a company in the steel industry, we have developed a prototype of serious game in order to implement this scripting framework. The prototype is based on a generic game platform, on a tablet with use of RFID tag, and with an immersive virtual reality Head-Mounted-Display (HMD type Oculus®). In this article we will present the actions realized in the context of a professional activity related to the manipulation of a bridge crane.","Formatting; Interaction; Serious games; Virtual reality","Helmet mounted displays; Steelmaking; Virtual reality; Engineering science; Formatting; Head mounted displays; Immersive virtual reality; Interaction; Professional activities; Security compliance; Serious games; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-84955134969
"Kretch K.S., Adolph K.E.","36053114500;7006292490;","Active vision in passive locomotion: Real-world free viewing in infants and adults",2015,"Developmental Science","18","5",,"736","750",,22,"10.1111/desc.12251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938074291&doi=10.1111%2fdesc.12251&partnerID=40&md5=354c7c6501815667be803212061bba7b","Department of Psychology, New York University, United States","Kretch, K.S., Department of Psychology, New York University, United States; Adolph, K.E., Department of Psychology, New York University, United States","Visual exploration in infants and adults has been studied using two very different paradigms: free viewing of flat screen displays in desk-mounted eye-tracking studies and real-world visual guidance of action in head-mounted eye-tracking studies. To test whether classic findings from screen-based studies generalize to real-world visual exploration and to compare natural visual exploration in infants and adults, we tested observers in a new paradigm that combines critical aspects of both previous techniques: free viewing during real-world visual exploration. Mothers and their 9-month-old infants wore head-mounted eye trackers while mothers carried their infants in a forward-facing infant carrier through a series of indoor hallways. Demands for visual guidance of action were minimal in mothers and absent for infants, so both engaged in free viewing while moving through the environment. Similar to screen-based studies, during free viewing in the real world low-level saliency was related to gaze direction. In contrast to screen-based studies, only infants - not adults - were biased to look at people, participants of both ages did not show a classic center bias, and mothers and infants did not display high levels of inter-observer consistency. Results indicate that several aspects of visual exploration of a flat screen display do not generalize to visual exploration in the real world. © 2014 John Wiley & Sons Ltd.",,"adult; aging; attention; depth perception; environment; eye movement; female; human; infant; locomotion; male; physiology; videorecording; vision; young adult; Adult; Aging; Attention; Environment; Eye Movements; Female; Humans; Infant; Locomotion; Male; Space Perception; Video Recording; Vision, Ocular; Young Adult",Article,"Final","",Scopus,2-s2.0-84938074291
"Li S., Chen Y., Whittinghill D., Vorvoreanu M.","56301468400;56301553700;43462227400;15053834400;","A pilot study exploring augmented reality to increase motivation of Chinese college students learning English",2015,"Computers in Education Journal","6","1",,"23","33",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052799998&partnerID=40&md5=0f3b5cdd673ccb1a1e8c6f20199654e4","Computer Graphics Technology Department, Purdue University, United States","Li, S., Computer Graphics Technology Department, Purdue University, United States; Chen, Y., Computer Graphics Technology Department, Purdue University, United States; Whittinghill, D., Computer Graphics Technology Department, Purdue University, United States; Vorvoreanu, M., Computer Graphics Technology Department, Purdue University, United States","With the advent and accelerated development of augmented reality (AR), an increasing number of studies have been conducted to test the effectiveness of this technique in education. Few, however, have investigated how AR might influence students' motivation toward the learning of a second language. To address this gap in the literature, we used a combination of convenience sampling and criterion sampling to select five Chinese college students to evaluate an English vocabulary learning application built upon augmented reality technology. To assess student motivation, the ARCS motivational model was adopted. A semi-structured interview with open-ended questions was used to collect data. Participants indicated that though they were attracted by this tool at the beginning, their motivation level decreased toward the end of the study. An interpretation of our observations in the context of the ARCS model suggests three motivational issues. First, predefined AR materials failed to establish relevance to subjects' personal interests and previous experiences. Secondly, subjects' confidence seemed to have been negatively influenced due to their difficulty in achieving the stated learning objectives. Lastly, technical issues delayed the computer quickly identifying the triggering image and thus resulted in a noticeable lack of system responsiveness. It seems this delay decreased subjects' satisfaction and distracted their attention from the learning task. These factors seemed most determinative in compromising AR's effectiveness as a tool to increase student motivation toward English vocabulary learning. It must be stressed that this study is a pilot with too low number of subjects from which to make any binding generalizations. Nonetheless, these findings should provide useful insights toward the successful application of AR in the educational realm. The authors recommend further study with a larger number of subjects with a wider range of vocabulary samples and a more powerful computer capable of more quickly identifying the trigger image. © 2015 American Society for Engineering Education. All rights reserved.","ARCS Model; Augmented reality; English vocabulary learning; Learning motivation","Augmented reality; Computer aided instruction; Education computing; Motivation; ARCS model; Augmented reality technology; Learning motivation; Learning objectives; Open-ended questions; Semi structured interviews; Student motivation; Vocabulary learning; Students",Article,"Final","",Scopus,2-s2.0-85052799998
"Itoh Y., Klinker G.","56154865900;6603530980;","Performance and sensitivity analysis of INDICA: INteraction-Free DIsplay CAlibration for Optical See-Through Head-Mounted Displays",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948424,"171","176",,20,"10.1109/ISMAR.2014.6948424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945128507&doi=10.1109%2fISMAR.2014.6948424&partnerID=40&md5=a5f7e94ab15349a3b47d9c354073238e","Technische Universität München, Germany","Itoh, Y., Technische Universität München, Germany; Klinker, G., Technische Universität München, Germany","An issue in AR applications with Optical See-Through Head-Mounted Display (OST-HMD) is to correctly project 3D information to the current viewpoint of the user. Manual calibration methods give the projection as a black box which explains observed 2D-3D relationships well (Fig. 1). Recently, we have proposed an INteraction-free DIsplay CAlibration method (INDICA) for OST-HMD, utilizing camera-based eye tracking [7]. It reformulates the projection in two ways: a black box with an actual eye model (Recycle Setup), and a combination of an explicit display model and an eye model (Full Setup). Although we have shown the former performs more stably than a repeated SPAAM calibration, we could not yet prove whether the same holds for the Full Setup. More importantly, it is still unclear how the error in the calibration parameters affects the final results. Thus, the users can not know how accurately they need to estimate each parameter in practice. We provide: (1) the fact that the Full Setup performs as accurately as the Recycle Setup under a marker-based display calibration, (2) an error sensitivity analysis for both SPAAM and INDICA over the on-/offline parameters, and (3) an investigation of the theoretical sensitivity on an OST-HMD justified by the real measurements. © 2014 IEEE.","augmented; H.5.1 [Information Interfaces and Presentation]; Multimedia Information Systems - Artificial; virtual realities","Augmented reality; Calibration; Recycling; Sensitivity analysis; Sensory perception; Street traffic control; Technology transfer; Virtual reality; augmented; Calibration parameters; Display calibrations; Error sensitivity analysis; H.5.1 [Information interfaces and presentation]; Manual calibration; Multimedia information systems-artificial; Optical see-through head-mounted displays; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-84945128507
"Kurz D.","57197559989;","Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948403,"9","16",,8,"10.1109/ISMAR.2014.6948403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945143898&doi=10.1109%2fISMAR.2014.6948403&partnerID=40&md5=a116f7a8c70547675604edd4ef89e20f","Metaio GmbH, South Korea","Kurz, D., Metaio GmbH, South Korea","We present an approach that makes any real object a true touch interface for mobile Augmented Reality applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on visual object tracking using a visible light camera. Finally the 3D position of the touch is used by human machine interfaces for Augmented Reality providing natural means to interact with real and virtual objects. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. Based on tests with a variety of different materials and different users, we show that our method enables intuitive interaction for mobile Augmented Reality with most common objects. © 2014 IEEE.","Artificial, augmented, virtual realities - Evaluation/methodology; H.5.1 [Multimedia Information Systems]; H.5.2 [User Interfaces]: Input devices and strategies - Graphical user interfaces","Graphical user interfaces; Heat transfer; Helmet mounted displays; Thermography (imaging); Touch screens; User interfaces; Virtual reality; Wearable computers; Wearable technology; Evaluation/methodology; H.5.1 [multimedia information systems]; Handheld augmented realities; Human Machine Interface; Input devices and strategies; Intuitive interaction; Mobile augmented reality; Visual object tracking; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84945143898
"Covaci A., Olivier A.-H., Multon F.","56419559200;15761037000;6602128151;","Third person view and guidance for more natural motor behaviour in immersive basketball playing",2014,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"55","64",,19,"10.1145/2671015.2671023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911165254&doi=10.1145%2f2671015.2671023&partnerID=40&md5=96d2c2541f6beaa265768ac7208a6e8b","Middlesex University London, United Kingdom; Inria, France; M2S Lab, University Rennes2, Inria, France","Covaci, A., Middlesex University London, United Kingdom; Olivier, A.-H., Inria, France; Multon, F., M2S Lab, University Rennes2, Inria, France","The use of Virtual Reality (VR) in sports training is now widely studied with the perspective to transfer motor skills learned in virtual environments (VEs) to real practice. However precision motor tasks that require high accuracy have been rarely studied in the context of VE, especially in Large Screen Image Display (LSID) platforms. An example of such a motor task is the basketball free throw, where the player has to throw a ball in a 46cm wide basket placed at 4.2m away from her. In order to determine the best VE training conditions for this type of skill, we proposed and compared three training paradigms. These training conditions were used to compare the combinations of different user perspectives: first (1PP) and third-person (3PP) perspectives, and the effectiveness of visual guidance. We analysed the performance of eleven amateur subjects who performed series of free throws in a real and immersive 1:1 scale environment under the proposed conditions. The results show that ball speed at the moment of the release in 1PP was significantly lower compared to real world, supporting the hypothesis that distance is underestimated in large screen VEs. However ball speed in 3PP condition was more similar to the real condition, especially if combined with guidance feedback. Moreover, when guidance information was proposed, the subjects released the ball at higher - and closer to optimal - position (5-7% higher compared to no-guidance conditions). This type of information contributes to better understand the impact of visual feedback on the motor performance of users who wish to train motor skills using immersive environments. Moreover, this information can be used by exergames designers who wish to develop coaching systems to transfer motor skills learned in VEs to real practice. Copyright © 2014 by the Association for Computing Machinery, Inc (ACM).","Basketball training; Immersive room; Perception of distance in VR; Performance; Visual feedback","Virtual reality; Visual communication; Guidance feedbacks; Guidance information; Immersive; Immersive environment; Motor performance; Performance; Training conditions; Visual feedback; Sports",Conference Paper,"Final","",Scopus,2-s2.0-84911165254
"Kelly J.W., Hammel W.W., Siegel Z.D., Sjolund L.A.","55346243800;56095556100;56094810300;55547706500;","Recalibration of perceived distance in virtual environments occurs rapidly and transfers asymmetrically across scale",2014,"IEEE Transactions on Visualization and Computer Graphics","20","4", 6777445,"588","595",,35,"10.1109/TVCG.2014.36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897409643&doi=10.1109%2fTVCG.2014.36&partnerID=40&md5=20a9acf9c3a6fc0511fd9c4b60755273","Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Department of Psychology, Iowa State University, Ames, IA 50011, United States","Kelly, J.W., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Hammel, W.W., Department of Psychology, Iowa State University, Ames, IA 50011, United States; Siegel, Z.D., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Sjolund, L.A., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States","Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance. © 2014 IEEE.","Distance perception; recalibration; virtual reality","Depth perception; Experiments; Visual communication; Current projects; Distance perception; Immersive virtual reality; Improved distance; Perceived distances; Recalibrations; Visual feedback; Walking through; Virtual reality",Article,"Final","",Scopus,2-s2.0-84897409643
"Trojan J., Diers M., Fuchs X., Bach F., Bekrater-Bodmann R., Foell J., Kamping S., Rance M., Maaß H., Flor H.","56262402900;6603232814;56154824700;55695210800;42260980200;44760992500;6507437083;36791108000;7005301066;7006743137;","An augmented reality home-training system based on the mirror training and imagery approach",2014,"Behavior Research Methods","46","3",,"634","640",,28,"10.3758/s13428-013-0412-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904767056&doi=10.3758%2fs13428-013-0412-4&partnerID=40&md5=fe577624a9748b6fbfbe2bdb194b15b8","Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Department of Psychology, University of Koblenz-Landau, Landau, Germany; Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Psychology, Florida State University, Tallahassee, FL, United States","Trojan, J., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany, Department of Psychology, University of Koblenz-Landau, Landau, Germany; Diers, M., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Fuchs, X., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Bach, F., Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Bekrater-Bodmann, R., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Foell, J., Department of Psychology, Florida State University, Tallahassee, FL, United States; Kamping, S., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Rance, M., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Maaß, H., Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Flor, H., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany","Mirror training and movement imagery have been demonstrated to be effective in treating several clinical conditions, such as phantom limb pain, stroke-induced hemiparesis, and complex regional pain syndrome. This article presents an augmented reality home-training system based on the mirror and imagery treatment approaches for hand training. A head-mounted display equipped with cameras captures one hand held in front of the body, mirrors this hand, and displays it in real time in a set of four different training tasks: (1) flexing fingers in a predefined sequence, (2) moving the hand into a posture fitting into a silhouette template, (3) driving a ""Snake"" video game with the index finger, and (4) grasping and moving a virtual ball. The system records task performance and transfers these data to a central server via the Internet, allowing monitoring of training progress. We evaluated the system by having 7 healthy participants train with it over the course of ten sessions of 15-min duration. No technical problems emerged during this time. Performance indicators showed that the system achieves a good balance between relatively easy and more challenging tasks and that participants improved significantly over the training sessions. This suggests that the system is well suited to maintain motivation in patients, especially when it is used for a prolonged period of time. © 2013 The Author(s).","Augmented reality; Complex regional pain syndrome; Imagery; Mirror training; Phantom limb pain; Rehabilitation; Stroke; Virtual reality","adult; agnosia; article; cerebrovascular accident; complex regional pain syndrome; equipment design; female; finger; hand; hand strength; human; male; middle aged; movement (physiology); paresis; physiology; psychotherapy; recreation; reproducibility; young adult; Adult; Complex Regional Pain Syndromes; Equipment Design; Female; Fingers; Hand; Hand Strength; Humans; Imagery (Psychotherapy); Male; Middle Aged; Movement; Paresis; Phantom Limb; Reproducibility of Results; Stroke; Video Games; Young Adult",Article,"Final","",Scopus,2-s2.0-84904767056
"Larrue F., Sauzeon H., Wallet G., Foloppe D., Cazalets J.-R., Gross C., N'Kaoua B.","36139685200;7801452039;26425357400;55226000800;7003443071;7402265400;6603602499;","Influence of body-centered information on the transfer of spatial learning from a virtual to a real environment",2014,"Journal of Cognitive Psychology","26","8",,"906","918",,12,"10.1080/20445911.2014.965714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911413165&doi=10.1080%2f20445911.2014.965714&partnerID=40&md5=41a995e0a6dd9f99b864c6b847a8c3b1","Université Bordeaux, Handicap et Système Nerveux, EA 4136, Bordeaux, F-33000, France; INSERM, IFR Handicap, Handicap et Système Nerveux, Bordeaux, F-33000, France; Inria, Equipe Phoenix, Talence, F-33400, France; Université Bordeaux, CNRS UMR 5287, Institut de Neurosciences Cognitives et Intégratives d'Aquitaine, Bordeaux, F-33000, France; Laboratoire de Psychologie des Pays de Loire (UPRES EA 4638), LUNAM Université, Université d'Angers, Angers, France","Larrue, F., Université Bordeaux, Handicap et Système Nerveux, EA 4136, Bordeaux, F-33000, France, INSERM, IFR Handicap, Handicap et Système Nerveux, Bordeaux, F-33000, France; Sauzeon, H., Université Bordeaux, Handicap et Système Nerveux, EA 4136, Bordeaux, F-33000, France, INSERM, IFR Handicap, Handicap et Système Nerveux, Bordeaux, F-33000, France, Inria, Equipe Phoenix, Talence, F-33400, France; Wallet, G., Université Bordeaux, Handicap et Système Nerveux, EA 4136, Bordeaux, F-33000, France; Foloppe, D., Laboratoire de Psychologie des Pays de Loire (UPRES EA 4638), LUNAM Université, Université d'Angers, Angers, France; Cazalets, J.-R., Université Bordeaux, CNRS UMR 5287, Institut de Neurosciences Cognitives et Intégratives d'Aquitaine, Bordeaux, F-33000, France; Gross, C., Université Bordeaux, CNRS UMR 5287, Institut de Neurosciences Cognitives et Intégratives d'Aquitaine, Bordeaux, F-33000, France; N'Kaoua, B., Université Bordeaux, Handicap et Système Nerveux, EA 4136, Bordeaux, F-33000, France, INSERM, IFR Handicap, Handicap et Système Nerveux, Bordeaux, F-33000, France, Inria, Equipe Phoenix, Talence, F-33400, France","This study investigated the effects of body-centred information on the transfer of spatial learning using a wayfinding task and tasks that specifically probe the route and survey strategies of navigation. The subject learned a route in either a real or a virtual environment (VE; 3D scale model of a Bordeaux neighbourhood) and then reproduced it in the real environment. The involvement of body-based information was manipulated across the spatial learning conditions in the VE: participants learned with full body-based information (treadmill with rotation), with the translational component only (treadmill without rotation) or without body-based information (joystick). In the wayfinding task, the results showed a significant effect of the learning environment with the best scores obtained in the real and treadmill with rotation conditions. There was no significant difference between these two conditions, but the real condition was significantly different from the treadmill without rotation and joystick conditions. Also, the visual flow was sufficient to successfully perform the two egocentric tasks used as well as a direction estimation task (a survey task), in so far as there is no significant difference between the joystick and the treadmill conditions. By contrast, the distance estimates were improved by the treadmill condition including the translational component (but not the rotational component). Finally, our results show that treadmill with rotation promotes the transfer of spatial learning from a virtual to a real environment (compared to joystick and treadmill without rotation). Moreover, body-centred informations are more involved in allocentric (distance estimates) than egocentric navigational strategies. © 2014 Taylor & Francis.","Body-based information; Spatial memory and navigation; Virtual reality; Walking interface","adult; Article; body centered information; controlled study; ego; environment; female; human; information; learning environment; male; normal human; optic flow; spatial learning; spatial memory; task performance; treadmill; virtual reality; walking; young adult; head movement; human experiment; priority journal; treadmill test",Article,"Final","",Scopus,2-s2.0-84911413165
"Li S., Chen Y., Whittinghill D.M., Vorvoreanu M.","56301468400;56301553700;43462227400;15053834400;","Exploring the potential for augmented reality to motivate English vocabulary learning in Chinese college students",2014,"ASEE Annual Conference and Exposition, Conference Proceedings",,,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905166159&partnerID=40&md5=ccacf60a42c5c97267777a8c06ab2fa7","Purdue University, West Lafayette, United States; Dept. of Computer Graphics Technology and Computer and Information Technology, Purdue University, West Lafayette, United States","Li, S., Purdue University, West Lafayette, United States; Chen, Y., Purdue University, West Lafayette, United States; Whittinghill, D.M., Dept. of Computer Graphics Technology and Computer and Information Technology, Purdue University, West Lafayette, United States; Vorvoreanu, M., Purdue University, West Lafayette, United States","With the advent and accelerated development of augmented reality (AR), an increasing number of studies have been conducted to test the effectiveness of this technique in education. Few, however, have investigated how AR might influence students' motivation toward learning of a second language. To address this gap in the literature, we used a combination of convenience sampling and criterion sampling to select five Chinese college students to evaluate an English vocabulary learning application built upon augmented reality technology. To assess student motivation, the ARCS motivational model was adopted. A semi-structured interview with openended questions was used to collect data. Participants indicated that though they were attracted by this tool at the beginning, their motivation level decreased toward the end of the study session. An interpretation of our observations in the context of the ARCS model suggests three motivational issues. First, predefined AR materials failed to establish relevance to subjects' personal interests and previous experiences. Secondly, subjects' confidence seemed to have been negatively influenced due to their difficulty in achieving the stated learning objectives. Lastly, technical issues delayed the computer quickly identifying the triggering image and thus resulted in a noticeable lack of system responsiveness. It seems this delay decreased subjects' satisfaction and distracted their attention from the learning task. These factors seemed most determinative in compromising AR's effectiveness as a tool to increase student motivation toward English vocabulary learning. It must be stressed that this study is a low subject N exploratory pilot not intended to produce binding generalizations. Nonetheless, these findings should provide useful insights toward the successful application of AR in the educational realm and identify potential causal factors that could form the foundation of future experimental research. The authors recommend further study with a larger number of subjects with a wider range of vocabulary sample and a more powerful viewing device capable of more quickly identifying the trigger images. © American Society for Engineering Education, 2014.","ARCS model; Augmented reality; English vocabulary learning; Learning motivation","Augmented reality; Computer aided instruction; Engineering education; Motivation; ARCS model; Augmented reality technology; Experimental research; Learning motivation; Learning objectives; Open-ended questions; Semi structured interviews; Vocabulary learning; Students",Conference Paper,"Final","",Scopus,2-s2.0-84905166159
"Schulz C.M., Schneider E., Kohlbecher S., Hapfelmeier A., Heuser F., Wagner K.J., Kochs E.F., Schneider G.","57213664587;35323116000;25031842100;35798264200;18037080500;55540970800;55884169300;57207906803;","The influence of anaesthetists’ experience on workload, performance and visual attention during simulated critical incidents",2014,"Journal of Clinical Monitoring and Computing","28","5",,"475","480",,11,"10.1007/s10877-013-9443-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919463101&doi=10.1007%2fs10877-013-9443-8&partnerID=40&md5=b62a38e7cccd29a921d1fd9b1d0ec73f","Department of Anaesthesiology, Klinikum Rechts der Isar, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Institute of Clinical Neurosciences, Ludwig-Maximilians-Universität München, Marchionistr. 15, Munich, 81377, Germany; Institute of Medical Statistics and Epidemiology, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Department of Anaesthesiology I, University Witten/Herdecke, Helios Clinic Wuppertal, Heusnerstr. 40, Wuppertal, 42283, Germany","Schulz, C.M., Department of Anaesthesiology, Klinikum Rechts der Isar, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Schneider, E., Institute of Clinical Neurosciences, Ludwig-Maximilians-Universität München, Marchionistr. 15, Munich, 81377, Germany; Kohlbecher, S., Institute of Clinical Neurosciences, Ludwig-Maximilians-Universität München, Marchionistr. 15, Munich, 81377, Germany; Hapfelmeier, A., Institute of Medical Statistics and Epidemiology, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Heuser, F., Department of Anaesthesiology, Klinikum Rechts der Isar, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Wagner, K.J., Department of Anaesthesiology, Klinikum Rechts der Isar, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Kochs, E.F., Department of Anaesthesiology, Klinikum Rechts der Isar, Technische Universität München, Ismaninger Str. 22, Munich, 81675, Germany; Schneider, G., Department of Anaesthesiology I, University Witten/Herdecke, Helios Clinic Wuppertal, Heusnerstr. 40, Wuppertal, 42283, Germany","Development of accurate Situation Awareness (SA) depends on experience and may be impaired during excessive workload. In order to gain adequate SA for decision making and performance, anaesthetists need to distribute visual attention effectively. Therefore, we hypothesized that in more experienced anaesthetists performance is better and increase of physiological workload is less during critical incidents. Additionally, we investigated the relation between physiological workload indicators and distribution of visual attention. In fifteen anaesthetists, the increase of pupil size and heart rate was assessed in course of a simulated critical incident. Simulator log files were used for performance assessment. An eye-tracking device (EyeSeeCam) provided data about the anaesthetists’ distribution of visual attention. Performance was assessed as time until definitive treatment. T tests and multivariate generalized linear models (MANOVA) were used for retrospective statistical analysis. Mean pupil diameter increase was 8.1 % (SD ± 4.3) in the less experienced and 15.8 % (±10.4) in the more experienced subjects (p = 0.191). Mean heart rate increase was 10.2 % (±6.7) and 10.5 % (±8.3, p = 0.956), respectively. Performance did not depend on experience. Pupil diameter and heart rate increases were associated with a shift of visual attention from monitoring towards manual tasks (not significant). For the first time, the following four variables were assessed simultaneously: physiological workload indicators, performance, experience, and distribution of visual attention between “monitoring” and “manual” tasks. However, we were unable to detect significant interactions between these variables. This experimental model could prove valuable in the investigation of gaining and maintaining SA in the operation theatre. © 2013, Springer Science+Business Media New York.","Eye-tracking; EyeSeeCam; Gaze behaviour; Situation awareness; Visual attention; Workload","Anesthesiology; Decision making; Heart; Physiology; Eye-tracking; EyeSeeCam; Gaze behaviours; Situation awareness; Visual Attention; Workload; Behavioral research; airway resistance; anesthesist; Article; attention; blood pressure; crossover procedure; eye tracking; gaze; heart rate; human; job performance; pulmonary shunt; pupil; randomized controlled trial; visual attention; work experience; workload; anaphylaxis; anesthesiology; clinical competence; computer simulation; general anesthesia; medical informatics; oculography; operating room; pathophysiology; physiologic monitoring; retrospective study; task performance; vision; Anaphylaxis; Anesthesia, General; Anesthesiology; Attention; Clinical Competence; Computer Simulation; Eye Movement Measurements; Humans; Medical Informatics; Monitoring, Physiologic; Operating Rooms; Retrospective Studies; Task Performance and Analysis; Visual Perception; Workload",Article,"Final","",Scopus,2-s2.0-84919463101
"Di Stasi L.L., McCamy M.B., Macknik S.L., Mankin J.A., Hooft N., Catena A., Martinez-Conde S.","33067587700;55293614700;6603312653;55902184600;55901559800;6701594417;6603412728;","Saccadic eye movement metrics reflect surgical residents′ fatigue",2014,"Annals of Surgery","259","4",,"824","829",,41,"10.1097/SLA.0000000000000260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895927392&doi=10.1097%2fSLA.0000000000000260&partnerID=40&md5=ba51c91aeb8df6c96c6bc3d0ec4f59e4","Department of Neurobiology, Barrow Neurological Institute, 350 W Thomas Rd, Phoenix, AZ 85013, United States; Cognitive Ergonomics Group, Mind, Brain, and Behavior Research Center (CIMCYC), University of Granada, E-Granada, Spain; Joint Center University of Granada-Spanish Army Training and Doctrine Command, Spain; Department of Neurosurgery, Barrow Neurological Institute, Phoenix, AZ, United States; St.Joseph's Hospital and Medical Center, Phoenix, AZ, United States","Di Stasi, L.L., Department of Neurobiology, Barrow Neurological Institute, 350 W Thomas Rd, Phoenix, AZ 85013, United States, Cognitive Ergonomics Group, Mind, Brain, and Behavior Research Center (CIMCYC), University of Granada, E-Granada, Spain, Joint Center University of Granada-Spanish Army Training and Doctrine Command, Spain; McCamy, M.B., Department of Neurobiology, Barrow Neurological Institute, 350 W Thomas Rd, Phoenix, AZ 85013, United States; Macknik, S.L., Department of Neurobiology, Barrow Neurological Institute, 350 W Thomas Rd, Phoenix, AZ 85013, United States, Department of Neurosurgery, Barrow Neurological Institute, Phoenix, AZ, United States; Mankin, J.A., St.Joseph's Hospital and Medical Center, Phoenix, AZ, United States; Hooft, N., St.Joseph's Hospital and Medical Center, Phoenix, AZ, United States; Catena, A., Cognitive Ergonomics Group, Mind, Brain, and Behavior Research Center (CIMCYC), University of Granada, E-Granada, Spain; Martinez-Conde, S., Department of Neurobiology, Barrow Neurological Institute, 350 W Thomas Rd, Phoenix, AZ 85013, United States","OBJECTIVE:: Little is known about the effects of surgical residentsÊ fatigue on patient safety. We monitored surgical residentsÊ fatigue levels during their call day using (1) eye movement metrics, (2) objective measures of laparoscopic surgical performance, and (3) subjective reports based on standardized questionnaires. BACKGROUND:: Prior attempts to investigate the effects of fatigue on surgical performance have suffered from methodological limitations, including inconsistent definitions and lack of objective measures of fatigue, and nonstandardized measures of surgical performance. Recent research has shown that fatigue can affect the characteristics of saccadic (fast ballistic) eye movements in nonsurgical scenarios. Here we asked whether fatigue induced by time-on-duty (∼24 hours) might affect saccadic metrics in surgical residents. Because saccadic velocity is not under voluntary control, a fatigue index based on saccadic velocity has the potential to provide an accurate and unbiased measure of the residentÊs fatigue level. METHODS:: We measured the eye movements of members of the general surgery resident team at St. JosephÊs Hospital and Medical Center (Phoenix, AZ) (6 males and 6 females), using a head-mounted video eye tracker (similar configuration to a surgical headlight), during the performance of 3 tasks: 2 simulated laparoscopic surgery tasks (peg transfer and precision cutting) and a guided saccade task, before and after their call day. Residents rated their perceived fatigue level every 3 hours throughout their 24-hour shift, using a standardized scale. RESULTS:: Time-on-duty decreased saccadic velocity and increased subjective fatigue but did not affect laparoscopic performance. These results support the hypothesis that saccadic indices reflect graded changes in fatigue. They also indicate that fatigue due to prolonged time-on-duty does not result necessarily in medical error, highlighting the complicated relationship among continuity of care, patient safety, and fatigued providers. CONCLUSIONS:: Our data show, for the first time, that saccadic velocity is a reliable indicator of the subjective fatigue of health care professionals during prolonged time-on-duty. These findings have potential impacts for the development of neuroergonomic tools to detect fatigue among health professionals and in the specifications of future guidelines regarding residentsÊ duty hours. © 2013 by Lippincott Williams & Wilkins.","Eyemetrics; fatigue assessment; neuroergonomics; patient safety; saccades; shift work; surgical skills assessment; time-on-task","accuracy; article; Epworth sleepiness scale; eye movement; fatigue; general surgery; health care personnel; human; job performance; laparoscopic surgery; oculography; patient safety; postgraduate education; priority journal; questionnaire; resident; risk management; saccadic eye movement; sleep waking cycle; Adult; Arizona; Clinical Competence; Fatigue; Female; General Surgery; Humans; Internship and Residency; Laparoscopy; Linear Models; Male; Personnel Staffing and Scheduling; Physicians; Questionnaires; Saccades; Time Factors; Work Schedule Tolerance",Article,"Final","",Scopus,2-s2.0-84895927392
"Zheng B., Tien G., Atkins S.M., Swindells C., Tanin H., Meneghetti A., Qayumi K.A., Neely O., Panton M.","35294347300;35772913000;7102310616;57203349817;37119599100;6701853632;6602302777;46062316500;46062521800;","Surgeon's vigilance in the operating room",2011,"American Journal of Surgery","201","5",,"673","677",,45,"10.1016/j.amjsurg.2011.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955698114&doi=10.1016%2fj.amjsurg.2011.01.016&partnerID=40&md5=bfeba409047cee5edb45898d2478dc48","Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Computing Science, Simon Fraser University, Burnaby, BC, Canada; University of Victoria, Locarna Systems, Inc., Victoria, BC, Canada","Zheng, B., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Tien, G., Computing Science, Simon Fraser University, Burnaby, BC, Canada; Atkins, S.M., Computing Science, Simon Fraser University, Burnaby, BC, Canada; Swindells, C., University of Victoria, Locarna Systems, Inc., Victoria, BC, Canada; Tanin, H., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Meneghetti, A., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Qayumi, K.A., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Neely, O., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada; Panton, M., Department of Surgery, University of British Columbia, 3602-910 W 10th Avenue, Vancouver, BC V5Z 4E3, Canada","Objective: Surgeons' vigilance regarding patient condition was assessed using eye-tracking techniques during a simulated laparoscopic procedure. Methods: Surgeons were required to perform a partial cholecystectomy in a virtual reality trainer (SurgicalSim; METI Inc, Sarasota, FL) while wearing a lightweight head-mounted eye-tracker (Locarna systems Inc, Victoria, British Columbia, Canada). Half of the patients were preprogrammed to present a mildly unstable cardiac condition during the procedure. Surgical performance (evaluated by task time, instrument trajectory, and errors), mental workload (by the National Aeronautics and Space Administration Task Load Index), and eye movement were recorded and compared between 13 experienced and 10 novice surgeons. Results: Experienced surgeons took longer to complete the task and also made more errors. The overall workload reported by surgeons was similar, but expert surgeons reported a higher level of frustration and a lower level of physical demands. Surgeon workload was greater when operating on the unstable patient than on the stable patient. Novices performed faster but focused more of their attention on the surgical task. In contrast, experts glanced more frequently at the anesthetic monitor. Conclusions: This study shows the usefulness of using eye-tracking technology to measure a surgeon's vigilance during an operation. Eye-tracking observations can lead to inferences about a surgeon's behavior for patient safety. The unsatisfactory performance of expert surgeons on the VR simulator suggests that the fidelity of the virtual simulator needs to improve to enable surgeons to transfer their clinical skills. This, in turn, suggests using caution when having clinical experts as instructors to teach skills with virtual simulators. © 2011 Elsevier Inc. All rights reserved.","Eye-tracking; Intraoperating room performance; Patient safety; Simulation; Vigilance","adult; alertness; article; cholecystectomy; controlled study; eye tracking; frustration; heart disease; human; laparoscopy; medical expert; operating room; priority journal; simulation; surgeon; surgical error; workload",Article,"Final","",Scopus,2-s2.0-79955698114
"Mania K., Badariah S., Coxon M., Watten P.","6602471750;10240878700;12790137700;6506615337;","Cognitive transfer of spatial awareness states from immersive virtual environments to reality",2010,"ACM Transactions on Applied Perception","7","2",,"","",,27,"10.1145/1670671.1670673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872409581&doi=10.1145%2f1670671.1670673&partnerID=40&md5=98491225a5534b5c6b72286852e4c6e7","Department of Electronic and Computer Engineering, Technical University of Crete, Kounoupidiana, 73100, Chania, Crete, Greece; University of Sussex UK, Brighton, BN1 9QJ, United Kingdom; York St John University, Lord Mayor's Walk, York, YO31 7EX, United Kingdom","Mania, K., Department of Electronic and Computer Engineering, Technical University of Crete, Kounoupidiana, 73100, Chania, Crete, Greece, University of Sussex UK, Brighton, BN1 9QJ, United Kingdom; Badariah, S., University of Sussex UK, Brighton, BN1 9QJ, United Kingdom; Coxon, M., York St John University, Lord Mayor's Walk, York, YO31 7EX, United Kingdom; Watten, P., University of Sussex UK, Brighton, BN1 9QJ, United Kingdom","An individual's prior experience will influence how new visual information in a scene is perceived and remembered. Accuracy of memory performance per se is an imperfect reflection of the cognitive activity (awareness states) that underlies performance in memory tasks. The aim of this research is to investigate the effect of varied visual fidelity of training environments on the transfer of training to the real world after exposure to immersive simulations representing a real-world scene. A between groups experiment was carried out to explore the effect of rendering quality on measurements of location-based recognition memory for objects and associated states of awareness. The immersive simulation consisted of one room that was either rendered flat-shaded or using radiosity rendering. The simulation was displayed on a stereo head-tracked head mounted display. Post exposure to the synthetic simulation, participants completed a memory recognition task conducted in a real-world scene by physically arranging objects in their physical form in a real-world room. Participants also reported one of four states of awareness following object recognition. They were given several options of awareness states that reflected the level of visual mental imagery involved during retrieval, the familiarity of the recollection and related guesses. The scene incorporated objects that ""fitted"" into the specific context of the real-world scene, referred to as consistent objects, and objects that were not related to the specific context of the real-world scene, referred to as inconsistent objects. A follow-up studywas conducted a week after the initial test. Interestingly, results revealed a higher proportion of correct object recognition associated with mental imagery when participants were exposed to low-fidelity flat-shaded training scenes rather than the radiosity rendered ones. Memory psychology indicates that awareness states based on visual imagery require stronger attentional processing in the first instance than those based on familiarity. A tentative claim would, therefore, be that those immersive environments that are distinctive because of their variation from ""real,"" such as flat-shaded environments, recruit stronger attentional resources. This additional attentional processing may bring about a change in participants' subjective experiences of ""remembering"" when they later transfer the training from that environment into a real-world situation. © 2010 ACM.","Human-computer interaction; Perceptual graphics","Cognitive activities; Head mounted displays; Immersive environment; Immersive virtual environments; Perceptual graphics; Real world situations; Subjective experiences; Transfer of trainings; Human computer interaction; Sensory perception; Virtual reality; Object recognition",Article,"Final","",Scopus,2-s2.0-84872409581
"Francis G., Rash C.E.","35748500900;7004825500;","Cognitive considerations for helmet-mounted display design",2010,"Proceedings of SPIE - The International Society for Optical Engineering","7688",, 76880D,"","",,,"10.1117/12.848930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953641789&doi=10.1117%2f12.848930&partnerID=40&md5=5ee099ddc14b578442a26b45fc2d3347","Psychological Sciences, Purdue University, West Lafayette, IN 47907, United States; U.S. Army Aeromedical Research Laboratory, PO Box 620577, Fort Rucker, AL 36330, United States","Francis, G., Psychological Sciences, Purdue University, West Lafayette, IN 47907, United States; Rash, C.E., U.S. Army Aeromedical Research Laboratory, PO Box 620577, Fort Rucker, AL 36330, United States","Helmet-mounted displays (HMDs) are designed as a tool to increase performance. To achieve this, there must be an accurate transfer of information from the HMD to the user. Ideally, an HMD would be designed to accommodate the abilities and limitations of users' cognitive processes. It is not enough for the information (whether visual, auditory, or tactual) to be displayed; the information must be perceived, attended, remembered, and organized in a way that guides appropriate decision-making, judgment, and action. Following a general overview, specific subtopics of cognition, including perception, attention, memory, knowledge, decision-making, and problem solving are explored within the context of HMDs. © 2010 Copyright SPIE - The International Society for Optical Engineering.","attention; cognition; decision making; Helmet-mounted display; HMD; memory; perception; problem solving","Cognitive process; Perception problems; Transfer of information; Aviators; Decision making; Design; Problem solving; Safety devices; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-77953641789
"Stoelen M.F., Akin D.L.","36175043400;7006727725;","Assessment of fitts law for quantifying combined rotational and translational movements",2010,"Human Factors","52","1",,"63","77",,15,"10.1177/0018720810366560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954189290&doi=10.1177%2f0018720810366560&partnerID=40&md5=e91a74aa18207093954a85c3c3cd24b7","Neutral Buoyancy Research Facility, University of Maryland, 382 Technology Drive, College Park, MD 20742, United States","Stoelen, M.F., Neutral Buoyancy Research Facility, University of Maryland, 382 Technology Drive, College Park, MD 20742, United States; Akin, D.L., Neutral Buoyancy Research Facility, University of Maryland, 382 Technology Drive, College Park, MD 20742, United States","Objective: To develop a model for human performance in combined translational and rotational movements based on Fitts law. Background: Fitts law has been successfully applied to translational movements in the past, providing generalization beyond a specific task as well as performance predictions. For movements involving both translations and rotations, no equivalent theory exists, making comparisons of input devices for these movements more ambiguous. Method: The study consisted of three experiments. In the first two, participants performed either pure translational or pure rotational movements of 1 degree of freedom. The third experiment involved the same movements combined. Results: On average, the performance times for combined movements were equal to the sum of the times for equivalent separate rotational and translational movements. A simple Fitts law equivalent for combined movements with a similar slope as the separate components was proposed. In addition, a significant degree of coordination of the combined movements was found. This had a strong bias toward a parallel execution in 12 out of 13 participants. Conclusion: Combined movements with rotations and translations of 1 degree of freedom can be approximated using a simple Fitts law equivalent. The rotational and translational components appear to be coordinated by the central nervous system to generate a parallel execution. Application: The results may help drive human interface designs and provide insights into the coordination of combined movements. Future extensions may be possible for the movements of higher degrees of freedom used in robot teleoperation and virtual reality applications. © 2010, Human Factors and Ergonomics Society.","central nervous system; combined movements; computer systems; degrees of freedom; Fitts law; human performance modeling; human-computer interaction (HCI); input devices; interface evaluation; movement coordination; performance prediction; robot teleoperation; rotational and translational movements; usability; virtual reality","central nervous system; Central nervous systems; Degrees of freedom; Human performance modeling; Input devices; Interface evaluation; Movement coordination; Performance prediction; Robot teleoperation; Experiments; Human robot interaction; Knobs; Knowledge management; Mechanics; Remote control; Rotation; Virtual reality; Human computer interaction; adolescent; adult; article; biological model; biomechanics; clinical trial; computer interface; female; human; information processing; male; movement (physiology); physiology; psychological model; psychomotor performance; reaction time; rotation; Adolescent; Adult; Biomechanics; Data Display; Female; Humans; Male; Models, Neurological; Models, Psychological; Movement; Psychomotor Performance; Reaction Time; Rotation; User-Computer Interface; Young Adult",Article,"Final","",Scopus,2-s2.0-77954189290
"Smit F., van Liere R., Beck S., Froehlich B.","18435203500;57195257466;18433681600;18433968300;","A shared-scene-graph image-warping architecture for VR: Low latency versus image quality",2010,"Computers and Graphics (Pergamon)","34","1",,"3","16",,5,"10.1016/j.cag.2009.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149145853&doi=10.1016%2fj.cag.2009.10.006&partnerID=40&md5=5f88c5a0467d50504dd7fd4c9fffe7d5","CWI, Amsterdam, Netherlands; Bauhaus-Universität Weimar, Germany","Smit, F., CWI, Amsterdam, Netherlands; van Liere, R., CWI, Amsterdam, Netherlands; Beck, S., Bauhaus-Universität Weimar, Germany; Froehlich, B., Bauhaus-Universität Weimar, Germany","Designing low end-to-end latency system architectures for virtual reality is still an open and challenging problem. We describe the design, implementation and evaluation of a client-server depth-image warping architecture that updates and displays the scene graph at the refresh rate of the display. Our approach works for scenes consisting of dynamic and interactive objects. The end-to-end latency is minimized as well as smooth object motion generated. However, this comes at the expense of image quality inherent to warping techniques. To improve image quality, we present a novel way of detecting and resolving occlusion errors due to warping. Furthermore, we investigate the use of asynchronous data transfers to increase the architecture's performance in a multi-GPU setting. Besides polygonal rendering, we also apply image-warping techniques to iso-surface rendering. Finally, we evaluate the architecture and its design trade-offs by comparing latency and image quality to a conventional rendering system. Our experience with the system confirms that the approach facilitates common interaction tasks such as navigation and object manipulation. © 2009 Elsevier Ltd. All rights reserved.","Image-based rendering; Latency; Virtual reality","Air navigation; Architecture; Computer architecture; Data transfer; Economic and social effects; Image enhancement; Image quality; Quality control; Virtual reality; Asynchronous data transfers; End to end latencies; Image based rendering; Interactive objects; Latency; Object manipulation; System architectures; Warping techniques; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-75149145853
"Ziemer C.J., Plumert J.M., Cremer J.F., Kearney J.K.","57203775068;6701849760;7102717840;7101792387;","Estimating distance in real and virtual environments: Does order make a difference?",2009,"Attention, Perception, and Psychophysics","71","5",,"1095","1106",,39,"10.3758/APP.71.5.1096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949170650&doi=10.3758%2fAPP.71.5.1096&partnerID=40&md5=02a4c788f67f7c620b0f58d307950d81","University of Iowa, Iowa City, Iowa, United States","Ziemer, C.J., University of Iowa, Iowa City, Iowa, United States; Plumert, J.M., University of Iowa, Iowa City, Iowa, United States; Cremer, J.F., University of Iowa, Iowa City, Iowa, United States; Kearney, J.K., University of Iowa, Iowa City, Iowa, United States","In this investigation, we examined how the order in which people experience real and virtual environments influences their distance estimates. Participants made two sets of distance estimates in one of the following conditions: (1) real environment first, virtual environment second; (2) virtual environment first, real environment second; (3) real environment first, real environment second; or (4) virtual environment first, virtual environment second. In Experiment 1, the participants imagined how long it would take to walk to targets in real and virtual environments. The participants' first estimates were significantly more accurate in the real than in the virtual environment. When the second environment was the same as the first environment (real-real and virtual- virtual), the participants' second estimates were also more accurate in the real than in the virtual environment. When the second environment differed from the first environment (real-virtual and virtual-real), however, the participants' second estimates did not differ significantly across the two environments. A second experiment, in which the participants walked blindfolded to targets in the real environment and imagined how long it would take to walk to targets in the virtual environment, replicated these results. These subtle yet persistent order effects suggest that memory can play an important role in distance perception. © 2009 The Psychonomic Society, Inc.",,"adult; article; computer interface; decision making; distance perception; ego; female; human; imagination; learning; male; orientation; pattern recognition; perceptive discrimination; sensory deprivation; short term memory; walking; Discrimination (Psychology); Distance Perception; Female; Generalization (Psychology); Humans; Imagination; Judgment; Male; Memory, Short-Term; Orientation; Pattern Recognition, Visual; Reality Testing; Reversal Learning; Sensory Deprivation; User-Computer Interface; Walking; Young Adult",Article,"Final","",Scopus,2-s2.0-68949170650
"Dias P., Campos G., Santos V., Casaleiro R., Seco R., Santos B.S.","22333370800;8603962700;35616433200;24477040900;24477807400;7006476948;","3D reconstruction and Auralisation of the ""painted dolmen"" of antelas",2008,"Proceedings of SPIE - The International Society for Optical Engineering","6805",, 68050Y,"","",,2,"10.1117/12.766607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949110073&doi=10.1117%2f12.766607&partnerID=40&md5=8cf6b741616e6b2cc561071a4c67c4e0","DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal","Dias, P., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Campos, G., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Santos, V., Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal; Casaleiro, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Seco, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Santos, B.S., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal","This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with ""free-space nodes"", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of ""free-space nodes"" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.","3D acquisition; Augmented reality; Auralisation; Laser range finder; Virtual reality","Absorption; Acoustic wave absorption; Acoustics; Architectural acoustics; Audio acoustics; Computational efficiency; Display devices; Energy absorption; Functions; Headphones; Helmet mounted displays; Image reconstruction; Imaging systems; Impulse response; Lasers; Loudspeakers; Medical imaging; Mergers and acquisitions; Range finders; Range finding; Reflection; Restoration; Reverberation; Surfaces; Three dimensional computer graphics; Virtual reality; 3D acquisition; 3D geometries; 3D reconstructions; 3d visual models; Acoustic absorption coefficients; Archaeological sites; Audio sources; Audio visuals; Augmented reality; Auralisation; Dark rooms; Geometric acoustics; Geometric models; Head Related Transfer Functions; In-situ; Irregular surfaces; Iterative algorithms; Iterative Closest points; Laser range finder; Orientation sensors; Reconstruction softwares; Reverberation times; Software computes; Sound waves; Source localisation; Stereo headphones; Uniform grids; Visual scenes; Visualization toolkits; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-47949110073
"Sudra G., Speidel S., Fritz D., Muller-Stich B.P., Gutt C., Dillmann R.","8722467000;22433983800;8657896200;14322034300;7005133005;35599949300;","MEDIASSIST - MEDIcal assistance for intraoperative skill transfer in minimally invasive surgery using augmented reality",2007,"Progress in Biomedical Optics and Imaging - Proceedings of SPIE","6509","PART 2", 65091O,"","",,7,"10.1117/12.709300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048892887&doi=10.1117%2f12.709300&partnerID=40&md5=440be17ab520451036dacbc9fe5ddfa7","Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany","Sudra, G., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Speidel, S., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Fritz, D., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Muller-Stich, B.P., Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Gutt, C., Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Dillmann, R., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany","Minimally invasive surgery is a highly complex medical discipline with various risks for surgeon and patient, but has also numerous advantages on patient-side. The surgeon has to adapt special operation-techniques and deal with difficulties like the complex hand-eye coordination, limited field of view and restricted mobility. To alleviate with these new problems, we propose to support the surgeon's spatial cognition by using augmented reality (AR) techniques to directly visualize virtual objects in the surgical site. In order to generate an intelligent support, it is necessary to have an intraoperative assistance system that recognizes the surgical skills during the intervention and provides context-aware assistance surgeon using AR techniques. With MEDIASSIST we bundle our research activities in the field of intraoperative intelligent support and visualization. Our experimental setup consists of a stereo endoscope, an optical tracking system and a head-mounted-display for 3D visualization. The framework will be used as platform for the development and evaluation of our research in the field of skill recognition and context-aware assistance generation. This includes methods for surgical skill analysis, skill classification, context interpretation as well as assistive visualization and interaction techniques. In this paper we present the objectives of MEDIASSIST and first results in the fields of skill analysis, visualization and multi-modal interaction. In detail we present a markerless instrument tracking for surgical skill analysis as well as visualization techniques and recognition of interaction gestures in an AR environment.","Augmented reality; Calibration; Computer guided surgery; Minimally invasive surgery; Tracking; Visualization","Computer guided surgery; Interaction techniques; Minimally invasive surgery; Health risks; Intelligent control; Neurosurgery; Object recognition; Virtual reality; Medical imaging",Conference Paper,"Final","",Scopus,2-s2.0-35048892887
"Akinladejo F.O.","18433788000;","Computer-based physical therapy: A case study on four post-acute stroke patients",2007,"Conference Proceedings - IEEE SOUTHEASTCON",,, 4147457,"370","377",,,"10.1109/SECON.2007.342927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547689155&doi=10.1109%2fSECON.2007.342927&partnerID=40&md5=b3f2b17a9ae3e6212a1c79a325fa790f","School of Computing and Information Technology, University of Technology Jamaica, 237 Old Hope Road, Kingston 6, Jamaica","Akinladejo, F.O., School of Computing and Information Technology, University of Technology Jamaica, 237 Old Hope Road, Kingston 6, Jamaica","This research investigates the outcome of using a computer-based therapy program in ambulatory training for post-acute stroke patients. Patients with stroke typically suffer dysfunctions that impair the complex set of motions involved in walking. The limited amount of therapy and resources offered by the current health care system do not provide the frequency and intensity of training needed for functional recovery of the walking skills in patients following stroke assaults. This non-traditional intervention research technique therefore sought to develop an alternative method capable of providing the frequency and intensity needed for improving the walking skills in post-acute stroke patients. The work also attempted to show how skills gained in virtual environments transfer to the real world. The work employed the case study method to report the results observed from four post-acute stroke patients who trained on the non-traditional intervention program for about half an hour per day, five days a week, for a period of four consecutive weeks at the out-patient department of the Sir John Golding Rehabilitation Center. The patients performed a computer-based painting exercise with their hemiplegic legs using a head-mounted display, and their gait variables were recorded and analyzed to determine the usefulness of the program in ambulatory training for post-acute stroke patients. A follow up examination conducted one week after the intervention sought to determine whether the patients could perform the skills learned on the computer-based intervention program in the real world. The results of the research showed that all the patients improved on their gait parameters and could walk better. An observational gait analysis conducted one week post-intervention showed that the skills gained in the virtual environment transferred to real-world conditions. The study contributes to the current effort to provide wider access to therapeutic intervention techniques using computer technology. © 2007 IEEE.",,"Diseases; Gait analysis; Patient rehabilitation; Personnel training; Virtual reality; Walking aids; Ambulatory training; Computer based physical therapy; Functional recovery; Health care systems; Patient treatment",Conference Paper,"Final","",Scopus,2-s2.0-34547689155
"Richardson A.R., Waller D.","55418762900;7203026309;","Interaction with an immersive virtual environment corrects users' distance estimates",2007,"Human Factors","49","3",,"507","517",,78,"10.1518/001872007X200139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249060033&doi=10.1518%2f001872007X200139&partnerID=40&md5=73e04dca28393a44f1ee912e20a16e94","Miami University, Oxford, OH, United States; Boeing Company, P. O. Box 3707, Seattle, WA 98124, United States; Boeing Company, Seattle, WA, United States; Psychology Department, Miami University, United States","Richardson, A.R., Miami University, Oxford, OH, United States, Boeing Company, P. O. Box 3707, Seattle, WA 98124, United States, Boeing Company, Seattle, WA, United States; Waller, D., Miami University, Oxford, OH, United States, Psychology Department, Miami University, United States","Objective: Two experiments examined whether prior interaction within an immersive virtual environment (VE) enabled people to improve the accuracy of their distance judgments and whether an improved ability to estimate distance generalized to other means of estimating distances. Background: Prior literature has consistently found that users of immersive VEs underestimate distances by approximately 50%. Method: In each of the two experiments, 16 participants viewed objects in an immersive VE and estimated their distance to them by means of blindfolded walking tasks before and after interacting with the VE. Results: The interaction task significantly corrected users' underestimation bias to nearly veridical. Differences between pre- and postinteraction mean distance estimation accuracy were large (d = 4.63), and significant (p < .001), and they generalized across response task. Conclusion: This finding limits the generality of the underestimation effect in VEs and suggests that distance underestimation in VEs may not be a roadblock to the development of VE applications. Application: Potential or actual applications of this research include the improvement of VE systems requiring accurate spatial awareness. Copyright © 2007, Human Factors and Ergonomics Society. All rights reserved.",,"Distance judgments; Response tasks; Spatial awareness; Distance measurement; Human engineering; Virtual reality; accuracy; adult; article; awareness; depth perception; female; human; human experiment; human factors research; male; normal human; spatial orientation; task performance; virtual reality; walking; Adult; Distance Perception; Female; Humans; Male; Maze Learning; Orientation; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-34249060033
"Fink P.W., Foo P.S., Warren W.H.","7101805360;7004202886;7202784806;","Obstacle Avoidance During Walking in Real and Virtual Environments",2007,"ACM Transactions on Applied Perception","4","1",,"2","",,73,"10.1145/1227134.1227136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980025512&doi=10.1145%2f1227134.1227136&partnerID=40&md5=5c76da40635f56d3f0055f7c40814ce6","Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, Florida 33431, United States; Department of Psychology, University of North Carolina at Asheville, North Cardina 28801, Asheville, United States; Department of Cognitive and Linguistic Sciences, Brown University, Rhode Island 02912, Providence, United States","Fink, P.W., Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, Florida 33431, United States; Foo, P.S., Department of Psychology, University of North Carolina at Asheville, North Cardina 28801, Asheville, United States; Warren, W.H., Department of Cognitive and Linguistic Sciences, Brown University, Rhode Island 02912, Providence, United States","Immersive virtual environments are a promising research tool for the study of perception and action, on the assumption that visual–motor behavior in virtual and real environments is essentially similar. We investigated this issue for locomotor behavior and tested the generality of Fajen and Warren's [2003] steering dynamics model. Participants walked to a stationary goal while avoiding a stationary obstacle in matched physical and virtual environments. There were small, but reliable, differences in locomotor paths, with a larger maximum deviation (Δ = 0.16 m), larger obstacle clearance (Δ = 0.16 m), and slower walking speed (Δ = 0.13 m/s) in the virtual environment. Separate model fits closely captured the mean virtual and physical paths (R2 > 0.98). Simulations implied that the path differences are not because of walking speed or a 50% distance compression in virtual environments, but might be a result of greater uncertainty about the egocentric location of virtual obstacles. On the other hand, paths had similar shapes in the two environments with no difference in median curvature and could be modeled with a single set of parameter values (R2 > 0.95). Fajen and Warren's original parameters successfully generalized to new virtual and physical object configurations (R2 > 0.95). These results justify the use of virtual environments to study locomotor behavior. © 2007, ACM. All rights reserved.","Experimentation; Human Factors; Locomotion; modeling; Perception; virtual reality",,Article,"Final","",Scopus,2-s2.0-84980025512
"Sebok A., Nystad E.","6701790629;15926058300;","Procedural training in virtual reality: A comparison of technology types",2006,"5th International Topical Meeting on Nuclear Plant Instrumentation Controls, and Human Machine Interface Technology (NPIC and HMIT 2006)","2006",,,"1240","1248",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047163611&partnerID=40&md5=ed9be34a0b4e793f61f481de5fff2243","Alion Science and Technology, MA and D Operation, 4949 Pearl East Circle, Boulder, CO 80301, United States; OECD Halden Reactor Project, PO Box 173, N-1751 Halden, Norway","Sebok, A., Alion Science and Technology, MA and D Operation, 4949 Pearl East Circle, Boulder, CO 80301, United States, OECD Halden Reactor Project, PO Box 173, N-1751 Halden, Norway; Nystad, E., Alion Science and Technology, MA and D Operation, 4949 Pearl East Circle, Boulder, CO 80301, United States, OECD Halden Reactor Project, PO Box 173, N-1751 Halden, Norway","This paper describes a study investigating questions of learning effectiveness in different VR technology types. Four VR display technology types were compared in terms of their ability to support procedural learning. The VR systems included two desktop displays (monoscopic and stereoscopic view), a large screen stereoscopic display, and a monoscopic head-mounted display. Twenty-four participants completed procedural training scenarios on these different display types. Training effectiveness was assessed in terms of objective task performance. Following the training session, participants performed the procedure they had just learned using the same VR display type they used for training. Time to complete the procedure and errors were recorded. Retention and transfer of training were evaluated in a talk-through session 24 hours after the training. In addition, subjective questionnaire data were gathered to investigate perceived workload, Sense of Presence, simulator sickness, perceived usability, and ease of navigation. While no difference was found for the short-term learning, the study results indicate that retention and transfer of training were better supported by the large screen stereoscopic condition.",,"Head-mounted display; Procedural training; Simulator sickness; Display devices; Error analysis; Personnel training; Simulators; Stereo vision; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-34047163611
"Bakdash J.Z., Augustyn J.S., Proffitt D.R.","6504035313;25821912500;7005887383;","Large displays enhance spatial knowledge of a virtual environment",2006,"Proceedings - APGV 2006: Symposium on Applied Perception in Graphics and Visualization",,,,"59","62",,19,"10.1145/1140491.1140503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250722532&doi=10.1145%2f1140491.1140503&partnerID=40&md5=d7b070ea83b474ecf26e91d5d0c72921","Department of Psychology, University of Virginia, United States; U.S. Army Natick Soldier Systems Center, United States","Bakdash, J.Z., Department of Psychology, University of Virginia, United States; Augustyn, J.S., U.S. Army Natick Soldier Systems Center, United States; Proffitt, D.R., Department of Psychology, University of Virginia, United States","Previous research has found performance for several egocentric tasks to be superior on physically large displays relative to smaller ones, even when visual angle is held constant. This finding is believed to be due to the more immersive nature of large displays. In our experiment, we examined if using a large display to learn a virtual environment (VE) would improve egocentric knowledge of the target locations. Participants learned the location of five targets by freely exploring a desktop large-scale VE of a city on either a small (25'' diagonally) or large (72'' diagonally) screen. Viewing distance was adjusted so that both displays subtended the same viewing angle. Knowledge of the environment was then assessed using a head-mounted display in virtual reality, by asking participants to stand at each target and paint at the other unseen targets. Angular pointing error was significantly lower when the environment was learned on a 72'' display. Our results suggest that large displays are superior for learning a virtual environment and the advantages of learning an environment on a large display may transfer to navigation in the real world. Copyright © 2006 by the Association for Computing Machinery, Inc.","Display size; Immersion; Navigation; Presence; Virtual reality","Angular pointing error; Egocentric tasks; Immersive nature; Spatial knowledge; Data acquisition; Helmet mounted displays; Knowledge acquisition; Target tracking; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-34250722532
"Mohler B.J., Creem-Regehr S.H., Thompson W.B.","10239373300;6602436425;36985831800;","The influence of feedback on egocentric distance judgments in real and virtual environments",2006,"Proceedings - APGV 2006: Symposium on Applied Perception in Graphics and Visualization",,,,"93","100",,84,"10.1145/1140491.1140493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250700905&doi=10.1145%2f1140491.1140493&partnerID=40&md5=3aeaa29111ed8a2a56507c9e6ce22e2a","University of Utah, United States","Mohler, B.J., University of Utah, United States; Creem-Regehr, S.H., University of Utah, United States; Thompson, W.B., University of Utah, United States","A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications. Copyright © 2006 by the Association for Computing Machinery, Inc.","Adaptation; Feedback; Space perception; Virtual environments","Egocentric distance; Feedback on egocentric distance; Space perception; Adaptive algorithms; Cognitive systems; Feedback; Parameter estimation; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-34250700905
"Sarlat L., Warusfel O., Viaud-Delmon I.","14057073800;14057179800;6603053540;","Ventriloquism aftereffects occur in the rear hemisphere",2006,"Neuroscience Letters","404","3",,"324","329",,15,"10.1016/j.neulet.2006.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746712945&doi=10.1016%2fj.neulet.2006.06.007&partnerID=40&md5=be89133dab8af14e4a07fb76d4119b3c","CNRS - UPMC UMR 7593, Hôpital de la Salpêtrière, 47 boulevard l'Hopital, 75013 Paris, France; IRCAM - CNRS UMR 9912, 1 place Igor Stravinsky, 75004 Paris, France","Sarlat, L., CNRS - UPMC UMR 7593, Hôpital de la Salpêtrière, 47 boulevard l'Hopital, 75013 Paris, France; Warusfel, O., IRCAM - CNRS UMR 9912, 1 place Igor Stravinsky, 75004 Paris, France; Viaud-Delmon, I., CNRS - UPMC UMR 7593, Hôpital de la Salpêtrière, 47 boulevard l'Hopital, 75013 Paris, France, IRCAM - CNRS UMR 9912, 1 place Igor Stravinsky, 75004 Paris, France","After exposure to a consistent spatial disparity of auditory and visual stimuli, subjective localization of sound sources is usually shifted in the direction of the visual stimuli. This study investigates whether such aftereffects can be observed in humans after exposure to a conflicting bimodal stimulation in virtual reality and whether these aftereffects are confined to the trained locations. Fourteen subjects participated in an adaptation experiment, in which auditory stimuli were convolved with non-individual head-related transfer functions, delivered via headphones. First, we assessed the auditory localization of subjects in darkness. They indicated the perceived direction of a sound using an angular pointer. We then immersed the subjects in a virtual environment by means of a head-mounted display. They were asked to reproduce sequences of movements of virtual objects with a mouse click on the objects. However, we introduced a spatial disparity of 15° between the visual event and the concurrent auditory stimulation. After 20 min of exposure, we tested the subjects again in total darkness to determine whether their auditory localization system had been modified by the conflicting visual signals. We observed a shift of subjective localization towards the left in both dorsal and frontal hemifields of the subject, mainly for auditory stimuli located in the right hemispace. This result suggests that interaural difference cues and monaural spectral cues were not equally adapted, and that visual stimuli mainly influence the processing of binaural directional cues of sound localization. © 2006 Elsevier Ireland Ltd. All rights reserved.","Human; Multisensory integration; Sound localization; Space perception; Virtual reality","adult; article; auditory stimulation; darkness; directional hearing; female; hemisphere; human; human experiment; male; movement (physiology); priority journal; sound detection; spatial discrimination; virtual reality; visual stimulation; Acoustic Stimulation; Adaptation, Physiological; Adolescent; Adult; Brain Mapping; Female; Humans; Male; Photic Stimulation; Sound Localization; Visual Perception",Article,"Final","",Scopus,2-s2.0-33746712945
"Glencross M., Chalmers A.G., Lin M.C., Otaduy M.A., Gutierrez D.","12790427400;7102938771;57202428958;6507451572;7005194565;","Exploiting perception in high-fidelity virtual environments",2006,"SIGGRAPH 2006 - ACM SIGGRAPH 2006 Courses",,, 1,"","",,10,"10.1145/1185657.1185814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961784838&doi=10.1145%2f1185657.1185814&partnerID=40&md5=17070b6b6b5e67e8db5422cb0547eee0",,"Glencross, M.; Chalmers, A.G.; Lin, M.C.; Otaduy, M.A.; Gutierrez, D.","The objective of this course is to provide an introduction to the issues that must be considered when building high-fidelity 3D engaging shared virtual environments. The principles of human perception guide important development of algorithms and techniques in collaboration, graphical, auditory, and haptic rendering. We aim to show how human perception is exploited to achieve realism in high fidelity environments within the constraints of available finite computational resources. In this course we address the challenges faced when building such high-fidelity engaging shared virtual environments, especially those that facilitate collaboration and intuitive interaction. We present real applications in which such high-fidelity is essential. With reference to these, we illustrate the significant need for the combination of high-fidelity graphics in real time, better modes of interaction, and appropriate collaboration strategies. After introducing the concept of high-fidelity virtual environments and why these convey important information to the user, we cover the main issues in two parts linked by the common thread of exploiting human perception. First we explore perceptually driven techniques that can be employed to achieve high-fidelity graphical rendering in real-time, and how incorporating authentic lighting effects helps to convey a sense of realism and scale in virtual re-constructions of historical sites. Secondly, we examine how intuitive interaction between participants, and with objects in the environment, also plays a key role in the overall experience. How perceptual methods can be used to guide interest management and distribution choices, is considered with an emphasis on avoiding potential pitfalls when distributing physically-based simulations. An analysis of real network conditions and the implications of these for distribution strategies that facilitate collaboration is presented. Furthermore, we describe technologies necessary to provide intuitive interaction in virtual environments, paying particular attention to engaging multiple sensory modalities, primarily through physically-based sound simulation and perceptually high-fidelity haptic interaction. The combination of realism and intuitive compelling interaction can lead to engaging virtual environments capable of exhibiting skills transfer, an illusive goal of many virtual environment applications.","Collaborative environments; Haptics; High-fidelity rendering; Human-computer interaction; Multi-user; Networked applications; Perception; Virtual reality","Computer graphics; Curricula; Distributed computer systems; Haptic interfaces; Human computer interaction; Interactive computer graphics; Sensory perception; Virtual addresses; Collaborative environments; Haptics; High-fidelity renderings; Multi-user; Networked applications; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84961784838
"Brown L.D., Hua H.","56362429100;7103212541;","Magic lenses for augmented virtual environments",2006,"IEEE Computer Graphics and Applications","26","4",,"64","73",,34,"10.1109/MCG.2006.84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746172973&doi=10.1109%2fMCG.2006.84&partnerID=40&md5=6d24ed9eb3b57f1ea89b9a44acb30945","Department of Computer Science, The University of Arizona, Tucson, AZ, United States; College of Optical Sciences, Department of Computer Science, The University of Arizona, Tucson, AZ, United States","Brown, L.D., Department of Computer Science, The University of Arizona, Tucson, AZ, United States; Hua, H., College of Optical Sciences, Department of Computer Science, The University of Arizona, Tucson, AZ, United States","The authors developed a Magic Lens framework for Scape, an augmented virtual environment (AVE). They generalize the functional characteristics of Magic Lenses in terms of 3D visualization in AVEs and present two tangible Magic Lens-enabled devices with complementary interface capabilities. The authors also demonstrate their Magic Lens devices through testbed applications relevant to urban planning and medical training. © 2006 IEEE.",,"Computer graphics; Lenses; Optical devices; Three dimensional; Augmented virtual environment; Magic lenses; Testbed applications; Three dimensional visualization; Virtual reality; algorithm; article; computer assisted diagnosis; computer graphics; computer interface; computer program; computer simulation; environment; methodology; optical instrumentation; photography; theoretical model; three dimensional imaging; Algorithms; Computer Graphics; Computer Simulation; Environment; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Lenses; Models, Theoretical; Photogrammetry; Software; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33746172973
"Del Río A., Fischer J., Köbele M., Bartz D., Straßer W.","55393205800;8345270000;55758712600;56031275800;7004523663;","Augmented reality interaction for semiautomatic volume classification",2005,"9th International Workshop on Immersive Projection Technology - 11th Eurographics Symposium on Virtual Environments, IPT/EGVE 2005",,,,"113","120",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878735362&partnerID=40&md5=07f911d08719b4b9fe156487f81e3046","WSI/GRIS-VCM, University of Tübingen, Germany","Del Río, A., WSI/GRIS-VCM, University of Tübingen, Germany; Fischer, J., WSI/GRIS-VCM, University of Tübingen, Germany; Köbele, M., WSI/GRIS-VCM, University of Tübingen, Germany; Bartz, D., WSI/GRIS-VCM, University of Tübingen, Germany; Straßer, W., WSI/GRIS-VCM, University of Tübingen, Germany","In the visualization of 3D medical data, the appropriateness of the achieved result is highly dependent on the application. Therefore, an intuitive interaction with the user is of utter importance in order to determine the particular aim of the visualization. In this paper, we present a novel approach for the visualization of 3D medical data with volume rendering combined with AR-based user interaction. The utilization of augmented reality (AR), with the assistance of a set of simple tools, allows the direct manipulation in 3D of the rendered data. The proposed method takes into account regions of interest defined by the user and employs this information to automatically generate an adequate transfer function. Machine learning techniques are utilized for the automatic creation of transfer functions, which are to be used during the classification stage of the rendering pipeline. The validity of the proposed approach for medical applications is illustrated. © The Eurographics Association 2005.",,"Automatic creations; Direct manipulation; Intuitive interaction; Machine learning techniques; Regions of interest; Rendering pipelines; User interaction; Volume classifications; Augmented reality; Learning systems; Medical applications; Optical projectors; Transfer functions; Virtual reality; Visualization; Volume rendering; Data visualization",Conference Paper,"Final","",Scopus,2-s2.0-84878735362
"Hale K.S., Jones D., Stanney K., Milham L.","35373227800;57208458929;7004487266;6507040467;","Adding modalities to VE training systems enhances spatial knowledge acquisition",2005,"Proceedings of the Human Factors and Ergonomics Society",,,,"2253","2257",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44349091461&partnerID=40&md5=7676e57fbf09fd5a440a3538c8c4cf0b","Industrial Engineering and Management Systems Department, University of Central Florida; Design Interactive, Inc.","Hale, K.S., Industrial Engineering and Management Systems Department, University of Central Florida, Design Interactive, Inc.; Jones, D., Industrial Engineering and Management Systems Department, University of Central Florida, Design Interactive, Inc.; Stanney, K., Industrial Engineering and Management Systems Department, University of Central Florida, Design Interactive, Inc.; Milham, L., Design Interactive, Inc.","An empirical study was completed to investigate the effects of audition on spatial knowledge acquisition and workload within a virtual training environment. Four levels of audio were investigated including no audio, non-spatialized audio, and two forms of spatialized audio. While all training conditions led to significant decreases in workload, mental demand associated with knowledge of relative locations of dangerous areas was significantly less when trained with sound cues. The results also indicated that training with generalized spatial audio enhanced concentration. Results from this study outline the benefits of training with metaphoric audio cues to enhance spatial awareness. Future research will empirically examine effects of metaphoric haptic cues.",,"Spatial knowledge acquisition; VE training systems; Knowledge acquisition; Personnel training; Sensory perception; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-44349091461
"Bergamasco M., Perotti S., Avizzano C.A., Angerilli M., Carrozzino M., Ruffaldi E.","7003907071;56685324600;6603815755;6507036695;12800022300;15023001900;","Fork-lift truck simulator for training in industrial environment",2005,"IEEE International Conference on Emerging Technologies and Factory Automation, ETFA","1 2 VOLS",, 1612593,"689","693",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847322431&partnerID=40&md5=34b45331d1e9b0d6d64a230856a954c5","PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy","Bergamasco, M., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy; Perotti, S., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy; Avizzano, C.A., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy; Angerilli, M., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy; Carrozzino, M., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy; Ruffaldi, E., PERCRO, Viale R.Piaggio, 34, 56125 Pontedera (PI), Italy","Since their first usage simulators have been employed in training staff in civil aeronautics and in military fields to improve driving skills without compromising safety of people and machines. The system proposed in this paper is suited to transfer this activity into the industrial field, in particular an innovative fork-lift simulator is presented. Actually most frequent causes of accident with fork-lifts are wrong manoeuvres accomplished by drivers. This simulator aims to improve skills in driving and handling materials using a fork-lift. The system can reply inertial feedback on the operator and allows the user to control all the tasks of a fork-lift as driving and handling materials. The paper presents an overall view of the system including Stewart platform, supporting structure, physical based model, wash-out filter, system architecture and a video system mounted on board of the moving platform. © 2005 IEEE.",,"Fork-lift truck simulator; Stewart platform; Supporting structure; Wash out filters; Automobile drivers; Automobile simulators; Civil aviation; Materials handling; Military operations; Personnel training; Systems analysis; Industrial management",Conference Paper,"Final","",Scopus,2-s2.0-33847322431
"Pierno A.C., Caria A., Glover S., Castiello U.","6507726606;23109962900;7103344807;7006266454;","Effects of increasing visual load on aurally and visually guided target acquisition in a virtual environment",2005,"Applied Ergonomics","36","3",,"335","343",,9,"10.1016/j.apergo.2004.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-18044369879&doi=10.1016%2fj.apergo.2004.11.002&partnerID=40&md5=c5a058419da8a0d5790c3b2026607ead","Department of Psychology, Royal Holloway- University of London, Egham, Surrey, United Kingdom; Dipartimento di Psicologia Generale, Università di Padova, Via Venizia 8, 35131 Padova, Italy","Pierno, A.C., Department of Psychology, Royal Holloway- University of London, Egham, Surrey, United Kingdom; Caria, A., Department of Psychology, Royal Holloway- University of London, Egham, Surrey, United Kingdom; Glover, S., Department of Psychology, Royal Holloway- University of London, Egham, Surrey, United Kingdom; Castiello, U., Department of Psychology, Royal Holloway- University of London, Egham, Surrey, United Kingdom, Dipartimento di Psicologia Generale, Università di Padova, Via Venizia 8, 35131 Padova, Italy","The aim of the present study is to investigate interactions between vision and audition during a target acquisition task performed in a virtual environment. We measured the time taken to locate a visual target (acquisition time) signalled by auditory and/or visual cues in conditions of variable visual load. Visual load was increased by introducing a secondary visual task. The auditory cue was constructed using virtual three-dimensional (3D) sound techniques. The visual cue was constructed in the form of a 3D updating arrow. The results suggested that both auditory and visual cues reduced acquisition time as compared to an uncued condition. Whereas the visual cue elicited faster acquisition time than the auditory cue, the combination of the two cues produced the fastest acquisition time. The introduction of secondary visual task differentially affected acquisition time depending on cue modality. In conditions of high visual load, acquiring a target signalled by the auditory cue led to slower and more error-prone performance than acquiring a target signalled by either the visual cue alone or by both the visual and auditory cues. © 2005 Elsevier Ltd. All rights reserved.","Dual task; Head related transfer functions; Multisensory integration; Target acquisition; Three-dimensional sound; Virtual reality; Visual load","Acoustic waves; Error analysis; Signal processing; Three dimensional; Virtual reality; Auditory cues; Target acquisition; Visual cues; Visual load; Vision; adult; article; association; auditory stimulation; controlled study; error; hearing; human; human experiment; normal human; sound; task performance; time; virtual reality; vision; visual stimulation",Article,"Final","",Scopus,2-s2.0-18044369879
"Pierno A.C., Caria A., Castiello U.","6507726606;23109962900;7006266454;","Comparing effects of 2-D and 3-D visual cues during aurally aided target acquisition",2004,"Human Factors","46","4",,"728","737",,11,"10.1518/hfes.46.4.728.56815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-13144259667&doi=10.1518%2fhfes.46.4.728.56815&partnerID=40&md5=2c30a5f815893f9587819b40941dfd39","Royal Holloway, University of London, Egham, United Kingdom; Department of General Psychology, University of Padua, Via Venezia 8, 35131 Padova, Italy; Department of General Psychology, University of Padua, Padova, Italy; Dept. of Cogn. Sci. and Formation, University of Trento, Italy; University of Genoa, Italy; Department of Psychology, Royal Holloway, University of London, Egham, United Kingdom; University of Melbourne, Australia","Pierno, A.C., Royal Holloway, University of London, Egham, United Kingdom, Department of General Psychology, University of Padua, Via Venezia 8, 35131 Padova, Italy, Department of General Psychology, University of Padua, Padova, Italy; Caria, A., Royal Holloway, University of London, Egham, United Kingdom, Dept. of Cogn. Sci. and Formation, University of Trento, Italy, University of Genoa, Italy; Castiello, U., Royal Holloway, University of London, Egham, United Kingdom, Department of Psychology, Royal Holloway, University of London, Egham, United Kingdom, University of Melbourne, Australia","The aim of the present study was to investigate interactions between vision and audition during a visual target acquisition task performed in a virtual environment. In two experiments, participants were required to perform an acquisition task guided by auditory and/or visual cues. In both experiments the auditory cues were constructed using virtual 3-D sound techniques based on nonindividualized head-related transfer functions. In Experiment 1 the visual cue was constructed in the form of a continuously updated 2-D arrow. In Experiment 2 the visual cue was a nonstereoscopic, perspective-based 3-D arrow. The results suggested that virtual spatial auditory cues reduced acquisition time but were not as effective as the virtual visual cues. Experiencing the 3-D perspective-based arrow rather than the 2-D arrow produced a faster acquisition time not only in the visually aided conditions but also when the auditory cues were presented in isolation. Suggested novel applications include providing 3-D nonstereoscopic, perspective-based visual information on radar displays, which may lead to a better integration with spatial virtual auditory information.",,"Audition; Information dissemination; Radar displays; Transfer functions; Virtual reality; Vision; Auditory cues; Spatial virtual auditory information; Target acquisition; Visual cues; Behavioral research; adult; article; association; hearing; hearing acuity; human; human experiment; normal human; psychophysics; spatial discrimination; task performance; virtual reality; vision; visual adaptation; visual information; Adolescent; Adult; Auditory Perception; Cohort Studies; Cues; Female; Humans; Male; Orientation; Pattern Recognition, Visual; Reaction Time; Sensitivity and Specificity; Task Performance and Analysis; User-Computer Interface; Visual Perception",Article,"Final","",Scopus,2-s2.0-13144259667
"Zhou Z., Cheok A.D., Yang X., Qiu Y.","23986510500;7003447496;22236065700;36724983800;","An experimental study on the role of 3D sound in augmented reality environment",2004,"Interacting with Computers","16","6",,"1043","1068",,23,"10.1016/j.intcom.2004.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-9944230931&doi=10.1016%2fj.intcom.2004.06.016&partnerID=40&md5=c593a65389cf3e596874fc1cfe187c82","Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Zhou, Z., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Cheok, A.D., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Yang, X., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Qiu, Y., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Investigation of augmented reality (AR) environments has become a popular research topic for engineers, computer and cognitive scientists. Although application oriented studies focused on audio AR environments have been published, little work has been done to vigorously study and evaluate the important research questions of the effectiveness of three-dimensional (3D) sound in the AR context, and to what extent the addition of 3D sound would contribute to the AR experience. Thus, we have developed two AR environments and performed vigorous experiments with human subjects to study the effects of 3D sound in the AR context. The study concerns two scenarios. In the first scenario, one participant must use vision only and vision with 3D sound to judge the relative depth of augmented virtual objects. In the second scenario, two participants must cooperate to perform a joint task in a game-based AR environment. Hence, the goals of this study are (1) to access the impact of 3D sound on depth perception in a single-camera AR environment, (2) to study the impact of 3D sound on task performance and the feeling of 'human presence and collaboration', (3) to better understand the role of 3D sound in human-computer and human-human interactions, (4) to investigate if gender can affect the impact of 3D sound in AR environments. The outcomes of this research can have a useful impact on the development of audio AR systems, which provide more immersive, realistic and entertaining experiences by introducing 3D sound. Our results suggest that 3D sound in AR environment significantly improves the accuracy of depth judgment and improves task performance. Our results also suggest that 3D sound contributes significantly to the feeling of human presence and collaboration and helps the subjects to 'identify spatial objects'. © 2004 Elsevier B.V. All rights reserved.","3D sound; Augmented reality; User study","Acoustic waves; Cameras; Cognitive systems; Feedback; Human computer interaction; Multiprocessing systems; Three dimensional; Three dimensional computer graphics; Transfer functions; Head related transfer functions (HRTF); Human-human interactions; Three-dimensional (3D) sound; User study; Virtual reality",Article,"Final","",Scopus,2-s2.0-9944230931
"Simons R., Melzer J.E.","7202166557;7004881040;","HMD-based Training for the U.S. Army's AVCATT-A Collective Aviation Training Simulator",2003,"Proceedings of SPIE - The International Society for Optical Engineering","5079",,,"1","6",,5,"10.1117/12.487192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242467447&doi=10.1117%2f12.487192&partnerID=40&md5=ed9309ebf802ad27c70e33a106f88c2c","Visual Systems Engineer, AVCATT-A, U.S. Army PEO STRI, Orlando, FL, United States; Head-Mounted and Miniature Displays, Kaiser Electro-Opt. Rockwell C.C., Carlsbad, CA, United States","Simons, R., Visual Systems Engineer, AVCATT-A, U.S. Army PEO STRI, Orlando, FL, United States; Melzer, J.E., Head-Mounted and Miniature Displays, Kaiser Electro-Opt. Rockwell C.C., Carlsbad, CA, United States","The Aviation Combined Arms Tactical Trainer - Aviation (AVCATT-A) reconfigurable manned simulator is the Army's newest helicopter training simulator. AVCATT-A is a dynamic reconfigurable system developed for use for combined arms collective training and mission rehearsal through a suite of networked simulators in a simulated battlefield environment. The main purpose of AVCATT-A is to effectively train aviation aircrews in a collective training environment. The AVCATT-A visual system is required to support realistic collective/combined arms training with the required fidelity to fly nap-of-the- earth (NOE) or conduct multi-ship operations. To meet this high level of expectation requires that the visual system be capable of performing all necessary collective tasks and supporting individual tasks with no negative training transfer or physical impacts on crewmembers. In addition to these stringent training requirements for the visual system, the AVCATT-A simulator must be reconfigurable for five different cockpits and be readily deployable. The solution was found in a combination of state-of-the-art technology where the standard projection dome is replaced by a combination of rear-screen projection displays and a Head-Mounted Display (HMD) which provides all the Out-The-Window (OTW) visual imagery.","AVCATT-A; Helicopter simulation and training; HMD; SIM EYE","Cockpits (aircraft); Flight simulators; Image sensors; Military aviation; Military helicopters; Personnel training; Projection screens; Training aircraft; Multifunction displays (MFD); Pilots; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-0242467447
"Aguilar M., Barniv Y., Garrett A.","13906594900;6602101989;7006787625;","Prediction of Pitch and Yaw Head Movements via Recurrent Neural Networks",2003,"Proceedings of the International Joint Conference on Neural Networks","4",,,"2813","2818",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141704220&partnerID=40&md5=06dd51a30052ea378e1de01885f931b1","Knowledge Systems Laboratory, Jacksonville State University, Jacksonville, AL 36265, United States; NASA/Ames Research Center, Human Factors Research Division, Moffett Field, CA 94035-1000, United States; MCIS Department, Jacksonville State University, Jacksonville, AL 36265, United States","Aguilar, M., Knowledge Systems Laboratory, Jacksonville State University, Jacksonville, AL 36265, United States; Barniv, Y., NASA/Ames Research Center, Human Factors Research Division, Moffett Field, CA 94035-1000, United States; Garrett, A., MCIS Department, Jacksonville State University, Jacksonville, AL 36265, United States","In Virtual-Environment (VE) Applications, where virtual objects are presented in a head-mounted display, virtual images must be continuously stabilized in space against the user's head motion. Latencies in head-motion compensation cause virtual objects to swim around instead of being stable in space. This results in an unnatural feel, disorientation, and simulation sickness in addition to errors in fitting/matching of virtual and real objects. Visual update delays are a critical technical obstacle for implementation of head-mounted displays in a wide variety of applications. To address this problem, we propose to use machine learning techniques to define a forward model of head movement based on angular velocity information. In particular, we utilize recurrent neural network to capture the temporal pattern of pitch and yaw motion. Our results demonstrate an ability to predict head motion up to 40 ms. ahead thus eliminating the main source of latencies. The accuracy of the system is tested for conditions akin to those encountered in virtual environments. These results demonstrate successful generalization by the learning system.",,"Learning systems; Mathematical models; Virtual reality; Virtual-environment (VE); Recurrent neural networks",Conference Paper,"Final","",Scopus,2-s2.0-0141704220
"Brewster S., Lumsden J., Bell M., Hall M., Tasker S.","7006514160;35610886600;8598005600;56250522000;7006836962;","Multimodal 'eyes-free' interaction techniques for wearable devices",2003,"Conference on Human Factors in Computing Systems - Proceedings",,,,"473","480",,144,"10.1145/642611.642694","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038713858&doi=10.1145%2f642611.642694&partnerID=40&md5=5678e6f69e54393ab79ba7c616caf9ae","Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; National Research Council, Inst. for Info. Technol. - e-Bus., 46 Dineen Drive, Fredericton, NB E3B 9W4, Canada","Brewster, S., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Lumsden, J., National Research Council, Inst. for Info. Technol. - e-Bus., 46 Dineen Drive, Fredericton, NB E3B 9W4, Canada; Bell, M., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Hall, M., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Tasker, S., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, 'eyes-free' device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users' gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.","Gestural interaction; Wearable computing","Computer keyboards; Gesture recognition; Graphical user interfaces; Mobile computing; Three dimensional; Transfer functions; Gesture recognition; Human computer interaction; Human engineering; Mobile devices; User interfaces; Wearable computers; Eyes free interaction; 3D audio; Gestural interaction; Gesture recognition system; Input/output; Interaction techniques; Interface designs; Multi-modal; Multimodal interaction techniques; Pie menus; Screen space; Task completion time; Visual Attention; Walking speed; Wearable computing; Wearable devices; Wearable computers; Audio acoustics",Conference Paper,"Final","",Scopus,2-s2.0-0038713858
"Arnold P., Farrell M.J., Pettifer S., West A.J.","7202944024;7202679231;6602097064;7202674494;","Performance of a skilled motor task in virtual and real environments",2002,"Ergonomics","45","5",,"348","361",,10,"10.1080/00140130110120510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037091920&doi=10.1080%2f00140130110120510&partnerID=40&md5=134b431ac4de031b9630b406988c9e9b","Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom","Arnold, P., Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Farrell, M.J., Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Pettifer, S., Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; West, A.J., Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom","Three experiments compared the performances of adult participants (three groups of 10) on a perceptuo-motor task in both real world (RW) and virtual environments (VEs). The task involved passing a hoop over a bent wire course, and three versions of the task were used: a 3-D wire course with no background, a flattened version of the 3-D course (2 1/2-D course) with no background, and the 2 1/2-D course with added background to provide spatial context. In all three experiments the participants had to prevent the hoop from touching the wire as they moved it. In the first experiment, the VE condition produced about 18 times more errors than the RW task. The VE 2 1/2-D task was found to be as difficult as the 3-D, and the 2 1/2-D with the added background produced more errors than the other two experiments. Taken together, the experiments demonstrate the difficulty of performing fine motor tasks in VEs, a phenomenon that has not been given due attention in many previous studies of motor control in VEs.","Head mounted display (HMD); Motor skill; Real world analogue; Virtual environment","Motors; Virtual reality; Motor task; Ergonomics; adult; article; comparative study; controlled study; human; human experiment; motor activity; motor control; motor performance; movement perception; perception; physical performance; sensorimotor function; skill; spatial summation; task performance; virtual reality; work environment; Adolescent; Adult; Analysis of Variance; Depth Perception; Humans; Psychomotor Performance; Regression Analysis; Transfer (Psychology); User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0037091920
"Mania K., Chalmers A.","6602471750;7102938771;","The effects of levels of immersion on memory and presence in virtual environments: A reality centered approach",2001,"Cyberpsychology and Behavior","4","2",,"247","264",,119,"10.1089/109493101300117938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035025714&doi=10.1089%2f109493101300117938&partnerID=40&md5=26852521731b4b257f035d8219dcfc28","University of Bristol, Bristol, United Kingdom","Mania, K., University of Bristol, Bristol, United Kingdom; Chalmers, A., University of Bristol, Bristol, United Kingdom","Simulation fidelity is characterized as the extent to which a Virtual Environment (VE) and relevant interactions with it are indistinguishable from a user's interaction with a real environment. The growing number of VE training applications which target a high level of simulation fidelity mainly for transfer of training in the real world have made it crucial to examine the manner in which these particular implementations and designs are evaluated. The methodology presented in this study focuses on real versus simulated virtual worlds comparing participants' level of presence task performance and cognition state employed to complete a memory task. A 15-minute seminar was presented in four different conditions including real 3D desktop 3D Head Mounted Display (HMD) and Audio-only (between-subjects design). Four independent groups of 18 participants took part in the experiment which investigated the effects of levels of immersion on participants' memory recall and memory awareness state (relevant to episodic and semantic memory types) as well as on their perception of the experimental space and sense of presence for every condition. The level of reported presence was not positively associated with accurate memory recall in all conditions although the scores for both presence and seminar memory recall in the ""real"" condition were statistically higher. Memory awareness states' analysis gave a invaluable insight into ""how"" participants remembered both communicated information and space as opposed to ""what, "" most interestingly across specific conditions where results for presence and accurate memory recall were not proven to be significant.",,"article; auditory stimulation; awareness; cognition; female; human; human experiment; image display; immersion; male; memory; normal human; recall; simulation; task performance; virtual reality; Adult; Female; Humans; Internet; Male; Memory; Motion Sickness; Questionnaires; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0035025714
"Gourlay D., Lun K.C., Lee Y.N., Tay J.","15739566700;7004523484;15739807900;57196704926;","Virtual reality for relearning daily living skills",2000,"International Journal of Medical Informatics","60","3",,"255","261",,42,"10.1016/S1386-5056(00)00100-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034532146&doi=10.1016%2fS1386-5056%2800%2900100-3&partnerID=40&md5=e662b908faaf96c9ec30e221d781378e","Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore","Gourlay, D., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Lun, K.C., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Lee, Y.N., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Tay, J., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore","The explosive increase in the power of computers has enabled the creation of fast, interactive 3D environments, sometimes called virtual reality (VR). This technology, often associated with arcade games, is increasingly being used for more serious applications. This paper describes research showing transfer of skills from a virtual environment to the real world. We then describe our VR authoring tool and an application to help cognitively impaired individuals relearn important daily living skills. Additionally we describe the development of a prototype networked system to enable a doctor to monitor remotely the rehabilitation of a group of patients. © 2000 Elsevier Science Ireland Ltd.",,"Computer applications; Health care; Patient monitoring; Patient rehabilitation; Telemedicine; Head mounted display; Traumatic brain injury; Virtual reality; cognitive defect; computer; daily life activity; human; learning; patient monitoring; physician attitude; priority journal; rehabilitation medicine; review; technology; virtual reality; Activities of Daily Living; Brain Injuries; Cerebrovascular Accident; Cognition Disorders; Humans; Learning; Therapy, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-0034532146
"Oman C.M., Shebilske W.L., Richards J.T., Tubré T.C., Beall A.C., Natapoff A.","7003298987;7004118469;7403706715;6506541103;7103222399;6506499780;","Three dimensional spatial memory and learning in real and virtual environments.",2000,"Spatial cognition and computation","2","4",,"355","372",,34,"10.1023/a:1015548105563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244377277&doi=10.1023%2fa%3a1015548105563&partnerID=40&md5=15de2fc503e6da8f30ae6fc0a4d44ad4","Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Oman, C.M., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Shebilske, W.L., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Richards, J.T., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Tubré, T.C., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Beall, A.C., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Natapoff, A., Man Vehicle Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Human orientation and spatial cognition partly depends on our ability to remember sets of visual landmarks and imagine their relationship to us from a different viewpoint. We normally make large body rotations only about a single axis which is aligned with gravity. However, astronauts who try to recognize environments rotated in 3 dimensions report that their terrestrial ability to imagine the relative orientation of remembered landmarks does not easily generalize. The ability of human subjects to learn to mentally rotate a simple array of six objects around them was studied in 1-G laboratory experiments. Subjects were tested in a cubic chamber (n = 73) and a equivalent virtual environment (n = 24), analogous to the interior of a space station node module. A picture of an object was presented at the center of each wall. Subjects had to memorize the spatial relationships among the six objects and learn to predict the direction to a specific object if their body were in a specified 3D orientation. Percent correct learning curves and response times were measured. Most subjects achieved high accuracy from a given viewpoint within 20 trials, regardless of roll orientation, and learned a second view direction with equal or greater ease. Performance of the subject group that used a head mounted display/head tracker was qualitatively similar to that of the second group tested in a physical node simulator. Body position with respect to gravity had a significant but minor effect on performance of each group, suggesting that results may also apply to weightless situations. A correlation was found between task performance measures and conventional paper-and-pencil tests of field independence and 2&3 dimensional figure rotation ability.",,"adolescent; adult; article; behavior; body posture; comparative study; computer interface; depth perception; gravity; human; information processing; learning; memory; NASA Discipline Neuroscience; NASA Program Biomedical Research and Countermeasures; Non-NASA Center; orientation; psychomotor performance; space flight; NASA Discipline Neuroscience; NASA Program Biomedical Research and Countermeasures; Non-NASA Center; Adolescent; Adult; Data Display; Gravitation; Humans; Learning; Memory; Orientation; Posture; Psychomotor Performance; Space Perception; Space Simulation; Spatial Behavior; User-Computer Interface; MLCS; MLOWN",Article,"Final","",Scopus,2-s2.0-16244377277
"Biocca F.","6701454382;","New media technology and youth: Trends in the evolution of new media",2000,"Journal of Adolescent Health","27","2 SUPPL.",,"22","29",,27,"10.1016/S1054-139X(00)00136-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034255822&doi=10.1016%2fS1054-139X%2800%2900136-1&partnerID=40&md5=379aeb571d6a6ed2c6d67b7f67aac7f3","Media Interface/Network Des. Labs, Michigan State University, 409 Commun. Arts and Sci. Building, East Lansing, MI 48824-1292, United States","Biocca, F., Media Interface/Network Des. Labs, Michigan State University, 409 Commun. Arts and Sci. Building, East Lansing, MI 48824-1292, United States","An information environment is emerging from the simultaneous, rapid, and interconnected evolution of transmission systems, interfaces, and content quantity, quality, and structure. It will be easy to underestimate the collective impact of the sum of these changes on how young people communicate and absorb information. Ultimately, it will be more important to understand how these technologies will facilitate, amplify, or alter the cognitive processes and/or social behavior of the Internet generation. The article analyzes the impact of the following trends on media use and cognition among youthful users: Information expansion and overload: Accessible networked information will continue to grow at a rapid pace for at least the next 10-20 years. Rapid increase in interface diffusion: The number of access points into the Internet is expanding in number, variety, and mobility. Evolution toward more embodied computing: Interfaces are evolving to use more of the sensorimotor system to transfer information to and from the user. The evolution of more intelligent sensors to interpret use behavior and intentions. Evolution toward anthropomorphic agent techniques: Computers are evolving to use more social and interpersonal communication techniques to interact with the user. Copyright (C) 2000 Society for Adolescent Medicine.","Evolution of technology; Human-computer interaction; Internet; Media and cognitive development; Virtual reality","adolescent; computer; conference paper; environment; human; information processing; Internet; juvenile; mass communication; priority journal; technology; Adolescent; Child; Female; Forecasting; Humans; Information Science; Internet; Male; Mass Media; Policy Making; Research Design; Technology; United States; User-Computer Interface",Conference Paper,"Final","",Scopus,2-s2.0-0034255822
"McGee J.S., Van Der Zaag C., Buckwalter J.G., Thiebaux M., Van Rooyen A., Neumann U., Sisemore D., Rizzo A.A.","7102023121;6507558646;7103071789;6701770846;6701803717;7103378063;55297389800;57189943380;","Issues for the assessment of visuospatial skills in older adults using virtual environment technology",2000,"Cyberpsychology and Behavior","3","3",,"469","482",,26,"10.1089/10949310050078931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033922544&doi=10.1089%2f10949310050078931&partnerID=40&md5=1fcd853f3831221f78ca63d182897805","Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States; Information Sciences Institute, University of Southern California, Los Angeles, CA, United States; Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Integrated Media Systems Center, University of Southern California, 3715 McClintock Avenue, Los Angeles, CA 90089-0191, United States","McGee, J.S., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States, Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Van Der Zaag, C., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Buckwalter, J.G., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Thiebaux, M., Information Sciences Institute, University of Southern California, Los Angeles, CA, United States; Van Rooyen, A., Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Neumann, U., Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States; Sisemore, D., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Rizzo, A.A., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States, Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States, Integrated Media Systems Center, University of Southern California, 3715 McClintock Avenue, Los Angeles, CA 90089-0191, United States","Virtual Environment (VE) technology offers clinical assessment and rehabilitation options that are currently not available using traditional neuropsychological methods. Advancements in this type of immersive information technology could produce tools that enhance the scientific study of human cognitive/functional processes and improve our capacity to more accurately assess and treat impairments found in persons with central nervous system (CNS) dysfunction. Through the creation of dynamic three-dimensional (3D) stimulus environments, in which all behavioral responding can be recorded, VE technology offers the possibility to more sensitively address a range of age-related CNS disorders including Alzheimer's Disease, Vascular Dementia, Parkinson's Disease, and stroke. Advances in this area could impact quality of life issues for an increasingly aging world population. The VE Laboratory at the University of Southern California has developed a suite of ImmersaDesk-format, 3D projection-based VEs. These scenarios target assessment of visuospatial skills including visual field-specific reaction time, depth perception, 3D field dependency (virtual rod and frame test), static and dynamic manual target tracking in 3D space, and spatial rotation. The current project tested healthy older adults (ages of 65 and 92). Participants were administered a standard neuropsychological battery and a suite of VE-delivered visuospatial tasks. Issues addressed in this project include: the occurrence of VE-related side effects in healthy older adults; the relationship between performance on VE measures and standard neuropsychological tests; the assessment of gender specific performance differences; the relationship between immersive tendencies, presence ratings, and VE performance in older adults; learning and generalization; and VE visuospatial performance differences between younger and older participants. This article will address the motivation, rationale, and relevant issues for use of VEs with older adults. A description of our VE system/methodology in the context of a recent study targeting assessment and possible rehabilitation of visuospatial skills with this population will then be detailed.",,"Alzheimer disease; central nervous system; conference paper; depth perception; information; neuropsychological test; psychologic assessment; rating scale; reaction time; virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0033922544
"Nemire K.","6602500249;","Individual combatant simulator for tactics training and mission rehearsal",1998,"Proceedings of SPIE - The International Society for Optical Engineering","3295",,,"435","441",,1,"10.1117/12.307192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032224291&doi=10.1117%2f12.307192&partnerID=40&md5=fb459fea6a57d3716ddbdcbb1bd3e990","Interface Technologies Corporation, 1840 Forty-First Avenue, Suite 102, Capitola, CA 95010, United States","Nemire, K., Interface Technologies Corporation, 1840 Forty-First Avenue, Suite 102, Capitola, CA 95010, United States","This individual combatant simulator (ICS) provides ground force leaders opportunities to practice tactical skills on the simulated battlefield by directing dismounted computer-generated forces in combatant and non-combatant exercises. Integrated hardware and software systems allow leaders to operate on the simulated battlefield as they would on the physical battlefield using combinations of voice commands, arm signals, virtual tools, and virtual weapons. Hardware components include image generator, head-mounted display, 3-D sound, spatial tracking, instrumented glove, synthesized speech, and voice recognition systems. The training simulator can be operated on a network. Four types of evaluations, including performance of authentic tasks and subjective evaluations, were conducted using dismounted infantry soldiers and university students as participants. The results indicated that the ICS was easy to learn and use, could be used to conduct training exercises, supported skillful performance in training exercises, and was engaging and compelling for the users. These initial evaluations indicated ease of learning and use of the simulator, as well as the potential for training effectiveness.","Dismounted infantry; Electronic battlefield; Immersive; Mission rehearsal; Simulator; Training; Virtual","Computer hardware; Data transfer; Decision making; Interoperability; Military applications; Simulators; Speech recognition; User interfaces; Individual combatant simulator; Tactics training; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0032224291
"Pausch R., Proffitt D., Williams G.","57204319989;7005887383;55469196100;","Quantifying immersion in virtual reality",1997,"Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997",,,,"13","18",,38,"10.1145/258734.258744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009782003&doi=10.1145%2f258734.258744&partnerID=40&md5=8abf9e12747b0103845b53e53bc673a6","Carnegie Mellon University and University of Virginia, United States; University of Virginia, United States; Fakespace, Mountain View, Inc. 241 Polaris AveCA, United States","Pausch, R., Carnegie Mellon University and University of Virginia, United States; Proffitt, D., University of Virginia, United States; Williams, G., Fakespace, Mountain View, Inc. 241 Polaris AveCA, United States","Virtual Reality (VR) has generated much excitement but little formal proof that it is useful. Because VR interfaces are difficult and expensive to build, the computer graphics community needs to be able to predict which applications will benefit from VR. In this paper, we show that users with a VR interface complete a search task faster than users with a stationary monitor and a hand-based input device. We placed users in the center of the virtual room shown in Figure 1 and told them to look for camouflaged targets. VR users did not do significantly better than desktop users. However, when asked to search the room and conclude if a target existed, VR users were substantially better at determining when they had searched the entire room. Desktop users took 41% more time, re-examining areas they had already searched. We also found a positive transfer of training from VR to stationary displays and a negative transfer of training from stationary displays to VR. Copyright © 1997 by the Association for Computing Machinery, Inc.",,"Computer graphics; Virtual reality; Formal proofs; Input devices; Search tasks; Transfer of trainings; Virtual rooms; Interactive computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85009782003
"Pausch Randy, Proffitt Dennis, Williams George","57204319989;7005887383;55469196100;","Quantifying immersion in virtual reality",1997,"Proceedings of the ACM SIGGRAPH Conference on Computer Graphics",,,,"13","18",,155,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030684946&partnerID=40&md5=5a107a9475f718f238fe210450502fab","Univ of Virginia, Charlottesville, United States","Pausch, Randy, Univ of Virginia, Charlottesville, United States; Proffitt, Dennis, Univ of Virginia, Charlottesville, United States; Williams, George, Univ of Virginia, Charlottesville, United States","Virtual Reality (VR) has generated much excitement but little formal proof that it is useful. Because VR interfaces are difficult and expensive to build, the computer graphics community needs to be able to predict which applications will benefit from VR. In this paper, we show that users with a VR interface complete a search task faster than users with a stationary monitor and a hand-based input device. We placed users in the center of the virtual room shown in Figure 1 and told them to look for camouflaged targets. VR users did not do significantly better than desktop users. However, when asked to search the room and conclude if a target existed, VR users were substantially better at determining when they had searched the entire room. Desktop users took 41% more time, re-examining areas they had already searched. We also found a positive transfer of training from VR to stationary displays and a negative transfer of training from stationary displays to VR.",,"Display devices; Graphical user interfaces; Human computer interaction; Subjective testing; Three dimensional computer graphics; Head mounted display (HMD); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0030684946
"Pioch Nicholas J., Roberts Bruce, Zeltzer David","6506235587;7402979005;6701417238;","Virtual environment for learning to pilot remotely operated vehicles",1997,"Proceedings of the Annual International Conference on Virtual Systems and Multimedia, VSMM",,,,"218","226",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030716629&partnerID=40&md5=7f0c52dce04e1b4cb040f24c162b8650","BBN Corp, Cambridge, United States","Pioch, Nicholas J., BBN Corp, Cambridge, United States; Roberts, Bruce, BBN Corp, Cambridge, United States; Zeltzer, David, BBN Corp, Cambridge, United States","Remotely Operated Vehicles (ROVs) are used extensively for underwater search and salvage, inspection, surveying, scientific exploration, and mine countermeasures. ROV pilots must learn to rely on limited data from video and sonar displays and a few other positional indicators to maintain a sense of their vehicle, its tether, and its surroundings. Pilot training typically occurs on-the-job, where equipment is placed at risk, controlled learning situations are hard to create, and time for instruction is minimal. The TRANSoM project is developing a training environment that combines two emerging technologies to overcome these limitations. A Virtual Environment (VE) simulates the ROV and its surroundings as well as instructional enhancements such as external views of the ROV, directional cues, and alternate modes of interaction with the vehicle; e.g. a head-tracked head-mounted display (HMD). An Intelligent Tutoring System (ITS) monitors student pilot behavior and offers verbal and graphical feedback, mission review, and performance assessment. Near-transfer experiments comparing the utility of different artificial viewpoints have been completed, with full transfer experiments involving a real ROV to follow.",,"Artificial intelligence; Computer aided instruction; Computer simulation; Learning systems; Personnel training; Submersibles; Intelligent tutoring systems (ITS); Remotely operated vehicles (ROV); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0030716629
"Szalavári Z., Gervautz M.","6507879085;6701389426;","The personal interaction panel - A two-handed interface for augmented reality",1997,"Computer Graphics Forum","16","3",,"C335","C346",,124,"10.1111/1467-8659.00171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031552897&doi=10.1111%2f1467-8659.00171&partnerID=40&md5=f9ff4637f6c8a299b478e6d5e0613cc3","Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria","Szalavári, Z., Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria; Gervautz, M., Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria","This paper describes the introduction of a new interaction paradigm to augmented reality applications. The everyday tool handling experience of working with pen and notebooks is extended to create a three dimensional two-handed interface, that supports easy-to-understand manipulation tasks in augmented and virtual environments. In the design step we take advantage from the freedom, given by our very low demands on hardware and augment form and functionality to this device. On the basis of examples from object manipulation, augmented research environments and scientific visualization we show the generality of applicability. Although being in the first stages implementation, we consider the wide spectrum of suitability for different purposes.","3D user interface; Augmented reality; Two handed interaction","Interactive computer graphics; Personal computers; Three dimensional computer graphics; User interfaces; Virtual reality; Visualization; Augmented reality; Object manipulation; Two handed interaction; Human computer interaction",Article,"Final","",Scopus,2-s2.0-0031552897
"Nemire Kenneth","6602500249;","Evaluating visual and auditory enhancements to a virtual object- manipulation task",1996,"Proceedings of SPIE - The International Society for Optical Engineering","2653",,,"249","260",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029709617&partnerID=40&md5=abcd4bdef78dba494d05dd3205db9e08","Interface Technologies Corp.,, Capitola, CA, USA, United States","Nemire, Kenneth, Interface Technologies Corp.,, Capitola, CA, USA, United States","Most off-the-shelf immersive virtual environment (IVE) systems do not provide adequate depth cues to allow quick and accurate manual interactions with virtual objects. Studies of teleoperation tasks show that using stereoscopic displays improves performance, especially in situations with increased scene complexity and decreased object visibility. However, many aspects of these studies prevent generalization of the results to IVE systems. Further, the additional costs of high-resolution stereoscopic displays preclude their widespread use in business and educational settings. In this paper, the effects of various visual and auditory display enhancements were evaluated to determine whether they may replace depth peg in one location and placed it on a virtual target in another location, provided a common test situation in which to compare various enhancements. Participants wore a commercial head-mounted display and spatial trackers on the head and hand. Results indicated conditions under which visual and auditory enhancements to monocular displays resulted in performance that was not different from using stereoscopic displays. Theoretical foundations for the findings and implications of the results for other tasks in VEs are discussed.",,"Auditory enhancement; Head-mounted displays; Stereoscopic displays; Virtual environments; Audition; Binocular vision; Display devices; Image enhancement; Performance; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0029709617
"Smith A.Brandon","57198626791;","Using virtual reality for the simulation of infrared environments for human training",1995,"Proceedings of the Annual Southeast Conference",,,,"101","109",,1,"10.1145/1122018.1122036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029194995&doi=10.1145%2f1122018.1122036&partnerID=40&md5=743b5f6665d892a81f96cb967c10f886","Clemson Univ, Clemson, SC, United States","Smith, A.Brandon, Clemson Univ, Clemson, SC, United States","This paper describes a method to dynamically simulate heat transfer and the generation of infrared images, all in real time. Although there do exist models that produce infrared images of environments these models are generally static and only show one view at one moment in time, usually the time at temperature equilibrium. The model presented in this paper however, is not confined to one view point or moment in time. Instead, a user is emersed within a virtual environment, through the use of a head mounted display, and is presented with a dynamically changing scene that reflects the changing temperatures of the objects as they seek equilibrium within their environment. The view point is also dynamic allowing the user to move around the scene, and to interact and change certain elements. In this way the user is free to turn on or off lights, motors, and other heat sources and observe the effects that this will have on the other objects in the environment, all of this in real time.",,"Computer simulation; Data structures; Heat transfer; Image processing; Infrared imaging; Personnel training; Thermal effects; Helmet mounted displays; Infrared environments; Temperature equilibrium; Changing temperature; Head mounted displays; Heat sources; Real time; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0029194995
"Regan E.C.","36485413200;","Some evidence of adaptation to immersion in virtual reality",1995,"Displays","16","3",,"135","139",,28,"10.1016/0141-9382(96)81213-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029333228&doi=10.1016%2f0141-9382%2896%2981213-3&partnerID=40&md5=fe619bd10a4a2b59d7b086181c8b8094","Department of Psychological Sciences, DRA Centre for Human Sciences, DRA Farnborough, Hants GU14 6TD, United Kingdom","Regan, E.C., Department of Psychological Sciences, DRA Centre for Human Sciences, DRA Farnborough, Hants GU14 6TD, United Kingdom","This paper describes a study in which 30 participants, all of whom had reported symptoms of malaise during their first immersion in virtual reality, were re-immersed in the virtual reality system on three further occasions in order to investigate the possibility that repeated immersions may produce a decrease in malaise as participants adapt to immersion in virtual reality. It was found that 18 of the participants reported a decrease in malaise during their second immersion from their highest malaise rating reported during their first immersion. Five reported a decrease during their third second immersion, and 13 reported a decrease during their fourth immersion. Although some participants did report an increase in malaise, whilst others reported no change, the decrease in reported symptoms from the first immersion to the fourth was none the less quite pronounced. Over half of the participants ceased to report symptoms, and reports of nausea became very low. There was very little change in the pattern of reported symptoms between the second and third immersions, but quite marked changes between the first and second immersions and between the third and fourth immersions. © 1995.","adaptation; immersion; virtual reality","Display devices; Health hazards; Health risks; Risk perception; Societies and institutions; Technology transfer; Immersion adaptation; Malaise; Side effects; Virtual reality",Article,"Final","",Scopus,2-s2.0-0029333228
