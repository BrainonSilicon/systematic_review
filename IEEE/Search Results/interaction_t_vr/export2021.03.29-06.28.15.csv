"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Gesture Controlled Virtual Reality Based Conferencing","K. Deherkar; G. Martin; N. George; V. Maurya","Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India","2018 International Conference on Smart City and Emerging Technology (ICSCET)","18 Nov 2018","2018","","","1","4","The technology available today for interacting with a virtual environment involves wired or wireless hand controls with limited buttons or a large setup involving a camera and/or a sensor to capture movements. The cost of such a setup is such that it makes it inaccessible to most. The project aims at providing a cost effective VR solution which can produce the same effect with precision and flexibility, which is accessible to all. The proposed idea is to provide a hardware device that provides the user with an immersive VR experience and uses hand gestures captured via a camera placed on the device to control and interact with the VR environment. As an application of the project, an interactive workspace environment would be simulated, using a single hardware component that provides the user the viewing interface as well as can be controlled by simple hand gestures eliminating the need for additional hand-held devices controls. The project will also delve into the field of supervised learning to make possible the implementation of gesture recognition.","","978-1-5386-1185-2","10.1109/ICSCET.2018.8537334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537334","Virtual Reality;VR;Gesture Recognition;Sensor;Hand Gesture Controls;VOIP;File transfer;Supervised learning","Tracking;Virtual environments;Cameras;Servers;Gesture recognition;Python","gesture recognition;human computer interaction;learning (artificial intelligence);virtual reality","virtual reality;virtual environment;wireless hand controls;cost effective VR solution;hardware device;immersive VR experience;VR environment;interactive workspace environment;single hardware component;gesture recognition;hand gestures;hand-held devices controls;gesture controlled virtual reality based conferencing;supervised learning","","","","20","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Simulation-Based Engineering with Hybrid Testbeds","B. Sondermann; M. Emde; M. Rast; J. Rossmann","Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany","2013 European Modelling Symposium","31 Mar 2014","2013","","","471","476","The development of new software applications in mobile robotics is very challenging due to the complexity oftasks and the multitude of involved components. The hybridtestbed idea, combining the advantages of virtual and realtestbeds, shortens the development cycle with little effort.Hybrid testbeds provide the possibility to exchange real andsimulated components at any time without influencing thefurther data processing pipeline. In this paper, we describea robust communication model based on generic interfacesallowing the transfer of data and commands across system borders. Based on this generic interface concept, hybrid testbeds, containing simulated as well as real hardware components, can easily be set up. Furthermore, the presented concept additionally allows physically separated systems, and thus supports simulation-based engineering combining development strategies like hardware-in-the-loop, co-design and concurrent engineering. The applicability of the introduced approach is shown in the development process of a mobile localization unit. It consists of several sensors and an internal processing unit for position estimation. Starting in a virtual environment, the unit has been implemented by replacing the simulated components successively by their physical counterparts leading to a final setup with real hardware. Finally, the simulation system itself is used to command the localization unit and to visualize the raw sensor and processed data in the virtual environment.","","978-1-4799-2578-0","10.1109/EMS.2013.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779890","virtual testbed;hardware-in-the-loop;simulationbsed control;hardware/software co-design","Hardware;Software;Sensors;Mobile communication;Synchronization;Data models;Testing","concurrent engineering;control engineering computing;data visualisation;hardware-software codesign;mobile robots;user interfaces;virtual reality","simulation-based engineering;hybrid testbeds;software applications;mobile robotics;virtual testbeds;real testbeds;data processing pipeline;robust communication model;generic interfaces;data transfer;command transfer;hardware components;hardware-in-the-loop;hardware-software codesign;concurrent engineering;mobile localization unit;internal processing unit;position estimation;virtual environment;raw sensor data visualization","","7","","16","","31 Mar 2014","","","IEEE","IEEE Conferences"
"Investigating the effect of immersive virtual reality and planning on the outcomes of simulation-based learning: A media and method experiment","J. Nie; B. Wu","East China Normal University,Department of Educational Informational Technology,Shanghai,China; East China Normal University,Department of Educational Informational Technology,Shanghai,China","2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)","4 Aug 2020","2020","","","329","332","The application of the immersive virtual reality (VR) has injected new vitality into educational innovation, but there are also some voices of doubt on its practical learning effects. Considering two aspects of media and instructional methods, the study investigated the effect of immersive virtual reality and planning strategy on simulation-based learning by a 2 × 2 experimental cross - panel design. The results showed that both of them had a significant main effect, indicating that the immersive VR and planning led to better behavioral transfer performance and immersive VR increased sense of presence and self-efficacy as well. No interaction effect between media and method was found.","2161-377X","978-1-7281-6090-0","10.1109/ICALT49669.2020.00106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155971","immersive virtual reality;planning strategy;virtual simulation;simulation-based learning","Xenon","computer aided instruction;virtual reality","immersive VR;interaction effect;immersive virtual reality;simulation-based learning;practical learning effects;2 × 2 experimental cross-panel design;educational innovation;planning strategy","","","","18","","4 Aug 2020","","","IEEE","IEEE Conferences"
"The effects of presentation method and simulation fidelity on psychomotor education in a bimanual metrology training simulation","J. Bertrand; A. Bhargava; K. C. Madathil; A. Gramopadhye; S. V. Babu","Clemson University, USA; Clemson University, USA; Clemson University, USA; Clemson University, USA; Clemson University, USA","2017 IEEE Symposium on 3D User Interfaces (3DUI)","6 Apr 2017","2017","","","59","68","In this study, we empirically evaluated the effects of presentation method and simulation fidelity on task performance and psychomotor skills acquisition in an immersive bimanual simulation towards precision metrology education. In a 2 × 2 experiment design, we investigated a large-screen immersive display (LSID) with a head-mounted display (HMD), and the presence versus absence of gravity. Advantages of the HMD include interacting with the simulation in a more natural manner as compared to using a large-screen immersive display due to the similarities between the interactions afforded in the virtual compared to the real-world task. Suspending the laws of physics may have an effect on usability and in turn could affect learning outcomes. Our dependent variables consisted of a pre and post cognition questionnaire, quantitative performance measures, perceived workload and system usefulness, and a psychomotor assessment to measure to what extent transfer of learning took place from the virtual to the real world. Results indicate that the HMD condition was preferable to the immersive display in several metrics while the no-gravity condition resulted in users adopting strategies that were not advantageous for task performance.","","978-1-5090-6716-9","10.1109/3DUI.2017.7893318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893318","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities","Solid modeling;Training;Aerospace electronics;Resists;Metrology;Visualization;Gravity","computer based training;digital simulation;helmet mounted displays;measurement;virtual reality","presentation method;simulation fidelity;psychomotor education;bimanual metrology training simulation;psychomotor skill acquisition;immersive bimanual simulation;precision metrology education;large-screen immersive display;LSID;head-mounted display;HMD;no-gravity condition","","3","","36","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Interface Design of a Human-Robot Interaction System for Dual-Manipulators Teleoperation Based on Virtual Reality*","F. Bian; R. Li; L. Zhao; Y. Liu; P. Liang","The State Key Lab of Robotics and System, Harbin Institute of Technology, Nangang, Harbin, China; The State Key Lab of Robotics and System, Harbin Institute of Technology, Nangang, Harbin, China; The State Key Lab of Robotics and System, Harbin Institute of Technology, Nangang, Harbin, China; Quanzhou HIT Institute of Engineering and Technology, Fengze, Quanzhou, China; Quanzhou HIT Institute of Engineering and Technology, Fengze, Quanzhou, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","1361","1366","This paper presents a human-robot interaction interface for dual-manipulators teleoperation based on virtual reality. Using this method, the human operator is able to control the robot at a distance to complete complicated tasks in unstructured environment. Virtual reality technology is integrated into the system to provide the first person perspective of the robot to the operator. Based on the three-dimensional coordinates of shoulder, elbow, wrist and hand captured by Kinect, a geometric model of the human arm is built. Then a geometric vector method is proposed to calculate the joint angles of human upper limbs which are translated into movement commands for a robot. To access the performance of our proposed human-robot interaction interface, teleoperation experiments are conducted on the Baxter robot, illustrating the effectiveness and feasibility of our proposed method.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812457","Human-Robot Interaction;Virtual reality;Motion transfer;Teleoperation","Three-dimensional displays;Robot sensing systems;Visualization;Robot kinematics;Head","control engineering computing;human-robot interaction;manipulators;telerobotics;vectors;virtual reality","interface design;human-robot interaction system;dual-manipulators teleoperation;human-robot interaction interface;human operator;virtual reality technology;human arm;geometric vector method;human upper limbs;Baxter robot","","","","16","","26 Aug 2019","","","IEEE","IEEE Conferences"
"“Woodlands” - a Virtual Reality Serious Game Supporting Learning of Practical Road Safety Skills","K. Szczurowski; M. Smith","Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland; Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland","2018 IEEE Games, Entertainment, Media Conference (GEM)","1 Nov 2018","2018","","","1","9","In developed societies road safety skills are taught early and often practiced under the supervision of a parent, providing children with a combination of theoretical and practical knowledge. At some point children will attempt to cross a road unsupervised, at that point in time their safety depends on the effectiveness of their road safety education. To date, various attempts to supplement road safety education with technology were made. Most common approach focus on addressing declarative knowledge, by delivering road safety theory in an engaging fashion. Apart from expanding on text based resources to include instructional videos and animations, some stakeholders (e.g.: Irish Road Safety Authority) attempt to take advantage of game-based learning [1]. However, despite the high capacity for interaction being common in Virtual Environments, available game-based solutions to road safety education are currently limited to delivering and assessing declarative knowledge. With recent advancements in the field of Virtual Reality (VR) Head Mounted Displays, procedural knowledge might also be addressed in Virtual Environments. This paper describes the design and development process of a computer-supported learning system that attempts to address psycho-motor skills involved in crossing a road safely, changing learners' attitude towards road safety best practices, and enabling independent practice of transferable skills. By implementing game-based learning principles and following best practice for serious game design (such as making educational components essential to successful game-play, or instructional scaffolding) we hope to make it not only more effective, but also engaging, allowing us to rely on learners' intrinsic motivation [2], to increase their independent practice time and provide them with feedback that will help to condition safe behaviour and increase retention. Presence in Virtual Reality might evoke responses to Virtual Environment as if it was real (RAIR) [3] and enable learners to truly experience learning scenarios. In consequence leading to formation of autobiographical memories constructed from multisensory input, which should result in an increased knowledge retention and transfer [4].","","978-1-5386-6304-2","10.1109/GEM.2018.8516493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516493","Virtual Reality;VR;Road Safety;Serious Game;Experiential Learning;Game-Based Learning;Virtual Environment","Games;Road safety;Virtual environments;Training","computer aided instruction;helmet mounted displays;road safety;serious games (computing);virtual reality","road safety education;declarative knowledge;road safety theory;road safety best practices;serious game design;virtual reality serious game;virtual environment;virtual reality head mounted displays;Woodlands;game-based learning","","4","","34","","1 Nov 2018","","","IEEE","IEEE Conferences"
"Object location memory error in virtual and real environments","M. Xu; M. Murcia-López; A. Steed","University College London, UK; University College London, UK; University College London, UK","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","315","316","We aim to further explore the transfer of spatial knowledge from virtual to real spaces. Based on previous research on spatial memory in immersive virtual reality (VR) we ran a study that looked at the effect of three locomotion techniques (joystick, pointing-and-teleporting and walking-in-place) on object location learning and recall. Participants were asked to learn the location of a virtual object in a virtual environment (VE). After a short period of time they were asked to recall the location by placing a real version of the object in the real-world equivalent environment. Results indicate that the average placement error, or distance between original and recalled object location, is approximately 20cm for all locomotion technique conditions. This result is similar to the outcome of a previous study on spatial memory in VEs that used real walking. We report this unexpected finding and suggest further work on spatial memory in VR by recommending the replication of this study in different environments and using objects with a wider diversity of properties, including varying sizes and shapes.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892303","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality","Legged locomotion;Virtual environments;Navigation;Visualization;Standards","brain;human computer interaction;realistic images;virtual reality","object location memory error;virtual environment;real environment;spatial knowledge transfer;spatial memory;immersive virtual reality;locomotion technique;joystick;pointing-and-teleporting;walking-in-place;object location learning;object location recall;placement error","","3","","9","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Virtual reality simulation training and assisted surgery: AYRA: Virtual and physical biomodels in surgery","C. Suárez-Mejías; G. G. Ciriza; C. P. Calderón; T. G. Cía; P. Gacto-Sánchez","Innovation Technologies Group, Virgen del Rocío, University Hospital, Seville, Spain; Innovation Technologies Group, Virgen del Rocío, University Hospital, Seville, Spain; Innovation Technologies Group, Virgen del Rocío, University Hospital, Seville, Spain; Plastic and Reconstructive Surgery Department, Virgen del Rocío, University Hospital, Seville, Spain; Plastic and Reconstructive Surgery Department, Virgen del Rocío, University Hospital, Seville, Spain","2012 18th International Conference on Virtual Systems and Multimedia","3 Dec 2012","2012","","","437","444","In this paper we present a surgical application based on virtual reality called AYRA. AYRA allows surgeons training, planning and optimization of surgical procedures. AYRA was developed under a research, development and innovation project financed by the Andalusia Department of Health called VirSSPA. AYRA is very important for the health organization since the surgeons can be trained and helped by AYRA in decision making issues. Nowadays AYRA has been successfully used in more than 489 real cases and surgeons have declared their satisfaction with the results. After proving its efficiency it has been introduced in the clinical practice during the surgical planning sessions at the Virgen del Rocío University Hospital. For example, the use of AYRA reduces by 45 % the complication rate and in two hours the operating room time in breast microvascular reconstruction by DIEAP free flap. The success rate is increased because physicians can see the exact anatomy of the case and the extent of the injury that the patient suffers before proceeding with the surgery. In addition, surgeons can see and interact in 3D with other similar cases made in AYRA by other surgeon colleagues, thus promoting the training of surgeons and the knowledge transfer in this field.","","978-1-4673-2563-9","10.1109/VSMM.2012.6365956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365956","surgeons training;surgical planning;virtual reality;Computer assisted surgery;Simulation based training;biomodel printing","Surgery;Training;Solid modeling;Planning;Servers;DICOM;Virtual reality","biomedical education;computer based training;digital simulation;injuries;knowledge management;medical computing;surgery;virtual reality","virtual reality simulation training;AYRA;virtual biomodels;physical biomodels;surgical application;innovation project;Andalusia Department of Health;VirSSPA health organization;decision making issues;clinical practice;surgical planning sessions;Virgen del Rocio University Hospital;breast microvascular reconstruction;DIEAP free flap;injury;3D interaction;surgeon training;knowledge transfer;computer assisted surgery","","","","36","","3 Dec 2012","","","IEEE","IEEE Conferences"
"Towards Evaluating the Effects of Stereoscopic Viewing and Haptic Interaction on Perception-Action Coordination","D. Brickler; S. V. Babu; J. Bertrand; A. Bhargava",Clemson University; Clemson University; Clemson University; Clemson University,"2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","1","516","This paper details the results of an initial empirical evaluation conducted to examine how stereoscopic viewing and haptic feedback affects fine motor actions in a pick-and-place task, similar to the peg transfer task in an FLS training curriculum for laproscopic sugical training. In a between subjects experiment, we examined the effect of stereoscopic viewing and simulated tactile feedback during the fine motor actions of a participants' actions in the near field on the number of collisions and time to complete the task. We found that stereo and haptic feedback contributed to the effectiveness of task performance in different ways. Specifically, we found that the mean time to complete the trials was significantly higher in the abcense of tactile feedback as compared to when it was present, and the mean number of collisions was significantly higher in the presence of stereo as compared to when it was absent.","","978-1-5386-3365-6","10.1109/VR.2018.8446227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446227","3D Interaction [Haptics]: Human-Computer Interaction-Perception-Action Coordination","Haptic interfaces;Task analysis;Stereo image processing;Three-dimensional displays;Solid modeling;Electronic mail;Performance evaluation","computer based training;feedback;haptic interfaces;medical image processing;stereo image processing;surgery","stereoscopic viewing;haptic feedback;fine motor actions;peg transfer task;FLS training curriculum;laproscopic sugical training;tactile feedback;stereo feedback;task performance;haptic interaction;perception-action coordination;empirical evaluation;pick-and-place task","","1","","3","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Intelligent Human-Computer Interaction for Web-Based Human Meridian Research Support System","G. Chen; X. Zheng; Y. Lin; H. Huang; L. Yu","Coll. of Phys. & Inf. Eng., Fuzhou Univ., Fuzhou, China; Coll. of Phys. & Inf. Eng., Fuzhou Univ., Fuzhou, China; Coll. of Phys. & Inf. Eng., Fuzhou Univ., Fuzhou, China; Coll. of Phys. & Inf. Eng., Fuzhou Univ., Fuzhou, China; Coll. of Phys. & Inf. Eng., Fuzhou Univ., Fuzhou, China","2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery","28 Dec 2009","2009","7","","550","554","Human meridian theory is the kernel of traditional Chinese medicine and is nowadays being accepted by the whole world. With people's increasing interest in studying human meridian theory, the Web-based human meridian research support system is becoming more and more important. How to visualize the meridian information interactively in real time is a great challenge. A new architecture of human-computer interaction based on context-aware is presented in this paper. The system supports the adaptive presentation and data transfer according to the user's context. Binary data that user requests and possibly needs is transferred while network is idle. And only a Web browser with X3D plug-in is needed for using this system. The experiment shows that the system can be used efficiently and friendly.","","978-0-7695-3735-1","10.1109/FSKD.2009.728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360070","Human Meridian;Research Support System;Human-Computer Interaction;Web","Human computer interaction;Virtual reality;Medical diagnostic imaging;Telecommunication computing;Kernel;Costs;Hardware;Internet;Graphics;Usability","human computer interaction;Internet;medicine;ubiquitous computing","human computer interaction;human meridian research support system;Internet;context awareness;X3D plug-in browser;Chinese medicine","","","","18","","28 Dec 2009","","","IEEE","IEEE Conferences"
"Ubii: Physical World Interaction Through Augmented Reality","S. Lin; H. F. Cheng; W. Li; Z. Huang; P. Hui; C. Peylo","HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; Deutsche Telekom AG Laboratories, Ernst-Reuter-Platz 7, Berlin, Germany","IEEE Transactions on Mobile Computing","6 Feb 2017","2017","16","3","872","885","We describe a new set of interaction techniques that allow users to interact with physical objects through augmented reality (AR). Previously, to operate a smart device, physical touch is generally needed and a graphical interface is normally involved. These become limitations and prevent the user from operating a device out of reach or operating multiple devices at once. Ubii (Ubiquitous interface and interaction) is an integrated interface system that connects a network of smart devices together, and allows users to interact with the physical objects using hand gestures. The user wears a smart glass which displays the user interface in an augmented reality view. Hand gestures are captured by the smart glass, and upon recognizing the right gesture input, Ubii will communicate with the connected smart devices to complete the designated operations. Ubii supports common inter-device operations such as file transfer, printing, projecting, as well as device pairing. To improve the overall performance of the system, we implement computation offloading to perform the image processing computation. Our user test shows that Ubii is easy to use and more intuitive than traditional user interfaces. Ubii shortens the operation time on various tasks involving operating physical devices. The novel interaction paradigm attains a seamless interaction between the physical and digital worlds.","1558-0660","","10.1109/TMC.2016.2567378","General Research Fund; Research Grants Council of Hong Kong; Innovation and Technology Fund; Hong Kong Innovation and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469860","Augmented reality;wearable computers;user interfaces;human computer interaction;mobile computing","Computers;Mobile handsets;Glass;Graphical user interfaces;Augmented reality","augmented reality;gesture recognition;interactive devices","augmUbii;augmented reality;smart device;graphical interface;ubiquitous interface and interaction;hand gestures;smart glass;file transfer;printing;projecting;device pairing;image processing","","23","","55","","13 May 2016","","","IEEE","IEEE Journals"
"3D collaborative interaction for aerospace industry","D. Clergeaud; F. Guillaume; P. Guitton",Inria; Airbus Group; Universite de Bordeaux,"2016 IEEE Third VR International Workshop on Collaborative Virtual Environments (3DCVE)","12 Sep 2016","2016","","","13","15","3D collaborative interaction raises issues like awareness of the Virtual Environment that have been well known for some time. They have been the subject of research activities that have led to some interesting results described in the literature. Unfortunately, most of these results have not yet been not implemented in the software systems daily used by industry and remains only theoretical concepts. That is why we recently started a project to transfer some of these results in the aerospace industry for the Airbus Group. Beyond this first transfer target, we also intend to measure the real gains (in real industrial conditions) for the users, and then for the company. This second goal is essential because in most publications, user testing is not satisfying (lack of real users, lack of real procedures, non significant number of subjects). In this paper, we describe the first step of this work in progress and more precisely, the basic interaction features we have developed. Currently, we are designing the first user tests.","","978-1-5090-2138-3","10.1109/3DCVE.2016.7563560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563560","","Collaboration;Three-dimensional displays;Trajectory;Visualization;Cameras;Aerospace industry;Virtual environments","aerospace industry;groupware;virtual reality","basic interaction features;user testing;Airbus group;virtual environment;aerospace industry;3D collaborative interaction","","1","","","","12 Sep 2016","","","IEEE","IEEE Conferences"
"3D simulation-based user interfaces for a highly-reconfigurable industrial assembly cell","C. Schlette; E. G. Kaigom; D. Losch; G. Grinshpun; M. Emde; R. Waspe; N. Wantia; J. Roßmann","Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany; Institute for Man-Machine Interaction (MMI), RWTH Aachen University, 52074, Germany","2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)","7 Nov 2016","2016","","","1","6","Although SMEs would benefit from robotic solutions in assembly, the required invests and efforts for their implementation are often too risky and costly for them. Here, the Horizon 2020 project “ReconCell” aims at developing a new type of highy-reconfigurable multi-robot assembly cell which adresses the particular needs of SMEs. At the Institute for Man- Machine Interaction (MMI), we are developing 3D simulation-based user interfaces for ReconCell as the central technology to enable the fast, easy and safe programming of the system. ReconCell heavily builds on previous developments that are transferred from research and prepared for industrial partners with real use cases and demands. Thus, in this contribution, we describe MMI's software platform that will be the basis of the desired user interfaces for robot simulation and control, assembly simulation and execution, Visual Programming and sensor simulation.","","978-1-5090-1314-2","10.1109/ETFA.2016.7733703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733703","","Robot kinematics;Solid modeling;Robot sensing systems;Dynamics;Three-dimensional displays;User interfaces","CAD;digital simulation;man-machine systems;multi-robot systems;production engineering computing;robotic assembly;small-to-medium enterprises;user interfaces;visual programming","3D simulation-based user interfaces;highly-reconfigurable industrial assembly cell;SMEs;robotic assembly solutions;Horizon 2020 project;ReconCell;highy-reconfigurable multirobot assembly cell;man- machine interaction;system programming;MMI software platform;robot simulation;robot control;assembly simulation;assembly execution;visual programming;sensor simulation","","3","","22","","7 Nov 2016","","","IEEE","IEEE Conferences"
"The role of dimensional symmetry on bimanual psychomotor skills education in immersive virtual environments","J. Bertrand; D. Brickler; S. Babu; K. Madathil; M. Zelaya; T. Wang; J. Wagner; A. Gramopadhye; J. Luo",Clemson University; Clemson University; Clemson University; Clemson University; Clemson University; Clemson University; Clemson University; Clemson University; Clemson University,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","3","10","The need for virtual reality applications for education and training involving bimanual dexterous activities has been increasing in recent years. However, it is unclear how the amount of correspondence between a virtual interaction metaphor to the real-world equivalent, otherwise known as dimensional symmetry, affects bimanual pscyhomotor skills training and how skills learned in the virtual simulation transfer to the real world. How does the number of degrees of freedom enhance or hinder the learning process? Does the increase in dimensional symmetry affect cognitive load? In an empirical evaluation, we compare the effectiveness of a natural 6-DOF interaction metaphor to a simplified 3-DOF metaphor. Our simulation interactively educates users in the step-by-step process of taking a precise measurement using calipers and micrometers in a simulated technical workbench environment. We conducted a usability study to evaluate the user experience and pedagogical benefits using measures including a pre and post cognition questionnaire over all levels of Bloom's taxonomy, workload assessment, system usability, and real world psychomotor assessment tasks. Results from the pre and post cognition questionnaires suggest that learning outcomes improved throughout all levels of Bloom's taxonomy for both conditions, and trends in the data suggest that the 6-DOF metaphor was more effective in real-world skill transference compared to the 3-DOF metaphor.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223317","Bimanual interaction;psychomotor skills education;dimensional symmetry","Atmospheric measurements;Particle measurements;Instruments;Training;Metrology;Solid modeling","computer aided instruction;user interfaces;virtual reality","dimensional symmetry role;bimanual psychomotor skills education;immersive virtual environment;virtual reality applications;education application;training application;virtual simulation;learning process;cognitive load;natural 6-DOF interaction;degrees-of-freedom;calipers;micrometers;user experience;pedagogical benefits;psychomotor assessment tasks;Bloom taxonomy;bimanual dexterous activities","","9","","27","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Impact of World Wide Web, Java, and virtual environments on education in computational science and engineering","T. Singh; M. Zhu; U. Thakkar; U. Ravaioli","Beckman Inst. for Adv. Sci. & Technol., Illinois Univ., Urbana, IL, USA; NA; NA; NA","Technology-Based Re-Engineering Engineering Education Proceedings of Frontiers in Education FIE'96 26th Annual Conference","6 Aug 2002","1996","3","","1007","1010 vol.3","The World Wide Web (WWW) on the Internet has been recognized as an effective environment to create new distributed applications that have the potential to bring instruction beyond the bounds of the classroom. The availability of browsers (e.g. Mosaic, Netscape, Hot Java) has enormously simplified the access to the WWW, and there have been numerous initiatives to take advantage of this new technology for teaching. This work will illustrate recent developments of tools for engineering education and technology transfer which take advantage of WWW browsers, Java applets, and virtual reality. We have developed modules based on WWW browsers incorporating educational and research software, including advanced visualization, which find use for multimedia classroom presentations accessible by Internet users, and which can also improve interaction among academic groups and industry. Therefore, the material is suitable for asynchronous distance learning and technology transfer. Examples of interactive WWW applications include device simulation, semiconductor band structure calculation, numerical techniques, and electromagnetics.","0190-5848","0-7803-3348-9","10.1109/FIE.1996.567665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=567665","","Web sites;Java;Virtual environment;World Wide Web;Internet;Technology transfer;Engineering education;Virtual reality;Visualization;Computer industry","Internet;engineering education;virtual reality;technology transfer;multimedia systems;computer science education","World Wide Web;Java;virtual environments;computational science education;engineering education;Internet;browsers;Mosaic;Netscape;teaching;technology transfer;virtual reality;educational and research software;multimedia classroom presentations;asynchronous distance learning;device simulation;semiconductor band structure calculation;numerical techniques;electromagnetics","","","1","5","","6 Aug 2002","","","IEEE","IEEE Conferences"
"On-Road Evaluation of Autonomous Driving Training","D. Sportillo; A. Paljic; L. Ojeda","PSL Research University, Center for Robotics, MINES ParisTech Groupe PSA, Paris, France; PSL Research University, Center for Robotics, MINES ParisTech, Paris, France; Groupe PSA, Velizy-Villacoublay, France","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","25 Mar 2019","2019","","","182","190","Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology.","2167-2148","978-1-5386-8555-6","10.1109/HRI.2019.8673277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673277","Automated Vehicles;Virtual Reality;Augmented Reality;Transfer of Training;TOR;Wizard of Oz;Human-Vehicle Interaction","Automobiles;Training;Autonomous vehicles;Roads;Automation;Augmented reality","augmented reality;computer based training;traffic engineering computing","driving scenarios;public roads;knowledge acquisition;road evaluation;autonomous driving training;driver interaction;car equipment;on-board video tutorial;VR training;between-subject study;virtual reality simulator;augmented reality program;WoZ protocol;Wizard of Oz;AR training","","3","","36","","25 Mar 2019","","","IEEE","IEEE Conferences"
"Supporting User Interfaces in Ubiquitous Virtual Reality","B. H. Thomas","Wearable Comput. Lab., Univ. of South AustraliaUniversity of South Australia, SA, Australia","2009 International Symposium on Ubiquitous Virtual Reality","4 Sep 2009","2009","","","15","18","Ubiquitous virtual reality focuses on the widespread access of digital information to the user with the fusion and extension of a number of computer science disciplines. This paper will focus on ways users can conveniently and easily transfer between these different modes of interacting with digital information. Each of these domains has a particular display and interaction technologies that current support their form of information presentation. In additional these domains have software and metaphor support for their user interfaces. This paper will focus on methodologies to perform a number of tasks: 1) transitions between different presentations of information, 2) unifying technologies to better bring together these domains, and 3) articulate the important aspects of interactions within a ubiquitous virtual reality system. This paper will explore a number of possible technologies, such as input devices, clipboard technologies, and software frameworks. The paper will also highlight areas that need future exploration and possible pitfalls to avoid.","","978-1-4244-4437-3","10.1109/ISUVR.2009.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232246","User Interface;Ubiquitous Virtual Reality;Ubiquitous Computing","User interfaces;Virtual reality;Wearable computers;Augmented reality;Pervasive computing;Computer science;Computer displays;Ubiquitous computing;Haptic interfaces;Mobile computing","ubiquitous computing;user interfaces;virtual reality","ubiquitous virtual reality system;user interface;digital information interaction;software framework;input device;clipboard technology;information presentation;computer science discipline","","","","15","","4 Sep 2009","","","IEEE","IEEE Conferences"
"Audio-visual attractors for capturing attention to the screens when walking in CAVE systems","F. Grani; S. Serafin; F. Argelaguet; V. Gouranton; M. Badawi; R. Gaugne; A. Lecuyer","Aalborg University Copenhagen; Aalborg University Copenhagen; Inria-IRISA, Rennes; Inria-IRISA, Rennes; Inria-IRISA, Rennes; Inria-IRISA, Rennes; Inria-IRISA, Rennes","2014 IEEE VR Workshop: Sonic Interaction in Virtual Environments (SIVE)","15 Jan 2015","2014","","","3","6","In four-sided CAVE-like VR systems, the absence of the rear wall has been shown to decrease the level of immersion and can introduce breaks in presence. In this paper it is investigated to which extent user's attention can be driven by visual and auditory stimuli in a four-sided CAVE-like system. An experiment was conducted in order to analyze how user attention is diverted while physically walking in a virtual environment, when audio and/or visual attractors are present. The foursided CAVE used in the experiment allowed to walk up to 9m in straight line. An additional key feature in the experiment is the fact that auditory feedback was delivered through binaural audio rendering techniques via non-personalized head related transfer functions (HRTFs). The audio rendering was dependent on the user's head position and orientation, enabling localized sound rendering. The experiment analyzed how different ""attractors"" (audio and/or visual, static or dynamic) modify the user's attention. The results of the conducted experiment show that audio-visual attractors are the most efficient attractors in order to keep the user's attention toward the inside of the CAVE. The knowledge gathered in the experiment can provide guidelines to the design of virtual attractors in order to keep the attention of the user and avoid the ""missing wall"".","","978-1-4799-5781-1","10.1109/SIVE.2014.7006282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006282","","Visualization;Legged locomotion;Virtual environments;Rendering (computer graphics);Navigation;Analysis of variance","audio user interfaces;audio-visual systems;rendering (computer graphics);transfer functions;virtual reality","four-sided CAVE-like VR systems;auditory stimuli;visual stimuli;user attention;virtual environments;auditory feedback;binaural audio rendering techniques;nonpersonalized head related transfer functions;user head position;user head orientation;localized sound rendering;audio-visual attractors;attention capturing;screens","","3","","12","","15 Jan 2015","","","IEEE","IEEE Conferences"
"Virtual Training: Learning Transfer of Assembly Tasks","P. Carlson; A. Peters; S. B. Gilbert; J. M. Vance; A. Luse","Department of Human-Computer Interaction (HCI), Iowa State University, Ames, IA; Department of Human-Computer Interaction (HCI), Iowa State University, Ames, IA; Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA; Department of Mechanical Engineering, Iowa State University, Ames, IA; Department of Management Science and Information Systems, Oklahoma State University, Stillwater, OK","IEEE Transactions on Visualization and Computer Graphics","29 Apr 2015","2015","21","6","770","782","In training assembly workers in a factory, there are often barriers such as cost and lost productivity due to shutdown. The use of virtual reality (VR) training has the potential to reduce these costs. This research compares virtual bimanual haptic training versus traditional physical training and the effectiveness for learning transfer. In a mixed experimental design, participants were assigned to either virtual or physical training and trained by assembling a wooden burr puzzle as many times as possible during a twenty minute time period. After training, participants were tested using the physical puzzle and were retested again after two weeks. All participants were trained using brightly colored puzzle pieces. To examine the effect of color, testing involved the assembly of colored physical parts and natural wood colored physical pieces. Spatial ability as measured using a mental rotation test, was shown to correlate with the number of assemblies they were able to complete in the training. While physical training outperformed virtual training, after two weeks the virtually trained participants actually improved their test assembly times. The results suggest that the color of the puzzle pieces helped the virtually trained participants in remembering the assembly process.","1941-0506","","10.1109/TVCG.2015.2393871","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014246","learning transfer;haptics;assembly;Learning transfer;Haptics;virtual reality;assembly;training","Training;Assembly;Virtual environments;Color;Haptic interfaces;Testing;Educational institutions","computer based training;virtual reality","virtual training;learning transfer;assembly tasks;assembly worker training;virtual reality;virtual bimanual haptic training;physical training;wooden burr puzzle;physical puzzle;colored physical parts;natural wood colored physical pieces;spatial ability;mental rotation;puzzle pieces;virtually trained participants;assembly process","","34","","56","","19 Jan 2015","","","IEEE","IEEE Journals"
"Design and implementation of a virtual-real interaction system","F. Zhang; Y. Zhao; J. Li","College of Computer Science and Technology North China University of Technology Beijing, China; College of Computer Science and Technology North China University of Technology Beijing, China; College of Computer Science and Technology North China University of Technology Beijing, China","2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","26 Feb 2018","2017","","","1","5","We design and implement a virtual-real interaction system using the optical motion capture device. First, we construct immersive interaction environment, such as setting OptiTrack parameter, calibrating cameras and transferring data, which play an important role in building environment. Second, we research virtual-real interaction technology based on optical motion capture device. And we acquire the motion data of human and rigid body, transferring them to the virtual scene. So we can accurately rotate and translate the corresponding virtual objects in real time. Finally, we design and implement natural immersive virtual-real interaction system. The experiment result shows that our system can preferably implement virtual-real interaction such as human and rigid body with markers, human and virtual objects.","","978-1-5386-1937-7","10.1109/CISP-BMEI.2017.8302079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302079","Motion Capture;Motion Control;Virtual-Real Interaction","Cameras;Calibration;Optical devices;Real-time systems;Biomedical optical imaging;Solid modeling;Optical signal processing","human computer interaction;interactive systems;virtual reality","optical motion capture device;immersive interaction environment;motion data;human body;rigid body;virtual scene;virtual objects;virtual-real interaction system;virtual-real interaction technology","","","","7","","26 Feb 2018","","","IEEE","IEEE Conferences"
"BIM-based Mixed Reality Application for Supervision of Construction","P. Raimbaud; R. Lou; F. Merienne; F. Danglade; P. Figueroa; J. T. Hernández","LiSPEN, Arts et Métiers, Institut Image, Chalon/Saône, France; LiSPEN, Arts et Métiers, Institut Image, Chalon/Saône, France; LiSPEN, Arts et Métiers, Institut Image, Chalon/Saône, France; LiSPEN, Arts et Métiers, Institut Image, Chalon/Saône, France; Systems and Computing Engineering, Imagine Group, Universidad de los Andes, Bogota, D.C., Colombia; Systems and Computing Engineering, Imagine Group, Universidad de los Andes, Bogota, D.C., Colombia","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1903","1907","Building Information Modelling (BIM) is an up-and-coming methodology and technology used in the Architecture, Engineering and Construction (AEC) industry, that allows data centralization and stakeholders' collaboration. But to check the accuracy of the work done on the worksite, it is necessary first to go on site and then to modify the BIM model. This paper presents a mixed reality (MR) application based on BIM data and drone videos, allowing off-site construction supervision. It permits to make annotations about differences between what has been planned in BIM and what has been built, using superimposition of the two sources. Then these ones can be transferred to the BIM model for corrections. Finally, we evaluate our work with building construction experts, providing to them a questionnaire to grade the application and to get feedback. Our major result is that as for them the application does really help to do construction supervisions; however, they suggest that the application should provide more interactions with the 3D model and with the videos.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797784","BIM;mixed reality;construction;superimposition;drone;videos;I.2.1 [Human-centered computing]: Interaction design—Interaction design process and methods;J.6.4[Computing methodologies]: Computer graphics—Graphics systems and interfaces","Conferences;Virtual reality;Three-dimensional displays;User interfaces;Erbium","building information modelling;building management systems;construction industry;remotely operated vehicles;structural engineering computing;video signal processing;virtual reality","data centralization;building information model;architecture-engineering-construction industry;stakeholder collaboration;mixed reality model;building construction experts;3D model;drone videos","","","","22","","15 Aug 2019","","","IEEE","IEEE Conferences"
"A Publish/Subscribe based correlative matching method for multi-domain virtual environment","L. Liu; W. Wu; H. Chen","School of Computer Science and Information Engineering, Beijing Technology and Business University, 100048, China; State Key Laboratory of Virtual Reality Technology, Beihang University, Beijing 100083, China; School of Computer Science and Information Engineering, Beijing Technology and Business University, 100048, China","2011 IEEE International Symposium on VR Innovation","29 Apr 2011","2011","","","169","174","The multi-domain virtual environment is an important research area of the distributed virtual reality. However, frequent interaction, great quantity of messages and the discrepant need of each individual simulation domain resulted in numerous redundant messages. In this paper, we propose a Publish/Subscribe based correlative matching method for multi-domain virtual environment. This method focus on data filtering of domains' gateways in addition to the DDM scheme. The paper introduces the correlative matching algorithm, the binary representation and the matching method which based on publish/subscribe relationship. Analysis and experiments show that this new solution effectively reduces the redundant data transferring among gateways, thus greatly enhances the performance of the virtual environment, and that the cost of matching method is relative small in multi-domain virtual environment application.","","978-1-4577-0054-5","10.1109/ISVRI.2011.5759625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759625","Publish/subscribe;matching method;multi-domain","Logic gates;Virtual environment;Bandwidth;Filtering;Local area networks;Wide area networks;Servers","middleware;virtual reality","correlative matching method;multidomain virtual environment;distributed virtual reality;data filtering;correlative matching algorithm","","","","10","","29 Apr 2011","","","IEEE","IEEE Conferences"
"Influences on the Elicitation of Interpersonal Space with Virtual Humans","D. M. Krum; S. Kang; T. Phan","University of Southern California, Institute for Creative Technologies; University of Southern California, Institute for Creative Technologies; University of Southern California, Institute for Creative Technologies","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","223","9","The emergence of low cost virtual and augmented reality systems has encouraged the development of immersive training applications for medical, military, and many other fields. Many of the training scenarios for these various fields may require the presentation of realistic interactions with virtual humans. It is thus vital to determine the critical factors of fidelity required in those interactions to elicit naturalistic behavior on the part of trainees. Negative training may occur if trainees are inadvertently influenced to react in ways that are unexpected and unnatural, hindering proper learning and transfer of skills and knowledge back into real world contexts. In this research, we examined whether haptic priming (presenting an illusion of virtual human touch at the beginning of the virtual experience) and different locomotion techniques (either joystick or physical walking) might affect proxemic behavior in human users. The results of our study suggest that locomotion techniques can alter proxemic behavior in significant ways. Haptic priming did not appear to impact proxemic behavior, but did increase rapport and other subjective social measures. The results suggest that designers and developers of immersive training systems should carefully consider the impact of even simple design and fidelity choices on trainee reactions in social interactions.","","978-1-5386-3365-6","10.1109/VR.2018.8446235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446235","Virtual humans;virtual reality;immersive training;fidelity;proxemics;haptic priming;locomotion techniques.: Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality;Human-centered computing-Interaction design-Interaction design process and methods-User interface design","Haptic interfaces;Training;Legged locomotion;Virtual environments;Atmospheric measurements;Particle measurements","augmented reality;haptic interfaces;interactive devices;training","elicitation;interpersonal space;virtual humans;augmented reality systems;immersive training applications;training scenarios;realistic interactions;naturalistic behavior;negative training;proper learning;virtual human touch;virtual experience;human users;impact proxemic behavior;immersive training systems;trainee reactions;social interactions;locomotion techniques","","","","28","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Task-oriented teleoperation through natural 3D user interaction","H. Kim; J. Kim; K. Ryu; S. Cheon; Y. Oh; J. Park","Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea; Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea; Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea; Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea; Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea; Human-centered Interaction & Robotics Research Center, Korea Institute of Science and Technology, Seoul, 136-791, Republic of Korea","2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","12 Mar 2015","2014","","","335","338","Teleoperation provides a technical means to perform desired tasks in remote environments. Teleoperation using direct control and haptic-mediated interactions requires significant effort for unskilled users to understand how to operate the robot. Task-oriented teleoperation supports human-level understanding to directly transfer the meaning of tasks from the user to the robot. In this paper, we propose a natural 3D interface to transfer task knowledge to a remote robot. We design a 3D manipulation system in a mixed reality environment that combines human hand gestures and a 3D view of the remote robot. We demonstrate that the remote robot successfully executes the ordered task even in dynamic environments.","","978-1-4799-5333-2","10.1109/URAI.2014.7057536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057536","Teleoperation;natural user interface;3D manipulation;mixed reality","Three-dimensional displays;Robot kinematics;Cameras;User interfaces;Aerospace electronics;Grasping","control engineering computing;haptic interfaces;human-robot interaction;manipulators;telerobotics","task-oriented teleoperation;natural 3D user interaction;direct control;haptic-mediated interactions;natural 3D interface;remote robot;3D manipulation system;human hand gestures","","","","11","","12 Mar 2015","","","IEEE","IEEE Conferences"
"Approach on a new methodology for skills transfer using a parallel planar robot with visuo-vibrotactile feedback","P. Humblot-Niño; O. Sandoval-González; I. Herrera-Aguilar; D. Rangel-Peñuelas; A. Flores-Cuautle; B. González-Sánchez","División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México; División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México; División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México; División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México; CONACYT - División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México; División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Orizaba, México","2017 14th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)","16 Nov 2017","2017","","","1","5","An approach of a new methodology for skills transfer from machine to human is proposed in this research. This methodology transmits a haptic feed-back using vibrotactile perception to transfer motor skills using a parallel planar robot and virtual reality environments. During the experimentation, the participants tried to learn a specific motion trajectory given by the system. During the process, the system computes the current position and generates a vibrotactile feed-back proportional to the error computed between the actual and the desired position of the motion trajectory. The results of the user studies showed this system can help with learning new skills.","","978-1-5386-3406-6","10.1109/ICEEE.2017.8108861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8108861","Skill Transfer;Parallel Planar Robot;Methodology","Haptic interfaces;Virtual environments;Robot sensing systems;Training;End effectors;Trajectory","feedback;haptic interfaces;human computer interaction;human-robot interaction;position control;virtual reality","haptic feedback;motion trajectory;visuo-vibrotactile feedback;virtual reality environments;parallel planar robot;motor skills;vibrotactile perception;skills transfer","","","","16","","16 Nov 2017","","","IEEE","IEEE Conferences"
"A framework for a multi-sensory VR effect system with motional display","Byounghyun Yoo; Moohyun Cha; Soonhung Han","Dept. of Mech. Eng., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea; Dept. of Mech. Eng., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea; Dept. of Mech. Eng., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea","2005 International Conference on Cyberworlds (CW'05)","6 Feb 2006","2005","","","8 pp.","244","Virtual reality simulators have been developed as tools for the transfer of knowledge and education. Concurrently, the demand for exhibition systems of science education and cultural experiences has also increased. Existing VR (virtual reality) simulators, which are based on the calculation of dynamics equations, cannot easily be adapted to changes in simulation content. In order to transfer knowledge and maintain interest through educational applications, an experiential system that has multi-sensory effects as well as motion effects is needed. The authors proposed a method for motion generation that is tailored to riding systems and multi-sensory VR effects. In this study, both sense of motion, which is generated from movement of the viewpoint of the visual image, and motion effects, which are prepared in advance, are blended to realize motion simulation by the proposed method. Motion effects can easily be added by interaction between the user and the riding system. Various sensory cues are also implemented to the riding system","","0-7695-2378-1","10.1109/CW.2005.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587539","","Virtual reality;Displays;Cultural differences;Vehicle dynamics;Vehicles;Libraries;Equations;Aerodynamics;Aerospace materials;Aerospace simulation","computer aided instruction;virtual reality","virtual reality simulators;knowledge transfer;science education;multisensory effects;visual image;motion simulation;user interaction","","","1","17","","6 Feb 2006","","","IEEE","IEEE Conferences"
"A Virtual Testbed for Human-Robot Interaction","J. Rossmann; E. G. Kaigom; L. Atorf; M. Rast; C. Schlette","Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany; Inst. for Man-Machine Interaction, RWTH Aachen Univ., Aachen, Germany","2013 UKSim 15th International Conference on Computer Modelling and Simulation","10 Jun 2013","2013","","","277","282","Many research efforts in human-robot interaction (HRI) have so far focussed on the mechanical design of intrinsically safe robots, as well as impedance control for tasks in which human and robot will work together. However, comparatively little attention is paid to an approach that ensures safety and permits close HRI while dispensing human with the necessity of physical presence in close proximity to the robot. In this work we acquire real human manipulation gestures using Microsoft's Kinect sensor and project them in realtime into the workspace of a simulated, impedance controlled robot manipulator. This way, we can remotely and therefore safely superimpose human motion over the robot's dynamic motion, wherever the human operator is located. The simulated robot state is then transferred to the real robot as input, so as to physically perform the intended task. The Virtual Testbed approach might not only be useful for HRI pre-analysis, testing and validation goals but particularly advantageous for telepresence, industrial and harzardous tasks as well as training purposes. Simulation results are provided to show the effectiveness of the approach.","","978-0-7695-4994-1","10.1109/UKSim.2013.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527428","eRobotics;Virtual Testbed;human-robot interaction;Kinect;simulation","Robot sensing systems;Impedance;Solid modeling;Robot kinematics;Joints","control engineering computing;design engineering;humanoid robots;human-robot interaction;manipulator dynamics;motion control;sensors","human-robot interaction;HRI;mechanical design;intrinsically safe robot;real human manipulation gesture;Microsoft Kinect sensor;impedance controlled robot manipulator;human motion;robot dynamic motion;human operator;robot state simulation;virtual testbed approach;telepresence;industrial task;harzardous task","","9","","15","","10 Jun 2013","","","IEEE","IEEE Conferences"
"A Virtual Assembly Design Environment","S. Jayaram; Yong Wang; U. Jayaram; K. Lyons; P. Hart","Dept. of Mech. & Mater. Eng., Washington State Univ., Pullman, WA, USA; NA; NA; NA; NA","Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)","6 Aug 2002","1999","","","172","179","The Virtual Assembly Design Environment (VADE) is a virtual reality (VR) based engineering application which allows engineers to evaluate, analyze, and plan the assembly of mechanical systems. This system focuses on utilizing an immersive virtual environment tightly coupled with commercial computer aided design (CAD) systems. Salient features of VADE include: data integration (two-way) with a parametric CAD system; realistic interaction of the user with parts in the virtual environment; creation of valued design information in the virtual environment; reverse data transfer of design information back to the CAD system; significant interactivity in the virtual environment; collision detection; and physically-based modeling. This paper describes the functionality and applications of VADE. A discussion of the limitations of virtual assembly and a comparison with automated assembly planning systems are presented. Experiments conducted using real-world engineering models are also described.","1087-8270","0-7695-0093-5","10.1109/VR.1999.756948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756948","","Virtual reality;Design automation;Design engineering;Virtual environment;Process planning;Integrated circuit modeling;Power engineering and energy;NIST;Assembly systems;Manufacturing systems","virtual reality;user interfaces;CAD;mechanical engineering computing;assembly planning","Virtual Assembly Design Environment;VADE;virtual reality;engineering application;mechanical systems;immersive virtual environment;computer aided design;CAD;data integration;user interaction;data transfer;collision detection;physically-based modeling;assembly planning systems;experiments","","29","4","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Virtual Reality for training - The impact of smell on presence, cybersickness, fatigue, stress and knowledge transfer","D. Narciso; M. Bessa; M. Melo; J. Vasconcelos-Raposo","UTAD,Engineering Department,Vila Real,Portugal; UTAD,Engineering Department,Vila Real,Portugal; INESC TEC,Porto,Portugal; UTAD,Departament of Education and Psychology,Vila Real,Portugal","2019 International Conference on Graphics and Interaction (ICGI)","14 Jan 2020","2019","","","115","121","The area of professional training using virtual reality technologies has received considerable investment due to the advantages that virtual reality provides over traditional training. In this paper, we present an experiment whose goal was to analyse the impact that an additional stimulus has on the effectiveness of a virtual environment designed to train firefighters. The additional stimulus is a smell, more specifically the smell of burnt wood, which is consistent with the audiovisual content presented, and the effectiveness of the VE is measured through participant's feeling of presence, cybersickness, fatigue, stress and transfer of knowledge. The results indicate that, although the VE was successful in transferring knowledge, the addition of smell did not influence any of the measured variables. In the discussion section, we present the various factors that we believe have influenced this result. As future work, more experiments will be performed, with other stimuli, to understand better which stimuli increase participant's feeling of presence in the VE.","","978-1-7281-6378-9","10.1109/ICGI47575.2019.8955071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955071","Virtual Reality;Olfactory Sense;Multisensory Stimulation","Training;Olfactory;Fatigue;Stress;Knowledge transfer;Atmospheric measurements","computer based training;continuing professional development;virtual reality","smell;stress;professional training;virtual reality technologies;additional stimulus;virtual environment;cybersickness;fatigue;knowledge transfer;firefighters;audiovisual content","","1","","44","","14 Jan 2020","","","IEEE","IEEE Conferences"
"Sound and tangible interface for shape evaluation and modification","M. Bordegoni; F. Ferrise; S. Shelley; M. A. Alonso; D. Hermes","Dipartimento di Meccanica, Politecnico di Milano, Via La Masa, 1 - 20156 Milano, Italy; Dipartimento di Meccanica, Politecnico di Milano, Via La Masa, 1 - 20156 Milano, Italy; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands","2008 IEEE International Workshop on Haptic Audio visual Environments and Games","21 Nov 2008","2008","","","148","153","One of the recent research topics in the area of design and virtual prototyping is offering designers tools for creating and modifying shapes in a natural and interactive way. Multimodal interaction is part of this research. It allows conveying to the users information through different sensory channels. The use of more modalities than touch and vision augments the sense of presence in the virtual environment and can be useful to present the same information in various ways. In addition, multimodal interaction can sometimes be used to augment the perception of the user by transferring information that is not generally perceived in the real world, but which can be emulated by the virtual environment. The paper presents a prototype of a system that allows designers to evaluate the quality of a shape with the aid of touch, vision and sound. Sound is used to communicate geometrical data, relating to the virtual object, which are practically undetectable through touch and vision. In addition, the paper presents the preliminary work carried out on this prototype and the results of the first tests made in order to demonstrate the feasibility. The problems related to the development of this kind of application and the realization of the prototype itself are highlighted. This paper also focuses on the potentialities and the problems relating to the use of multimodal interaction, in particular the auditory channel.","","978-1-4244-2668-3","10.1109/HAVE.2008.4685315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685315","haptic;audio & visual sensors and displays;human-computer interaction","Haptic interfaces;Prototypes;Virtual environment;Product design;Shape control;Virtual reality;Application software;Conferences;Humans;Virtual prototyping","computational geometry;data visualisation;haptic interfaces;human computer interaction;virtual reality","sound interface;tangible interface;shape evaluation;shape modification;virtual prototyping;interactive system;multimodal human computer interaction;sensory channel;virtual object geometrical data;auditory channel;haptic system;visualization system","","8","1","18","","21 Nov 2008","","","IEEE","IEEE Conferences"
"Implement Multi-Character Display in Virtual Reality Environment Based on Unet and Tracker","Y. Zou; P. Wang; Q. Tang; Y. Sun","Zunyi Power Supply Bureau,Safety Supervision Department,Zunyi,Guizhou,China; Zunyi Power Supply Bureau,Safety Supervision Department,Zunyi,Guizhou,China; Zunyi Power Supply Bureau,Safety Supervision Department,Zunyi,Guizhou,China; Zunyi Power Supply Bureau,Safety Supervision Department,Zunyi,Guizhou,China","2019 2nd International Conference on Safety Produce Informatization (IICSPI)","19 May 2020","2019","","","530","532","Power grid operation is a very safe operation site, it is very important to be able to carry out related operations in accordance with the safe operation process, the existing traditional safe operation process, mainly in the traditional teaching mode or simulated real scene operation mode. In the links of management and training, operators lack intuitive experience and interaction, and they are not enough to show the serious consequences of illegal operation; As the head-mounted display becomes the most important interactive virtual reality display device, the 3d interactive display and the expressive force of the game engine are gradually improved, significantly improving participants' sense of immersion. Virtual reality technology can be used to build a simulation teaching platform for authentic visual experience, authentic representation of work flow and convincing interactive feedback. Virtual reality technology is used to establish a practical environment for grid operation in the virtual three-dimensional space of the computer that integrates class a illegal operation events. Users participating in the learning practice can operate in the virtual space according to the process. In this paper, based on unity built-in HAPI unet and third-party plug-in VRTK, the complex interaction of large-scale training business process is realized. Through VRTK, steam VR interface, the handle button state is obtained and information is transmitted on multiple instances, so as to determine the transfer and pickup state of interactive objects.","","978-1-7281-4566-2","10.1109/IICSPI48186.2019.9096025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096025","Multiplayer networking technology;Virtual reality technology;Multiplayer online interaction;Inverse dynamics","Games;Training;Servers;Virtual reality;Synchronization;Production;Safety","helmet mounted displays;interactive systems;power engineering computing;power grids;user interfaces;virtual reality","virtual reality environment;power grid operation;head-mounted display;virtual reality technology;simulation teaching platform;interactive feedback;large-scale training business process;interactive virtual reality display device;3D interactive display;multicharacter display;Unet;Tracker;VR interface;VRTK","","","","8","","19 May 2020","","","IEEE","IEEE Conferences"
"Anthropomorphic robotic system with 6 DOF for space positioning in the virtual reality applications for human machine interaction","M. Chavez-Gamboa; I. Herrera-Aguilar; O. Sandoval-Gonzalez; F. Malagon-Gonzalez; J. M. Jacinto-Villegas","The Technological Institute of Orizaba; Electronics Department, Orizaba; Veracruz, Mexico; The Technological Institute of Orizaba; Electronics Department, Orizaba; Veracruz, Mexico; The Technological Institute of Orizaba; Electronics Department, Orizaba; Veracruz, Mexico; The Technological Institute of Orizaba; Electronics Department, Orizaba; Veracruz, Mexico; The Technological Institute of Orizaba; Electronics Department, Orizaba; Veracruz, Mexico","CONIELECOMP 2013, 23rd International Conference on Electronics, Communications and Computing","13 Jun 2013","2013","","","212","217","This paper presents a spatial hand tracking system using a 6 DOF anthropomorphic robot applied in human machine interaction. The main objective of this mechatronic system is to obtain information about the spatial position of a user's hand movements in order to be used like a skills trainer to accelerate the skills transfer from the machine to the human by integrating the laws of physics of virtual objects and adapting different design techniques and use of computer software for three-dimensional virtual reality.","","978-1-4673-6155-2","10.1109/CONIELECOMP.2013.6525788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525788","DOF;virtual reality;skills transfer;upper limbs;physical human-computer interaction;computational design","Virtual environments;Robots;Sensors;Software;Assembly;Kinematics","anthropology;gesture recognition;human-robot interaction;mechatronics;position control;robots;virtual reality","6 DOF anthropomorphic robotic system;space positioning;virtual reality applications;human machine interaction;spatial hand tracking system;mechatronic system;user hand movements;virtual objects;computer software;three-dimensional virtual reality","","","","18","","13 Jun 2013","","","IEEE","IEEE Conferences"
"Phantom-based multimodal interactions for medical education and training: the Munich knee joint simulator","R. Riener; M. Frey; T. Proll; F. Regenfelder; R. Burgkart","Autom. Control Lab., Swiss Fed. Inst. of Technol., Zurich, Switzerland; NA; NA; NA; NA","IEEE Transactions on Information Technology in Biomedicine","7 Jun 2004","2004","8","2","208","216","Simulation environments based on virtual reality technologies can support medical education and training. In this paper, the novel approach of an ""interactive phantom"" is presented that allows a realistic display of haptic contact information typically generated when touching and moving human organs or segments. The key idea of the haptic interface is to attach passive phantom objects to a mechanical actuator. The phantoms look and feel as real anatomical objects. Additional visualization of internal anatomical and physiological information and sound generated during the interaction with the phantom yield a multimodal approach that can increase performance, didactic value, and immersion into the virtual environment. Compared to classical approaches, this multimodal display is convenient to use, provides realistic tactile properties, and can be partly adjusted to different, e.g., pathological properties. The interactive phantom is exemplified by a virtual human knee joint that can support orthopedic education, especially for the training of clinical knee joint evaluation. It is suggested that the technical principle can be transferred to many other fields of medical education and training such as obstetrics and dentistry.","1558-0032","","10.1109/TITB.2004.828885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1303564","","Knee;Medical simulation;Imaging phantoms;Displays;Haptic interfaces;Humans;Virtual reality;Educational technology;Actuators;Visualization","bone;orthopaedics;dentistry;biomedical education;training;phantoms;haptic interfaces;biomechanics;virtual reality;display devices;actuators;medical robotics;digital simulation;computer animation","phantom-based multimodal interaction;medical education;medical training;Munich knee joint simulator;virtual reality technologies;VR;haptic contact information;mechanical actuator;internal anatomical information;physiological information;multimodal sound approach;didactic value;realistic tactile properties;pathological properties;virtual human knee joint;orthopedic education;clinical knee joint evaluation;obstetrics;dentistry;acoustic display;animation;biomechanics;graphical display;multimodal display;robotics","Computer Graphics;Computer Simulation;Education, Medical;Environment;Germany;Humans;Knee Joint;Models, Anatomic;Models, Biological;Online Systems;Phantoms, Imaging;Robotics;Robotics;Teaching;User-Computer Interface","21","","33","","7 Jun 2004","","","IEEE","IEEE Journals"
"Intuitive volume rendering on mobile devices","Y. Xin; H. Wong","Faculty of Information Technology, Macau University of Science and Technology, China; Faculty of Information Technology, Macau University of Science and Technology, China","2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","16 Feb 2017","2016","","","696","701","Nowadays, mobile devices, virtual reality and augment reality technologies are developing faster and faster. With a variety of equipments, people are no longer only using PC to handle tasks. Some traditional system frameworks are migrating to these new technology areas, and direct volume rendering is one of them. In this paper, we propose a real-time and intuitive volume data exploration framework on mobile devices. Our framework introduces a direct-touch transfer function design method that is able for the user to pick voxels directly on a volume rendered image. With one-pass shader, user interaction and volume rendering can be handled efficiently in real-time. The user only needs to learn a few related knowledge to explore a volume data and get its rendered image. As a result, the time cost of transfer function design for direct volume rendering is significantly reduced. Our framework is implemented with OpenGL ES 3.0 and GLSL shader. Experimental results show the advantages of our framework. Researchers, and even general users, can easily obtain volume rendered images of volume data.","","978-1-5090-3710-0","10.1109/CISP-BMEI.2016.7852799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852799","Direct volume rendering;mobile devices;transfer function design;volume data;direct touch;user interface design","Transfer functions;Rendering (computer graphics);Mobile handsets;Histograms;Image color analysis;Two dimensional displays","augmented reality;mobile computing;rendering (computer graphics);transfer functions","mobile devices;virtual reality;augment reality;real-time intuitive volume data exploration;intuitive volume rendering;direct-touch transfer function design;one-pass shader;user interaction;OpenGL ES 3.0;GLSL shader","","","","13","","16 Feb 2017","","","IEEE","IEEE Conferences"
"Large screen multi-touch system integrated with multi-projector","Xiaoqiong Wang; Liwei Wang","School of Information, Guizhou Institute of Finance and Economics, Guiyang, China; School of Economics and Management, Guizhou Normal University, Guiyang, China","Proceedings of 2011 International Conference on Computer Science and Network Technology","12 Apr 2012","2011","1","","455","458","A large screen multi-touch system integrated with multi-projector is presented in this paper. It uses multiple infrared cameras behind the screen to capture the image changes in multi-user finger contact surface, filters the visible light images projected by projectors, collects the remaining infrared image and transfers it into a computer, then detects and tracks various operations of multi-fingers via image recognition algorithm. The experimental results show that this system can support multi-users to use their fingers to accurately simulate actions of mouse clicks, synchronous positioning and trigger detection, and operations of selecting, moving, rotating and zooming target objects on large screen. This system has wide application prospects in real-time human-machine interaction.","","978-1-4577-1587-7","10.1109/ICCSNT.2011.6181996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181996","multi-touch control;large screen;integrated with multi-projector;KNN algorithm","Cameras;Pressing;Films","haptic interfaces;human computer interaction;image recognition;infrared imaging;optical projectors","large screen multitouch system;multiprojector;multiple infrared cameras;multiuser finger contact surface;image recognition algorithm;mouse clicks;synchronous positioning;trigger detection;real-time human-machine interaction;visible light images;infrared image;KNN algorithm","","","1","8","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Human-Robot Assembly: Methodical Design and Assessment of an Immersive Virtual Environment for Real-World Significance","J. Höcherl; A. Adam; T. Schlegl; B. Wrede","OTH Regensburg,Regensburg Robotics Research Unit,Regensburg,Germany; OTH Regensburg,RRRU,Regensburg,Germany; OTH Regensburg,RRRU,Regensburg,Germany; OTH Regensburg,RRRU,Regensburg,Germany","2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)","5 Oct 2020","2020","1","","549","556","Virtual reality is a powerful tool for industrial applications. The article at hand addresses designers of industrial virtual environments. It summarizes key aspects to design immersive and coherent virtual environments. Furthermore, relevant influencing factors for a high quality virtual environment and tools to quantify this quality are presented. So far, a methodology to design, evaluate, and transfer knowledge from virtual environments into reality has been missing and is of high value for industrial applications. The proposed methodical approach includes the steps application analysis, technology selection and integration, design of virtual environment, evaluation of simulator quality, as well as discussion of the real-world validity. The method is shown on the example of a virtual human-robot working cell used to analyze the human perception of robot behavior during mutual assembly processes. The quality of the virtual environment is evaluated to be adequate for those purposes and the transfer of knowledge gained in virtuality on a corresponding real-world application is discussed. To the best of our knowing a system like the presented one, including full-body tracking, finger tracking, a virtual avatar and a head-mounted display has not been used for industrial use cases and human-robot cooperation before.","1946-0759","978-1-7281-8956-7","10.1109/ETFA46521.2020.9212039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212039","virtual reality;human-robot cooperation;system design;fidelity;industrial applications;assembly;immersive and coherent virtual environment;knowledge transfer;simulator quality;real-world validity","","avatars;helmet mounted displays;human-robot interaction;robotic assembly","immersive virtual environment;virtual reality;industrial applications;industrial virtual environments;coherent virtual environments;high quality virtual environment;human-robot working cell;virtual avatar;human-robot cooperation;human-robot assembly","","","","35","","5 Oct 2020","","","IEEE","IEEE Conferences"
"ARWand: Phone-Based 3D Object Manipulation in Augmented Reality Environment","T. Ha; W. Woo","U-VR Lab., GIST, Gwangju, South Korea; U-VR Lab., GIST, Gwangju, South Korea","2011 International Symposium on Ubiquitous Virtual Reality","3 Nov 2011","2011","","","44","47","In this paper, we suggest a mobile phone-based indirect 3D object manipulation method that uses sensor information in an augmented reality environment. Specifically, we propose 1) a method that exploits a 2D touch screen, a 3DOF accelerometer, and compass sensors information to manipulate 3D objects in 3D space, 2) design transfer functions to map the control space of mobile phones to an augmented reality (AR) display space, and 3) confirm the feasibility of the transfer functions by implementation. Our work could be applicable to the design and implementation of a future mobile phone-based 3D user interface for AR application in normal indoor and outdoor environments without any special tracking installations.","","978-1-4577-0356-0","10.1109/ISUVR.2011.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068304","Augmented reality;HMD-based wearable computing;3D object manipulation;sensor based interaction;mobile phone","Three dimensional displays;Mobile handsets;Vectors;Aerospace electronics;Transfer functions;Equations;Mathematical model","augmented reality;helmet mounted displays;mobile computing;mobile handsets;object tracking;sensor fusion;solid modelling;touch sensitive screens;transfer functions;user interfaces","ARWand;mobile phone-based indirect 3D object manipulation method;sensor information;2D touch screen;3DOF accelerometer;compass sensors;transfer functions;augmented reality display space;mobile phone-based 3D user interface;tracking installations","","11","","10","","3 Nov 2011","","","IEEE","IEEE Conferences"
"PhD Forum: Strengthening Social Emotional Skills for Individuals with Developmental Disabilities Through Virtual Reality Games","T. Thang","Comput. Media, Univ. of California, Santa Cruz, Santa Cruz, CA, USA","2018 IEEE International Conference on Smart Computing (SMARTCOMP)","30 Jul 2018","2018","","","242","243","Defining qualities of developmental disabilities include deficits in social-emotional skills, especially with respect to emotion recognition. This study aims to assist adults with developmental disabilities in strengthening emotion recognition skills through the research and development of virtual reality games to increase accessibility to life-changing therapies. Work previously accomplished in this study includes the development of EmotionVR, a virtual reality game created for the HTC Vive. EmotionVR takes traditional methods of therapy aimed at teaching emotion recognition and translates it into an interesting and interactive narrative that provides users the opportunity to learn and practice emotion recognition in realistic settings. This work observed and analyzed the interaction between adults with developmental disabilities, and the HTC Vive and virtual environments. While the game supports the idea that virtual reality is a feasible method of providing such therapies, users found some discomfort with using the HTC Vive, and had slight difficulty translating what they had learned from the game in different situational contexts. To resolve those issues, we investigate interactions between adults with developmental disabilities and other virtual reality systems, and develop a 360 video based virtual reality game to assist with transfer of skills.","","978-1-5386-4705-9","10.1109/SMARTCOMP.2018.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421355","assistive technology;virtual reality;human computer interaction;developmental disabilities","Conferences","computer aided instruction;computer games;emotion recognition;handicapped aids;teaching;virtual reality","social-emotional skills;adults;developmental disabilities;emotion recognition skills;HTC Vive;virtual reality systems;EmotionVR;teaching;interactive narrative;virtual environments;video based virtual reality game","","","","9","","30 Jul 2018","","","IEEE","IEEE Conferences"
"A Mixed Reality Approach for Merging Abstract and Concrete Knowledge","J. Quarles; S. Lampotang; I. Fischler; P. Fishwick; B. Lok","Dept. of CISE, University of Florida, email: jpq@cise.ufl.edu; Dept. of Anesthesiology, University of Florida, email: SLampotang@anest.ufl.edu; Dept. of Psychology, University of Florida, email: ifisch@ufl.edu; Dept. of CISE, University of Florida, email: fishwick@cise.ufl.edu; Dept. of CISE, University of Florida, email: lok@cise.ufl.edu","2008 IEEE Virtual Reality Conference","4 Apr 2008","2008","","","27","34","Mixed reality's (MR) ability to merge real and virtual spaces is applied to merging different knowledge types, such as abstract and concrete knowledge. To evaluate whether the merging of knowledge types can benefit learning, MR was applied to an interesting problem in anesthesia machine education. The virtual anesthesia machine (VAM) is an interactive, abstract 2D transparent reality simulation of the internal components and invisible gas flows of an anesthesia machine. It is widely used in anesthesia education. However when presented with an anesthesia machine, some students have difficulty transferring abstract VAM knowledge to the concrete real device. This paper presents the augmented anesthesia machine (AAM). The AAM applies a magic-lens approach to combine the VAM simulation and a real anesthesia machine. The AAM allows students to interact with the real anesthesia machine while visualizing how these interactions affect the internal components and invisible gas flows in the real world context. To evaluate the AAM's learning benefits, a user study was conducted. Twenty participants were divided into either the VAM (abstract only) or AAM (concrete+abstract) conditions. The results of the study show that MR can help users bridge their abstract and concrete knowledge, thereby improving their knowledge transfer into real world domains.","2375-5334","978-1-4244-1971-5","10.1109/VR.2008.4480746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480746","J.3 [Computer Applications]: Life and Medical Sciences ¿ Health;Mixed Reality;Modeling and Simulation;Anesthesiology;Psychology;User Studies","Virtual reality;Merging;Concrete;Anesthesia;Active appearance model;Fluid flow;Machine learning;Visualization;Bridges;Knowledge transfer","medical computing;user interfaces;virtual reality","mixed reality approach;anesthesia machine education;virtual anesthesia machine;2D transparent reality simulation;augmented anesthesia machine","","19","12","22","","4 Apr 2008","","","IEEE","IEEE Conferences"
"Quality of service support of distributed interactive virtual environment applications in IP networks","Hong Yu; Quanyou Zhou; D. Makrakis; N. D. Georganas; E. Petriu","Broadband Wireless & Internetworking Res. Lab., Ottawa Univ., Ont., Canada; NA; NA; NA; NA","2001 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (IEEE Cat. No.01CH37233)","7 Aug 2002","2001","1","","196","199 vol.1","This paper reports on the performance of Distributed Interactive Virtual Environment (DIVE) applications, deployed through Internet. Our goal is to understand the behaviour of a DIVE application, its interaction with competing traffic streams, as well as its network resource requirements for a satisfactory performance. As DIVE is becoming the building block of new applications and services, impacting several existing or new and growing sectors of our economy (email electronic commerce, tele-training, transportation), it is important that we understand the resource requirements of these applications, as well as the ""stress"" they will impose on the network.","","0-7803-7080-5","10.1109/PACRIM.2001.953556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953556","","Quality of service;Virtual environment;Intelligent networks;IP networks;Diffserv networks;Internet;Application software;Telecommunication traffic;Computational modeling;Asynchronous transfer mode","distributed processing;virtual reality;Internet;quality of service;computer network management","Distributed Interactive Virtual Environment applications;Internet;DIVE application;competing traffic streams;network resource requirements;DiffServ;quality of service","","2","","6","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Virtual Reality Instruction Followed by Enactment Can Increase Procedural Knowledge in a Science Lesson","N. K. Andreasen; S. Baceviciute; P. Pande; G. Makransky",Aalborg University; University of Copenhagen; Roskilde University; Aalborg University,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","840","841","A 2×2 between-subjects experiment (a) investigated and compared the instructional effectiveness of immersive virtual reality (VR) versus video as media for teaching scientific procedural knowledge, and (b) examined the efficacy of enactment as a generative learning strategy in combination with the respective instructional media. A total of 117 high school students (74 females) were randomly distributed across four instructional groups - VR and enactment, video and enactment, only VR, and only video. Outcome measures included declarative knowledge, procedural knowledge, knowledge transfer, and subjective ratings of perceived enjoyment. Results indicated that there were no main effects or interactions for the outcomes of declarative knowledge or transfer. However, there was a significant interaction between media and method for the outcome of procedural knowledge with the VR and enactment group having the highest performance. Furthermore, media also seemed to have a significant effect on student perceived enjoyment, indicating that the groups enjoyed the VR simulation significantly more than the video. The results deepen our understanding of how we learn with immersive technology, as well as suggest important implications for implementing VR in schools.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797755","Virtual Reality;generative learning strategy;enactment;learning;procedural knowledge","Media;Biological system modeling;Virtual reality;Education;Solid modeling;Psychology;Knowledge transfer","computer aided instruction;teaching;virtual reality","generative learning strategy;instructional groups;declarative knowledge;knowledge transfer;student perceived enjoyment;VR simulation;immersive virtual reality;instructional media;high school students;virtual reality instruction;scientific procedural knowledge teaching;science lesson","","","","9","","15 Aug 2019","","","IEEE","IEEE Conferences"
"3D Sound Rendering in a Virtual Environment to Evaluate Pedestrian Street Crossing Decisions at a Roundabout","H. Wu; D. H. Ashmead; H. Adams; B. Bodenheimer","Vanderbilt University, USA; Vanderbilt University, USA; Vanderbilt University, USA; Vanderbilt University, USA","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","16 Dec 2018","2018","","","1","6","Crossing streets is a potentially hazardous activity for pedestrians, and it is made more hazardous when the complexity of the intersection increases beyond a simple linear bisection, as it does in the case of a roundabout. We are interested in how pedestrians make decisions about when to cross at such intersections. Simulating these intersections in immersive virtual reality provides a safe and effective way to do this. In this context, we present a traffic simulation designed to assess how pedestrians make dynamic gap affordance judgments related to crossing the street when supplied with spatialized sound cues. Our system uses positional information to generate generic head-related transfer functions (HRTFs) for spatializing audio sources. In this paper we evaluate the utility of using spatialized sound for these gap crossing judgments. In addition, we evaluate our simulation against varying levels of visual degradation to better understand how those with visual deficits might rely on their auditory senses. Our results indicate that spatialized sound enhances the experience of the virtual traffic simulation; its effects on gap crossing behavior are more subtle.","","978-1-5386-5713-3","10.1109/SIVE.2018.8577195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8577195","Virtual reality;spatialized sound;gap affordance;traffic simulation;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtal Reality;J.4 [Computer Applications]: Social and Behavioral Sciences—Psychology","Virtual environments;Solid modeling;Three-dimensional displays;Visualization;Biological system modeling;Acoustics;Automobiles","acoustic signal processing;audio signal processing;digital simulation;pedestrians;rendering (computer graphics);solid modelling;traffic engineering computing;transfer functions;virtual reality","virtual reality;head-related transfer functions;pedestrian street crossing decisions;HRTF;audio sources;gap crossing behavior;virtual traffic simulation;roundabout;virtual environment;3d sound rendering","","","","50","","16 Dec 2018","","","IEEE","IEEE Conferences"
"A framework for volume segmentation and visualization using Augmented Reality","T. Tawara; K. Ono","RIKEN, Japan; RIKEN, Japan","2010 IEEE Symposium on 3D User Interfaces (3DUI)","29 Apr 2010","2010","","","121","122","We propose a two-handed direct manipulation system to achieve complex volume segmentation of CT/MRI data in Augmented Reality with a remote controller attached to a motion tracking cube. At the same time segmented data is displayed by direct volume rendering using a programmable GPU. Our system achieves visualization of real time modification of volume data with complex shading including transparency control by changing transfer functions, displaying any cross section, and rendering multi materials using a local illumination model. Our goal is to build a system that facilitates direct manipulation of volumetric CT/MRI data for segmentation in Augmented Reality. Volume segmentation is a challenging problem and segmented data has an important role for visualization and analysis.","","978-1-4244-6847-8","10.1109/3DUI.2010.5444707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444707","","Augmented reality;Data visualization;Data mining;Magnetic resonance imaging;Tracking;Humans;Motion control;Control systems;Shape control;Needles","augmented reality;biomedical MRI;computerised tomography;data visualisation;image segmentation;medical image processing","volume segmentation;volume visualization;augmented reality;direct manipulation system;CT/MRI data;motion tracking cube;real time modification","","2","","4","","29 Apr 2010","","","IEEE","IEEE Conferences"
"Immersive virtual environments for tacit knowledge transfer focusing on gestures: A workflow","H. Esmaeili; H. Thwaites; P. C. Woods","Centre for Research-Creation in Digital Media, School of Arts, Sunway University, Malaysia; Centre for Research-Creation in Digital Media, School of Arts, Sunway University, Malaysia; Faculty of Creative Multimedia, Multimedia University, Malaysia","2017 23rd International Conference on Virtual System & Multimedia (VSMM)","26 Apr 2018","2017","","","1","6","This study presents a workflow for creating immersive virtual environments for tacit knowledge transfer. The main focus is on gestures, which are related to skill, performance, or physical emotion (not facial) e.g. sports, martial arts, playing instruments, acting, etc. The initial idea behind this design is to provide a virtual practice environment mainly for actors in order to learn new gestures or moves. However, this virtual environment can also be used by many other target audiences based on their needs. Sometimes, ambiguity is part of knowledge transfer and becomes more salient or critical when it comes to tacit knowledge, especially at early stages of transfer. Performance while maintaining believable gesture is a must have requirement for actors. Visual references (mainly video in absence of trainer) are commonly used by actors in order to learn specific moves or gestures. However, videos are limited to 2D screen view (even if stereoscopic or 360°) and do not provide chance of studying a freezing moment from all angles, simultaneously. Although this can be partly mimicked using multi-camera rig, it is still limited to the number of shots taken and only provides a linear frame sequence (mostly used as VFX). Immersive virtual environments not only eliminate this limitation but also provide one to one scale experience. In this study, the process of creating such environment is discussed in detail. This includes planning, concept design, selecting tools, establishing the environment, properly selecting or creating the virtual character(s), capturing the motion or using existing ones from different Mocap libraries, actor's interaction with VR equipment, user experience, etc. In addition to studying reference moves and gestures (frame by frame and from any angle), the user is able to observe his/her performance in VR. This can be achieved using motion capture cameras installed at the practice location. The captured content is later assigned to the user's virtual representative i.e. a 3d character created based on his/her physical body features for side by side analysis with the reference. This provides countless interaction possibilities that cannot be achieved in the real world. Few examples are: multiplying the reference character and freeze two or more different moments (frames) and create a walkthrough, creating an immersive timeline based on the actor's progress (also requires multiplying), assigning reference moves to the user's avatar to be compared with his/her movements by himself/herself or anyone else (different from side by side comparison with the reference character), and many others. What has been discussed above is fully illustrated and described in this paper including detailed figures. The contribution of this study can be extended to various fields from acting and sport to stop motion and creative art, as the processes presented in the paper are designed in the most affordable way, using hardware and software currently available to basic users.","2474-1485","978-1-5386-4494-2","10.1109/VSMM.2017.8346255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8346255","immersive virtual environments;immersive virtual reality;tacit knowledge transfer;gesture;virtual reality;mocap;learning in virtual reality;htc vive;oculus rift;unity;steam vr","Teleportation;Knowledge transfer;Observers;Virtual environments;Art;Tools;Three-dimensional displays","gesture recognition;virtual reality","reference character;reference moves;immersive virtual environments;tacit knowledge transfer;virtual practice environment;believable gesture;virtual character;virtual representative","","1","","10","","26 Apr 2018","","","IEEE","IEEE Conferences"
"An Interactive Mixed Reality Framework for Virtual Humans","A. Egges; G. Papagiannakis; N. Magnenat-Thalmann","University of Geneva, Switzerland; University of Geneva, Switzerland; University of Geneva, Switzerland","2006 International Conference on Cyberworlds","19 Dec 2006","2006","","","165","172","In this paper, we present a simple and robust mixed reality (MR) framework that allows for real-time interaction with virtual humans in real and virtual environments under consistent illumination. We look at three crucial parts of this system: interaction, animation and global illumination of virtual humans for an integrated and enhanced presence. The interaction system comprises of a dialogue module, which is interfaced with a speech recognition and synthesis system. Next to speech output, the dialogue system generates face and body motions, which are in turn managed by the virtual human animation layer. Our fast animation engine can handle various types of motions, such as normal key-frame animations, or motions that are generated on-the-fly by adapting previously recorded clips. All these different motions are generated and blended on-line, resulting in a flexible and realistic animation. Our robust rendering method operates in accordance with the previous animation layer, based on an extended for virtual humans precomputed radiance transfer (PRT) illumination model, resulting in a realistic display of such interactive virtual characters in mixed reality environments. Finally, we present a scenario that illustrates the interplay and application of our methods, glued under a unique framework for presence and interaction in MR","","0-7695-2671-3","10.1109/CW.2006.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4030841","","Virtual reality;Humans;Lighting;Robustness;Facial animation;Virtual environment;Speech recognition;Speech synthesis;Face;Engines","avatars;computer animation;interactive systems","interactive mixed reality framework;real-time interaction;virtual environment;interaction system;speech recognition;speech synthesis;dialogue system;virtual human animation;rendering method;precomputed radiance transfer illumination model;interactive virtual character","","5","","32","","19 Dec 2006","","","IEEE","IEEE Conferences"
"Exploring social presence transfer in real-virtual human interaction","S. Daher; K. Kim; M. Lee; A. Raij; R. Schubert; J. Bailenson; G. Welch",University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida and UNC-Chapel Hill; Stanford University; University of Central Florida,"2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","165","166","We explore whether a peripheral observation of apparent mutual social presence between a real human (RH) and a virtual human (VH) can in turn increase a subject's sense of social presence with the VH. In other words, we explore whether social presence can “transfer” from one RH-VH interaction to another. Specifically, we carried out an experiment where human subjects were asked to play a game with a VH. As they entered the game room, approximately half of the subjects were exposed to a brief but apparently engaging conversation between an RH and the VH. The subjects who were exposed to the brief RH-VH interaction had significantly higher measures of both emotional connection and the attentional allocation dimension of social presence for the VH, compared to those who were not. We describe the motivation, the experiment, and the results.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504705","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented, and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology","Games;Electronic mail;Psychology;Resource management;Medical services;Virtual environments;Training","computer games;graphical user interfaces;social sciences computing;virtual reality","attentional allocation dimension;emotional connection;apparently engaging conversation;game room;RH-VH interaction;apparent mutual social presence;peripheral observation;real-virtual human interaction;social presence transfer","","2","","7","","7 Jul 2016","","","IEEE","IEEE Conferences"
"A comparison between measured and modelled head-related transfer functions for an enhancement of real-time 3D audio processing for virtual reality environments","A. S. Suarez; J. Tissieres; L. S. Vieira; R. Hunter-McHardy; S. K. Sernavski; S. Serafin",Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen,"2017 IEEE 3rd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","17 Apr 2017","2017","","","1","9","Sound in Virtual Reality (VR) has been explored in a variety of algorithms which try to enhance the illusion of presence, improving sound localization and spatialization in the virtual environment. As new systems are developed, different models are applied. There is still the need to evaluate and understand the main advantages of each of these approaches. In this study, a performance comparison of two methods for real-time 3D binaural sound tested preferences and quality of presence for headphones in a VR experience. Both the mathematical based HRTF and the convolution based measured HRTF from the MIT KEMAR show a general similarity in the participants sense of localization, depth and presence. Nevertheless, the tests also indicate a preference in elevation perception for the convolution-based measured HRTF. Further experiments with new tools, techniques, contexts, and guidelines are therefore required to highlight the importance and differences between these two methods and other implementations.","","978-1-5386-0459-5","10.1109/SIVE.2017.7901609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7901609","","Ear;Three-dimensional displays;Convolution;Computational modeling;Databases;Mathematical model;Azimuth","audio signal processing;real-time systems;virtual reality","real-time 3D audio processing enhancement;virtual reality environments;sound localization;sound spatialization;binaural sound tested preferences;quality of presence;headphones;VR experience;mathematical based HRTF;MIT KEMAR;convolution","","2","2","11","","17 Apr 2017","","","IEEE","IEEE Conferences"
"Poster: A pilot study on stepwise 6-DoF manipulation of virtual 3D objects using smartphone in wearable augmented reality environment","Taejin Ha; Woontack Woo","KAIST UVR Lab., 305-701, S. Korea; KAIST UVR Lab., 305-701, S. Korea","2013 IEEE Symposium on 3D User Interfaces (3DUI)","5 Sep 2013","2013","","","137","138","In general, 6 degrees of freedom (DoF) manipulation of 3D virtual objects requires cumbersome external tracking installations to localize and track the input device in a normal wearable augmented reality (WAR) environment. In this paper, a smartphone-based 6-DoF manipulation method is proposed to avoid the use of complicated installations. First, the proposed 6-DoF manipulation method (3-DoF translation and 3-DoF rotation) exploits various embedded sensor information of the smartphone. Transfer functions are then designed to transfer the phone's 6-DoF control gain to the 3D display space's object manipulation gain. According to the experimental results, the quadratic transfer function was superior to the linear and acceleration functions with less completion time and errors.","","978-1-4673-6098-2","10.1109/3DUI.2013.6550216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550216","Wearable Augmented Reality Environment;Non-External Tracking Environment;3D User Interaction","Abstracts;Indexes;Wireless sensor networks;Wireless communication;Magnetic resonance imaging;Cameras;Image resolution","augmented reality;smart phones;user interfaces","stepwise 6-DoF manipulation;6 degrees-of-freedom manipulation;smart phone;wearable augmented reality environment;3D virtual object;WAR environment;3-DoF translation;3-DoF rotation;embedded sensor information;quadratic transfer function;linear function;acceleration function","","","","3","","5 Sep 2013","","","IEEE","IEEE Conferences"
"Development of A Virtual Environment to Realize Human-Machine Interaction of Forklift Operation","J. Y. Chew; K. Okayama; T. Okuma; M. Kawamoto; H. Onda; N. Kato","National Institute of Advanced Industrial Science and Technology,Japan; National Institute of Advanced Industrial Science and Technology,Japan; National Institute of Advanced Industrial Science and Technology,Japan; National Institute of Advanced Industrial Science and Technology,Japan; National Institute of Advanced Industrial Science and Technology,Japan; National Institute of Advanced Industrial Science and Technology,Japan","2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA)","16 Dec 2019","2019","","","112","118","This study presents an experimental concept to develop realistic Human-Machine Interaction (HMI) for a Virtual Environment (VE) and a novel evaluation methodology of such system. Such evaluation is motivated by the need to facilitate transfer of model/knowledge from VE to the Real Environment (RE), where it is crucial for the VE to trigger similar user behavior as in the RE. This paper discusses the application of such concept to evaluate interactions of forklift operation in the VE. First, a Virtual Reality (VR) forklift simulator is developed using motion capture and 3D reconstruction methods to mimic HMI of the real forklift operation. Then, the Dynamic Time Warping (DTW) algorithm is used for temporal evaluation of operation behaviors in VE and RE. Results of DTW (i.e. distance and correlation) are used as objective measures to evaluate fidelity of VE during forklift operations on the simulator. Results suggest the proposed forklift simulator triggers operation behavior which is similar (highly correlated) to that of real forklift operation. The contributions of this paper are (a) the novel VR forklift simulator system to realize interactions of real forklift in the VE, and (b) the proposed objective measures for temporal evaluation of the fidelity of VE.","","978-1-7281-3118-4","10.1109/RITAPP.2019.8932837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8932837","","Visualization;Task analysis;Three-dimensional displays;Resists;Image reconstruction;Tracking;Man-machine systems","fork lift trucks;human computer interaction;man-machine systems;virtual reality","operation behaviors;temporal evaluation;virtual environment;human-machine interaction;evaluation methodology;Virtual Reality forklift simulator;forklift simulator;forklift simulator;VR forklift;Real Environment;motion capture;3D reconstruction methods;real forklift operation;Dynamic Time Warping","","","","26","","16 Dec 2019","","","IEEE","IEEE Conferences"
"Interaction with an agent in Blended Reality","Y. Kanai; H. Ohsawa; M. Imai","Keio University, 3-14-1, Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223-8522, Japan; Keio University, 3-14-1, Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223-8522, Japan; Keio University, 3-14-1, Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223-8522, Japan","2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","21 Mar 2013","2013","","","153","154","This paper proposes a Blended Reality Agent called “BReA” that can exist in both the real and virtual world and has communicative advantages that is more than simply the sum of those of a robotic agent and an on-screen agent. BReA can seamlessly transfer between the real world and virtual world and users can communicate with BReA without interruption during the transference. We conducted a field test and showed that users can recognize when it points from the virtual world to a real world object after it moves from the real world to virtual world. This result supports a hypothesis that communicating with BReA helps users recognize the link between real world and virtual world objects and virtual world information.","2167-2148","978-1-4673-3101-2","10.1109/HRI.2013.6483547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6483547","blended reality;human-agent interaction;field test","Robots;Interrupters;Interactive systems;Augmented reality;Augmented virtuality;Organic light emitting diodes;Cameras","human-robot interaction;multi-agent systems","blended reality agent;BReA;on-screen agent;robotic agent;real world object;virtual world objects","","","","5","","21 Mar 2013","","","IEEE","IEEE Conferences"
"VREX: Virtual reality education expansion could help to improve the class experience (VREX platform and community for VR based education)","L. Ying; Z. Jiong; S. Wei; W. Jingchun; G. Xiaopeng","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Dept. for Cyber Online Popularization of Science, Chinese Science and Technology Museum, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","2017 IEEE Frontiers in Education Conference (FIE)","14 Dec 2017","2017","","","1","5","This paper proposed an innovative education platform-VREX (Virtual Reality based Education eXpansion), with combination of online and offline, to improve the curriculum building and teaching experience. VREX is based on Virtual Reality (VR) and we believe VR can revolutionize the education ecosystem. With some trials, we found VR can be used to promote curriculum effectiveness in an immersive environment so that students can have intuitive sense to understand some abstract knowledge, which is always hard for teachers to describe. We have tried to transfer slides into VR scenes, for the students to learn knowledge in a rather real but totally virtual world. The main contributions were made: (1) VREX build an open and immersion virtual O2O classroom with internet and VR devices so that real classrooms might be used in a different way in the future. (2) VREX provides a distributed mode for students to experience an interactive learning process at anytime, anywhere and any-frequency. (3) VREX can be used to support education in different disciplines, from K-12 to Universities, and we provided some practical cases, like `Marine Life' to show creatures in deep sea, which provides immersive experience to makes students feel they were there. Finally, the feasibility and advantage of VREX are proved by the actual statistical data in the 3rd season 2017.","","978-1-5090-5920-1","10.1109/FIE.2017.8190660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190660","Virtual Reality;Immersion;Interaction;Virtual classroom;VR Cloud Platform","Cloud computing;Games;Computer architecture;Hardware;Courseware;Training","computer aided instruction;educational courses;interactive systems;Internet;teaching;virtual reality","immersive experience;virtual reality education expansion;VR based education;innovative education platform-VREX;curriculum building;teaching experience;education ecosystem;VR scenes;immersion virtual O2O classroom;class experience improvement;VREX platform;open virtual O2O classroom;interactive learning process","","5","","10","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Syncretic Post-Biological Digital Identity: Hybridizing Mixed Reality Data Transfer Systems","J. Stadon; R. Grasset","Curtin Univ., Perth, WA, Australia; NA","2011 IEEE/ACM 15th International Symposium on Distributed Simulation and Real Time Applications","20 Oct 2011","2011","","","120","125","This paper offers a contribution to an emerging culturally orientated discourse regarding mixed reality interaction. It seeks to analyse syncretic, hybridized agency, particularly in mixed reality data transfer systems. Recent developments in bridging autonomous relationships with digital representation through mixed reality interfacing, have brought about the need for further analysis of these new 'post-biological', hybridized states of being that traverse traditional paradigms of time and space. Roy Ascott's concept of syncretism may facilitate further understanding of multi-layered world views, both material and metaphysical, that are emerging from our engagement with such pervasive computational technologies and post-biological systems. Syncretism has traditionally been regarded as an attempt to harmonise and analogise [1] Citing recent examples of practical research outcomes, this paper will analyse what Gilles Deleuze and Fèlix Guattari have called 'deterritorialisation' of the human body through its dispersion throughout multiple reality manifestations and how mixed reality data transfer might constitute a 'reterritorialising' effect on syncretic post-biological digital identity construction [2].","1550-6525","978-1-4577-1643-0","10.1109/DS-RT.2011.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051788","Art;Media art;Performing Arts;Metaverse;dual reality;cross-reality;transhumanism;post-humanism","Art;Second Life;Avatars;Augmented reality;Collaboration;Humans","art;augmented reality;biology computing;data handling;history;human computer interaction;ubiquitous computing","syncretic post-biological digital identity construction;mixed reality data transfer systems;cultural orientated discourse;mixed reality interaction;pervasive computational technologies;post-biological systems","","","","22","","20 Oct 2011","","","IEEE","IEEE Conferences"
"Augmented reality @ Siemens: ""The Workflow Designer Project"" & ""Augmented reality PDA""","A. Raczynski; C. Reimann; P. Gussmann; C. Matysczok; A. Grow; W. Rosenbach",NA; NA; NA; NA; NA; NA,"The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","2 pp.","","In this paper we describe two AR projects at Siemens: ""The Workflow Designer Project"" and ""AR-PDA - A Personal Digital Assistant for VR/AR Content"". The Workflow Designer makes use of pattern recognition and tracking to augment real-time video with 3D models, allowing a remote expert to guide a worker through a complicated workflow step-by-step. The general idea of the AR-PDA project to the development of applications for new mobile devices like 3/sup rd/ generation mobile telephones, organizer or PDA for the consumer market. In this context augmented reality is used to support efficiently final users during their daily tasks and especially in service situations. The AR-PDA enhances real camera images by virtual objects (3D-animations, 2D-graphics or text) and allows personalized user interactions with the augmented scene.","","0-7803-7680-3","10.1109/ART.2002.1106974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106974","","Augmented reality;Layout;Pattern recognition;Wireless networks;Video compression;Image coding;Prototypes;Animation;Displays;Personal digital assistants","3G mobile communication;augmented reality;workflow management software;notebook computers;pattern recognition;real-time systems;computer animation","augmented reality;Siemens;The Workflow Designer Project;Augmented reality PDA;A Personal Digital Assistant for VR/AR Content;AR-PDA;pattern recognition;pattern tracking;real-time video;3D models;3rd generation mobile telephones;virtual objects;3D animations;2D graphics;text;personalized user interactions;augmented scene","","","3","3","","6 Jan 2003","","","IEEE","IEEE Conferences"
"Online Identification of Interaction Behaviors From Haptic Data During Collaborative Object Transfer","A. Kucukyilmaz; I. Issak","School of Computer Science, University of Lincoln, Lincoln, U.K.; School of Computer Science, University of Lincoln, Lincoln, U.K.","IEEE Robotics and Automation Letters","21 Nov 2019","2020","5","1","96","102","Joint object transfer is a complex task, which is less structured and less specific than what is existing in several industrial settings. When two humans are involved in such a task, they cooperate through different modalities to understand the interaction states during operation and mutually adapt to one another's actions. Mutual adaptation implies that both partners can identify how well they collaborate (i.e. infer about the interaction state) and act accordingly. These interaction states can define whether the partners work in harmony, face conflicts, or remain passive during interaction. Understanding how two humans work together during physical interactions is important when exploring the ways a robotic assistant should operate under similar settings. This study acts as a first step to implement an automatic classification mechanism during ongoing collaboration to identify the interaction state during object co-manipulation. The classification is done on a dataset consisting of data from 40 subjects, who are partnered to form 20 dyads. The dyads experiment in a physical human-human interaction (pHHI) scenario to move an object in an haptics-enabled virtual environment to reach predefined goal configurations. In this study, we propose a sliding-window approach for feature extraction and demonstrate the online classification methodology to identify interaction patterns. We evaluate our approach using 1) a support vector machine classifier (SVMc) and 2) a Gaussian Process classifier (GPc) for multi-class classification, and achieve over 80% accuracy with both classifiers when identifying general interaction types.","2377-3766","","10.1109/LRA.2019.2945261","Engineering and Physical Sciences Research Council; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854968","Classification;Feature Extraction;Force and Tactile Sensing;Haptics and Haptic Interfaces;Human Factors and Human-in-the-Loop;Learning and Adaptive Systems;Physical Human-Human Interaction;Physical Human-Robot Interaction;Recognition","Task analysis;Collaboration;Haptic interfaces;Feature extraction;Robots;Microsoft Windows;Taxonomy","feature extraction;Gaussian processes;haptic interfaces;human-robot interaction;image classification;robot vision;support vector machines;virtual reality","interaction state;physical interactions;interaction patterns;physical human-human interaction;general interaction types;interaction behaviors;collaborative object transfer;joint object transfer;online identification;haptic data;industrial settings;object co-manipulation;haptics-enabled virtual environment;sliding-window approach;feature extraction;online classification methodology;support vector machine classifier;SVMc;Gaussian process classifier;multiclass classification;general interaction type identification;automatic classification mechanism","","","","27","IEEE","2 Oct 2019","","","IEEE","IEEE Journals"
"Filter optimization for human-machine interaction in aviation","J. Boril; K. Zaplatilek; R. Jalovecky","University of Defence, Faculty of Military Technology, Kounicova 65, 662 10 Brno, Czech Republic; University of Defence, Faculty of Military Technology, Kounicova 65, 662 10 Brno, Czech Republic; University of Defence, Faculty of Military Technology, Kounicova 65, 662 10 Brno, Czech Republic","2013 IEEE/ASME International Conference on Advanced Intelligent Mechatronics","22 Aug 2013","2013","","","448","452","Pilot's responses to a sudden step change in flight altitude were measured on a flight simulator. The measured data was then mathematically analyzed in the MATLAB® environment, providing the input and output data for the filter optimization. The aim of such an optimal filter design is to obtain the best transfer function parameters (pilot gain and time constants) of human behavior while controlling an aircraft. Due to the measurements and the filter optimization, the pilot behavior model can be more realistic in identifying any potential imperfections in the aircraft's flight control systems at an early stage of its development.","2159-6255","978-1-4673-5320-5","10.1109/AIM.2013.6584132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6584132","","Optimization;Aircraft;Mathematical model;Filtering algorithms;Aerospace control;MATLAB;Atmospheric modeling","aerospace simulation;aircraft control;filtering theory;human computer interaction;human factors;mathematical analysis;optimisation;psychology;transfer functions","human-machine interaction;aviation;flight simulator;pilot response measurement;flight altitude change;mathematical analysis;MATLAB environment;input data;output data;filter optimization;transfer function parameters;optimal filter design;human behavior;aircraft control;pilot behavior model;flight control systems","","2","","13","","22 Aug 2013","","","IEEE","IEEE Conferences"
"SSVEP Stimulus Layout Effect on Accuracy of Brain-Computer Interfaces in Augmented Reality Glasses","X. Zhao; C. Liu; Z. Xu; L. Zhang; R. Zhang","School of Information Engineering, Zhengzhou University, Zhengzhou, China; School of Information Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China","IEEE Access","13 Jan 2020","2020","8","","5990","5998","Steady-state visual evoked potentials-based brain-computer interfaces (SSVEP-BCI) has the advantage of high information transfer rate (ITR) and little user training, and it has a high application value in the field of disability assistance and human-computer interaction. Generally SSVEP-BCI requires a personal computer screen (PC) to display several repetitive visual stimuli for inducing the SSVEP response, which reduces its portability and flexibility. Using augmented reality (AR) glasses worn on the head to display the repetitive visual stimuli could solve the above drawbacks, but whether it could achieve the same accuracy as PC screen in the case of reduced brightness and increased interference is unknown. In current study, we firstly designed 4 stimulus layouts and displayed them with Microsoft HoloLens (AR-SSVEP) glasses, comparison analysis showed that the classification accuracies are influenced by the stimulus layout when the stimulus duration is less than 3s. When the stimulus duration exceeds 3s, there is no significant accuracy difference between the 4 layouts. Then we designed a similar experimental paradigm on PC screen (PC-SSVEP) based on the best layout of AR. Classification results showed that AR-SSVEP achieved similar accuracy with PC-SSVEP when the stimulus duration is more than 3s, but when the stimulus duration is less than 2s, the accuracy of AR-SSVEP is lower than PC-SSVEP. Brain topological analysis indicated that the spatial distribution of SSVEP responses is similar, both of which are strongest in the occipital region. Current study indicated that stimulus layout is a key factor when building SSVEP-BCI with AR glasses, especially when the stimulation time is short.","2169-3536","","10.1109/ACCESS.2019.2963442","National Natural Science Foundation of China; Key Project at Central Government Level; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947980","Steady-state visual evoked potentials (SSVEP);brain–computer interfaces (BCI);augmented reality (AR);optical see-through (OST);human–computer interaction","Layout;Glass;Visualization;Electroencephalography;Correlation;Microsoft Windows;Brain-computer interfaces","augmented reality;brain;brain-computer interfaces;electroencephalography;glass;handicapped aids;medical signal processing;neurophysiology;visual evoked potentials","SSVEP stimulus layout effect;augmented reality glasses;steady-state visual evoked potentials-based brain-computer interfaces;high information transfer rate;high application value;disability assistance;human-computer interaction;personal computer screen;repetitive visual stimuli;SSVEP response;PC screen;classification accuracies;stimulus duration;significant accuracy difference;PC-SSVEP;AR-SSVEP achieved similar accuracy;brain topological analysis;building SSVEP-BCI;stimulus layouts;time 3.0 s;time 2.0 s","","","","45","CCBY","1 Jan 2020","","","IEEE","IEEE Journals"
"Perceptual Study of Near-Field Binaural Audio Rendering in Six-Degrees-of-Freedom Virtual Reality","O. S. Rummukainen; S. J. Schlecht; T. Robotham; A. Plinge; E. A. P. Habets","International Audio Laboratories Erlangen, A joint institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; International Audio Laboratories Erlangen, A joint institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; International Audio Laboratories Erlangen, A joint institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; International Audio Laboratories Erlangen, A joint institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; International Audio Laboratories Erlangen, A joint institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","448","454","Auditory localization cues in the near-field are significantly different than in the far-field. The near-field region is within an arm's length of the listener allowing to integrate proprioceptive cues to determine the location of an object in space. This perceptual study compares three non-individualized methods to apply head-related transfer functions (HRTFs) in six-degrees-of-freedom near-field audio rendering, namely, far-field measured HRTFs, multi-distance measured HRTFs, and spherical-model-based HRTFs with near-field extrapolation. To set our findings in context, we provide a real-world hand-held audio source for comparison along with a distance-invariant condition. Two modes of interaction are compared in an audio-visual virtual reality: one allowing the participant to move the audio object dynamically and the other with a stationary audio object but a freely moving listener.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798177","Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual Reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies","Rendering (computer graphics);Loudspeakers;Virtual reality;Visualization;Solid modeling;Engines;Acoustic measurements","acoustic signal processing;audio signal processing;hearing;human computer interaction;virtual reality","near-field binaural audio rendering;six-degrees-of-freedom virtual reality;auditory localization cues;proprioceptive cues;head-related transfer functions;six-degrees-of-freedom near-field audio rendering;spherical-model-based HRTFs;near-field extrapolation;audio-visual virtual reality;handheld audio source;human computer interaction;binaural localization cues","","","","23","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Learning social affordance grammar from videos: Transferring human interactions to human-robot interactions","T. Shu; X. Gao; M. S. Ryoo; S. Zhu","Center for Vision, Cogntion, Learning, and Autonomy, University of California, Los Angeles, USA; Department of Electronic Engineering, Fudan University, China; School of Informatics and Computing, Indiana University, Bloomington, USA; Center for Vision, Cogntion, Learning, and Autonomy, University of California, Los Angeles, USA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","1669","1676","In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989197","","Grammar;Robots;Videos;Human-robot interaction;Spatiotemporal phenomena;Grounding;Real-time systems","graph theory;humanoid robots;human-robot interaction;inference mechanisms;learning (artificial intelligence);video signal processing","social affordance grammar learning;human interactions;human-robot interactions;HRI;spatiotemporal AND-OR graph;ST-AOG;humanoid grammar;real-time motion inference;Gibbs sampling;weakly supervised grammar learning;interaction hierarchical representation;RGB-D video dataset;Baxter simulation;human evaluation;real Baxter test;human-like behavior generation","","6","","31","","24 Jul 2017","","","IEEE","IEEE Conferences"
"The wobbly table: Increased social presence via subtle incidental movement of a real-virtual table","M. Lee; K. Kim; S. Daher; A. Raij; R. Schubert; J. Bailenson; G. Welch",University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida and UNC-Chapel Hill; Stanford University; University of Central Florida,"2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","11","17","While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced - however subtly - by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of “Twenty Questions,” where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the “wobbly” group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504683","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology","Electronic mail;Measurement by laser beam;Virtual environments;Games;Training;Haptic interfaces;Force","human computer interaction;virtual reality","wobbly table;social presence;real-virtual table;subtle incidental movement;object movement;human-virtual human interaction;human-VH interaction;shared real-virtual object;real-virtual room;projection screen;virtual world;Twenty Questions game","","17","","42","","7 Jul 2016","","","IEEE","IEEE Conferences"
"Empirical Evidence of Priming, Transfer, Reinforcement, and Learning in the Real and Virtual Trillium Trails","M. C. R. Harrington","University of Pittsburgh, Pittsburgh","IEEE Transactions on Learning Technologies","9 Jun 2011","2011","4","2","175","186","Over the past 20 years, there has been a debate on the effectiveness of virtual reality used for learning with young children, producing many ideas but little empirical proof. This empirical study compared learning activity in situ of a real environment (Real) and a desktop virtual reality (Virtual) environment, built with video game technology, for discovery-based learning. The experiences were in the form of two field trips featuring statistically identical wildflower reserves. While the results support that the Real is superior for learning activity, they also show that the Virtual is useful for priming and reinforcing in-curriculum material, or for situations when the real environment is inaccessible. Offering the Virtual first primes for learning activity in the Real; if used second, it reinforces the Real experience, as supporting evidence shows significant transfer effects. Thus, the Virtual may serve educational goals, if used appropriately, and can come close to the Real. As informal learning environments, such as field trips and video games, are accepted as motivational, an attitudinal survey was conducted postexperiences to capture motivational factors at play, to aid in comparison and contrast, and to provide context to the empirical results on learning activity in situ; however, more work is needed.","1939-1382","","10.1109/TLT.2010.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5539765","Child-computer-environment interaction;child-computer interface;discovery-based learning;educational simulation;evaluation/methodology;serious games;human factors in software design;human-computer interaction;human information processing;informal learning;intrinsic learning;salient events;simulation;modeling;visualization;software psychology;virtual reality;user-centered design;user interfaces.","Biological system modeling;Solid modeling;Software;Virtual environment;Games;Environmental factors","computer aided instruction;computer games;virtual reality","virtual trillium trails;virtual reality used;learning activity;video game technology;discovery-based learning;informal learning environments","","17","","51","","5 Aug 2010","","","IEEE","IEEE Journals"
"User Performance of VR-Based Dissection: Direct Mapping and Motion Coupling of a Surgical Tool","F. Trejo; Y. Hu","Dept. of Electr. & Comput. Eng., Univ. of Calgary, Calgary, AB, Canada; Dept. of Electr. & Comput. Eng., Univ. of Calgary, Calgary, AB, Canada","2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","17 Jan 2019","2018","","","3039","3044","Robot-assisted surgical systems aim at enhancing surgeon's skills. Nonetheless, the learning curve for mastering such systems is very slow due to the motion-coupling mode that is usually presented in these systems for manipulating a surgical tool. This mode has limitations compared to the direct mapping mode used in open surgery for manipulating a tool. Virtual reality (VR) surgical simulators may reduce the learning time for transferring the surgeon's skills from direct mapping to motion coupling of tool manipulation. This may be accomplished by adding two features to the simulator. First, force models of tool-tissue interaction can be implemented in the haptic interface of the simulator. Second, VR-based surgical tasks can be designed to recreate directmapping mode and motion coupling mode of tool manipulation, as in open and robot-assisted surgeries, respectively. This may permit to transfer the surgeon's skills from open surgery to robotassisted surgery in a timely manner. This work presents a preliminary study on the effect of direct mapping mode and motion coupling mode of tool manipulation on the performance of naïve participants for VR-based brain tissue dissection. An Analytic force model of soft-tissue dissection was implemented in the simulator along with visual feedback of a predefined tool speed of 1 mm/s, which is observed in neurosurgery. The outcomes indicated that the motion quality of the tool via direct mapping was significantly better than with motion coupling. Thus, the study might serve as a first step toward the assessment of user's skills for VR-based robot-assisted dissection.","2577-1655","978-1-5386-6650-0","10.1109/SMC.2018.00516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616512","VR-based surgical simulation;direct mapping;motion coupling;robot-assisted surgery;soft-tissue dissection","Tools;Surgery;Task analysis;Robots;Couplings;Analytical models;Force","biological tissues;brain;haptic interfaces;medical computing;medical robotics;surgery;virtual reality","VR-based dissection;surgical tool;robot-assisted surgical systems;surgeon;motion-coupling mode;direct mapping mode;open surgery;surgical simulators;tool manipulation;tool-tissue interaction;VR-based surgical tasks;motion coupling mode;robot-assisted surgeries;VR-based brain tissue dissection;predefined tool speed;motion quality;VR-based robot-assisted dissection;direct-mapping mode;velocity 1.0 mm/s","","","","22","","17 Jan 2019","","","IEEE","IEEE Conferences"
"How much is your virtual environment worth? Considering economical requests in online 3D-worlds","M. Kurze","T-Nova, Deutsche Telekom, Germany","Proceedings IEEE Virtual Reality 2001","7 Aug 2002","2001","","","295","296","While for 2D Web sites there are means to evaluate user navigation from page to page and thus rate a sites value, there is nothing like that available for 3D environments. The user tracing approach described here provides information about users' navigation and interaction in 3D environments which can be utilized to rate a 3D Web site's value and also enhance the usability of the overall site. All information is collected on the client and then transferred to the server to be available for later evaluation.","","0-7695-0948-7","10.1109/VR.2001.913804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=913804","","Virtual environment;Environmental economics;Navigation;Databases;Protocols;Environmental management;Usability;Portals;Hardware;HTML","information resources;client-server systems;information retrieval;virtual reality","virtual environment;economical requests;online 3D worlds;user navigation evaluation;user tracing approach;user interaction;3D Web site;usability;value rating;client;server","","","","2","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Methods of Efficiency Enhancement of Network Interaction in Distributed Systems of Virtual Reality","V. Y. Kharitonov",Moscow power engineering institute,"2nd International Conference on Dependability of Computer Systems (DepCoS-RELCOMEX '07)","16 Jul 2007","2007","","","305","308","In this paper the main problems taking place when organizing consistent network interaction in distributed systems of a virtual reality are considered. In particular, the physical limitations influencing reliability of data exchanges between users are considered, and their analytical estimation is given. Then an overview of modern methods that allow to overcome these limitations is provided, such as dead reckoning. Finally, the new adaptive dead reckoning method is proposed to reduce data exchanges between clients considerably while saving desirable state prediction accuracy of corresponded objects. Moreover this method allows to lower the reliability requirements for data transfer in a network using less reliable, but faster and more efficient protocols, such as UDP.","","0-7695-2850-3","10.1109/DEPCOS-RELCOMEX.2007.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272923","","Virtual reality;Delay;Bandwidth;Virtual environment;Coherence;Computer network reliability;Dead reckoning;Workstations;Size measurement;Time measurement","computer network reliability;virtual reality","network interaction enhancement;distributed systems;virtual reality;dead reckoning;data transfer;UDP","","2","","5","","16 Jul 2007","","","IEEE","IEEE Conferences"
"VR-OOS: The DLR's virtual reality simulator for telerobotic on-orbit servicing with haptic feedback","M. Sagardia; K. Hertkorn; T. Hulin; S. Schätzle; R. Wolff; J. Hummel; J. Dodiya; A. Gerndt","German Aerospace Center (DLR), Muenchener Str. 20, 82234 Wessling, Germany; German Aerospace Center (DLR), Muenchener Str. 20, 82234 Wessling, Germany; German Aerospace Center (DLR), Muenchener Str. 20, 82234 Wessling, Germany; German Aerospace Center (DLR), Muenchener Str. 20, 82234 Wessling, Germany; German Aerospace Center (DLR), Lilienthalplatz 7, 38108 Braunschweig, Germany; German Aerospace Center (DLR), Lilienthalplatz 7, 38108 Braunschweig, Germany; German Aerospace Center (DLR), Lilienthalplatz 7, 38108 Braunschweig, Germany; German Aerospace Center (DLR), Lilienthalplatz 7, 38108 Braunschweig, Germany","2015 IEEE Aerospace Conference","8 Jun 2015","2015","","","1","17","The growth of space debris is becoming a severe issue that urgently requires mitigation measures based on maintenance, repair, and de-orbiting technologies. Such on-orbit servicing (OOS) missions, however, are delicate and expensive. Virtual Reality (VR) enables the simulation and training in a flexible and safe environment, and hence has the potential to drastically reduce costs and time, while increasing the success rate of future OOS missions. This paper presents a highly immersive VR system with which satellite maintenance procedures can be simulated interactively using visual and haptic feedback. The system can be used for verification and training purposes for human and robot systems interacting in space. Our framework combines unique realistic virtual reality simulation engines with advanced immersive interaction devices. The DLR bimanual haptic device HUG is used as the main user interface. The HUG is equipped with two light-weight robot arms and is able to provide realistic haptic feedback on both human arms. Additional devices provide vibrotactile and electrotactile feedback at the elbow and the fingertips. A particularity of the realtime simulation is the fusion of the Bullet physics engine with our haptic rendering algorithm, which is an enhanced version of the Voxmap-Pointshell Algorithm. Our haptic rendering engine supports multiple objects in the scene and is able to compute collisions for each of them within 1 msec, enabling realistic virtual manipulation tasks even for stiff collision configurations. The visualization engine ViSTA is used during the simulation to achieve photo-realistic effects, increasing the immersion. In order to provide a realistic experience at interactive frame rates, we developed a distributed system architecture, where the load of computing the physics simulation, haptic feedback and visualization of a complex scene is transferred to dedicated machines. The implementations are presented in detail and the performance of the overall system is validated. Additionally, a preliminary user study in which the virtual system is compared to a physical test bed shows the suitability of the VR-OOS framework.","1095-323X","978-1-4799-5380-6","10.1109/AERO.2015.7119040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119040","","Computational modeling;Force;Haptic interfaces;Robots;Satellites;Solid modeling;Space vehicles","aerospace computing;aerospace robotics;aerospace safety;haptic interfaces;realistic images;real-time systems;rendering (computer graphics);space debris;telerobotics;virtual reality","DLR virtual reality simulator;telerobotic on-orbit servicing missions;space debris;mitigation measures;repair;de-orbiting technologies;OOS missions;immersive VR system;satellite maintenance procedures;visual feedback;robot systems;realistic virtual reality simulation engines;immersive interaction devices;DLR bimanual haptic device HUG;user interface;robot arms;realistic haptic feedback;vibrotactile feedback;electrotactile feedback;realtime simulation;Bullet physics engine;haptic rendering algorithm;Voxmap-Pointshell algorithm;haptic rendering engine;realistic virtual manipulation tasks;stiff collision configurations;visualization engine ViSTA;photo-realistic effects;realistic experience;interactive frame rates;distributed system architecture;physics simulation;complex scene;virtual system;VR-OOS framework","","9","1","43","","8 Jun 2015","","","IEEE","IEEE Conferences"
"Two-handed direct interaction with ARTootKit","S. Veigl; A. Kaltenbach; F. Ledermann; G. Reitmayr; D. Schmalstieg","Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria; Vienna Univ. of Technol., Austria","The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","2 pp.","","A mobile augmented reality system for 3D interaction requires an easy to use, interactive input device. In our work on the Studierstube project, we had long-term experience with two-handed direct manipulation interfaces based on tracked props. However, for a mobile setup, props are no longer appropriate as they permanently occupy the user's hands and prevent the user from performing everyday tasks such as opening doors. Instead, we resorted to the common tool which is very intuitive to use and can be applied for nearly all interactions: our hands. A big advantage of hand based interaction is the intuitive use of gestures such as pointing, grabbing or stretching.","","0-7803-7680-3","10.1109/ART.2002.1106984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106984","","Optical sensors;Thumb;Cameras;Fingers;Mechanical sensors;Control systems;Robustness;Bicycles;Cotton;Fires","augmented reality;data gloves;gesture recognition","mobile augmented reality;3D interaction;interactive input device;Stradierstube project;two-handed direct manipulation;hand based interaction;gestures;virtual reality;glove","","3","","5","","6 Jan 2003","","","IEEE","IEEE Conferences"
"VirtualTablet: Extending Movable Surfaces with Touch Interaction","A. H. Hoppe; F. Marek; F. van de Camp; R. Stiefelhagen","Karlsruhe Institute of Technology (KIT), IAR; Karlsruhe Institute of Technology (KIT); Fraunhofer IOSB; Karlsruhe Institute of Technology (KIT), IAR","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","980","981","Immersive output and effortless input are two core aspects of a virtual reality (VR) experience. We transfer ubiquitous touch interaction with haptic feedback into a virtual environment (VE). The movable and cheap real world object supplies an accurate touch detection equal to a ray-casting interaction with a controller. Moreover, the virtual tablet extends the functionality of a real world tablet. Additional information is displayed in mid-air around the touchable area and the tablet can be turned over to interact with both sides. It allows easy to learn and precise system interaction and can even augment the established touch metaphor with new paradigms.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797993","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality","Three-dimensional displays;Cameras;Haptic interfaces;Visualization;Virtual environments;User interfaces","haptic interfaces;human computer interaction;notebook computers;touch sensitive screens;ubiquitous computing;virtual reality","VirtualTablet;movable surfaces;virtual reality experience;VR;ubiquitous touch interaction;haptic feedback;virtual environment;ray-casting interaction;touch metaphor;VE","","","","9","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Training Transfer of Bimanual Assembly Tasks in Cost-Differentiated Virtual Reality Systems","S. Shen; H. -T. Chen; T. W. Leong","School of Software, University of Technology, Sydney; School of Software, University of Technology, Sydney; School of Software, University of Technology, Sydney","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1152","1153","Recent advances of the affordable virtual reality headsets make virtual reality training an economical choice when compared to traditional training. However, these virtual reality devices present a range of different levels of virtual reality fidelity and interactions. Few works have evaluated their validity against the traditional training formats. This paper presents a study that compares the learning efficiency of a bimanual gearbox assembly task among traditional training, virtual reality training with direct 3D inputs (HTC VIVE), and virtual reality training without 3D inputs (Google Cardboard). A pilot study was conducted and the result shows that HTC VIVE brings the best learning outcomes.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797917","Assistive systems;Head-mounted Display;Virtual Reality;Learning Transfer","Training;Task analysis;Virtual reality;Three-dimensional displays;Headphones;Google;Software","assembling;gears;helmet mounted displays;industrial training;production engineering computing;virtual reality","virtual reality training;bimanual gearbox assembly task;cost-differentiated virtual reality systems;virtual reality headsets;traditional training;head-mounted display","","","","3","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Tangible interaction for conceptual architectural design","J. M. S. Dias; P. Santos; N. Diniz; L. Monteiro; R. Silvestre; R. Bastos","ADETTI/UNIDE, Lisbon, Portugal; ADETTI/UNIDE, Lisbon, Portugal; ADETTI/UNIDE, Lisbon, Portugal; ADETTI/UNIDE, Lisbon, Portugal; ADETTI/UNIDE, Lisbon, Portugal; ADETTI/UNIDE, Lisbon, Portugal","The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","9 pp.","","This work reports and experiments a tangible mixed reality system for architectural design usage scenarios, such as conceptual design, client brief or architectural design education. The system provides the means for an architect to intuitively interact with an augmented version of a real scale model, in normal working settings, where he can observe 3D virtual objects registered to the real ones. Intuitive tangible interfaces, implemented with paddles, are used to aid design and editing tasks in the mixed environment. By means of paddle gesturing recognition, it is possible to activate a menu, browse and choose menu options or pick, move, rotate and scale 3D virtual objects, within the scale model working area. To transport the user from augmented reality to a fully virtual environment (supporting real-time scene navigation, while in VR mode) and back, paddle gesturing recognition is also used. Complex architectural designs, described in the VRML 97 data format, can be imported into our system, which provides a platform, for testing now design concepts while seamlessly transporting the architect in a truly mixed reality environment: from reality (RE) to augmented reality (AR) and then through augmented virtuality (AV), towards a full virtual environment (VE), and back.","","0-7803-7680-3","10.1109/ART.2002.1106951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106951","","Virtual reality;Testing;Design automation;Augmented reality;Cameras;Layout;Programmable logic arrays;Process design;Shape;Visualization","augmented reality;architectural CAD;gesture recognition;virtual reality languages","conceptual architectural design;tangible interaction;tangible mixed reality system;client brief;architectural design education;real scale model;3D virtual objects;intuitive tangible interfaces;editing;paddle gesture recognition;menu;virtual environment;VRML 97 data format","","3","5","13","","6 Jan 2003","","","IEEE","IEEE Conferences"
"A novel approach for addressing extensibility issue in collaborative virtual environment","Choo Tian Fook; Lin Qingping; Zhang Liang","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore","Proceedings. 2003 International Conference on Cyberworlds","8 Jan 2004","2003","","","78","84","In order to achieve entity evolvement, enrich socialization of virtual community and optimize computer processing in the collaborative virtual environment (CVE), efficient collaborative interaction management (CIM) is a key to handle these issues. In this paper, we focus on functional extensibility in CVE, and give the solution by using the conceptual idea of our proposed CIM approach, called task-oriented interaction management. This approach has four important components, which are scene interaction manager, virtual entity interaction manager, interaction task agent, and scene role & rules. They have certain capabilities, like data distribution, load-balance, tasks migration, ownership transfer, entities' states consistency maintenance, database persistency management and etc. The motivation of this research is to develop an extensible and evolving CVE with extensible functionalities for creating large-scale collaborative interaction experience to users in a large-scale interconnected 3D virtual environment. Details of the conceptual framework of the task-oriented interaction management will be presented in this paper.","","0-7695-1922-9","10.1109/CYBER.2003.1253438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253438","","Collaboration;Virtual environment;Virtual reality;Computer integrated manufacturing;Layout;Large-scale systems;Graphics;Collaborative work;Environmental management;Technology management","virtual reality;groupware;task analysis;software architecture;computer networks;object-oriented programming;object-oriented methods","collaborative virtual environment;CVE;collaborative interaction management;CIM;functional extensibility;scene integration manager;virtual entity interaction manager;interaction task agent;data distribution;load balance;tasks migration;ownership transfer;database persistency management;entitie state consistency maintenance;3D virtual environment;task-oriented interaction management","","2","","16","","8 Jan 2004","","","IEEE","IEEE Conferences"
"Coupled-clay: Physical-virtual 3D collaborative interaction environment","K. Ozacar; T. Hagiwara; J. Huang; K. Takashima; Y. Kitamura",Research Institute of Electrical Communication Tohoku University; Research Institute of Electrical Communication Tohoku University; Research Institute of Electrical Communication Tohoku University; Research Institute of Electrical Communication Tohoku University; Research Institute of Electrical Communication Tohoku University,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","255","256","We propose Coupled-clay, a bi-directional 3D collaborative interactive environment that supports the 3D modeling work between groups of users at remote locations. Coupled-clay consists of two network-connected workspaces, the Physical Interaction Space and the Virtual Interaction Space. The physical interaction space allows a user to directly manipulate a physical object whose shape and position are precisely tracked. This tracked 3D information is transferred to the virtual interaction space in real time. The virtual interaction space is made of an interactive multi-user stereoscopic 3D tabletop, or other 3D displays with adequate interaction device. The users at the virtual interaction space observe the virtual 3D object which corresponds to the physical object and manipulate its geometrical attributes (e.g., translation, rotation and scaling). Additionally, they can control the graphical attributes of the virtual object such as color and texture. Information about changes in geometrical and graphical attributes are sent back to the physical interaction space in real time and reflected to the object in the physical interaction space by a robotic arm and a top-mounted projector. Coupled-clay can be used to remotely collaborate on 3D modeling tasks such as between a skilled designer and novice learners. This paper details our Coupled-clay implementation and presents its interaction capabilities.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223392","H.5.3 [INFORMATION INTERFACES AND PRESENTATION]: Group and Organization Interfaces — Collaborative computing;I.3.7 [Computing Methodologies];COMPUTER GRAPHICS — Three-Dimensional Graphics and Realism","Three-dimensional displays;Shape;Real-time systems;Collaboration;Bidirectional control;Electronic mail;Robots","interactive devices;solid modelling;stereo image processing;three-dimensional displays;virtual reality","coupled-clay;physical-virtual 3D collaborative interaction environment;bidirectional 3D collaborative interactive environment;remote location;network-connected workspace;physical interaction space;virtual interaction space;3D information;multiuser stereoscopic 3D tabletop;3D display;interaction device;virtual 3D object;physical object;geometrical attribute;graphical attribute;virtual object;robotic arm;top-mounted projector;3D modeling task","","","","4","","27 Aug 2015","","","IEEE","IEEE Conferences"
"The avatar navigation of distributed virtual environment by using multiview client","ManKyu Sung; ChanJong Park","Human Comput. Interaction Dept., Syst. Eng. Res. Inst., Taejun, South Korea; NA","Proceedings. 3rd Asia Pacific Computer Human Interaction (Cat. No.98EX110)","6 Aug 2002","1998","","","108","113","The multi participant VR system can more enhance the sense of reality than existing single user VR systems. But, we should be careful when designing the system, since the current Internet using TCP/IP cannot afford to deliver such a massive amount of information required for the real time constraint of a VR system. We introduce distributed network structure and protocols for multiple user virtual space navigation and interaction through their avatars. This network structure is composed of heterogeneous multi servers, multiview clients and protocols. The multiview client can give proper information to the avatar when navigating the distributed virtual environment. And, for minimizing traffic on the network, we designed the VSTP-the Virtual Space Transfer Protocol to meet real time constraints. The avatars have nine behaviors and recognize the state of each other by using the pre-defined VSTP protocol.","","0-8186-8347-3","10.1109/APCHI.1998.704167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=704167","","Avatars;Navigation;Virtual environment;Protocols;Virtual reality;Internet;TCPIP;Real time systems;Network servers;Telecommunication traffic","virtual reality;multi-access systems;real-time systems;transport protocols;user interfaces;client-server systems;interactive systems;human factors","avatar navigation;distributed virtual environment;multiview client;multi participant VR system;single user VR system;Internet;TCP/IP;real time constraint;VR system;distributed network structure;protocols;multiple user virtual space navigation;network structure;heterogeneous multi servers;Virtual Space Transfer Protocol;VSTP protocol","","","7","10","","6 Aug 2002","","","IEEE","IEEE Conferences"
"The synthetic character Ritchie: first steps towards a virtual companion for mixed reality","K. Dorfinuller-Ulhaas; E. Andre","Augsburg Univ., Germany; Augsburg Univ., Germany","Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)","5 Dec 2005","2005","","","178","179","Unlike most existing work on traversable interfaces, we focus on the use of synthetic characters to accompany the user in mixed reality (MR) applications. We examine virtual companions as a promising means to design smooth transitions between different worlds and to avoid orientation problems. We propose a taxonomy to describe the spatial relationship between character and user which has an important impact on the style of interaction. To flexibly transfer user and character into different spaces, we have created a platform that supports the design of interfaces derived from the proposed taxonomy as well as transitions between them.","","0-7695-2459-1","10.1109/ISMAR.2005.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544682","","Virtual reality;Space technology;Taxonomy;Humans;User interfaces;Cities and towns;Displays;Switches;Geometry;Augmented reality","virtual reality;user interfaces","synthetic character Ritchie;virtual reality;mixed reality application;traversable interfaces;design interfaces","","2","","5","","5 Dec 2005","","","IEEE","IEEE Conferences"
"VR training system with adaptive operational assistance considering straight-line transfer operation","M. Yoneda; F. Arai; T. Fukuda; K. Miyata; T. Naito","Dept. of Micro Syst. Eng., Nagoya Univ., Japan; NA; NA; NA; NA","8th IEEE International Workshop on Robot and Human Interaction. RO-MAN '99 (Cat. No.99TH8483)","6 Aug 2002","1999","","","142","147","This paper deals with an operational assistance system of a rough terrain crane. A proper control rule to operate the payload in the straight-line motion is proposed. The system assists straight-line operation with force display based on the rule. The strength of assistance can be adapted to operator's skill level by changing the gain of force display. We present some experiments on a crane simulator, and show the effectiveness of the proposed operational assistance system.","","0-7803-5841-4","10.1109/ROMAN.1999.900330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=900330","","Virtual reality;Adaptive systems;Cranes;Control systems;Automatic control;Displays;Proposals;Payloads;Systems engineering and theory;Motion control","cranes;computer based training;virtual reality;force feedback;simulation;adaptive systems","training;operational assistance system;straight-line motion;force display;operator skill;crane simulator;virtual reality","","1","","6","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Rock or Roll – Locomotion Techniques with a Handheld Spherical Device in Virtual Reality","D. Englmeier; F. Fan; A. Butz","LMU Munich,Munich,Germany; LMU Munich,Munich,Germany; LMU Munich,Munich,Germany","2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","14 Dec 2020","2020","","","618","626","We investigate the use of a handheld spherical object as a controller for locomotion in VR. Rotating the object controls avatar movement in two different ways: As a zero order controller, it is continuously rotated to the target position as if rolling a ball on the floor. As a first order controller, it is tilted like a joystick to determine the direction and speed of movement. We describe how our prototype was built from low-cost commercially available hardware and discuss our design decisions. Then we evaluate both locomotion techniques in a user study (N=20) and compare them to established methods using handheld VR controllers. Our prototype matched and in some cases outperformed these methods regarding task time and accuracy. All results were obtained without any usage instructions, indicating easy learnability. Some of our insights may transfer to interaction with other naturally shaped objects in VR experiences.","1554-7868","978-1-7281-8508-8","10.1109/ISMAR50242.2020.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284676","Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality","Avatars;Prototypes;Rocks;Hardware;Task analysis;Floors;Augmented reality","avatars;human computer interaction;interactive devices","design decisions;locomotion techniques;handheld VR controllers;naturally shaped objects;VR experiences;handheld spherical device;virtual reality;handheld spherical object;avatar movement;zero order controller","","","","48","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Augmented Reality applied to the teaching/learning environment","P. Magalhães; A. Castro; C. V. Carvalho","GILT (Graphics, Interaction and Learning Technologies), Instituto Superior de Engenharia do Porto, Portugal; GILT (Graphics, Interaction and Learning Technologies), Instituto Superior de Engenharia do Porto, Portugal; GILT (Graphics, Interaction and Learning Technologies), Instituto Superior de Engenharia do Porto, Portugal","6th Iberian Conference on Information Systems and Technologies (CISTI 2011)","4 Aug 2011","2011","","","1","5","In the last decades we have witnessed a technological advancement at all levels with particular emphasis in hardware and mobile devices. These became increasingly lighter and cheaper, and transferred from the office to the car, to equipment and other utensils. The amount of (digital) information available in the environment has increased exponentially, requiring a technological response in order to improve and facilitate its access and assimilation. The concept of Augmented Reality acts as a bridge between real and digital inviting to new models of user interaction. The AR incorporation is intended primarily to make systems more usable by decreasing the need of cognitive load inherent to their use. This paper presents a case study, proposing a model for a Learning Object using Augmented Reality, studying the integration of augmented reality and multimedia techniques combined with other conventional materials researching how they may contribute to increase motivation and perceived towards knowledge.","2166-0735","978-989-96247-5-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974235","Learning Objects;Human-Computer Interaction;Augmented Reality;Usability;Health","Visualization;Augmented reality;Software;Biological system modeling;Laboratories;Browsers","augmented reality;computer aided instruction;multimedia systems;user interfaces","augmented reality;teaching-learning environment;user interaction;learning object model;multimedia technique","","1","","8","","4 Aug 2011","","","IEEE","IEEE Conferences"
"Cognitive psychology and human factors engineering of virtual reality","A. K. T. Ng","The University of Hong Kong, Hong Kong, China","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","407","408","This position paper summarizes the author's research interest in Cognitive Psychology and Human-Computer Interaction in the imseCAVE, a CAVE-like system in the University of Hong Kong. Several areas of interest were explored while finding the thesis topic for the Ph.D. research. They include a perception research on distance estimation with proposed error correction mechanism, neurofeedback meditation with EEG in VR and the effect with audio and video, the study of training transfer in VR training, the comparison and research of cybersickness between HMD and the imseCAVE, and comparing VR gaming in TV, HMD, and the imseCAVE by performance, activity level and time perception. With a broad interest, the exact direction is still in the search and requires future exploration.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892349","Virtual environment;cognitive psychology;HCI","Training;Psychology;Resists;Virtual reality;Human factors;Ergonomics;Human computer interaction","cognition;human factors;virtual reality","cognitive psychology;human factors engineering;virtual reality;CAVE-like system;error correction mechanism;neurofeedback meditation;EEG;VR;imseCAVE;distance estimation","","","","13","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Pinch-n-Paste: Direct texture transfer interaction in augmented reality","A. Umakatsu; T. Mashita; K. Kiyokawa; H. Takernura",Osaka University; Osaka University; Osaka University; Osaka University,"2013 IEEE Virtual Reality (VR)","9 Sep 2013","2013","","","73","74","Our Pinch-n-Paste allows a user to touch or pinch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object. In this poster, we will describe the basic idea, implementation details, and example interaction results and a preliminary user study.","2375-5334","978-1-4673-4796-9","10.1109/VR.2013.6549369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549369","Augmented Reality;texture transfer;3D physics interaction","Brushes;Three-dimensional displays;Electronic mail;Clothing;Augmented reality;Cameras;Mathematical model","augmented reality;image texture;least squares approximations;user interfaces","texture transfer interaction;pinch-n-paste interaction technique;augmented reality environment;least square conformal map technique;LSCM technique;texture image generation;cross-boundary brush;moving least squares;MLS;object mapping","","","","7","","9 Sep 2013","","","IEEE","IEEE Conferences"
"Multi-user networked framework for virtual reality platform","A. Rai; R. J. Kannan; S. Ramanathan","School of Computing Science & Engineering, VIT University, Chennai, India; School of Computing Science & Engineering, VIT University, Chennai, India; School of Computing Science & Engineering, VIT University, Chennai, India","2017 14th IEEE Annual Consumer Communications & Networking Conference (CCNC)","20 Jul 2017","2017","","","584","585","Networked Virtual Environments (NEV) are sets of disjointed virtual worlds which are topographically set apart from the users yet connected through the communication network to give them the illusion of being in the same virtual location. NVEs are increasingly receiving consideration from business and research point of view. Contrasted with various sorts of possible applications, NVEs require a high responsiveness to ensure continuous streaming of live data for full immersion. It is as yet difficult to develop such applications since it require to create virtual reality framework over distributed databases; as every user needs locally adequate data flow to concurrently construct a scene as with other users and able to accommodate multi-user interactions in the virtual environment. Such information requires simultaneous computation and communication of streaming data. In this study we built up a prototype of a NVE in view of the blend of Minority Game (MG) and Online Induction Algorithm (OIA) with lossless transfer functions.","2331-9860","978-1-5090-6196-9","10.1109/CCNC.2017.7983177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983177","Networked Virtual Environments;Online Learning;Data Stream control;Adaptive Networking","Rendering (computer graphics);Virtual environments;Databases;Feature extraction;Conferences;Tracking","game theory;inference mechanisms;transfer functions;virtual reality","multiuser networked framework;NVE;networked virtual environments;minority game;MG;online induction algorithm;OIA;lossless transfer functions;virtual reality platform","","","","3","","20 Jul 2017","","","IEEE","IEEE Conferences"
"Comparing HMD-Based and Paper-Based Training","S. Werrlich; A. Daniel; A. Ginger; P. Nguyen; G. Notni","BMW Group, Munich, Germany; BMW Group, Munich, Germany; BMW Group, Munich, Germany; BMW Group, Munich, Germany; Tech. Univ. Ilmenau, Ilmenau, Germany","2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","17 Jan 2019","2018","","","134","142","Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ.","1554-7868","978-1-5386-7459-8","10.1109/ISMAR.2018.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613759","Augmented Reality;Evaluation;Head Mounted Displays;Training","Task analysis;Training;Engines;Software;Resists;Atmospheric measurements;Particle measurements","augmented reality;computer based training;groupware;helmet mounted displays;human computer interaction;human factors;user interfaces;virtual reality","paper-based training;Collaborative Systems;smartwatches;head-mounted displays;industrial domain;scientific laboratory studies;augmented reality;training tasks;inadequate task complexity;training transfer;engine assembly training task;user satisfaction;system usability scale;user experience questionnaire;Nasa Task Load Index;paper-based instructions;assistive systems;multimodal HMD-based learning","","5","","28","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Interactive features based augmented reality authoring tool","J. Shim; M. Kong; Y. Yang; J. Seo; T. Han","Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Yonsei University, Seoul, Republic of Korea","2014 IEEE International Conference on Consumer Electronics (ICCE)","20 Mar 2014","2014","","","47","50","This paper intends to propose an authoring tool system that allows users to easily create augmented reality content with the application of marker based and gesture interactions. It is possible to generate a simplified form of marker based augmented reality content with the user interface by using Kinect that enables gesture interaction. We seek to provide methods for the user to actively interact with augmented reality content. The interaction method we put forth in this study is a gesture interaction approach capable of Transfer, Rotate, Enlarge, and Shrink an object by recognizing and tracking the user's bare hand. As for marker based interaction methods, there is one employing two markers and another using marker occlusion. We ascertain the positive results of this study from the user evaluation of an authoring tool system and marker based and gesture interactions.","2158-4001","978-1-4799-1291-9","10.1109/ICCE.2014.6775902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6775902","","Augmented reality;Thumb;Image segmentation;User interfaces;Real-time systems;Computer vision","augmented reality;authoring systems;gesture recognition;interactive devices;object recognition;object tracking","interactive features based augmented reality authoring tool;marker based interaction;gesture interactions;marker based augmented reality content;user interface;Kinect;transfer;rotate;enlarge;shrink;user bare hand recognition;user bare hand tracking;marker occlusion","","4","","12","","20 Mar 2014","","","IEEE","IEEE Conferences"
"Enlightening Patients with Augmented Reality","A. Jakl; A. Lienhart; C. Baumann; A. Jalaeefar; A. Schlager; L. Schöffer; F. Bruckner","University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria; University of Applied Sciences,Institute of Creative\Media/Technologies,St. Pölten,Austria","2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","11 May 2020","2020","","","195","203","Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures.","2642-5254","978-1-7281-5608-8","10.1109/VR46266.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089476","Human-centered computing;Mixed / augmented reality Human-centered computing;Interface design prototyping Human-centered computing;Interaction design theory;concepts and paradigms Human-centered computing;Usability testing","Education;Three-dimensional displays;Augmented reality;Surgery;Human computer interaction;Usability;Prototypes","augmented reality;computer aided instruction;eye;medical computing;surgery;virtual reality","EPAR;patient education;medical procedures;enlightening patients with augmented reality;interactive storytelling","","","","57","","11 May 2020","","","IEEE","IEEE Conferences"
"Increasing the effective egocentric field of view with proprioceptive and tactile feedback","Ungyeon Yang; G. Jounghyun Kim","Virtual Reality Div., Electr. & Telecom. Res. Inst., Daejeon, South Korea; NA","IEEE Virtual Reality 2004","12 Jul 2004","2004","","","27","34","Multimodality often exhibits synergistic effects: each modality compliments and compensates for other modalities in transferring coherent, unambiguous, and enriched information for higher interaction efficiency and improved sense of presence. In this paper, we explore one such phenomenon: a positive interaction among the geometric field of view, proprioceptive interaction, and tactile feedback. We hypothesize that, with proprioceptive interaction and tactile feedback, the geometric field of view and thus visibility can be increased such that it is larger than the physical field of view, without causing a significant distortion in the user's distance perception. This, in turn, would further help operation of the overall multimodal interaction scheme as the user is more likely to receive the multimodal feedback simultaneously. We tested our hypothesis with an experiment to measure the user's change in distance perception according to different values of egocentric geometric field of view and feedback conditions. Our experimental results have shown that, when coupled with physical interaction, the GFOV could be increased by up to 170 percent of the physical field of view without introducing significant distortion in distance perception. Second, when tactile feedback was introduced, in addition to visual and proprioceptive cues, the GFOV could be increased by up to 200 percent. The results offer a useful guideline for effectively utilizing of modality compensation and building multimodal interfaces for close range spatial tasks in virtual environments. In addition, it demonstrates one way to overcome the shortcomings of the narrow (physical) fields of views of most contemporary HMDs.","1087-8270","0-7803-8415-6","10.1109/VR.2004.1310052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310052","","Feedback;Virtual reality;Space technology;Layout;Telecommunications;Computer science;Testing;Distortion measurement;Guidelines;Virtual environment","virtual reality;image motion analysis;helmet mounted displays;distortion;haptic interfaces","tactile feedback;geometric field of view;proprioceptive interaction;multimodal interaction;multimodal feedback;physical interaction;GFOV;proprioceptive cues;modality compensation;multimodal interfaces;spatial tasks;virtual environments;HMDs","","2","","33","","12 Jul 2004","","","IEEE","IEEE Conferences"
"Touch-n-Paste: Direct texture transfer interaction in AR environments","A. Umakatsu; T. Mashita; K. Kiyokawa; H. Takemura","Osaka University, Japan; Osaka University, Japan; Osaka University, Japan; Osaka University, Japan","2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","7 Jan 2013","2012","","","325","326","Our Touch-n-Paste allows a user to touch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object.","","978-1-4673-4662-7","10.1109/ISMAR.2012.6402596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402596","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;and virtual realities I.3.7 [Computer Graphics]: Three-dimensional Graphics and Realism—Color;shading;shadowing;texture","Clothing;Educational institutions;Electronic mail;Solid modeling;Image color analysis;Brushes;Computer graphics","augmented reality;image texture;least squares approximations;tactile sensors","touch-n-paste;direct texture transfer interaction;AR environments;augmented reality environment;image texture;least square conformal map technique;LSCM technique;cross-boundary brushes;target areas of interest;target texel values;source texels;moving least squares;MLS","","1","","6","","7 Jan 2013","","","IEEE","IEEE Conferences"
"MoVEROffice: Virtual Reality for Upper Limbs Rehabilitation","R. V. Aranha; L. V. Araújo; C. B. M. Monteiro; T. D. Da Silva; F. L. S. Nunes","Lab. de Aplic. de Inf. em Saude, Univ. de Sao Paulo, Sao Paulo, Brazil; Lab. de Aplic. de Inf. em Saude, Univ. de Sao Paulo, Sao Paulo, Brazil; Lab. de Aplic. de Inf. em Saude, Univ. de Sao Paulo, Sao Paulo, Brazil; Lab. de Aplic. de Inf. em Saude, Univ. de Sao Paulo, Sao Paulo, Brazil; Lab. de Aplic. de Inf. em Saude, Univ. de Sao Paulo, Sao Paulo, Brazil","2016 XVIII Symposium on Virtual and Augmented Reality (SVR)","21 Jul 2016","2016","","","160","169","Considering the complexity involved in the motor rehabilitation process, this paper presents the development of a serious game with virtual reality and natural interaction to act as a support tool for physical therapy professionals. The objective of the developed application is to enable that patients acquire skill in performing tasks in the virtual environment to transfer them later to the real environment. An experiment was conducted to compare the performance of people with and without mobility limitations in the use of the virtual environment. The results showed that the time spent for performing tasks tend to be reduced when the users familiarize them selves with natural interaction, and size and position of objects have influence in the interaction. The study allowed inferring in this sample evaluated that the motor limitations of the patients did not have influence in the performance of the volunteers.","","978-1-5090-4149-7","10.1109/SVR.2016.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517270","Virtual Reality;Natural Interaction;Serious Game;Virtual Rehabilitation","Visualization;Games;Virtual environments;Mice;Augmented reality;Art","medical computing;patient treatment;serious games (computing);virtual reality","MoVEROffice;virtual reality;upper limbs rehabilitation;motor rehabilitation process;natural interaction;physical therapy professionals;virtual environment;mobility limitations","","2","","","","21 Jul 2016","","","IEEE","IEEE Conferences"
"The interactive haptics technique of virtual objects on augmented reality environment","C. Huang; S. Tang","Graduate Institute of Electrical Engineering, National Kaohsiung First University of Science and Technology, Taiwan; Graduate Institute of Electrical Engineering, National Kaohsiung First University of Science and Technology, Taiwan","11th IEEE International Conference on Control & Automation (ICCA)","7 Aug 2014","2014","","","584","589","In this study, a new technique that combines with augmented reality (AR) and haptic device technologies has been proposed. In terms of augmented reality technology, the vision and its interactions are quite importance, but the perceptions of haptic could not break through. The major reason is the scene and the 3D objects only display on a screen, such that reality and attention level down in augmented reality environment, so that the user can not engage with the sensors of vision and touch at the same time. Therefore, this research focus on the haptic behaviors of 3D objects display on augmented reality transfer to haptic device and user can receive the force feedback when touch the 3D objects. The both diverse interface possibilities and influencing people's perception in this two different ways (vision and haptic).","1948-3457","978-1-4799-2837-8","10.1109/ICCA.2014.6870984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6870984","","Phantoms;Augmented reality;Cameras;Three-dimensional displays;Robot vision systems;Force feedback","augmented reality;force feedback;haptic interfaces;natural scenes","interactive haptic technique;virtual objects;augmented reality environment;AR;haptic device technologies;haptic perceptions;3D objects;natural scene;attention level;vision sensors;force feedback;people perception","","","","7","","7 Aug 2014","","","IEEE","IEEE Conferences"
"A syncretic approach to artistic research in mixed reality data transfer","J. Stadon; R. Grasset",Curtin University of Technology; Graz University of Technology,"2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities","1 Dec 2011","2011","","","67","72","This paper offers a contribution to an emerging culturally orientated discourse regarding mixed reality interaction. It seeks to define syncretic, or hybridized agency, particularly in social mixed reality data transfer artworks. Recent developments in bridging relationships with digital representation of identity through mixed reality interfacing, have brought about the need for further analysis of these new post-biological, hybridized states of being that traverse traditional paradigms of biophysics. time and space, and artistic practice. Roy Ascott's interpretation of syncretism within digital networks may facilitate further understanding of multi-layered consciousness, both material and metaphysical, that are emerging from our engagement with such pervasive computational technologies and post-biological systems. Syncretism has traditionally been regarded as an attempt to harmonise and analogise [1] Citing recent examples of practical research outcomes, this paper will cite what Deleuze and Guattari have called `deterritorialisation' of the human body and its dispersion throughout multiple reality manifestations and how mixed reality data transfer might constitute a `reterritorialising' effect that creates a syncretic post-biological digital identity for the user [2].","2381-8360","978-1-4673-0059-9","10.1109/ISMAR-AMH.2011.6093659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093659","Art;Media Art;Performing Arts;Metaverse;Dual Reality;Cross-reality;Post-humanism","Art;Second Life;Avatars;Augmented reality;Humans;Virtual environments","art;ubiquitous computing;virtual reality","syncretic approach;artistic research;mixed reality interaction;social mixed reality data transfer artworks;multilayered consciousness;pervasive computational technologies;post-biological systems;human body deterritorialisation;reterritorialising effect;syncretic post-biological digital identity","","","","22","","1 Dec 2011","","","IEEE","IEEE Conferences"
"Research of inter-domain data filtering method for distributed interactive simulation","L. Li; C. Hongqian; T. Li","School of Computer Science and Information Engineering, Beijing Technology and Business, University, Beijing 100048; School of Computer Science and Information Engineering, Beijing Technology and Business, University, Beijing 100048; School of Computer Science and Information Engineering, Beijing Technology and Business, University, Beijing 100048","2010 International Conference on Audio, Language and Image Processing","10 Jan 2011","2010","","","1451","1456","The distributed interactive simulation of multidomain is an important research area of the distributed virtual reality. However, frequent interaction, great quantity of messages and the discrepant need of each individual simulation domain resulted in numerous redundant messages. Filtering out the redundant data and relieving the transfer loads become a pressing necessity. We analysis the principle of data filtering of HLA standard and introduce a data filtering algorithm based on publish/subscribe. Utilized the data locality feature of distributed interaction simulation, this algorithm can be used in multi-domain either. The experiment shows that the method we introduced provides reliable data filtering performance with incurred incremental costs on purchasing information endurable by our system. In addition, the data filtering matching efficiency is well.","","978-1-4244-5858-5","10.1109/ICALIP.2010.5684393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5684393","","Filtering;Logic gates;Subscriptions;Data models;Bandwidth;Distributed databases;Wide area networks","digital simulation;message passing;middleware;virtual reality","inter-domain data filtering method;distributed interactive simulation;distributed virtual reality;HLA standard;publish-subscribe;distributed interaction simulation","","2","","8","","10 Jan 2011","","","IEEE","IEEE Conferences"
"A virtual environment for learning to pilot remotely operated vehicles","N. J. Pioch; B. Roberts; D. Zeltzer","BBN Corp., Cambridge, MA, USA; NA; NA","Proceedings. International Conference on Virtual Systems and MultiMedia VSMM '97 (Cat. No.97TB100182)","6 Aug 2002","1997","","","218","226","Remotely operated vehicles (ROVs) are used extensively for underwater searching and salvage, inspection, surveying, scientific exploration and mine countermeasures. ROV pilots must learn to rely on limited data from video and sonar displays and a few other positional indicators to maintain a sense of their vehicle, its tether and its surroundings. Pilot training typically occurs on-the-job, where equipment is placed at risk, controlled learning situations are hard to create, and time for instruction is minimal. The TRANSoM (TRAiNing for remote Sensing and Manipulation) project is developing a training environment that combines two emerging technologies to overcome these limitations. A virtual environment (VE) simulates the ROV and its surroundings, and also has instructional enhancements such as external views of the ROV, directional cues and alternate modes of interaction with the vehicle, e.g. a head-tracked head-mounted display (HMD). An intelligent tutoring system (ITS) monitors student pilot behavior and offers verbal and graphical feedback, a mission review and a performance assessment. Near-transfer experiments comparing the utility of different artificial viewpoints have been completed, with full transfer experiments involving a real ROV to follow.","","0-8186-8150-0","10.1109/VSMM.1997.622350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622350","","Virtual environment;Remotely operated vehicles;Inspection;Sonar equipment;On the job training;Remote sensing;Intelligent systems;Artificial intelligence;Computer displays;Feedback","marine systems;virtual reality;intelligent tutoring systems;telecontrol;computer based training;control engineering computing","virtual environment;remotely operated vehicle simulation;pilot training;underwater vehicles;video displays;sonar displays;positional indicators;TRANSoM project;remote sensing;remote manipulation;instructional enhancements;external views;directional cues;interaction modes;head-tracked head-mounted display;intelligent tutoring system;student pilot behavior monitoring;verbal feedback;graphical feedback;mission review;performance assessment;near-transfer experiments;artificial viewpoints","","5","","15","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Mobile Augmented Reality Authoring Tool","Y. Yang; J. Shim; S. Chae; T. Han","Media Syst. Lab., Yonsei Univ., Seoul, South Korea; Media Syst. Lab., Yonsei Univ., Seoul, South Korea; Media Syst. Lab., Yonsei Univ., Seoul, South Korea; Media Syst. Lab., Yonsei Univ., Seoul, South Korea","2016 IEEE Tenth International Conference on Semantic Computing (ICSC)","24 Mar 2016","2016","","","358","361","This paper propose a mobile Augmented Reality authoring tool interface that allows users to easily create Augmented Reality (AR) content with marker based and touch based interactions. It is possible to generate a simplified form of marker based AR content with the user interface. As for marker based interaction methods, there is one employing two markers and another using marker occlusion. This suggesting AR authoring tool system is providing another interaction method which is touch based interaction. This mobile AR authoring tool system recognizes user rotate, transfer, zoom-in and zoom-out gestures for virtual object. We seek to provide approaches for the user to actively interact with augmented reality content and ascertain the positive results of this study from the user evaluation of an authoring tool system and interaction.","","978-1-5090-0662-5","10.1109/ICSC.2016.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439364","Mobile device;Mobile AR;Authoring Tool","Mobile communication;Augmented reality;Thumb;Usability;Engines;User interfaces","augmented reality;authoring systems;computer vision;human computer interaction;mobile computing;user interfaces","mobile augmented reality;mobile AR;authoring tool system;marker based interaction;touch based interaction;user interface","","8","","9","","24 Mar 2016","","","IEEE","IEEE Conferences"
"A Novel Robot Teaching System Based on Mixed Reality","Y. Xu; C. Yang; X. Liu; Z. Li","Key Lab of Autonomous Systems and Networked Control, School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Key Lab of Autonomous Systems and Networked Control, School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Hohai University, Jiangsu, China; Key Lab of Autonomous Systems and Networked Control, School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","2018 3rd International Conference on Advanced Robotics and Mechatronics (ICARM)","13 Jan 2019","2018","","","250","255","In this paper, we have developed a robot teaching system where the robot learns the point-to-point motions from human demonstrations. In the demonstration phase, the operator's gestures and palm position are used to guide the robot to complete the task. At the same time, the scene of robots and its work environment, and the virtual model of the palms are integrated into Unity. Then the mixed reality scene is transferred to a virtual reality(VR) helmet, which provides operator real-time visual feedback. In the learning and reproduction phase, the Extreme Learning Machine(ELM) is used to generate a new trajectory from the training data. The experimental result shows that the robot teaching system based on mixed reality has more realistic and natural interaction. And the learning algorithm based on ELM has a good fitting ability.","","978-1-5386-7066-8","10.1109/ICARM.2018.8610861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610861","Robot Teaching;Mixed Reality;Teaching by Demonstration;Extreme Learning Machine","Robot kinematics;Education;Zirconium;Service robots;Virtual reality;End effectors","gesture recognition;human-robot interaction;learning (artificial intelligence);virtual reality","reproduction phase;novel robot teaching system;point-to-point motions;human demonstrations;demonstration phase;mixed reality scene;operator real-time visual feedback;extreme learning machine;virtual reality helmet","","2","","22","","13 Jan 2019","","","IEEE","IEEE Conferences"
"Virtual reality based rehabilitation system for Parkinson and multiple sclerosis patients","M. M. Kılıc; O. C. Muratlı; C. Catal","Istanbul Kültür University, Department of Computer Engineering, Bakırköy, İstanbul, 34156; Istanbul Kültür University, Department of Computer Engineering, Bakırköy, İstanbul, 34156; Istanbul Kültür University, Department of Computer Engineering, Bakırköy, İstanbul, 34156","2017 International Conference on Computer Science and Engineering (UBMK)","2 Nov 2017","2017","","","328","331","The aim of this study is to present a virtual reality based rehabilitation system for Parkinson and Multiple Sclerosis (MS) patients. In this study, physical rehabilitation software has been developed using Virtual Reality (VR) technology. The VR environment was used to find a unique solution to the problems of balance, tremor and movement coordination that MS and Parkinson patients suffer from. Virtual reality environment was designed and implemented using UNITY 3D game engine. The generated virtual environment was supported by Kinect, delivered to the virtual reality viewer of Google Cardboard with third party software, and this whole system provided the virtual reality environment. The interaction of the patients with the virtual environment helped these patients to tackle with their problems. The movements in the joint area of the patient were detected using the Microsoft Kinect human-machine interface. Kinect transferred user movements to the computer based on the serial communication. This prototype system will be deployed into a rehabilitation center in Turkey for in-depth analysis and experiments.","","978-1-5386-0930-9","10.1109/UBMK.2017.8093401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093401","Virtual Reality;Software Systems;Health Informatics;Biomedical Systems;Human-Computer Interface;Unity for Virtual Reality","Games;Multiple sclerosis;Fatigue;Sensors;Virtual environments","computer games;diseases;human computer interaction;medical computing;patient rehabilitation;virtual reality","virtual reality viewer;virtual reality based rehabilitation system;Parkinson;multiple sclerosis patients;physical rehabilitation software;VR environment;movement coordination;MS patients;tremor problem;balance problem;UNITY 3D game engine;third party software;Microsoft Kinect human-machine interface;serial communication","","1","","32","","2 Nov 2017","","","IEEE","IEEE Conferences"
"The Impact of Haptic and Visual Feedback on Teaching","K. Qi; D. Borland; E. Jackson; N. L. Williams; J. Minogue; T. C. Peck","Davidson College; RENCI, UNC Chapel Hill; North Carolina State University; University of Maryland,College Park; North Carolina State University; Davidson College","2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","11 May 2020","2020","","","612","613","Haptic feedback, an important aspect of learning in virtual reality, has been demonstrated in contexts such as surgical training. However, deploying haptic feedback in other educational practices remains understudied. Haptically-enabled science simulations enable students to experience abstract scientific concepts through concrete and observable lessons in which students can physically experience the concepts being taught through haptic feedback. The present study aims to investigate the effect of an educational simulation on the understanding of basic physics concepts related to buoyancy. Specifically, we hypothesize that a simulation with visual and haptic feedback will improve participant learning transfer.","","978-1-7281-6532-5","10.1109/VRW50115.2020.00157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090605","Human computer interaction (HCI);Interaction paradigms;Virtual reality;Education;Interactive learning environments","Haptic interfaces;Visualization;Training;Buoyancy;Solid modeling;Liquids","computer aided instruction;educational courses;haptic interfaces;physics education;teaching;virtual reality","haptic feedback;haptically-enabled science simulations;visual feedback;teaching;virtual reality learning;participant learning transfer;educational practices;physics concepts;buoyancy","","","","5","","11 May 2020","","","IEEE","IEEE Conferences"
"Virtual Reality Integrated Multi-Depth-Camera-System for Real-Time Telepresence and Telemanipulation in Caregiving","C. F. -v. Böhlen; A. Brinkmann; S. Mävers; S. Hellmers; A. Hein","Carl von Ossietzky University,Assistive Systems&Medical Technologies,Oldenburg,Germany; Carl von Ossietzky University,Assistive Systems&Medical Technologies,Oldenburg,Germany; Carl von Ossietzky University,Assistive Systems&Medical Technologies,Oldenburg,Germany; Carl von Ossietzky University,Assistive Systems&Medical Technologies,Oldenburg,Germany; Carl von Ossietzky University,Assistive Systems&Medical Technologies,Oldenburg,Germany","2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","15 Jan 2021","2020","","","294","297","Telepresence and telemanipulation robotics are suitable solutions to relieve humans from direct health risks and repetitive or unhealthy work. Through demographic changes in western countries and the COVID-19 pandemic, this relief is also considered for healthcare workers, especially caregivers, as the demands for them rises. The requirements are intuitively usable telerobotic and telepresence systems for remote assistance, to cut the high physical strain in manual patient transfers and the reduction of contact with infected patients. To ensure this, key technologies like 3D imaging and perception systems are essential. In this work, we present a novel, lightweight telepresence and telemanipulation system, specialized for caregiving. It allows an operator, wearing a virtual reality headset, to immerse into a sensor system captured scene on a distant location in real-time, with low latency of 250 ms and up to 30 fps refresh rate. Extensive measurement shows that 97.1% of the relevant point cloud data is below 1 cm error and 99.5 % is below 1.6 cm, making the system suitable for the application.","","978-1-7281-7463-1","10.1109/AIVR50618.2020.00059","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319102","sensor systems and applications;virtual reality;computer networks","Three-dimensional displays;Robot sensing systems;Robots;Headphones;Virtual reality;Medical services;Solid modeling","cameras;diseases;epidemics;haptic interfaces;health care;human-robot interaction;image capture;medical computing;medical robotics;real-time systems;robot vision;telemedicine;telerobotics;virtual reality","demographic changes;western countries;COVID-19 pandemic;healthcare workers;caregivers;telepresence systems;remote assistance;high physical strain;manual patient transfers;infected patients;perception systems;telemanipulation system;caregiving;virtual reality headset;sensor system captured scene;real-time telepresence;telemanipulation robotics;direct health risks;repetitive work;unhealthy work;virtual reality integrated multidepth-camera-system","","","","26","","15 Jan 2021","","","IEEE","IEEE Conferences"
"Non-contact hand interaction with smart phones using the wireless power transfer features","C. Liu; C. Gu; C. Li","Dept. of Electrical and Computer Engineering, Texas Tech University, Lubbock, 79409, USA; Marvell Semiconductor Inc., Santa Clara, CA 95054, USA; Dept. of Electrical and Computer Engineering, Texas Tech University, Lubbock, 79409, USA","2015 IEEE Radio and Wireless Symposium (RWS)","22 Jun 2015","2015","","","20","22","The interaction between human and smart phones is mainly based on press buttons and touch screens. As wireless charging based on wireless power transfer (WPT) coils is standardized and becoming a competitive feature for smart phones in recent years, it is also possible to control smart phones without contact by interacting with their wireless charging coils. In this paper, a non-contact method to interact with smart phones based on their wireless charging coils is proposed and investigated. A Colpitts oscillator is built and tested using a standard WPT coil as part of the resonant tank elements. Experiment results show that the resonant frequency of the coil is heavily influenced by hand movement, which could be further interpreted as the interaction between human and smart phones. By analyzing the resonant frequency changes of multiple coils embedded into a phone, it is possible to determine which hand movement is performed by a user.","2164-2974","978-1-4799-5507-7","10.1109/RWS.2015.7129750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7129750","non-contact smart phone interaction;wireless charging;Colpitts oscillator;spectrogram","Coils;Resonant frequency;Smart phones;Oscillators;Impedance;Inductive charging;Wireless communication","human computer interaction;inductive power transmission;oscillators;smart phones","noncontact hand interaction;smart phones;wireless power transfer features;press buttons;touch screens;wireless charging coils;wireless power transfer coils;WPT coils;Colpitts oscillator;resonant tank elements;resonant frequency change analysis;hand movement","","5","","5","","22 Jun 2015","","","IEEE","IEEE Conferences"
"Robust Sensorimotor Representation to Physical Interaction Changes in Humanoid Motion Learning","T. Shimizu; R. Saegusa; S. Ikemoto; H. Ishiguro; G. Metta","Department of Mechanical Engineering, Kobe City College of Technology, Kobe, Japan; Center for Human-Robot Symbiosis Research, Toyohashi University of Technology, Toyohashi, Japan; Institute for Academic Initiatives, Osaka University, Toyonaka, Japan; Department of Systems InnovationGraduate School of Engineering Science, Osaka University, Toyonaka, Japan; Department of Robotics, Brain and Cognitive Sciences, Istituto Italiano di Tecnologia, Genoa, Italy","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2015","26","5","1035","1047","This paper proposes a learning from demonstration system based on a motion feature, called phase transfer sequence. The system aims to synthesize the knowledge on humanoid whole body motions learned during teacher-supported interactions, and apply this knowledge during different physical interactions between a robot and its surroundings. The phase transfer sequence represents the temporal order of the changing points in multiple time sequences. It encodes the dynamical aspects of the sequences so as to absorb the gaps in timing and amplitude derived from interaction changes. The phase transfer sequence was evaluated in reinforcement learning of sitting-up and walking motions conducted by a real humanoid robot and compatible simulator. In both tasks, the robotic motions were less dependent on physical interactions when learned by the proposed feature than by conventional similarity measurements. Phase transfer sequence also enhanced the convergence speed of motion learning. Our proposed feature is original primarily because it absorbs the gaps caused by changes of the originally acquired physical interactions, thereby enhancing the learning speed in subsequent interactions.","2162-2388","","10.1109/TNNLS.2014.2333092","Department of Robotics, Brain and Cognitive Sciences, Istituto Italiano di Tecnologia, Genoa, Italy; Grants-in-Aid for Scientific Research, Japan Society for the Promotion of Science (JSPS); JSPS Core-to-Core Program A; Advanced Research Network; European Union (EU) FP7 Project; Cooperative Human Robot Interaction Systems; EU FP7 Project Xperience entitled the Robots Bootstrapped through Learning and Experience; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851925","Change detection;dimensionality reduction;learning from demonstration (LfD);physical human-robot interaction.;Change detection;dimensionality reduction;learning from demonstration (LfD);physical human-robot interaction","Robot sensing systems;Legged locomotion;Joints;Timing;Indexes;Robustness","humanoid robots;human-robot interaction;intelligent robots;learning (artificial intelligence);legged locomotion;motion control","robust sensorimotor representation;physical interaction changes;humanoid robot motion learning;learning-from-demonstration system;motion feature;phase transfer sequence;knowledge synthesis;humanoid whole-body motions;teacher-supported interactions;temporal order changing points;dynamical sequence aspect encoding;reinforcement learning;sitting-up motions;walking motions;convergence speed;learning speed enhancement","Computer Simulation;Functional Laterality;Humans;Knowledge;Learning;Motion;Motor Activity;Robotics;Symbolism;Time Factors;Transfer (Psychology);Walking","6","","24","","10 Jul 2014","","","IEEE","IEEE Journals"
"A Cross-Platform Classroom Training Simulator: Interaction Design and EvaluationA Cross-Platform Classroom Training Simulator: Interaction Design and Evaluation","A. Delamarre; C. Lisetti; C. Buche","Florida International University,VISAGE Lab, SCIS,Miami,USA; Florida International University,VISAGE Lab, SCIS,Miami,USA; LAB-STICC CNRS, ENIB,Brest,France","2020 International Conference on Cyberworlds (CW)","30 Oct 2020","2020","","","86","93","Virtual training environments experienced with different immersive technologies can accommodate users' preferences, proficiency, and platform availability. Whereas research comparing the effects of immersive technologies can provide important insights about their impact on users' experience (e.g. engagement, transfer of learning), current studies do not address how to design the user interface (UI) to ensure sound comparisons across platforms. For effective comparisons, however, the UI designs must be adapted for the platform used to provide comparable usability. In this article we describe our UI design methodology for the development of an effective and usable virtual classroom training simulator built for three technologies: (1) desktop; (2) Head-Mounted Display (HMD); and (3) Cave Automatic Virtual Environment (CAVE). Usability and other user experience factors were evaluated for each platform with concurrent think-aloud protocol and semi-structured interviews indicating that all three UIs were easy to use and to learn. We discuss insights for future development of cross-platform VTEs.","2642-3596","978-1-7281-6497-7","10.1109/CW49994.2020.00020","Florida International University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240533","Immersive Virtual Environment;Virtual Reality;Human Computer Interaction;User Study;Design","Training;Virtual environments;Resists;User interfaces;User experience;Usability;Interviews","computer based training;computer simulation;helmet mounted displays;user interfaces;virtual reality","interaction design;virtual training environments;platform availability;user interface;UI designs;comparable usability;UI design;usable virtual classroom training simulator;user experience factors;cross-platform VTEs;classroom training simulator;immersive technologies;cross-platform classroom training simulator;desktop;head-mounted display;cave automatic virtual environment","","","","24","","30 Oct 2020","","","IEEE","IEEE Conferences"
"Influence of information search cost on technology transfer based on multi-agent system","N. Ma; Q. Li","School of Economics and Management, Beijing Forestry University, Beijing, P. R. China; School of Economics and Management, Beijing Forestry University, Beijing, P. R. China","2009 16th International Conference on Industrial Engineering and Engineering Management","4 Dec 2009","2009","","","443","447","To establish cooperation between universities and enterprises is an important way to realize technology transfer. In reality there are many universities and enterprises, and the process of technology transfer is very complex. Thus it is difficult to resolve the multi-participant game problem through traditional economics method. Based on multi-agent system (MAS) thought and method, evolvement system, which consists of many self-determination and interaction agents, is set up to simulate the process of technology transfer from universities to enterprises. In the end of the paper, through a series of computational experimentations, conclusion is found that more choice chance and less information search cost are necessary conditions of technology transfer.","","978-1-4244-3671-2","10.1109/ICIEEM.2009.5344553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344553","Multi-agent system (MAS);search cost;simulation;technology transfer","Costs;Technology transfer;Multiagent systems;Power generation economics;Environmental economics;Object oriented modeling;Computational modeling;Intelligent agent;Technological innovation;Technology management","costing;educational institutions;game theory;multi-agent systems;technology transfer;virtual enterprises","multiagent system;technology transfer;information search cost;universities-enterprises cooperation;multiparticipant game problem;evolvement system","","","","23","","4 Dec 2009","","","IEEE","IEEE Conferences"
"Post-Biological Hypersurfacing: Embodied Mixed Reality Data Transfer","J. Stadon","Dept. of Art, Curtin Univ., Perth, WA, Australia","2011 International Conference on Cyberworlds","17 Nov 2011","2011","","","264","268","This paper focuses on the (d)evolving interface between cyber worlds and the real world, what Giannachi has called the hyper surface. This fusion of real and representation, linking cyber and real worlds constitutes mixed reality interaction as experienced by humans in the physical world, their avatars, agents, and virtual humans. Current mixed reality XML RPC (Remote Procedure Call) interfaces and real-time data transfer enhance the experience of the hyper surface for the audience beyond any previous virtual media types, such as, hypertext, HTML, VRML, virtual reality, etc. Previous research in mixed reality and interactive workspaces that use the concept of a bridge for data transfer have largely inspired this research and I aim to continue the development of new knowledge in this field by critically applying cultural discourse in order to develop a theory regarding the impact of such systems on the notion of post biological identity.","","978-1-4577-1453-5","10.1109/CW.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079377","virtual humans and avatars;human-computer interaction;social networking;networked collaborations;cyberworlds","Humans;DNA;Educational institutions;Cultural differences;Avatars;Collaboration","avatars;computer graphics;human computer interaction","post-biological hypersurfacing;mixed reality data transfer;cyber world;avatar;agent;virtual human;mixed reality XML RPC interface;remote procedure call interface","","","","12","","17 Nov 2011","","","IEEE","IEEE Conferences"
"Amplified Head Rotation in Virtual Reality and the Effects on 3D Search, Training Transfer, and Spatial Orientation","E. D. Ragan; S. Scerbo; F. Bacim; D. A. Bowman","Texas A&M University, College Station, TX; Virginia Tech, Blacksburg, VA; Virginia Tech, Blacksburg, VA; Virginia Tech, Blacksburg, VA","IEEE Transactions on Visualization and Computer Graphics","28 Jun 2017","2017","23","8","1880","1895","Many types of virtual reality (VR) systems allow users to use natural, physical head movements to view a 3D environment. In some situations, such as when using systems that lack a fully surrounding display or when opting for convenient low-effort interaction, view control can be enabled through a combination of physical and virtual turns to view the environment, but the reduced realism could potentially interfere with the ability to maintain spatial orientation. One solution to this problem is to amplify head rotations such that smaller physical turns are mapped to larger virtual turns, allowing trainees to view the entire surrounding environment with small head movements. This solution is attractive because it allows semi-natural physical view control rather than requiring complete physical rotations or a fully-surrounding display. However, the effects of amplified head rotations on spatial orientation and many practical tasks are not well understood. In this paper, we present an experiment that evaluates the influence of amplified head rotation on 3D search, spatial orientation, and cybersickness. In the study, we varied the amount of amplification and also varied the type of display used (head-mounted display or surround-screen CAVE) for the VR search task. By evaluating participants first with amplification and then without, we were also able to study training transfer effects. The findings demonstrate the feasibility of using amplified head rotation to view 360 degrees of virtual space, but noticeable problems were identified when using high amplification with a head-mounted display. In addition, participants were able to more easily maintain a sense of spatial orientation when using the CAVE version of the application, which suggests that visibility of the user's body and awareness of the CAVE's physical environment may have contributed to the ability to use the amplification technique while keeping track of orientation.","1941-0506","","10.1109/TVCG.2016.2601607","Office of Naval Research; Virginia Tech Research Computing's Visionarium Lab; VisCube; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547900","Virtual reality;spatial orientation;rotation amplification;3D interaction;search;cybersickness","Training;Legged locomotion;Three-dimensional displays;Games;Navigation;Visualization;Virtual reality","helmet mounted displays;virtual reality","amplified head rotation;virtual reality;3D search;training transfer;spatial orientation;VR systems;head movements;3D environment;virtual turns;seminatural physical view control;physical rotations;cybersickness;head-mounted display;surround-screen CAVE;VR search task","","13","","53","","19 Aug 2016","","","IEEE","IEEE Journals"
"Small Marker Tracking with Low-Cost, Unsynchronized, Movable Consumer Cameras For Augmented Reality Surgical Training","N. Rewkowski; A. State; H. Fuchs","UNC Chapel Hill, UMD College Park; UNC Chapel Hill; UNC Chapel Hill","2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","16 Dec 2020","2020","","","90","95","Surgeons improve their skills through repetition of training tasks in order to operate on living patients, ideally receiving timely, useful, and objective performance feedback. However, objective performance measurement is currently difficult without 3D visualization, with effective surgical training apparatus being extremely expensive or limited in accessibility. This is problematic for medical students, especially in situations such as the COVID-19 pandemic in which they are needed by the community but have few ways of practicing without lab access. In this work, we propose and prototype a system for augmented reality (AR) visualization of laparoscopic training tasks using cheap and widely-compatible borescopes, which can track small objects typical of surgical training. We use forward kinematics for calibration and multi-threading to attempt synchronization in order to increase compatibility with consumer applications, resulting in an effective AR simulation with low-cost devices and consumer software, while also providing dynamic camera and marker tracking. We test the system with a typical peg transfer task on the HoloLens 1 and MagicLeap One.","","978-1-7281-7675-8","10.1109/ISMAR-Adjunct51615.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288414","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality; Computing methodologies;Artificial intelligence;Computer vision;Computer vision problems;Tracking","Training;Visualization;Three-dimensional displays;Surgery;Cameras;Task analysis;Augmented reality","augmented reality;computer based training;medical computing;medical image processing;object tracking;surgery","MagicLeap One;HoloLens 1;surgical training apparatus;peg transfer task;dynamic camera;consumer software;low-cost devices;AR simulation;consumer applications;widely-compatible borescopes;laparoscopic training tasks;augmented reality visualization;COVID-19 pandemic;medical students;objective performance measurement;objective performance feedback;augmented reality surgical training;movable consumer cameras;marker tracking","","","","35","","16 Dec 2020","","","IEEE","IEEE Conferences"
