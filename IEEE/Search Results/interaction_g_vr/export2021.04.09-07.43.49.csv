"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Virtual reality as a complex interactive system: a multidimensional model of person artificial partner co-relations","A. Libin","Dept. of Psychol., Georgetown Univ., Rockville, MD, USA","Proceedings Seventh International Conference on Virtual Systems and Multimedia","6 Aug 2002","2001","","","652","657","The operational definition of virtual reality as a complex interactive system (CIS) represents it as a configuration of exchanges between a person and a digital world mediated by the artificial, partner. The adequate model for understanding the essence of CIS includes, in addition to three 'traditional' dimensions (physical-per-se, mental, and virtual), a fourth dimension defined as individual self-determination and formed by one's needs, preferences, profile of imagination, identities, and individual experiences. The analysis of 'person-artificial agent' interactions in the context of a complex systems approach is associated with the implementation of such interaction effects as compensation, optimization, adequacy, and effectiveness. Virtual entertainment can be viewed not only as a part of modern popular culture but also as a tool for constructive activities. The latter, employed by certain groups of users, can not only enhance their life as an enjoyment but also as training tool for sensory-motor, cognitive and emotional improvement, and their well-being in general.","","0-7695-1402-2","10.1109/VSMM.2001.969724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969724","","Virtual reality;Interactive systems;Multidimensional systems;Computational Intelligence Society;Humans;Visual perception;Cultural differences;Search engines;Psychology;Aging","virtual reality;man-machine systems;software agents;computer games;interactive systems","complex interactive system;virtual reality;artificial agent;individual preferences;man machine interactions;computer game","","4","","30","","6 Aug 2002","","","IEEE","IEEE Conferences"
"London calling: GIS, VR, and the Victorian period","R. F. Chevez; T. L. Milbank",NA; NA,"Proceedings Seventh International Conference on Virtual Systems and Multimedia","6 Aug 2002","2001","","","335","344","The Bolles Collection of Tufts University represents a comprehensive and integrated collection of sources on the history and topography of Victorian London. Texts, images, maps, and three-dimensional reconstructions are all interconnected forming a body of material that transcends the limits of print publication and exploits the flexibility of the electronic medium. The Perseus Digital Library has incorporated geographic information system and virtual reality technologies in a set of tools intended to help readers synthesize and visualize the numerous temporal and spatial interconnections between Bolles Collection materials. The tools, which are applicable to any large assemblage of related documents, also help readers grasp the complex temporal-spatial interactions that shape historical materials in general.","","0-7695-1402-2","10.1109/VSMM.2001.969688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=969688","","Geographic Information Systems;Virtual reality;Software libraries;Visualization;Books;Surfaces;Assembly;History;Image reconstruction;Shape","cartography;virtual reality;solid modelling;history;geographic information systems;interactive systems","Bolles Collection;Tufts University;Perseus Digital Library;geographic information system;virtual reality;London;3D image reconstructions;temporal-spatial interactions","","1","","3","","6 Aug 2002","","","IEEE","IEEE Conferences"
"The effect of sound on visual realism perception and task completion time in a cel-shaded serious gaming virtual environment","D. Rojas; B. Cowan; B. Kapralos; K. Colllins; A. Dubrowski","Institute of Medical Science, University of Toronto, Toronto, Canada; Faculty of Business and IT, University of Ontario Institute of Technology, Oshawa, Ontario, Canada; Faculty of Business and IT, University of Ontario Institute of Technology, Oshawa, Ontario, Canada; The Games Institute, University of Waterloo, Waterloo, Ontario, Canada; Faculty of Medicine, Memorial University. St. John's, Newfoundland, Canada","2015 Seventh International Workshop on Quality of Multimedia Experience (QoMEX)","6 Jul 2015","2015","","","1","6","Here we investigate the effect of sound on the perception of visual realism and the time required to complete a simple navigation-based task within a serious gaming (virtual) environment under various sound and visual conditions. Results indicate that the perception of visual realism and task completion time can be affected by sound. Designers and developers of serious games (and virtual environments in general) should be aware of the effects of sound on a user's perception of the visual scene and on task completion time, and they should thus ensure that sound is carefully considered when creating such environments.","","978-1-4799-8958-4","10.1109/QoMEX.2015.7148136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148136","Serious games;virtual simulation;multi-modal interactions;audio-visual interaction;realism","Visualization;Games;White noise;Surgery;Three-dimensional displays;Virtual environments;Training","human computer interaction;serious games (computing);virtual reality","cel-shaded serious gaming;visual realism perception;navigation-based task;virtual environment;sound conditions;visual conditions;task completion time;user perception;visual scene","","3","","22","","6 Jul 2015","","","IEEE","IEEE Conferences"
"Gesture recognition for virtual reality applications using data gloves and neural networks","J. Weissmann; R. Salomon","Dept. of Comput. Sci., Zurich Univ., Switzerland; NA","IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)","6 Aug 2002","1999","3","","2043","2046 vol.3","Explores the use of hand gestures as a means of human-computer interactions for virtual reality applications. For the application, specific hand gestures, such as ""fist"", ""index finger"" and ""victory sign"", have been defined. Most existing approaches use various camera-based recognition systems, which are rather costly and very sensitive to environmental changes. In contrast, this paper explores a data glove as the input device, which provides 18 measurement values for the angles of different finger joints. The paper compares the performance of different neural network models, such as backpropagation and radial-basis functions, which are used by the recognition system to recognize the actual gesture. Some network models achieve a recognition rate (training as well a generalization) of up to 100% over a number of test subjects. Due to its good performance, this recognition system is the first step towards virtual reality applications in which program execution is controlled by a sign language.","1098-7576","0-7803-5529-6","10.1109/IJCNN.1999.832699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=832699","","Virtual reality;Data gloves;Neural networks;Application software;Fingers;Wrist;Mice;Image recognition;Computer science;Testing","gesture recognition;virtual reality;data gloves;backpropagation;radial basis function networks","neural networks;hand gestures;human-computer interactions;fist;index finger;victory sign;radial-basis functions;sign language","","28","1","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Faster Multibeam Sonar Data Cleaning: Evaluation of Editing 3D Point Clouds using Immersive VR","A. H. Stevens; T. Butkiewicz","Center for Coastal and Ocean Mapping, University of New Hampshire,Durham,New Hampshire,USA; Center for Coastal and Ocean Mapping, University of New Hampshire,Durham,New Hampshire,USA","OCEANS 2019 MTS/IEEE SEATTLE","20 Jan 2020","2019","","","1","10","Remote sensing technologies routinely generate point cloud datasets with billions of points. While automatic data cleaning algorithms exist, safety-critical applications (such as waterway surveys) still require that data be processed and verified by a human. This presents a significant bottleneck in the pipeline from surveys into navigational maps. The recent proliferation of low-cost, high-quality virtual reality systems presents an opportunity to explore how these technologies might be integrated into the point cloud data processing pipeline. Prior research has shown that stereoscopic viewing, head-tracked perspective, and bimanual interactions can lead to faster 3D task completion times and lower errors compared to traditional monoscopic, mouse-and-keyboard desktop systems. In this paper, we present a human factors study that compares 3D point cloud editing performance between a traditional interface and type types of immersive virtual reality interfaces. Our results showed that for complex datasets, the immersive interfaces generally led to faster task completion times than when using the desktop interface. Participants also reported a strong subjective preference for the immersive interface.","0197-7385","978-0-578-57618-3","10.23919/OCEANS40490.2019.8962793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8962793","virtual reality;point clouds;editing;annotation","","data handling;graphical user interfaces;human factors;remote sensing;sonar imaging;stereo image processing;virtual reality","navigational maps;high-quality virtual reality systems;point cloud data processing pipeline;head-tracked perspective;human factors;3D point cloud editing performance;immersive virtual reality interfaces;complex datasets;immersive VR;remote sensing technologies;point cloud datasets;automatic data cleaning algorithms;safety-critical applications;waterway surveys;multibeam sonar data cleaning;3D task completion times;stereoscopic viewing;bimanual interactions","","","","55","","20 Jan 2020","","","IEEE","IEEE Conferences"
"An intestinal surgery simulator: real-time collision processing and visualization","L. Raghupathi; L. Grisoni; F. Faure; D. Marchal; M. -. Cani; C. Chaillou","GRAVIR/IMAG Lab., Montbonnet, France; NA; NA; NA; NA; NA","IEEE Transactions on Visualization and Computer Graphics","13 Sep 2004","2004","10","6","708","718","This research work is aimed toward the development of a VR-based trainer for colon cancer removal. It enables the surgeons to interactively view and manipulate the concerned virtual organs as during a real surgery. First, we present a method for animating the small intestine and the mesentery (the tissue that connects it to the main vessels) in real-time, thus enabling user interaction through virtual surgical tools during the simulation. We present a stochastic approach for fast collision detection in highly deformable, self-colliding objects. A simple and efficient response to collisions is also introduced in order to reduce the overall animation complexity. Second, we describe a new method based on generalized cylinders for fast rendering of the intestine. An efficient curvature detection method, along with an adaptive sampling algorithm, is presented. This approach, while providing improved tessellation without the classical self-intersection problem, also allows for high-performance rendering thanks to the new 3D skinning feature available in recent GPUs. The rendering algorithm is also designed to ensure a guaranteed frame rate. Finally, we present the quantitative results of the simulations and describe the qualitative feedback obtained from the surgeons.","1941-0506","","10.1109/TVCG.2004.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333668","Index Terms- Virtual reality;physically-based modeling;animation;curve and surface representation.","Intestines;Visualization;Oncological surgery;Animation;Colon;Cancer;Stochastic processes;Object detection;Sampling methods;Algorithm design and analysis","data visualisation;virtual reality;cancer;surgery;computer animation;rendering (computer graphics);medical image processing;user interfaces;simulation;computational geometry;sampling methods","intestinal surgery simulator;real-time collision processing;real-time visualization;VR-based trainer;colon cancer removal;virtual organ;user interaction;virtual surgical tool;stochastic approach;self-colliding object;animation complexity;self-intersection problem;3D skinning feature;virtual reality;physically-based model","Algorithms;Biomedical Engineering;Colonic Neoplasms;Computer Graphics;Computer Simulation;Computer Systems;Computer-Assisted Instruction;Digestive System Surgical Procedures;Digestive System Surgical Procedures;Humans;Intestine, Small;Intestine, Small;Models, Anatomic;User-Computer Interface","34","3","41","","13 Sep 2004","","","IEEE","IEEE Journals"
"The impact of motion in virtual environments on memorization performance","P. Häfner; C. Vinke; V. Häfner; J. Ovtcharova; W. Schotte","Institute for Information, Management in Engineering, Karlsruhe Institute of Technology, Zirkel 2, 76131, Germany; Institute for Information, Management in Engineering, Karlsruhe Institute of Technology, Zirkel 2, 76131, Germany; Institute for Information, Management in Engineering, Karlsruhe Institute of Technology, Zirkel 2, 76131, Germany; Institute for Information, Management in Engineering, Karlsruhe Institute of Technology, Zirkel 2, 76131, Germany; High Performance Computing Center Stuttgart, University of Stuttgart, Nobelstr. 19, 70569, Germany","2013 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)","3 Oct 2013","2013","","","104","109","Virtual environments are more and more used for educational and training purposes. In order to design virtual environments for these applications in particular, it is very important to get a deep understanding of the relevant design features supporting the user's process of learning and comprehension. Relevance and implementation of these features as well as the benefits of virtual learning environments over traditional educational approaches in general are rarely explored. Focusing on modes of interaction in this work, we examined the effect of different motion types on the knowledge acquisition of users in various virtual environments. For our study we chose a simple memorization task as approximation of low cognitive knowledge acquirement. We hypothesized motion types and immersion levels influence memorization performance in virtual environments. The memorization task was conducted in two virtual environments with different levels of immersion: A high-immersive Cave Automatic Virtual Environment (CAVE) and a low-immersive desktop virtual environment. Two motion types in virtual environments were explored: Physical and virtual walking. In the CAVE physical walking was implemented by using motion capturing and virtual walking was realized using a joystick-like input device. The results indicate neither motion types nor immersion levels in virtual environments affect memorization performance significantly.","2377-9322","978-1-4673-4703-7","10.1109/CIVEMSA.2013.6617404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617404","cognition;human computer interaction;learning;memory;motion;virtual environment;virtual reality","Virtual environments;Navigation;Legged locomotion;Tutorials;Stereo vision;Educational institutions","computer aided instruction;human computer interaction;human factors;interactive devices;virtual reality","training;education;CAVE physical walking;virtual walking;low-immersive desktop virtual environment;high-immersive CAVE automatic virtual environment;hypothesized motion types;cognitive knowledge acquirement;user knowledge acquisition;virtual learning environments;user process;virtual environment design;training purposes;memorization performance","","4","","12","","3 Oct 2013","","","IEEE","IEEE Conferences"
"Distance-based modeling and manipulation techniques using ultrasonic gloves","T. N. Hoang; B. H. Thomas","Wearable Computer Lab - University of South Australia, Australia; Wearable Computer Lab - University of South Australia, Australia","2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","7 Jan 2013","2012","","","287","288","We present a set of distance-based interaction techniques for modeling and manipulation, enabled by a new input device called the ultrasonic gloves. The ultrasonic gloves are built upon the original design of the pinch glove device for virtual reality systems with a tilt sensor and a pair of ultrasonic transducers in the palms of the gloves. The transducers are distance-ranging sensors that allow the user to specify a range of distances by natural gestures such as facing the palms towards each other or towards other surfaces. The user is able to create virtual models of physical objects by specifying their dimensions with hand gestures. We combine the reported distance with the tilt orientation data to construct virtual models. We also map the distance data to create a set of affine transformation techniques, including relative and fixed scaling, translation, and rotation. Our techniques can be generalized to different sensor technologies.","","978-1-4673-4662-7","10.1109/ISMAR.2012.6402577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402577","ultrasonic gloves;distance-based techniques;modeling;manipulation","Acoustics;Solid modeling;Ultrasonic variables measurement;Thumb;Ultrasonic transducers;Augmented reality","augmented reality;data gloves;sensors;solid modelling;user interfaces","distance-based modeling technique;distance-based manipulation technique;ultrasonic gloves;distance-based interaction technique;pinch glove device;virtual reality system;tilt sensor;ultrasonic transducer;distance-ranging sensor;virtual model;hand gesture;tilt orientation data;affine transformation technique;relative scaling technique;fixed scaling technique;translation technique;rotation technique;sensor technology","","4","","7","","7 Jan 2013","","","IEEE","IEEE Conferences"
"Shoulder flexion rehabilitation in patients with monoparesia using an exergame","J. B. Castano; J. D. Hoyos Escobar; J. E. Munoz Cardona; J. F. Lopez Herrera","Human Computer Interaction Group, Programa Ciencias del Deporte y la Recreación, Universidad Tecnológica de Pereira Pereira, Colombia; Human Computer Interaction Group Programa Ciencias del Deporte y la Recreación, Universidad Tecnológica de Pereira Pereira, Colombia; Human Computer Interaction Group Maestría en Ingeniería Eléctrica Pereira, Colombia; Programa Ciencias del Deporte y la Recreación, Universidad Tecnológica de Pereira Pereira, Colombia","2014 IEEE 3nd International Conference on Serious Games and Applications for Health (SeGAH)","26 Mar 2015","2014","","","1","5","Purpose - Pyramidal syndrome is a neuromotor disorder that affects quality of life of 1 out of 12,000 people around the world and most people in their middle- old age. Conventional methods are used generally for the rehabilitation of this disorder and studies are currently trying to rehabilitate patients through interaction with serious video games focused on health. This study proposes a combination of the two methods to find improvements in the flexion angle of shoulder affected by upper motor neurone lesion in patients of the “Clinica de Dolor del Eje Cafetero”. Methodology - 6 patients (3 patients with sequels of stroke, 1 patient with sequels of TBI and 2 patients with sequels of cerebral palsy) were taken into consideration. All patients suffered monoparesia in upper limb. Each patient had 7 sessions of rehabilitation. Each session of rehabilitation lasted for 30 minutes of assisted therapy and another 30 minutes with therapies of VR, where the movements of flexion of the shoulder affected through the Kinect sensor were recorded while the patient interacted with the video game. A biomechanical analysis with the Bio-Cirac software developed to load data from MoCap and show angle graphs was performed. Results - the patient who achieved the best results showed 21.0 % of improvement in the angle of flexion of the affected shoulder and improvement in muscular endurance and control of their affected limb. Conclusion - VR and serious video games specifically designed for particular pathologies are potentially useful technologies that can be combined with conventional methods to improve the angle of amplitude of flexion of the shoulder affected in patients with sequels of upper motor neurone lesion. In addition VR offers immersive experiences favorable for dissipation of pain, fatigue, setting goals and enjoyment of the activity.","","978-1-4799-4823-9","10.1109/SeGAH.2014.7067072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067072","pyramidal syndrome;Kinect;Rehabilitation;Exergame;Serious Game","Games;Medical treatment;Lesions;Neurons;Virtual reality;Pain;Muscles","biomechanics;interactive devices;medical computing;medical disorders;patient rehabilitation;serious games (computing);virtual reality","shoulder flexion rehabilitation;patient rehabilitation;monoparesia;exergame;serious video game;pyramidal syndrome;neuromotor disorder;Kinect sensor;biomechanical analysis;Bio-Cirac software;VR;virtual reality","","1","","19","","26 Mar 2015","","","IEEE","IEEE Conferences"
"Easy Extrinsic Calibration of VR System and Multi-camera Based Marker-Less Motion Capture System","K. Takahashi; D. Mikami; M. Isogawa; S. Sun; Y. Kusachi",NTT; NTT; NTT; NTT; NTT,"2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","9 Jan 2020","2019","","","83","88","This paper proposes a novel easy extrinsic calibration algorithm for an off-the-shelf VR system and a multi-camera based marker-less motion capture system. To realize interactions between 3D user motions and virtual objects reconstructed from multi-view videos in a common 3D space, the extrinsic calibration of the VR system and the multiple cameras must be conducted beforehand. This calibration, which involves estimating the pose and position of each coordinate system, is a key technology for handling 3D information in a system with various type of input sources. In general, extrinsic calibration is carried out by identifying and utilizing some common 3D points. However, since most of off-the-shelf VR systems do not include any imaging device, it is difficult to apply conventional automatic calibration approaches as there are no common points shared with the cameras. Against this problem, this paper introduces an easy calibration algorithm by generating corresponding points from the trajectories of the user's motion with VR devices and 3D human pose reconstructed from multi-view videos. Our study provides the following two contributions; (1) our method does not need to introduce additional devices, such as a chessboard and (2) our method does not need manual processes as the extrinsic parameters are automatically estimated. We demonstrate the performance of the proposed method in a practical scenario.","","978-1-7281-4765-9","10.1109/ISMAR-Adjunct.2019.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951990","calibration;motion;multi view","","cameras;image capture;image motion analysis;image reconstruction;pose estimation;stereo image processing;video signal processing;virtual reality","VR devices;multiview videos;extrinsic calibration algorithm;off-the-shelf VR system;3D user motions;common 3D space;multiple cameras;common 3D points;automatic calibration approaches;multicamera based markerless motion capture system;virtual objects reconstruction;3D human pose reconstruction","","","","18","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Multi-user interaction with public screens using mobile devices","H. Anzures; S. Mendoza","Computer Science Department. CINVESTAV, Av. Instituto Politécnico Nacional 2508. 07360 México D.F., México; Computer Science Department. CINVESTAV, Av. Instituto Politécnico Nacional 2508. 07360 México D.F., México","2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control","19 Dec 2011","2011","","","1","5","Public screens can now be found in many places, from shopping malls and airports to museums and restaurants, and even in the outside walls of tall buildings. These public screens usually convey contextual information to the public, and even though some of them support interactivity via touchscreen, the same information is always displayed to all its users regardless of their interests or preferences. On the other hand, mobile devices in general have shown great success by substantially increasing their numbers and their power. As such it is common to start viewing them as tools to achieve previously unattainable capabilities. In this article we engage the idea of using mobile devices to allow multiple users to simultaneously interact with public screens. As a matter of motivation, we consider a few potential applications that range from the marketing to the entertaining industry. We discuss some key characteristics that are desired on a platform that supports this kind of interaction. Finally we present PACMEN, a platform that supports the development of applications oriented to allow multiple users to simultaneously interact with public screens using mobile devices.","","978-1-4577-1013-1","10.1109/ICEEE.2011.6106647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106647","interactive systems;large-screen displays;mobile computing;multi-user interaction;application development platforms","Servers;Mobile handsets;Games;Multimedia communication;Computers;Browsers;Collaborative work","computer displays;entertainment;groupware;interactive systems;marketing;mobile computing","multiuser interaction;public screen;mobile device;shopping malls;airport;museum;restaurant;contextual information;interactivity;touchscreen;marketing;entertainment;PACMEN","","2","1","15","","19 Dec 2011","","","IEEE","IEEE Conferences"
"Simulation Framework for Evaluation of Indoor Navigation Systems","Y. Tao; A. Ganz","Department of Electrical and Computer Engineering, University of Massachusetts–Amherst, Amherst, MA, USA; Department of Electrical and Computer Engineering, University of Massachusetts–Amherst, Amherst, MA, USA","IEEE Access","3 Feb 2020","2020","8","","20028","20042","In this paper, we introduce the first simulation framework that provides cost-effective means to evaluate indoor navigation systems for different user groups (e.g., users with visual impairment), various positioning techniques, and navigation instructions algorithms. The simulation engine uses the Unity game engine, which tracks the virtual user interaction and motion in a virtual environment that represents the physical environment in which the user navigates. The framework includes the following modules that will be defined by the indoor navigation system developers: 1) Positioning module which simulates indoor localization techniques and their observed accuracy; 2) Indoor navigation algorithm module that generates the navigation instructions using natural language phrases, and 3) Virtual user model (VUM) that includes human perception and information processing that can understand navigation instructions, perceive the surrounding environment, and act based on this information. The framework also includes a performance analysis module that evaluates the indoor navigation system performance in terms of navigation success rate and route similarity.","2169-3536","","10.1109/ACCESS.2020.2968435","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964372","Blind and visually impaired;artificial intelligence;natural language processing;wayfinding;PERCEPT;navigation;virtual user model;simulation;ACT-R","Indoor navigation;Virtual environments;Solid modeling;Games;Sensors;Visualization","computer games;human computer interaction;indoor navigation;indoor radio;natural language processing;telecommunication computing;user modelling;virtual reality","virtual user model;natural language phrases;indoor navigation algorithm module;indoor navigation system performance;indoor localization techniques;virtual environment;virtual user interaction;Unity game engine;simulation engine;navigation instructions;positioning techniques;indoor navigation systems;simulation framework","","","","62","CCBY","22 Jan 2020","","","IEEE","IEEE Journals"
"Distributed Haptic Interactions with Physically Based 3D Deformable Models over Lossy Networks","Z. Tang; Y. Yang; X. Guo; B. Prabhakaran","Towson University, Towson; University of New Mexico, Albuquerque; University of Texas at Dallas, Richardson; University of Texas at Dallas, Richardson","IEEE Transactions on Haptics","9 Dec 2013","2013","6","4","417","428","Researchers have faced great challenges when simulating complicated 3D volumetric deformable models in haptics-enabled collaborative/cooperative virtual environments (HCVEs) due to the expensive simulation cost, heavy communication load, and unstable network conditions. When general network services are applied to HCVEs, network problems such as packet loss, delay, and jitter can cause severe visual distortion, haptic instability, and system inconsistency. In this paper, we propose a novel approach to support haptic interactions with physically based 3D deformable models in a distributed virtual environment. Our objective is to achieve real-time sharing of deformable and force simulations over general networks. Combining linear modal analysis and corotational methods, we can effectively simulate physical behaviors of 3D objects, even for large rotational deformations. We analyze different factors that influence HCVEs' performance and focus on exploring solutions for streaming over lossy networks. In our system, 3D deformation can be described by a fairly small amount of data (several KB) using accelerations in the spectral domain, so that we can achieve low communication load and effective streaming. We develop a loss compensation and prediction algorithm to correct the errors/distortions caused by network problem, and a force prediction method to simulate force at users' side to ensure the haptic stability, and the visual and haptic consistency. Our system works well under both the client-server and the peer-to-peer distribution structures, and can be easily extended to other topologies. In addition to theoretical analysis, we have tested the proposed system and algorithms under various network conditions. The experimental results are remarkably good, confirming the effectiveness, robustness, and validity of our approach.","2329-4051","","10.1109/TOH.2013.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604387","Collaborative virtual environments;3D deformation;haptics;real-time simulation;user interaction","Haptic interfaces;Deformable models;Three-dimensional displays;Computational modeling;Collaboration","client-server systems;haptic interfaces;peer-to-peer computing;virtual reality","spectral domain;loss compensation;force prediction method;visual consistency;haptic consistency;client-server distribution structures;peer-to-peer distribution structures;rotational deformations;3D objects;physical behaviors;corotational methods;linear modal analysis;force simulations;deformable simulations;distributed virtual environment;system inconsistency;haptic instability;visual distortion;jitter;delay;packet loss;network services;network conditions;communication load;simulation cost;HCVE;haptics-enabled collaborative/cooperative virtual environments;3D volumetric deformable models;lossy networks;physically based 3D deformable models;distributed haptic interactions","Algorithms;Computer Communication Networks;Computer Simulation;Humans;Imaging, Three-Dimensional;Models, Biological;Models, Theoretical;Signal Processing, Computer-Assisted;Touch;User-Computer Interface","4","","46","","19 Sep 2013","","","IEEE","IEEE Journals"
"An evaluation of two simple methods for representing heaviness in immersive virtual environments","J. Hummel; J. Dodiya; R. Wolff; A. Gerndt; T. Kuhlen","Simulation and Software Technology, German Aerospace Center (DLR), Germany; Simulation and Software Technology, German Aerospace Center (DLR), Germany; Simulation and Software Technology, German Aerospace Center (DLR), Germany; Simulation and Software Technology, German Aerospace Center (DLR), Germany; Virtual Reality Group, RWTH Aachen University, Germany","2013 IEEE Symposium on 3D User Interfaces (3DUI)","5 Sep 2013","2013","","","87","94","Weight perception in virtual environments generally can be achieved with haptic devices. However, most of these are hard to integrate in an immersive virtual environment (IVE) due to their technical complexity and the restriction of a user's movement within the IVE. We describe two simple methods using only a wireless light-weight finger-tracking device in combination with a physics simulated hand model to create a feeling of heaviness of virtual objects when interacting with them in an IVE. The first method maps the varying distance between tracked fingers and the thumb to the grasping force required for lifting a virtual object with a given weight. The second method maps the detected intensity of finger pinch during grasping gestures to the lifting force. In an experiment described in this paper we investigated the potential of the proposed methods for the discrimination of heaviness of virtual objects by finding the just noticeable difference (JND) to calculate the Weber fraction. Furthermore, the workload that users experienced using these methods was measured to gain more insight into their usefulness as interaction technique. At a hit ratio of 0.75, the determined Weber fraction using the finger distance based method was 16.25% and using the pinch based method was 15.48%, which corresponds to values found in related work. There was no significant effect of method on the difference threshold measured and the workload experienced, however the user preference was higher for the pinch based method. The results demonstrate the capability of the proposed methods for the perception of heaviness in IVEs and therefore represent a simple alternative to haptics based methods.","","978-1-4673-6098-2","10.1109/3DUI.2013.6550202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550202","","Force;Grasping;Thumb;Skin;Software","gesture recognition;haptic interfaces;virtual reality","heaviness representation;immersive virtual environments;IVE;weight perception;technical complexity;wireless light-weight finger-tracking device;physics simulated hand model;finger pinch intensity detection;grasping gestures;heaviness discrimination;just noticeable difference;JND;determined Weber fraction;finger distance based method;haptics based methods","","6","","27","","5 Sep 2013","","","IEEE","IEEE Conferences"
"Virtual reality for post-stroke shoulder-arm motor rehabilitation: Training system & assessment method","Shih-Ching Yeh; Si-Huei Lee; Jia-Chi Wang; Shuya Chen; Yu-Tsung Chen; Yi-Yung Yang; Huang-Ren Chen; Yen-Po Hung","Department of Computer Science and Information Engineering National Central University, China; Taipei Veterans General Hospital, Taiwan; Taipei Veterans General Hospital, Taiwan; Department of Physical Therapy, China Medical University, China; Department of Computer Science and Information Engineering National Central University, China; Taipei Veterans General Hospital, Taiwan; Department of Computer Science and Information Engineering National Central University, China; Department of Computer Science and Information Engineering National Central University, China","2012 IEEE 14th International Conference on e-Health Networking, Applications and Services (Healthcom)","13 Dec 2012","2012","","","190","195","Stroke is one of the major diseases around the world. The brain injury caused by stroke will derive sustaining neurological disorder of different forms, which in turn will lead to all kinds of limb and body exercise hindrance and will cause significant challenge to the life of the patient, that is, the quality of life of the patient is going to be strictly affected. Along with the development and popularity of technology, scholars in the medical care and rehabilitation fields are trying to integrate all kinds of new technologies to perform the development of new rehabilitation training system. This research aims at rehabilitation items of upper limbs, which include the reciprocating stretching of upper limb, reaching of the upper arm, bi-lateral coordination and balance of the body. In association with interactive technology, game technology, sensor technology and stereo image technology, virtual reality physical-based training task is developed, and initial pilot test is done on patient with stroke, meanwhile, multi-dimensional experimental results are acquired, which include clinical test assessment, task performance, historical data of exercise track and psychological emotional data. The research objectives are to verify the functionality of the system, to verify the effectiveness of the system on rehabilitation, to develop new assessment method and to investigate topics related to human machine interaction. The experimental results have verified the functionality of this rehabilitation training task in all aspects. Through the exercise analysis of the historical data of exercise track and the statistical analysis of task performance of the past therapy sessions, this system can acquire successfully reliable and valuable information to be used for future verification of medical therapeutic effectiveness and the development of new type of clinical assessment method. In the mean time, according to the measured psychological emotional data as perceived subjectively, this system indeed can urge the patient to engage continuously rehabilitation therapeutic session that is based on this training system and enjoy it, besides, the authors are very confident on the possibly generated rehabilitation effect of these two training tasks.","","978-1-4577-2040-6","10.1109/HealthCom.2012.6379398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6379398","","Training;Games;Virtual reality;Psychology;Solid modeling;Extremities;Medical treatment","computer games;diseases;medical disorders;medical image processing;patient rehabilitation;stereo image processing;virtual reality","post-stroke shoulder-arm motor rehabilitation;brain injury;neurological disorder;quality of life;medical care;rehabilitation training system;interactive technology;game technology;sensor technology;stereo image technology;virtual reality physical-based training task;human machine interaction;clinical assessment method","","2","","14","","13 Dec 2012","","","IEEE","IEEE Conferences"
"Deriving software engineering requirements specification for computer graphics simulation systems through a case study","M. Song; P. Grogono","Concordia University, Montreal, Quebec, Canada, H3G 1M8; Concordia University, Montreal, Quebec, Canada, H3G 1M8","The 3rd International Conference on Information Sciences and Interaction Sciences","3 Aug 2010","2010","","","285","291","In this work we derive a set of requirements from a detailed case study of the Softbody Simulation System design and specification in the attempt to come up with the requirements all similar systems need to have and to further generalize our findings to other physical-based simulation systems involving real-time computer graphics.","","978-1-4244-7386-1","10.1109/ICICIS.2010.5534757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5534757","requirements engineering;computer graphics;simulation systems;softbody simulation","Software engineering;Computer graphics;Computational modeling;Computer simulation;Object oriented modeling;Visualization;Open source software;Guidelines;Interactive systems;Computer languages","computer graphics;digital simulation;formal specification","software engineering;requirements specification;computer graphics simulation systems;Softbody simulation system design;physical-based simulation systems","","1","","32","","3 Aug 2010","","","IEEE","IEEE Conferences"
"Architectural approach to interoperability between multi-agent systems and 3D virtual worlds","H. B. Barón; R. G. Crespo; O. S. Martinez","Facultad de Ingeniería, Universidad Católica de Colombia, Bogotá, Colombia; Escuela de Ingeniería y Arquitectura, Universidad Pontificia de Salamanca, Madrid, Spain; Facultad de Ingeniería, Universidad Carlos III Madrid, Spain","2013 8th Computing Colombian Conference (8CCC)","21 Oct 2013","2013","","","1","6","The three-dimensional interfaces are approaching to virtual reality, because allowing to create simulation scenarios in which man interacts to likeness of the real live, however, a rich graphical environment and natural interaction itself neither warrants effective learning, it need a monitoring, evaluation and feedback. Therefore, We propose the integration of three-dimensional environments with multi agents system, it is looking customize the learning process to the individual needs of students, organize and distribute content efficiently and support student reflection on their learning. Different studies have made specific implementations of agents through markup languages, however the proposed model, looks for general purpose implementations. Our architectural model integrates the behaviors of multi-agent systems and virtual 3D World. Validation was performed through an implementation that integrates Java Agent Development Framework developed by Telecom Italia Lab and OpenSimulator.","","978-1-4799-1056-4","10.1109/ColombianCC.2013.6637528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637528","intelligent Agents;3D Virtual Worlds;Architectural Approach;Interoperability","Three-dimensional displays;Second Life;Solid modeling;Software;Unified modeling language;ISO standards;Servers","computer aided instruction;human computer interaction;Java;multi-agent systems;open systems;software architecture;virtual reality","interoperability;multiagent systems;3D virtual worlds;three-dimensional interfaces;virtual reality;simulation scenarios;graphical environment;natural interaction;learning process customization;individual student needs;content distribution;content organization;markup languages;general purpose implementations;architectural model;Java agent development framework;Telecom Italia Lab;OpenSimulator","","","","22","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Construction of virtual world using dynamics modules and interaction modules","T. Yoshikawa; H. Ueda","Dept. of Mech. Eng., Kyoto Univ., Japan; Dept. of Mech. Eng., Kyoto Univ., Japan","Proceedings of IEEE International Conference on Robotics and Automation","6 Aug 2002","1996","3","","2358","2364 vol.3","Recently force-feedback has been recognized to be important for virtual reality systems, and many studies have been done in this field. Although we have proposed several methods for displaying the operating feel of a virtual object by considering its dynamics, we have not dealt with cases of multiple objects or operators. In more advanced applications of haptic virtual reality, however, multiple operators and multiple objects will generally exist in the virtual world. In this paper, we propose a systematic method of constructing a virtual world by combining the dynamics modules and interaction modules, the former denoting elements of the virtual world (virtual objects or operators) and the latter denoting interactions between them. Since this method makes it possible to regard those elements as equal, we can construct a flexible and extendable virtual world to which we can easily add new elements or remove some of its elements. An experimental result using a prototype force display system shows the validity of the proposed method.","1050-4729","0-7803-2988-0","10.1109/ROBOT.1996.506516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506516","","Modular construction;Virtual reality;Haptic interfaces;Virtual prototyping;Auditory displays;Interference;Collaboration;Manipulator dynamics","virtual reality;dynamics;feedback;interactive systems;telerobotics","virtual world;dynamics modules;interaction modules;force-feedback;dynamics;haptic virtual reality;force display system","","14","40","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Diverse augmented reality exhibitions for differential users based upon private quick response code","P. Lin; C. Teng; Y. Chen","Department of Information Communication, and Innovation Center for Big Data and Digital Convergence, Yuan Ze University, Taiwan; Department of Information Communication, and Innovation Center for Big Data and Digital Convergence, Yuan Ze University, Taiwan; Department of M-Commerce and Multimedia Applications, Asia University, Taiwan","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","25 Feb 2016","2015","","","1121","1125","The marker-based augmented reality (AR) system can produce 3D virtual object to satisfy the user interaction in the real-world. The marker is a tag/pattern and used to help the AR system to locate the corresponding virtual object on the scene. The marker, however, needs to be designed firstly and the recognition capability is limited. The exhibited virtual object and the marker, unfortunately, are linked in the AR system. Hence, such conventional AR system treats all users as the same role. In this article, we proposed a diversity AR system that can provide the differential virtual objects exhibitions for specific users, such as the joint member and different degree person. The new schemes exploited the error correction capability of QR barcode to conceal the individual secret stream of users into a QR marker. Consequently, the general user can only observe the normal 3D virtual object on the marked QR tag. The assigned members can further reveal the various virtual objects from the same QR tag. The proposed diverse AR system can distinguish different users from same QR marker and thereby exhibit the corresponding virtual objects. The new system is practical and can be widely applied in specific customer AR applications.","","978-9-8814-7680-7","10.1109/APSIPA.2015.7415445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415445","","Augmented reality;Cameras;Uniform resource locators;Error correction codes;Three-dimensional displays;Error correction;Education","augmented reality","marker-based AR system;differential users;private quick response code;3D virtual object;user interaction;diversity AR system;virtual objects exhibitions;QR tag;customer AR applications","","4","","17","","25 Feb 2016","","","IEEE","IEEE Conferences"
"Visuo-motor tracking with coordinated wrist movements under different combinations of visual and kinesthetic disturbances","L. Masia; V. Squeri; M. Casadio; P. Morasso; V. Sanguineti; G. Sandini","Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Department of Informatics, Systems and Telecommunications, University of Genoa, Italy; Department of Informatics, Systems and Telecommunications, University of Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy","2009 2nd Conference on Human System Interactions","23 Jun 2009","2009","","","715","718","This study addresses a major problem in the design of HCI (human-computer interface) systems: how to avoid or reduce the long learning/adaptation process and the corresponding attentional load of the underlying hand-eye coordination task that frequently affects HCI systems. In particular, we considered a 2D tracking task with two degrees of freedom of the wrist to a visual target whose frame of reference was rotated with respect to a body-fixed frame in a time varying manner. We investigated it by means of a wrist robot coupled with a virtual reality system. The experimental protocol consisted of applying kinesthetic and visual disturbances in a unimodal or bimodal manner and observing the tracking performance. The kinesthetic disturbance was provided by passively rotating the forearm of the subjects by the third degree of freedom of the wrist robot, while the visual disturbance was provided by rotating the visual scene. The results suggest that the combination of a suitable proprioceptive feedback with the kinematic redundancy of the HCI system might be a rather general principle for improving the efficiency of HCI systems.","2158-2254","978-1-4244-3959-1","10.1109/HSI.2009.5091065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5091065","wrist robot;visuo-proprioceptive disturbance;tracking;virtual reality","Tracking;Wrist;Robot kinematics;Human computer interaction;Motor drives;Mice;Virtual reality;Brushes;Cognitive robotics;Informatics","control engineering computing;human computer interaction;robots;virtual reality","visuomotor tracking;coordinated wrist movements;visual-kinesthetic disturbances;human-computer interface;learning-adaptation process;hand-eye coordination task;virtual reality system;kinematic redundancy","","1","","12","","23 Jun 2009","","","IEEE","IEEE Conferences"
"The god-finger method for improving 3D interaction with virtual objects through simulation of contact area","A. Talvas; M. Marchal; A. Lécuyer","INSA/Inria Rennes, France; INSA/Inria Rennes, France; Inria Rennes, France","2013 IEEE Symposium on 3D User Interfaces (3DUI)","5 Sep 2013","2013","","","111","114","In physically-based virtual environments, interaction with objects generally happens through contact points that barely represent the area of contact between the user's hand and the virtual object. This representation of contacts contrasts with real life situations where our finger pads have the ability to deform slightly to match the shape of a touched object. In this paper, we propose a method called god-finger to simulate a contact area from a single contact point determined by collision detection, and usable in a rigid body physics engine. The method uses the geometry of the object and the force applied to it to determine additional contact points that will emulate the presence of a contact area between the user's proxy and the virtual object. It could improve the manipulation of objects by constraining the rotation of touched objects in a similar manner to actual finger pads. An implementation in a physics engine shows that the method could make for more realistic behaviour when manipulating objects while keeping high simulation rates.","","978-1-4673-6098-2","10.1109/3DUI.2013.6550206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550206","","Vectors;Haptic interfaces;Force;Geometry;Engines;Friction;Computational modeling","computational geometry;digital simulation;graphical user interfaces;virtual reality","god-finger method;3D interaction improvement;virtual objects;contact area simulation;physically-based virtual environments;contact points;finger pads;physics engine","","3","","12","","5 Sep 2013","","","IEEE","IEEE Conferences"
"Constraint Fluids","K. Bodin; C. Lacoursiere; M. Servin","Umeå University and Algoryx Simulation, Umeå; Umeå University and Algoryx Simulation, Umeå; Umeå University and Algoryx Simulation, Umeå","IEEE Transactions on Visualization and Computer Graphics","12 Jan 2012","2012","18","3","516","526","We present a fluid simulation method based on Smoothed Particle Hydrodynamics (SPH) in which incompressibility and boundary conditions are enforced using holonomic kinematic constraints on the density. This formulation enables systematic multiphysics integration in which interactions are modeled via similar constraints between the fluid pseudoparticles and impenetrable surfaces of other bodies. These conditions embody Archimede's principle for solids and thus buoyancy results as a direct consequence. We use a variational time stepping scheme suitable for general constrained multibody systems we call SPOOK. Each step requires the solution of only one Mixed Linear Complementarity Problem (MLCP) with very few inequalities, corresponding to solid boundary conditions. We solve this MLCP with a fast iterative method. Overall stability is vastly improved in comparison to the unconstrained version of SPH, and this allows much larger time steps, and an increase in overall performance by two orders of magnitude. Proof of concept is given for computer graphics applications and interactive simulations.","1941-0506","","10.1109/TVCG.2011.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708198","SPH;incompressible;constraints;fluid simulation;variational integrator.","Force;Equations;Mathematical model;Computer graphics;Approximation methods;Computational modeling;Stability analysis","computational fluid dynamics;computer graphics;digital simulation;hydrodynamics;iterative methods","constraint fluids;fluid simulation method;smoothed particle hydrodynamics;incompressibility conditions;boundary conditions;holonomic kinematic constraints;systematic multiphysics integration;fluid pseudoparticles;Archimedes principle;buoyancy;SPOOK;mixed linear complementarity problem;fast iterative method;computer graphics applications;interactive simulations","Algorithms;Computer Graphics;Computer Simulation;Hydrodynamics;Models, Theoretical","34","1","75","","4 Feb 2011","","","IEEE","IEEE Journals"
"The empty museum. Multi-user interaction in an immersive and physically walkable VR space","L. Hernandez; J. Taibo; A. Seoane; R. Lopez; R. Lopez","Eng., Archit. & Urban Design Visualisation Group, Univ. da Coruna, Spain; Eng., Archit. & Urban Design Visualisation Group, Univ. da Coruna, Spain; Eng., Archit. & Urban Design Visualisation Group, Univ. da Coruna, Spain; Eng., Archit. & Urban Design Visualisation Group, Univ. da Coruna, Spain; Eng., Archit. & Urban Design Visualisation Group, Univ. da Coruna, Spain","Proceedings. 2003 International Conference on Cyberworlds","8 Jan 2004","2003","","","446","452","Until quite recently, virtual reality systems consisted of fixed devices which enabled the user to feel immersed in a spot of the virtual space by means of the adequate hardware. The recent emergence of wireless systems for motion capture, together with the increase in graphic power of laptops, and the generalisation of wireless networks has allowed the appearance of the first systems in which at last the user is able to walk physically within a given space framed in the real one, and containing the objects and elements of the virtual space. Some examples of this hybrid space have already been accomplished worldwide. However, beyond the technical problems with the development of these systems, we must bear in mind the types of contents to be shown, making the most of the possibilities offered by the fact that the user him/herself is the pointer in this kind of virtual reality, while the space itself is the interface. The authors have recently developed a system similar to the ones described. This is a totally immersive, walkable and wireless system called the Empty Museum. The paper outlines its enlargement with the purpose of making it simultaneously usable by several persons. Besides, an example of content is provided which has been specifically designed in order to be experienced in multi-user mode with this equipment: the Virtual Art Gallery.","","0-7695-1922-9","10.1109/CYBER.2003.1253488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1253488","","Virtual reality;Head;Hardware;Displays;Design engineering;Visualization;Irrigation;Graphics;Portable computers;Wireless networks","virtual reality;art;wireless LAN","multi-user interaction;VR space;virtual reality;wireless system;wireless network;virtual art gallery","","2","","12","","8 Jan 2004","","","IEEE","IEEE Conferences"
"Domain-general simulation and planning with physical schemas","M. S. Atkin; D. L. Westbrook; P. R. Cohen","Dept. of Comput. Sci., Massachusetts Univ., Amherst, MA, USA; NA; NA","2000 Winter Simulation Conference Proceedings (Cat. No.00CH37165)","6 Aug 2002","2000","2","","1730","1738 vol.2","Physical schemas are representations of simple physically grounded relationships and interactions such as ""move"", ""push"", and ""contain"". We believe they are the conceptual primitives an agent employs to understand its environment. Physical schemas can be used at varying levels of abstraction across a variety of domains. We have designed a domain-general agent simulation and control testbed based on physical schemas. If a domain can be described in physical terms as agents moving and applying force, it can be simulated in this testbed. Furthermore, we show that physical schemas can be viewed as the basis for abstract plans and a domain-general planner, GRASP. Our simulation and planning system is currently being evaluated in a continuous, dynamic, and adversarial domain based on the game of Capture the Flag. The paper concludes with an example of how GRASP was applied to the problem of Course of Action generation and evaluation.","","0-7803-6579-8","10.1109/WSC.2000.899163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899163","","Grasping;Testing;Animals;Shape;Pediatrics;Computational modeling;Knowledge based systems;Computer science;Drives;Psychology","digital simulation;software agents;planning (artificial intelligence);inference mechanisms","domain-general simulation;planning;physical schemas;physically grounded relationships;conceptual primitives;abstraction levels;domain-general agent simulation;control testbed;abstract plans;domain-general planner;GRASP;adversarial domain;Capture the Flag;Course of Action generation;General Reasoning using Abstract Physics;least-commitment partial hierarchical planner","","3","","30","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Mobile augmented reality for interactive catalogue","N. F. Mohamed El-firjani; A. M. Maatuk","Faculty of Information Technology, Benghazi University, Libya; Faculty of Information Technology, Benghazi University, Libya","2016 International Conference on Engineering & MIS (ICEMIS)","17 Nov 2016","2016","","","1","4","Augmented reality has been widely recognized due to its dominance and increasing advancement of the technology as an extensive circulation of smart phones. It is a form of human computer interaction that provides an insight with surplus information to users in the form of reality. This has made the use of augmented reality an important concept in sales promotion. A print catalogue is a way for products promotion. Print catalogues contain short sentences, pictures and limited information which are non-interactive to readers. Many people have problems when they want to buy products in terms of specifications and insufficient information. This paper aims to develop an interactive catalogue that uses augmented reality to provide full product description to customers. We have employed a general design methodology for gathering the functional and non-functional requirements for the prototype development. The developed prototype has been evaluated by experts and normal users, and the evaluation showed that the prototype is perceived as useful and easy to use.","","978-1-5090-5579-1","10.1109/ICEMIS.2016.7745370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745370","Print catalogue;Human computer interaction;prototype","Smart phones;Mobile communication;Prototypes;Augmented reality;Videos;Mars","augmented reality;human computer interaction;interactive systems;mobile computing;promotion (marketing);sales management","mobile augmented reality;interactive catalogue;smart phones;human computer interaction;sales promotion;print catalogues;products promotion;full product description","","1","","25","","17 Nov 2016","","","IEEE","IEEE Conferences"
"A comparison between measured and modelled head-related transfer functions for an enhancement of real-time 3D audio processing for virtual reality environments","A. S. Suarez; J. Tissieres; L. S. Vieira; R. Hunter-McHardy; S. K. Sernavski; S. Serafin",Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen; Aalborg University Copenhagen,"2017 IEEE 3rd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","17 Apr 2017","2017","","","1","9","Sound in Virtual Reality (VR) has been explored in a variety of algorithms which try to enhance the illusion of presence, improving sound localization and spatialization in the virtual environment. As new systems are developed, different models are applied. There is still the need to evaluate and understand the main advantages of each of these approaches. In this study, a performance comparison of two methods for real-time 3D binaural sound tested preferences and quality of presence for headphones in a VR experience. Both the mathematical based HRTF and the convolution based measured HRTF from the MIT KEMAR show a general similarity in the participants sense of localization, depth and presence. Nevertheless, the tests also indicate a preference in elevation perception for the convolution-based measured HRTF. Further experiments with new tools, techniques, contexts, and guidelines are therefore required to highlight the importance and differences between these two methods and other implementations.","","978-1-5386-0459-5","10.1109/SIVE.2017.7901609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7901609","","Ear;Three-dimensional displays;Convolution;Computational modeling;Databases;Mathematical model;Azimuth","audio signal processing;real-time systems;virtual reality","real-time 3D audio processing enhancement;virtual reality environments;sound localization;sound spatialization;binaural sound tested preferences;quality of presence;headphones;VR experience;mathematical based HRTF;MIT KEMAR;convolution","","2","2","11","","17 Apr 2017","","","IEEE","IEEE Conferences"
"Multi-modal Geometry Tutoring System Using Speech and Touchscreen Figure Tracing","K. Kiyohara; R. Nishimura; N. Kitaoka","Graduate School of Technology, Industrial and Social Science, Tokushima University Tokushima, Japan; Graduate School of Technology, Industrial and Social Science, Tokushima University Tokushima, Japan; Graduate School of Technology, Industrial and Social Science, Tokushima University Tokushima, Japan","2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)","13 Dec 2018","2018","","","257","258","People sometimes use gestures with speech to exchange information. As such an example, people explain to someone a solution of a geometric problem of mathematics. In this paper, we introduce an application system, on which a user can explain a solution of a geometric problem displayed on a screen by speech and pointing. Then, the system understands these inputs, adds the explanations to the figure, and an explanation is translated to formulas. In addition, we aim to construct the system that can be provided as a learning application for the general public, which does not require special knowledge, technology, or device for introduction into ICT education.","2378-8143","978-1-5386-6309-7","10.1109/GCCE.2018.8574714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574714","speech recognition;finger pointing;touch screen;multimodal interaction;ICT education;geometric problems","Speech recognition;Tactile sensors;Education;Touch sensitive screens;Computers;Gesture recognition;Fasteners","computer aided instruction;geometry;mathematics computing;touch sensitive screens","multimodal geometry tutoring system;speech;touchscreen figure tracing;gestures;geometric problem;mathematics;application system;learning application","","","","8","","13 Dec 2018","","","IEEE","IEEE Conferences"
"A general simulation algorithm for the accurate assessment of isolated diesel-wind turbines systems interaction. I. A general multimachine power system model","G. S. Stavrakakis; G. N. Kariniotakis","Dept. of Electron. & Comput. Eng., Tech. Univ. of Crete, Chania, Greece; Dept. of Electron. & Comput. Eng., Tech. Univ. of Crete, Chania, Greece","IEEE Transactions on Energy Conversion","6 Aug 2002","1995","10","3","577","583","In the first part of this two-part paper, detailed dynamic equations for the power system and wind energy conversion system (WECS) components and their synthesis to a unified model are presented. This model is the basis for creating simulation software able to perform the transient stability analysis of isolated diesel-wind turbine power systems for accurate assessment of their interaction. Approximations in the various component models, when necessary, remain between limits that do not affect the accuracy of the analysis performed. A new general multimachine power system model is also developed which describes the topology and the complexity of wind-diesel power systems in a compact form which is easy to implement in the simulation software.<>","1558-0059","","10.1109/60.464885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=464885","","Power system modeling;Power system simulation;Power system analysis computing;Power system transients;Power system dynamics;Power system stability;Equations;Wind energy;Analytical models;Software performance","diesel-electric generators;diesel-electric power stations;wind power plants;wind turbines;power system interconnection;power system analysis computing;digital simulation","isolated diesel-wind turbine power systems;multimachine power system model;simulation algorithm;dynamic equations;computer simulation;wind energy conversion system;simulation software;wind turbine;approximations;accuracy;interconnection","","140","","25","","6 Aug 2002","","","IEEE","IEEE Journals"
"Application of contextual QR codes to augmented reality technologies","F. Gutiérrez; M. A. Abud; F. Vera; J. A. Sánchez","División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Veracruz, México; División de Estudios de Posgrado e Investigación, Instituto Tecnológico de Orizaba, Veracruz, México; Laboratory of Interactive and Cooperative Technologies, Universidad de las Américas Puebla, Puebla, México; Laboratory of Interactive and Cooperative Technologies, Universidad de las Américas Puebla, Puebla, México","CONIELECOMP 2013, 23rd International Conference on Electronics, Communications and Computing","13 Jun 2013","2013","","","264","269","Augmented reality is a technology that involves digital content deployed in the physical world in real time through the use of devices and sensors that allow users to interact with the resulting hybrid environment. Likewise, QR codes have become popular and accepted by the general public. They are useful for conveying information in the physical world and thus connect to the digital world in a practical and simple way. This paper presents an application of QR codes for generating augmented reality environments. Using the notion of contexts, a complete user interaction is enabled in terms of the relationship of QR codes and augmented reality. We present an analysis of augmented reality technologies supported by this approach as well as the architecture and operation of a system that implements these concepts.","","978-1-4673-6155-2","10.1109/CONIELECOMP.2013.6525798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525798","Contextual QR Code;Augmented Reality;Ubiquitous computing","Augmented reality;Context;XML;Hardware;Real-time systems;Software;Uniform resource locators","augmented reality;codes;human computer interaction","contextual QR codes;augmented reality technologies;digital content;digital world;complete user interaction","","2","","17","","13 Jun 2013","","","IEEE","IEEE Conferences"
"Mobile Augmented Reality Survey: From Where We Are to Where We Go","D. Chatzopoulos; C. Bermejo; Z. Huang; P. Hui","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Access","2 Jun 2017","2017","5","","6917","6950","The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality (MAR) from science fiction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated MAR applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of MAR, we present a categorization of the application fields together with some representative examples. Next, we introduce the reader to the user interface and experience in MAR applications and continue with the core system components of the MAR systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any MAR application and the network connectivity of the devices that run MAR applications together with its importance to the performance of the application. We continue with the importance of data management in MAR systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.","2169-3536","","10.1109/ACCESS.2017.2698164","General Research Fund from the Research Grants Council of Hong Kong; Innovation and Technology Fund from the Hong Kong Innovation and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912316","Mobile augmented reality;mobile computing;human computer interaction","Mobile communication;Mobile computing;Cloud computing;Augmented reality;Smart phones;Sensors","augmented reality;graphical user interfaces;human computer interaction;mobile computing","ubiquitous access;Internet access;cooperative networking;mobile cloud computing;computer vision;mobile augmented reality;MAR applications;remote servers;user interface;network connectivity;data management;systems performance;systems sustainability;mobile devices","","115","","316","","26 Apr 2017","","","IEEE","IEEE Journals"
"Navigation Power of MaxWhere: a Unique Solution","B. Berki","Széchenyi István University, Doctoral School of Multidisciplinary Engineering Sciences,Györ,Hungary","2020 11th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","2 Nov 2020","2020","","","000509","000514","Precise and accurate navigation is an elementary expectation in desktop virtual realities. In this paper, the navigation method of MaxWhere VR is presented, furthermore, it is analyzed in terms of interaction fidelity and controller mapping. The MaxWhere VR platform offers different navigational operations, that are optimized for an external mouse with a scroll wheel. Besides moving forward and backward, rotating the camera view and lifting, another navigation type is available, the spherical orbit. This navigational operation is known from engineering software, and its role is to look around a selected object and observe it from each side. This function can be useful in educational and exhibition spaces where 3D models are located. These navigational operations are considered low-fidelity interactions, compared to human motions, although with the directionally mapped general control devices, such as a mouse, high perceived naturalness can be achieved that can relate to a higher sense of spatial presence.","2380-7350","978-1-7281-8213-1","10.1109/CogInfoCom50765.2020.9237904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237904","virtual reality;navigation;spherical orbit;MaxWhere","Three-dimensional displays;Navigation;Wheels;Virtual reality;Orbits;Mice;Software","mouse controllers (computers);solid modelling;virtual reality","elementary expectation;desktop virtual realities;interaction fidelity;controller mapping;MaxWhere VR platform;navigational operations;external mouse;scroll wheel;camera view;lifting;navigation type;navigational operation;low-fidelity interactions;3D models","","","","36","","2 Nov 2020","","","IEEE","IEEE Conferences"
"Self-Integration in Mediated-Reality Systems: a Socio-Technical Perspective","J. Botev","University of Luxembourg 6, Avenue de la Fonte,Department of Computer Science,Esch-sur-Alzette,Luxembourg,4364","2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)","15 Sep 2020","2020","","","60","61","From a structural point of view, mediated-reality systems essentially constitute hybrid socio-technical and cyber-physical systems. As such, they are inherently characterized by highly heterogeneous constituents that need to continuously integrate in order to serve their intended purpose and provide the desired user experience. This position paper discusses the different dimensions specific to self-integration in mediated-reality systems, arguing in favor of a generally human-centric design approach for systems that involve user interaction beyond monitoring tasks. Following a classification of the central interaction modes and issues, an initial set of recommendations is derived for including essential human-in-the-loop aspects already at an early stage of the design and integration process.","","978-1-7281-8414-2","10.1109/ACSOS-C51401.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196390","Self-Integration;Mediated Reality;Interaction;Agency;Collaboration;Systems Design","Virtual reality;Sociotechnical systems;Collaboration;System analysis and design;Telepresence;Context modeling","cyber-physical systems;human computer interaction;user experience;user interfaces;virtual reality","mediated-reality systems;socio-technical perspective;cyber-physical systems;self-integration;user interaction;human-in-the-loop aspects;user experience","","","","13","","15 Sep 2020","","","IEEE","IEEE Conferences"
"Memory-Based Passivation Approach for Stable Haptic Interaction","J. Ryu; M. Yoon","School of Mechanical Engineering , Korea University of Technology and Education, Cheonan, Korea; School of Mechanical Engineering , Korea University of Technology and Education, Cheonan, Korea","IEEE/ASME Transactions on Mechatronics","28 Apr 2014","2014","19","4","1424","1435","Inspired from the graphical interpretation of passivity in a position versus force xy graph, this paper proposes a new concept of passivation method to increase the dynamic range of impedance in which a haptic interface can passively interact. The position versus force graph represents the energy behavior in a one-port haptic interface while the haptic interface is traveling in (pressed) or out of (released) a virtual environment (VE). It interprets that to make the one-port system passive, it is sufficient to bound the releasing path below the pressing path in the position versus force graph. To realize the bound, the computed force output from the VE is saved into a designated memory, addressed by current position, while the haptic interface is pressed. Thereafter, the saved force can be reused to upper-bound the releasing path below the saved pressing path while the haptic interface is released. The proposed method is generalized from preliminary work by including interaction with a moving virtual object and multi-DOF extension, together with more systematic and detailed explanations about the methods. In particular, we reconfigure the one-port moving virtual object as a two-port haptic controller and a connected one-port moving inertia. This reconfiguration allows implementation of the preliminary approach including the interaction with moving virtual objects by using an error versus force passivity instead of a position versus force passivity. This two-port extension also allows generalization of the proposed method to multi-DOF interactions by introducing virtual proxy as a virtual moving object. The proposed method is tested with a single and a multi-DOF haptic interface and shows better performance than the recently proposed field-programmable gate array-based time-domain passivity approach .","1941-014X","","10.1109/TMECH.2013.2285380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648463","Haptic interfaces;human--robot interaction;stability;telerobotics","Force;Pressing;Haptic interfaces;Damping;Frequency modulation;Radiation detectors;Passivation","control engineering computing;haptic interfaces;human-robot interaction;telerobotics;virtual reality","memory-based passivation approach;stable haptic interaction;graphical interpretation;passivation method;force graph;energy behavior;one-port haptic interface;virtual environment;one-port system;computed force output;pressing path;multiDOF extension;one-port moving virtual object;two-port haptic controller;one-port moving inertia;virtual objects;force passivity;multiDOF interaction;virtual proxy;virtual moving object;multiDOF haptic interface;field-programmable gate array-based time-domain passivity","","19","","23","","25 Oct 2013","","","IEEE","IEEE Journals"
"Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object","W. Cho; G. Park; W. Woo",KAIST UVR lab.; KAIST UVR lab.; KAIST UVR lab.,"2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","14 Dec 2020","2020","","","251","259","We propose a 3D hand tracking system using bare-hand depth inpainting from an RGB-depth image for a hand interacting with an object. The effectiveness of most existing hand-object tracking methods is impeded by the insufficiency of data, which do not include hand data occluded by the object, and their reliance on the information inferred from assuming the specific object type. We generate a sufficiently accurate bare-hand depth image from a hand interacting with an object using a conditional generative adversarial network, which is trained using the synthesized 2D silhouettes of the object to learn the morphology of the hand. We evaluate the proposed approach using a hierarchical particle filter-based hand tracker and prove that our approach utilizing the bare-hand tracker in the hand-object interaction dataset achieve state-of-the-art performance. The generalization of our work will enable visual-tactile interaction that is more natural in various wearable augmented reality applications.","1554-7868","978-1-7281-8508-8","10.1109/ISMAR50242.2020.00048","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284802","Artificial intelligence;Computer vision;Computer vision problems;Tracking;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality","Geometry;Three-dimensional displays;Two dimensional displays;Morphology;Semisupervised learning;Object tracking;Augmented reality","augmented reality;image colour analysis;learning (artificial intelligence);neural nets;object tracking;particle filtering (numerical methods)","bare-hand depth inpainting;RGB-depth image;bare-hand depth image;hierarchical particle filter-based hand tracker;3D hand tracking system;hand-object tracking methods;conditional generative adversarial network;2D silhouettes;hand morphology;wearable augmented reality applications;visual-tactile interaction","","","","68","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Workshop on intention recognition in HRI","S. Thill; A. Montebelli; T. Ziemke","Interaction Lab, School of Informatics, University of Skövde, Sweden; Interaction Lab, School of Informatics, University of Skövde, Sweden; Interaction Lab, School of Informatics, University of Skövde, Sweden","2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","14 Apr 2016","2016","","","585","586","The present workshop focuses on the topic of intention recognition in HRI. To be able to recognise intentions of other agents is a fundamental prerequisite to engage in, for instance, instrumental helping or mutual collaboration. It is a necessary aspect of natural interaction. In HRI, the problem is therefore bi-directional: not only does a robot need the ability to infer intentions of humans; humans also need to infer the intentions of the robot. From the human perspective, this inference draws both on the ability to attribute cognitive states to lifeless shapes, and the ability to understand actions of other agents through, for instance, embodied processes or internal simulations (i.e the human ability to form a theory of mind of other agents). How precisely, and to what degree these mechanisms are at work when interacting with social artificial agents remains unknown. From the robotic perspective, this lack of understanding of mechanisms underlying human intention recognition, or the capacity for theory of mind in general, is also challenging: the solution can, for instance, not simply be to make autonomous systems work “just like” humans by copying the biological solution and implementing some technological equivalent. It is therefore important to be clear about the theoretical framework(s) and inherent assumptions underlying technological implementations related to mutual intention. This remains very much an active research area in which further development is necessary. The core purpose of this workshop is thus to contribute to - and advance the state of the art in - this area.","2167-2148","978-1-4673-8370-7","10.1109/HRI.2016.7451868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451868","","Robots;Conferences;Context;Vehicles;Psychology;Neuroscience","human-robot interaction;multi-agent systems","human intention recognition;HRI;human-robot interaction;instrumental helping;mutual collaboration;cognitive state;social artificial agent;robotic perspective","","","","8","","14 Apr 2016","","","IEEE","IEEE Conferences"
"Efficient 3D content authoring framework based on mobile AR","S. Lee; J. Jung; J. Hong; S. Lee; H. Cho; H. S. Yang","AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea; AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea; AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea; AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea; AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea; AIM Lab, Dept. of Computer Science, KAIST Daejeon, Republic of Korea","2012 18th International Conference on Virtual Systems and Multimedia","3 Dec 2012","2012","","","95","102","Many applications for generating and authoring 3D content are dependent on interaction and visualization on a 2D screen. There have been various applications using 3D interaction to overcome the limited interaction in 2 dimensional space. Those applications, however, require special hardware [13] for interaction or marker-based techniques for scene recognition and tracking. Both approaches make the applications less intuitive and uncomfortable especially when one creates and manages 3D objects in the content. In order to overcome these limitations, in this paper we propose a novel application that creates 3D content such as 3D animations and 3D movies. In our method we make use of Augmented Reality (AR) techniques and hand gesture recognition in 3D AR space. To obtain the specific information of an open set, we use real-time object recognition and marker-less tracking based on the generic randomized forest (GRF) method, which facilitates the use of mobile devices like ultra-mobile PCs (UMPCs) for generating 3D content. Since in general it is difficult to generate realistic 3D content on the mobile devices in real-time, we apply a server-client model to our application so as to separate content construction and scene generation. Based on the techniques mentioned above, we develop an interactive and easy-to-use AR application using a natural user interface (NUI) to make 3D content for not only experts in the related fields but also for general users; we show the novelty and the efficiency of our proposed approach via making a short 3D animation.","","978-1-4673-2563-9","10.1109/VSMM.2012.6365912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365912","mobile AR;authoring framework;marker-less tracking;generic randomized forest;3d content generation","Cameras;Object recognition;Mobile handsets;Target tracking;Real-time systems;Mobile communication;Visualization","augmented reality;client-server systems;computer animation;content management;data visualisation;gesture recognition;image classification;mobile computing;object recognition;object tracking;screens (display);user interfaces","3D content authoring framework;mobile AR technique;3D content generation;2D screen interaction;2D screen visualization;3D interaction;scene recognition;scene tracking;augmented reality technique;hand gesture recognition;3D AR space;real-time object recognition;marker-less tracking;generic randomized forest method;GRF method;mobile devices;ultra-mobile PC;UMPC;server-client model;content construction;scene generation;natural user interface;NUI;3D animation","","","","13","","3 Dec 2012","","","IEEE","IEEE Conferences"
"SSVEP Stimulus Layout Effect on Accuracy of Brain-Computer Interfaces in Augmented Reality Glasses","X. Zhao; C. Liu; Z. Xu; L. Zhang; R. Zhang","School of Information Engineering, Zhengzhou University, Zhengzhou, China; School of Information Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; Henan Key Laboratory of Brain Science and Brain–Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, China","IEEE Access","13 Jan 2020","2020","8","","5990","5998","Steady-state visual evoked potentials-based brain-computer interfaces (SSVEP-BCI) has the advantage of high information transfer rate (ITR) and little user training, and it has a high application value in the field of disability assistance and human-computer interaction. Generally SSVEP-BCI requires a personal computer screen (PC) to display several repetitive visual stimuli for inducing the SSVEP response, which reduces its portability and flexibility. Using augmented reality (AR) glasses worn on the head to display the repetitive visual stimuli could solve the above drawbacks, but whether it could achieve the same accuracy as PC screen in the case of reduced brightness and increased interference is unknown. In current study, we firstly designed 4 stimulus layouts and displayed them with Microsoft HoloLens (AR-SSVEP) glasses, comparison analysis showed that the classification accuracies are influenced by the stimulus layout when the stimulus duration is less than 3s. When the stimulus duration exceeds 3s, there is no significant accuracy difference between the 4 layouts. Then we designed a similar experimental paradigm on PC screen (PC-SSVEP) based on the best layout of AR. Classification results showed that AR-SSVEP achieved similar accuracy with PC-SSVEP when the stimulus duration is more than 3s, but when the stimulus duration is less than 2s, the accuracy of AR-SSVEP is lower than PC-SSVEP. Brain topological analysis indicated that the spatial distribution of SSVEP responses is similar, both of which are strongest in the occipital region. Current study indicated that stimulus layout is a key factor when building SSVEP-BCI with AR glasses, especially when the stimulation time is short.","2169-3536","","10.1109/ACCESS.2019.2963442","National Natural Science Foundation of China; Key Project at Central Government Level; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947980","Steady-state visual evoked potentials (SSVEP);brain–computer interfaces (BCI);augmented reality (AR);optical see-through (OST);human–computer interaction","Layout;Glass;Visualization;Electroencephalography;Correlation;Microsoft Windows;Brain-computer interfaces","augmented reality;brain;brain-computer interfaces;electroencephalography;glass;handicapped aids;medical signal processing;neurophysiology;visual evoked potentials","SSVEP stimulus layout effect;augmented reality glasses;steady-state visual evoked potentials-based brain-computer interfaces;high information transfer rate;high application value;disability assistance;human-computer interaction;personal computer screen;repetitive visual stimuli;SSVEP response;PC screen;classification accuracies;stimulus duration;significant accuracy difference;PC-SSVEP;AR-SSVEP achieved similar accuracy;brain topological analysis;building SSVEP-BCI;stimulus layouts;time 3.0 s;time 2.0 s","","","","45","CCBY","1 Jan 2020","","","IEEE","IEEE Journals"
"Learning social affordance grammar from videos: Transferring human interactions to human-robot interactions","T. Shu; X. Gao; M. S. Ryoo; S. Zhu","Center for Vision, Cogntion, Learning, and Autonomy, University of California, Los Angeles, USA; Department of Electronic Engineering, Fudan University, China; School of Informatics and Computing, Indiana University, Bloomington, USA; Center for Vision, Cogntion, Learning, and Autonomy, University of California, Los Angeles, USA","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","1669","1676","In this paper, we present a general framework for learning social affordance grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human interactions, and transfer the grammar to humanoids to enable a real-time motion inference for human-robot interaction (HRI). Based on Gibbs sampling, our weakly supervised grammar learning can automatically construct a hierarchical representation of an interaction with long-term joint sub-tasks of both agents and short term atomic actions of individual agents. Based on a new RGB-D video dataset with rich instances of human interactions, our experiments of Baxter simulation, human evaluation, and real Baxter test demonstrate that the model learned from limited training data successfully generates human-like behaviors in unseen scenarios and outperforms both baselines.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989197","","Grammar;Robots;Videos;Human-robot interaction;Spatiotemporal phenomena;Grounding;Real-time systems","graph theory;humanoid robots;human-robot interaction;inference mechanisms;learning (artificial intelligence);video signal processing","social affordance grammar learning;human interactions;human-robot interactions;HRI;spatiotemporal AND-OR graph;ST-AOG;humanoid grammar;real-time motion inference;Gibbs sampling;weakly supervised grammar learning;interaction hierarchical representation;RGB-D video dataset;Baxter simulation;human evaluation;real Baxter test;human-like behavior generation","","6","","31","","24 Jul 2017","","","IEEE","IEEE Conferences"
"Withindows: A Framework for Transitional Desktop and Immersive User Interfaces","A. Hill; A. Johnson","University of Illinois at Chicago, e-mail: ahill@evl.uic.edu; University of Illinois at Chicago, e-mail: ajohnson@uic.edu","2008 IEEE Symposium on 3D User Interfaces","31 Mar 2008","2008","","","3","10","The uniqueness of 3D interaction is often used to justify levels of user fatigue that are significantly higher than those of desktop systems. Object manipulation and symbolic manipulation techniques based strictly on first person perspective are also generally less efficient than their desktop counterparts. Instead of considering the two environments as distinct, we have focused on the idea that desktop applications will likely need to transition smoothly into full immersion through intermediate states. The Withindows framework uses image-plane selection and through- the-lens techniques in an attempt to smooth the movement of both traditional and immersive applications across transitional states such as desktop stereo and multi-display setups. We propose using a virtual cursor in the dominant eye and a reinforcing cursor in the non-dominant eye to avoid ambiguity problems that have discouraged the use of image-plane selection in stereo. We show how image-plane selection resolves non-linear control-display relationships inherent in some approaches to desktop stereo. When combined with through-the-lens techniques, image-plane selection allows immersive viewpoint management and 2&‌#xBD;D object manipulation techniques analogous to those on the desktop. This approach resolves global search and scaling problems inherent in prior through-the-lens implementations. We describe extensions for 6 DOF input devices that do not supersede the default interaction method. We developed a single-authored virtual world builder as a proof of concept application of our framework. Our evaluations found alternate perspectives useful but our implementation of viewing windows proved fatiguing to some users.","","978-1-4244-2047-6","10.1109/3DUI.2008.4476584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476584","Through the Lens;Image Plane;Virtual Reality;Augmented Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, augmented, and virtual realities;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques","User interfaces;Fatigue;Virtual reality;Augmented reality;Aerospace simulation;Industrial training;Space technology;Displays;Image resolution;Application software","user interfaces;virtual reality","immersive user interface;desktop user interface;symbolic manipulation;object manipulation;stereo image plane selection;virtual cursor;Withindows framework;virtual reality","","1","2","32","","31 Mar 2008","","","IEEE","IEEE Conferences"
"Haptic interactions with under-actuated robots using virtual mechanisms","G. R. Luecke; J. A. Beckman","Iowa State University, Department of Mechanical Engineering, Ames, USA; Graduate Assistant at Iowa State University, USA","2008 IEEE International Conference on Robotics and Automation","13 Jun 2008","2008","","","2878","2883","Haptic interactions with computer generated simulations has become almost routine in Virtual Reality (VR) applications. While general interaction requires six degrees of freedom, some haptic devices are designed with fewer degrees of freedom, and so have problems representing general haptic contact. In this work, we present a new approach to the control of a general haptic device, and extend this approach to compensate for missing forces in under-actuated haptic devices. We present an experimental implementation using one of the most popular devices, the PHANTOMreg, and show experimental results for a simple case of haptic interaction.","1050-4729","978-1-4244-1646-2","10.1109/ROBOT.2008.4543646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4543646","","Haptic interfaces;Robots;Force feedback;Probes;Virtual environment;Virtual reality;Imaging phantoms;Manipulators;Robotics and automation;Actuators","control engineering computing;haptic interfaces;manipulators;virtual reality","haptic interaction;computer generated simulation;under-actuated robot;virtual reality;haptic device","","2","","6","","13 Jun 2008","","","IEEE","IEEE Conferences"
"A framework for multi-contact multi-body dynamic simulation and haptic display","D. Ruspini; O. Khatib","Robotics Lab., Stanford Univ., CA, USA; NA","Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)","6 Aug 2002","2000","2","","1322","1327 vol.2","Presents a general framework for the dynamic simulation and haptic exploration of complex virtual environments. This work builds on previous developments in simulation, haptics and operational space control. The relations between the dynamic models used in simulation and the models originally developed for robotic control are also presented. This framework has been used to develop a simulator that can model complex interactions between generalized articulated mechanical systems and permits direct ""hands-on"" interaction with the virtual environment through a haptic interface.","","0-7803-6348-5","10.1109/IROS.2000.893204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=893204","","Haptic interfaces;Fingers;Computational modeling;Orbital robotics;Virtual environment;Computer displays;Computer simulation;Rendering (computer graphics);Springs;Laboratories","mechanical contact;N-body problems;dynamics;digital simulation;haptic interfaces;virtual reality;mechanical engineering computing;robot dynamics","multi-contact multi-body dynamic simulation;haptic display;haptic exploration;complex virtual environments;operational space control;dynamic models;robotic control;complex interactions;generalized articulated mechanical systems;hands-on interaction;haptic interface","","21","","15","","6 Aug 2002","","","IEEE","IEEE Conferences"
"GISMOO: collaborative planning for civil infrastructure management in a simple Web-enhanced virtual environment","L. D. McCray; P. M. Jones","SRA Int., Fairfax, VA, USA; NA","Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0","6 Aug 2002","2000","2","","1063","1068 vol.2","The GISMOO project examined the collaborative planning behavior of an existing team of practitioners in the Public Works Division of a US Army base through the use of a simple Web-enhanced virtual environment. Comparisons were drawn between the behaviors of the work group while communicating face to face, and while communicating synchronously via an online virtual conference room based on MOO (Multi-user Object Oriented) technology enhanced by a Web interface. The results of this project showed that the use of an online virtual environment for collaborative planning is practical and could be beneficial, especially to physically dispersed work groups. The value of the conference room tools was not proven in this study, primarily as the structure of the scenarios did not support the asynchronous portion of the team's collaborative work well. The behavior of the work group using the collaboration tool changed in generally predictable ways as compared to face to face communication.","1062-922X","0-7803-6583-6","10.1109/ICSMC.2000.885992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885992","","Collaborative work;Virtual environment;Meeting planning;International collaboration;Online Communities/Technical Collaboration;Collaborative tools;Large-scale systems;Project management","military computing;virtual reality;groupware;information resources;object-oriented programming;user interfaces;human factors;teleconferencing","collaborative planning;civil infrastructure management;Web-enhanced virtual environment;GISMOO project;Public Works Division;US Army base;work group;face to face communication;online virtual conference room;MOO;multi-user object oriented technology;online virtual environment;physically dispersed work groups;conference room tools;collaborative work","","","","7","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Physiological Measurement for Emotion Recognition in Virtual Reality","L. B. Hinkle; K. K. Roudposhti; V. Metsis","Texas State University; Lahijan Branch, Islamic Azad University; Texas State University","2019 2nd International Conference on Data Intelligence and Security (ICDIS)","3 Oct 2019","2019","","","136","143","In this work, various non-invasive sensors are used to collect physiological data during subject interaction with virtual reality environments. The collected data are used to recognize the subjects' emotional response to stimuli. The shortcomings and challenges faced during the data collection and labeling process are discussed, and solutions are proposed. A machine learning approach is adopted for emotion classification. Our experiments show that feature extraction is a crucial step in the classification process. A collection of general purpose features that can be extracted from a variety of physiological biosignals is proposed. Our experimental results show that the proposed feature set achieves better emotion classification accuracy compared to traditional domain-specific features used in previous studies.","","978-1-7281-2080-5","10.1109/ICDIS.2019.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855312","Physiological measurement, emotion recognition, virtual reality, feature extraction, feature selection.","Sensors;Electrodes;Virtual reality;Feature extraction;Biomedical monitoring;Data collection;Physiology","data analysis;emotion recognition;feature extraction;human computer interaction;learning (artificial intelligence);medical signal processing;neurophysiology;sensor fusion;signal classification;virtual reality","machine learning;emotion classification;user interaction;data collection;virtual reality environments;physiological data;noninvasive sensors;emotion recognition;physiological measurement;physiological biosignals;feature extraction","","1","","24","","3 Oct 2019","","","IEEE","IEEE Conferences"
"A behavioral interface to simulate agent-object interactions in real time","M. Kallmann; D. Thalmann","Lab. of Comput. Graphics, EPFL, Lausanne, Switzerland; NA","Proceedings Computer Animation 1999","6 Aug 2002","1999","","","138","146","The paper shows a novel approach to model and control interactive objects for simulations with virtual human agents when real time interactivity is essential. A general conceptualization is made to model objects with behaviors that can provide: information about their functionality changes in appearance from parameterized deformations, and a complete plan for each possible interaction with a virtual human. Such behaviors are described with simple primitive commands, following the actual trend of many standard scene graph file formats that connects language with movements and events to create interactive animations. In our case, special attention is given to correctly interpret object behaviors in parallel; a situation that arrives when many human agents interact at the same time with one same object.","1087-4844","0-7695-0167-2","10.1109/CA.1999.781207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781207","","Humans;Computational modeling;Deformable models;Animation;Virtual environment;Shape;Computer simulation;Layout;Application software;Computer graphics","virtual reality;user interfaces;real-time systems;computer animation;digital simulation;software agents","behavioral interface;agent-object interaction simulation;interactive objects;virtual human agents;real time interactivity;functionality changes;parameterized deformations;simple primitive commands;standard scene graph file formats;interactive animations;object behavior;human agent interaction","","19","","23","","6 Aug 2002","","","IEEE","IEEE Conferences"
"VPARK - a Windows NT software platform for a virtual networked amusement park","H. Seo; C. Joslin; U. Berner; N. Magnenat-Thalmann; M. Jovovic; J. Esmerado; D. Thalmann; I. Palmer","MIRALab., Geneva Univ., Switzerland; NA; NA; NA; NA; NA; NA; NA","Proceedings Computer Graphics International 2000","6 Aug 2002","2000","","","309","315","Presents the VPARK (Virtual Park) system, which includes a networked virtual environment (NVE) system called W-VLNET and an ""attraction building system"" that is able to create and modify the attractions used in the NVE. Both systems have been developed in the Windows NT environment. The paper outlines the techniques for communication, scene management, facial and body animation, and general user interaction modules. The use of VRML97 and MPEG-4 SHNC is overviewed for the purpose of outlining the compatability of the system with other similar virtual reality systems. The software provides realistic virtual actors as well as sets of high-level actions that are applicable to them in real-time. Related issues on obtaining actor models and animating them in real time are presented. The creation process of an attraction incorporates assembling animation units through a timeline. Using this software, the users are able to bring their own scenario-based applications into a shared virtual environment.","","0-7695-0643-7","10.1109/CGI.2000.852347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852347","","Virtual environment;Humans;Animation;Computer graphics;Layout;Virtual reality;Real time systems;Windows;MPEG 4 Standard;Assembly","entertainment;virtual reality;computer animation;groupware;graphical user interfaces;network operating systems","VPARK;Virtual Park;Microsoft Windows NT software platform;virtual networked amusement park;networked virtual environment system;W-VLNET;attraction building system;communication techniques;scene management;facial animation;body animation;user interaction modules;VRML97;MPEG-4 SHNC;compatability;virtual reality system;realistic virtual actors;high-level actions;real-time actor model animation;animation unit assembly;timeline;scenario-based applications;shared virtual environment;virtual humans","","3","1","22","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Why should we use 3D Collaborative Virtual Environments for Cyber Security?","A. Kabil; T. Duval; N. Cuppens; G. L. Comte; Y. Halgand; C. Ponchel","IMT Atlantique Lab-STICC, UMR CNRS 6285; IMT Atlantique Lab-STICC, UMR CNRS 6285; IMT Atlantique Lab-STICC, UMR CNRS 6285; Société Générale; EDF; Airbus Defence and Space","2018 IEEE Fourth VR International Workshop on Collaborative Virtual Environments (3DCVE)","7 Feb 2019","2018","","","1","2","Cyber Security data analysis is an important growing domain: more and more data visualization systems are offered to operators in order to improve their threat detection performances or facilitate suspect behaviors characterization. As today Cyber Security trend is to regroup employees in structures such as Security Operations Center (SOC) or Computer Emergency Response Team (CERT), collaborative approach seems to be relevant in this context. We think that 3D Collaborative Virtual Environments (3DCVE) can be used in order to improve users Cyber Situational Awareness, as they can allow them to have a better understanding of a cyber situation by mediating interactions towards them and also by providing different points of view of the same data, on different scales.","","978-1-5386-5132-2","10.1109/3DCVE.2018.8637109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637109","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Collaborative interaction;Visualization;Visualization application domains;Visual analytics","Data visualization;Three-dimensional displays;Computer security;Collaboration;Data analysis;Virtual environments","data analysis;data visualisation;groupware;security of data;virtual reality","data visualization systems;threat detection performances;3DCVE;cyber situational awareness;3D collaborative virtual environments;cyber security data analysis","","2","","14","","7 Feb 2019","","","IEEE","IEEE Conferences"
"Applicability of Immersive Analytics in Mixed Reality: Usability Study","B. Hoppenstedt; T. Probst; M. Reichert; W. Schlee; K. Kammerer; M. Spiliopoulou; J. Schobel; M. Winter; A. Felnhofer; O. D. Kothgassner; R. Pryss","Institute of Databases and Information Systems, University of Ulm, Ulm, Germany; Department for Psychotherapy and Biopsychosocial Health, Danube University Krems, Krems an der Donau, Austria; Institute of Databases and Information Systems, University of Ulm, Ulm, Germany; Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany; Institute of Databases and Information Systems, University of Ulm, Ulm, Germany; Faculty of Computer Science, Otto von Guericke University Magdeburg, Magdeburg, Germany; Institute of Databases and Information Systems, University of Ulm, Ulm, Germany; Institute of Databases and Information Systems, University of Ulm, Ulm, Germany; Department of Pediatrics and Adolescent Medicine, Medical University of Vienna, Vienna, Austria; Department of Child and Adolescent Psychiatry, Medical University of Vienna, Vienna, Austria; Institute of Databases and Information Systems, University of Ulm, Ulm, Germany","IEEE Access","12 Jun 2019","2019","7","","71921","71932","Nowadays, visual analytics is mainly performed by programming approaches and viewing the results on a desktop monitor. However, due to the capabilities of smart glasses, new user interactions and representation possibilities become possible. This refers especially to 3D visualizations in the medical field, as well as, the industry domain, as valuable depth information can be related to the complex real-world structures and related data, which is also denoted as immersive analytics. However, the applicability of immersive analytics and its drawbacks, especially in the context of mixed reality, are quite unexplored. In order to validate the feasibility of immersive analytics for the aforementioned purposes, we designed and conducted a usability study with 60 participants. More specifically, we evaluated the effects of spatial sounds, performance changes from one analytics task to another, expert status, and compared an immersive analytics approach (i.e., a mixed-reality application) with a desktop-based solution. Participants had to solve several data analytics tasks (outlier's detection and cluster recognition) with the developed mixed-reality application. Thereby, the performance measures regarding time, errors, and movement patterns were evaluated. The separation into groups (low performer vs. high performer) was performed using a mental rotation pretest. When solving analytic tasks in mixed reality, participants changed their movement patterns in the mixed reality setting significantly, while the use of spatial sounds reduced the handling time significantly, but did not affect the movement patterns. Furthermore, the usage of mixed reality for cluster recognition is significantly faster than the desktop-based solution (i.e., a 2D approach). Moreover, the results obtained with self-developed questionnaires indicate 1) that wearing smart glasses are perceived as a potential stressor and 2) that the utilization of sounds is perceived very differently by the participants. Altogether, industry and researchers should consider immersive analytics as a suitable alternative compared to the traditional approaches.","2169-3536","","10.1109/ACCESS.2019.2919162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723024","Immersive analytics;mixed reality;spatial sounds;visual analytics;hololens","Task analysis;Virtual reality;Three-dimensional displays;Atmospheric measurements;Particle measurements;Two dimensional displays;Smart glasses","computer displays;data analysis;data visualisation;pattern clustering;user interfaces;virtual reality","visual analytics;desktop-based solution;immersive analytics;mixed-reality;data analytics;desktop monitor;user interactions;3D visualizations;medical field;cluster recognition","","1","","38","","27 May 2019","","","IEEE","IEEE Journals"
"Combined Wireless Hardware and Real-Time Computer Vision Interface for Tangible Mixed Reality","A. D. Cheok; Y. Qiu; K. Xu; K. G. Kumar","Nat. Univ. of Singapore, Singapore; Nat. Univ. of Singapore, Singapore; Nat. Univ. of Singapore, Singapore; NA","IEEE Transactions on Industrial Electronics","16 Jul 2007","2007","54","4","2174","2189","Recent advances in technology enable portable, even wearable, computers to be equipped with wireless interfaces, which allows data transactions even while mobile. Combined with mixed reality (MR), mobile computing exploits a promising field for wearable computers. Natural and nonobtrusive means of interaction call for new devices, which should be simple to use, and provide effective tracking methods in unprepared environments for MR. In this paper, a new interaction hardware tilt pad designed using accelerometers and wireless devices is introduced. This is combined with two new natural feature-tracking algorithms based on geometrical image constraints. The first is based on epipolar geometry and provides a general description of the constraints on image flow between two static scenes. The second is based on the calculation of a homography relationship between the current frame and a stored representation of the scene. We assessed these algorithms compared with the current optical flow calculation algorithm across a number of criteria including robustness, speed, and accuracy. Finally, we demonstrated an MR computer game application combining the new tracking method and the hardware tilt pad. Videos of the tilt pad and application of tilt pad Pacman game can be found at the web site: http://www.mixedrealitylab.org.","1557-9948","","10.1109/TIE.2007.895134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273657","Games;machine vision;tactile sensors;virtual reality (VR)","Hardware;Computer vision;Virtual reality;Wearable computers;Mobile computing;Layout;Games;Application software;Computer interfaces;Portable computers","augmented reality;computer games;computer vision;graphical user interfaces;mobile computing","real-time computer vision interface;tangible mixed reality;wireless interfaces;mobile data transactions;mobile computing;wearable computers;accelerometers;wireless devices;natural feature-tracking algorithms;geometrical image constraints;epipolar geometry;homography relationship;optical flow calculation algorithm;computer game;tracking method;hardware tilt pad;Pacman game","","12","","15","","16 Jul 2007","","","IEEE","IEEE Journals"
"A tutorial GENETIK simulation and scheduling","K. Concannon; P. Becker","Insight Int., Mississauga, Ont., Canada; Insight Int., Mississauga, Ont., Canada","1990 Winter Simulation Conference Proceedings","6 Aug 2002","1990","","","140","145","GENETIK is a powerful general-purpose visual interactive modeling system that includes both simulation and scheduling modules. The authors present a detailed description of the GENETIK system and its features, with examples of its use as a DSS generator and a simulation modeling tool, as well as its growing application in the role of planning and scheduling. It is noted that GENETIK provides a visually interactive means of giving analysts a fast, easy and powerful tool for developing customized simulation and scheduling systems. Data, pictures, logic and interactions are the four key requirements in building a GENETIK model. By using simple facilities to handle these four requirements, GENETIK makes it possible to build a model of any type of problem, however complex it is. Answers from a GENETIK simulation or scheduling model make it possible to re-assess what the real problem is or prompt fresh ideas for solving it.<>","","0-911801-72-3","10.1109/WSC.1990.129503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129503","","Tutorial;Job shop scheduling;Analytical models;Discrete event simulation;Animation;Standards organizations;Mice;Power system modeling;Graphics;Displays","decision support systems;digital simulation;interactive systems;scheduling;visual programming","general-purpose visual interactive modeling system;scheduling modules;GENETIK system;DSS generator;simulation modeling tool;planning;customized simulation;scheduling systems","","","","","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Position Control of an assistive robot via graphical interface interaction","E. Vázquez-Santacruz; C. Morales-Cruz; M. Gamboa-Zúñiga","Centro de Investigación y de Estudios Avanzados del I.P.N., Coordinación General de Tecnologías de la Información y las Comunicaciones, Av. I.P.N. No. 2508, Col. San Pedro Zacatenco 07360, México, D.F., México; Centro de Investigación y de Estudios Avanzados del I.P.N., Coordinación General de Tecnologías de la Información y las Comunicaciones, Av. I.P.N. No. 2508, Col. San Pedro Zacatenco 07360, México, D.F., México; Centro de Investigación y de Estudios Avanzados del I.P.N., Coordinación General de Tecnologías de la Información y las Comunicaciones, Av. I.P.N. No. 2508, Col. San Pedro Zacatenco 07360, México, D.F., México","2015 12th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)","17 Dec 2015","2015","","","1","6","Nowadays the technological community is concerned about the human-machine interface to increase the real device capabilities to achieve or overcome the user expectancy, as well as to increment the accurate interpretation of humans command by the machine. This study aimed at developing more understandable, efficient and reliable ways to communicate with devices. In this work, PC-based touch-screen acts as the principal Human-Machine Interface to interact with an assistive robot. This work is presented using a cognitive approach to improving understanding related to the way of thinking of the final user and give a tool to the designer to interpret this information.","","978-1-4673-7839-0","10.1109/ICEEE.2015.7357999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357999","Assistive robot;GUI;Cognitive approach;lift mechanism","Graphical user interfaces;Electrical engineering;Cities and towns;Actuators;Robot kinematics;Man machine systems","graphical user interfaces;human computer interaction;human-robot interaction;position control","position control;assistive robot;graphical interface interaction;technological community;user expectancy;humans command;PC-based touch-screen;principal human-machine interface;cognitive approach","","","","9","","17 Dec 2015","","","IEEE","IEEE Conferences"
"Spatial dialog for space system autonomy","S. Green; S. Richardson; V. Slavin; R. Stiles","Lockheed Martin Space Systems Co. Advanced Technology Center, O/ABCS B/153, 1111 Lockheed Martin Way, Sunnyvale, CA 94089, USA; Lockheed Martin Space Systems Co. Advanced Technology Center, O/ABCS B/153, 1111 Lockheed Martin Way, Sunnyvale, CA 94089, USA; Lockheed Martin Space Systems Co. Advanced Technology Center, O/ABCS B/153, 1111 Lockheed Martin Way, Sunnyvale, CA 94089, USA; Lockheed Martin Space Systems Co. Advanced Technology Center, O/ABCS B/153, 1111 Lockheed Martin Way, Sunnyvale, CA 94089, USA","2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)","30 Jul 2012","2007","","","341","348","Future space operations will increasingly demand cooperation between humans and autonomous space systems such as robots, observer satellites, and distributed components. Human team members use a combination of gestures, gaze, posture, deictic references and speech to communicate effectively. When a human team collaborates on a given task, they discuss the task, create a plan and then review this plan prior to execution to ensure success. This is exactly the process we envision for effective human-autonomous agent collaborative teamwork. Visual spatial information is a common reference for increased shared situation awareness between humans and autonomous systems. We use a spatial dialog approach to enable multiple humans to naturally and effectively communicate and work with multiple autonomous space systems. With this in mind, we have created a prototype spatial dialog system to support teamwork between humans and autonomous robotic agents. We combine augmented reality gesture interaction and visualization with speech to realize a spatial dialog capability. This paper describes our prototype, general approach, and related issues for team-oriented spatial dialog interaction with autonomous space systems.","2167-2148","978-1-59593-617-2","10.1145/1228716.1228762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6251710","Human-Robotic Interaction;Virtual Tele-presence;Augmented Reality;Multi-modal Interfaces;Spoken Dialog System;Adjustable Autonomy;Collaboration","Visualization;Abstracts;Robots;Monitoring;Humans","augmented reality;data visualisation;groupware;human-robot interaction;team working","spatial dialog approach;space system autonomy;space operations;human team members;observer satellites;distributed components;human-autonomous agent collaborative teamwork;visual spatial information;situation awareness;human-autonomous robotic agent teamwork;augmented reality gesture interaction;visualization;spatial dialog capability;team-oriented spatial dialog interaction","","","","26","","30 Jul 2012","","","IEEE","IEEE Conferences"
"A flexible model for real-time crowd simulation","J. Rossmann; N. Hempe; P. Tietjen","Institute of Man-Machine Interaction, RWTH Aachen University, Aachen, Germany; Institute of Man-Machine Interaction, RWTH Aachen University, Aachen, Germany; Institute of Man-Machine Interaction, RWTH Aachen University, Aachen, Germany","2009 IEEE International Conference on Systems, Man and Cybernetics","4 Dec 2009","2009","","","2085","2090","This paper will introduce the generic concept of a multi-agent based crowd simulation prototype. The prototype consists of many distinct components that contribute to the system as a whole. Based on the criteria for a human agent model, the agent's module-based, layered architecture is introduced. Subsequently, a closer examination of each module reveals a detailed insight into the agent's architecture. The examined modules are: The agent's finite state machine, its steering behavior, its locomotion model, the path planner and the messaging capability. Finally, with respect to the simulation's general architecture, the optimization techniques to further improve the simulation's runtime performance are discussed. This is especially important, since the simulation should run at interactive frame rates and allow the simulation of as many agents as possible for scalability reasons. The investigated optimization techniques are multi-threading and cell space partitioning. The simulation is implemented in our 3D simulation system VEROSIM, which also handles 3D graphics to visualize the results in real-time.","1062-922X","978-1-4244-2793-2","10.1109/ICSMC.2009.5346308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346308","Multi-agent simulation;interactive simulation;multi-threading;software design","Humans;Virtual prototyping;Man machine systems;Visualization;Application software;Management training;Real time systems;Automata;Runtime;Decision making","data visualisation;digital simulation;multi-agent systems;multi-threading;optimisation","multiagent based crowd simulation prototype;agent architecture;agent finite state machine;locomotion model;path planner;optimization techniques;interactive frame rates;multithreading;cell space partitioning;3D simulation system VEROSIM;3D graphics;data visualization;software design","","","","17","","4 Dec 2009","","","IEEE","IEEE Conferences"
"AliciaVR: Exploration of scientific articles in an immersive virtual environment with natural user interfaces","R. Linares; J. Herrera; L. Alfaro","UNSA, National University of Saint Agustin, Arequipa, Peru; UNSA, National University of Saint Agustin, Arequipa, Peru; UNSA, National University of Saint Agustin, Arequipa, Peru","2016 IEEE Ecuador Technical Chapters Meeting (ETCM)","24 Nov 2016","2016","","","1","6","Researchers share their results through articles that are published and indexed in scientific databases, which are useful to consult in future research. Generally, the user interface of these databases are traditional, restrict user interaction and limit their field of view. In this paper a new interface model is proposed, based on virtual reality and natural language processing, which together provide a excellent user experience and better use of human capabilities, such as intuition and spatial cognition. In this research they are used the devices Oculus Rif and Leap Motion hardware. The case study is the exploration of scientific articles using Alicia, which is the scientific database of Peru. The motivation for this research is to contribute with better tools to optimize the tasks of literature review and facilitate the work of researchers.","","978-1-5090-1629-7","10.1109/ETCM.2016.7750829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7750829","Virtual reality;human interaction computer;natural user interfaces;information retrieval;leap motion;oculus rift","Virtual environments;User interfaces;Three-dimensional displays;Two dimensional displays;Databases;Software","database management systems;natural language interfaces;natural language processing;scientific information systems;virtual reality","AliciaVR;scientific article exploration;immersive virtual environment;natural user interfaces;virtual reality;natural language processing;Oculus Rif;Leap Motion hardware;scientific database;Peru;literature review","","1","","19","","24 Nov 2016","","","IEEE","IEEE Conferences"
"Simulation based experiments using EDNAS: The Event-Driven Network Architecture Simulator","S. Salmon; H. ElAarag","Stetson University, 421 N Woodland Blvd, Deland, FL 32723, USA; Stetson University, 421 N Woodland Blvd, Deland, FL 32723, USA","Proceedings of the 2011 Winter Simulation Conference (WSC)","9 Feb 2012","2011","","","3261","3272","Computer networks serve billions of users all over the world. Research in this field could be performed by building test beds in labs. However, this approach is very expensive, inflexible and hard to reconfigure. It is also difficult and sometimes impossible to replicate some scenarios with test beds. Network simulation on the other hand overcomes all these difficulties. Network simulation can be easily used to study and debug network protocols, understand their interaction and predict how network changes will affect performance. In this paper, we introduce the Event-Driven Network Architecture Simulator, EDNAS. EDNAS is a general-purpose, portable and scalable simulator. We discuss its architecture and implementation. We demonstrate and analyze the results EDNAS provides using various performance measures that are hard to obtain using analytical models. This makes EDNAS very appealing in the study of communication networks.","1558-4305","978-1-4577-2109-0","10.1109/WSC.2011.6148023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148023","","Protocols;Computational modeling;NIST;IP networks;Computer architecture;Kernel;Delay","computer networks;digital simulation;protocols;telecommunication computing","simulation based experiments;event-driven network architecture simulator;computer network;network simulation;network protocol debugging;general-purpose simulator;portable simulator;scalable simulator;communication network","","4","","24","","9 Feb 2012","","","IEEE","IEEE Conferences"
"A gesture based interaction technique for a planning tool for construction and design","M. Rauterberg; M. Bichsel; M. Meier; M. Fjeld","Inst. of Hygiene & Appl. Physiol., Swiss Federal Inst. of Technol., Zurich, Switzerland; NA; NA; NA","Proceedings 6th IEEE International Workshop on Robot and Human Communication. RO-MAN'97 SENDAI","6 Aug 2002","1997","","","212","217","In this article we present a method that goes beyond the established approaches of human-computer interaction. We first bring a serious critique of traditional interface types, showing their major drawbacks and limitations. Promising alternatives are offered by virtual (or immersive) reality (VR) and by augmented reality (AR). The AR design strategy enables humans to behave in a nearly natural way. Natural interaction means human actions in the real world with other humans and/or with real world objects. Guided by the basic constraints of natural interaction, we derive a set of recommendations for the next generation of user interfaces: the natural user interface (NUI). Our approach to NUIs is discussed in the form of a general framework followed by a prototype. The prototype tool builds on video-based interaction and supports construction and plant layout. A first empirical evaluation is briefly presented.","","0-7803-4076-0","10.1109/ROMAN.1997.646984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646984","","Virtual reality;Humans;User interfaces;Prototypes;Mice;Computer displays;Command languages;Feedback;Physiology;Design methodology","user interfaces;man-machine systems;virtual reality;interactive systems","gesture based interaction;human-computer interaction;virtual reality;augmented reality;natural user interface;video-based interaction","","10","3","20","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multiagent interactions in distributed virtual worlds","F. Wang; S. J. Turner; L. Wang","Parallel & Distributed Comput. Centre, Nanyang Technol. Univ., Singapore; Parallel & Distributed Comput. Centre, Nanyang Technol. Univ., Singapore; Parallel & Distributed Comput. Centre, Nanyang Technol. Univ., Singapore","2004 IEEE Region 10 Conference TENCON 2004.","23 May 2005","2004","B","","345","348 Vol. 2","Multiagent systems provide a valuable tool for handling increasing software complexity and supporting rapid and accurate problem solving. This paper presents a general architecture in which autonomous agents are embedded in a distributed virtual environment and are enabled to communicate with each other. We focus on the interaction issues, using cooperation and coordination patterns. Different cases of the agents' teamwork to improve their efficiency are investigated and compared. A prototype system has been implemented, with message templates and algorithms identified. Finally, some experimental results are given.","","0-7803-8560-8","10.1109/TENCON.2004.1414602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1414602","","Problem-solving;Computational modeling;Distributed computing;Multiagent systems;Software tools;Autonomous agents;Virtual environment;Prototypes;Application software;Concurrent computing","multi-agent systems;mobile agents;software prototyping;virtual reality;software architecture","multiagent interactions;distributed virtual world;software complexity;autonomous agents;agent team working;prototype system","","1","","10","","23 May 2005","","","IEEE","IEEE Conferences"
"General proxy-based haptics for volume visualization","K. Lundin; B. Gudmundsson; A. Ynnerman","Norrkoping Visualization & Interaction Studio, Linkoping Univ., Sweden; Norrkoping Visualization & Interaction Studio, Linkoping Univ., Sweden; Norrkoping Visualization & Interaction Studio, Linkoping Univ., Sweden","First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference","4 Apr 2005","2005","","","557","560","We present a general, proxy-based approach to volume haptics founded upon the notion of 'haptic primitives'. Haptic modes each representing a different aspect of volumetric data, are created by defining sets of haptic primitives which reflect the local properties of the data. The proxy position for every time-frame in the haptic loop is found by balancing the force feedback in the haptic instrument with the force from the haptic primitives involved. The presented general framework allows for rapid development of haptic feedback modes for volumetric data. The approach also allows, in contrast to previous work on proxy-based volume haptics, combination of non-orthogonal constraints and thus allows free combination of various modes in haptic exploration of multivariate data. We demonstrate the effectiveness of our approach through the implementation of five different haptic modes.","","0-7695-2310-2","10.1109/WHC.2005.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407007","","Haptic interfaces;Force feedback;Probes;Data visualization;Instruments;Surface treatment;Impedance;Prototypes;Viscosity;Equations","haptic interfaces;data visualisation;force feedback","proxy-based haptics;volume visualization;volumetric data;force feedback;haptic instrument;haptic feedback modes;nonorthogonal constraint","","17","","8","","4 Apr 2005","","","IEEE","IEEE Conferences"
"Evaluation method of human-vehicle tactile interaction experience based on EEG","S. Han; Y. Kong; Z. Lei; Q. Zhao","Shanghai university,School of Mechatronic Engineering and Automation; Shanghai university,School of Mechatronic Engineering and Automation; Shanghai university,School of Mechatronic Engineering and Automation; Shanghai university,School of Mechatronic Engineering and Automation","2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","23 Sep 2020","2020","2","","75","78","Aiming at the problem of human-vehicle tactile interaction experience evaluation, this paper proposed a method based on the combination of EEG signals and subjective scoring evaluation. Generally, the evaluation method of human-machine interaction experience is based on the user's subjective evaluation. This kind of method causes the inaccuracy of the evaluation because different people have different judgments on feelings. This paper built a simulated driving environment, collects the EEG signals of the subjects in the interaction process of a car touchpad in different modes, and preprocesses, extracts and analyzes the signals. Then combining the user's subjective score evaluation to achieve an effective combination of quantitative analysis and qualitative analysis. The experimental results show that different modes of the tactile interaction system will affect users to generate EEG signals with different characteristics. The method of subjective and objective fusion can make the evaluation of human-vehicle tactile interaction experience more comprehensive.","","978-1-7281-6517-2","10.1109/IHMSC49165.2020.10095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204278","experience assessment;EEG signals;data analysis;tactile interaction","Electroencephalography;Vibrations;Man-machine systems;Automobiles;Frequency-domain analysis;Data analysis;Brain modeling","electroencephalography;haptic interfaces;human computer interaction;man-machine systems;medical signal processing","tactile interaction system;interaction process;human-machine interaction experience;subjective scoring evaluation;EEG signals;human-vehicle tactile interaction experience evaluation","","","","10","","23 Sep 2020","","","IEEE","IEEE Conferences"
"An approach to LEM modeling: construction, collision detection and dynamic simulation","K. Sundaraj; C. Laugier; I. F. Costa","INRIA, Montbonnot Saint Martin, France; NA; NA","Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)","6 Aug 2002","2001","4","","2196","2201 vol.4","This paper presents an approach to apply the long element method (LEM), a new method for physically based simulation of deformable objects, to a general polygonal mesh. The LEM is suitable for real time simulation and virtual environment interaction. The approach implements a static solution for elastic global deformation of objects filled with fluids based on Pascal's Principle and the volume conservation. The volumes are discretised into long elements, defining meshes several order of magnitudes smaller than tetrahedral or cubic meshes. We show how these volumes are constructed when the method is applied to a general mesh. We also propose a method for collision detection when two such objects interact. Finally, we show how we can perform dynamic simulation when the physics of the objects are modeled using bulk variables: pressure, density, volume and stress. This approach is particularly interesting for real time simulation of soft tissue undergoing small deformations.","","0-7803-6612-3","10.1109/IROS.2001.976396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=976396","","Deformable models;Computational modeling;Biological tissues;Object detection;Virtual reality;Biological system modeling;Virtual environment;Physics;Stress;Human anatomy","digital simulation;solid modelling;deformation;mesh generation;real-time systems","long element method;deformable objects;polygonal mesh;elastic global deformation;Pascal principle;collision detection;dynamic simulation;soft tissue;volume conservation;real time simulation","","11","1","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"The temporal limits of agency for reaching movements in augmented virtuality","G. Bernal; P. Maes; O. A. Kannape","MIT Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Media Lab, Massachusetts Institute of Technology, Cambridge, USA; School of Psychology, University of Central Lancashire, Preston, UK","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","002896","002899","The sense of agency (SoA) describes the feeling of being the author and in control of one's movements. It is closely linked to automated aspects of sensorimotor control and understood to depend on one's ability to monitor the details of one's movements. As such SoA has been argued to be a critical component of self-awareness in general and contribute to presence in virtual reality environments in particular. A common approach to investigating SoA is to ask participants to perform goal-directed movements and introducing spatial or temporal visuomotor mismatches in the feedback. Feedback movements are traditionally either switched with someone else's movements using a 2D video-feed or modified by providing abstracted feedback about one's actions on a computer screen. The aim of the current study was to quantify conscious monitoring and the SoA for ecologically valid, three dimensional feedback of the participants' actual limb and movements. This was achieved by displaying an Infra-Red (IR) feed of the participants' upper limbs in an augmented virtuality environment (AVE) using a head-mounted display (HMD). Movements could be fed back in real-time (46ms system delay) or with an experimental delay of up to 570ms. As hypothesized, participant's SoA decreased with increasing temporal visuomotor mismatches (p<;.001), replicating previous findings and extending them to AVEs. In-line with this literature, we report temporal limits of 222±60ms (50% psychometric threshold) in N=28 participants. Our results demonstrate the validity of the experimental platform by replicating studies in SoA both qualitatively and quantitatively. We discuss our findings in relation to the use of virtual and mixed reality in research and implications for neurorehabilitation therapies.","","978-1-5090-1897-0","10.1109/SMC.2016.7844679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844679","augmented virtuality;sense of agency;presence;sensory integration;movement feedback;motion capture","Delays;Visualization;Feeds;Real-time systems;Training;Conferences;Cybernetics","augmented reality;helmet mounted displays;human computer interaction","temporal agency limits;SoA;sense of agency;infrared feed;IR feed;augmented virtuality environment;head-mounted display;temporal visuomotor mismatches;mixed reality;HMD;AVE","","1","","19","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Go-in-Circles: A Directed Walking Algorithm for VR Navigation Based on One-Step-Ahead Strategy","M. Qi; G. Liu; J. Cui","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","IEEE Access","17 Mar 2020","2020","8","","49028","49037","In virtual reality (VR), how to map large-scale virtual spaces to small tracking area is a challenging problem. In this paper, we propose a novel steering algorithm for the redirected walking (RDW) technique to steer users away from the boundaries of the tracking area while reducing perceptible discrepancies. Inspired by the motion illusion that when a person is blindfolded, she goes in circles while thinking she is walking in a straight and infinitely long path, we map the path composed of straight-line segments in virtual space to a path composed of arcs in real space, and use one-step-ahead strategy and a buffer zone to reduce the number of user-boundary collisions. To verify the effectiveness of our algorithm, we conduct two experiments based on simulated in large-scale virtual spaces with a relatively small tracking area. The results indicate that our algorithm can effectively reduce collisions and perceptual distortion, and show the potential to address the challenge of simultaneously redirecting more than one user.","2169-3536","","10.1109/ACCESS.2020.2977363","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019652","Virtual Reality;Redirected Walking;Navigation","Navigation;Skeleton;Legged locomotion;Space exploration;Space vehicles;Tracking;Distortion","human computer interaction;virtual reality","directed walking algorithm;VR navigation;one-step-ahead strategy;virtual reality;tracking area;novel steering algorithm;redirected walking technique;perceptible discrepancies;motion illusion;straight path;infinitely long path;straight-line segments;virtual space;user-boundary collisions;go-in-circles;large-scale virtual spaces","","","","30","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"The use of computer simulation for marine terminal planning","B. C. Carpenter; T. Ward","Autom. Associates Inc., Solana Beach, CA, USA; NA","1990 Winter Simulation Conference Proceedings","6 Aug 2002","1990","","","802","804","A flexible over-model that represents the overall operations of a marine terminal and that also includes the effect of traffic interactions has been developed. This model has been tested and implemented for the proposed remodeling of several marine terminals for a major shipping company. A general finding from the simulation study indicates that rearranging the yard to achieve increased year productivity and efficient traffic routing does not necessarily reduce ship turn time. This is evident when the enhancement of dockside crane configurations to reduce crane cycle times fails to reduce ship turn time. These results suggest that the productivity of the dock and the year are linked, and the effect of increasing productivity of any isolated activity must also be examined on overall terminal operations.<>","","0-911801-72-3","10.1109/WSC.1990.129617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129617","","Computer simulation;Containers;Computational modeling;Costs;Traffic control;Microcomputers;Automation;Marine technology;Testing;Cranes","administrative data processing;digital simulation;transportation","computer simulation;marine terminal planning;marine terminal;traffic interactions;shipping company;year productivity;dockside crane configurations;crane cycle times;ship turn time","","1","","","","6 Aug 2002","","","IEEE","IEEE Conferences"
"LightSense: enabling spatially aware handheld interaction devices","A. Olwal","Sch. of Comput. Sci. & Commun., R. Inst. of Technol., Stockholm","2006 IEEE/ACM International Symposium on Mixed and Augmented Reality","5 Feb 2007","2006","","","119","122","The vision of spatially aware handheld interaction devices has been hard to realize. The difficulties in solving the general tracking problem for small devices have been addressed by several research groups and examples of issues are performance, hardware availability and platform independency. We present LightSense, an approach that employs commercially available components to achieve robust tracking of cell phone LEDs, without any modifications to the device. Cell phones can thus be promoted to interaction and display devices in ubiquitous installations of systems such as the ones we present here. This could enable a new generation of spatially aware handheld interaction devices that would unobtrusively empower and assist us in our everyday tasks.","","1-4244-0650-1","10.1109/ISMAR.2006.297802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4079264","","Displays;Cellular phones;Pervasive computing;Light emitting diodes;Handheld computers;Cameras;Virtual reality;Augmented reality;Ubiquitous computing;Computer interfaces","cellular radio;human computer interaction;interactive devices;LED displays;tracking","spatially aware handheld interaction device;tracking problem;robust cell phone LED tracking;LightSense","","12","3","18","","5 Feb 2007","","","IEEE","IEEE Conferences"
"Perceptually augmented simulator design through decomposition","T. Edmunds; D. K. Pai","Rutgers University, USA; University of British Columbia, USA","World Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","3 Apr 2009","2009","","","505","510","We approach the problem of determining a general method for augmenting haptic simulators to amplify the perceptually salient aspects of the interaction that induce effective skill transfer. Using such a method, we seek to simplify the design of haptic simulators that can improve training effectiveness without requiring expensive improvements in the capability of the rendering hardware. We present a decomposition approach to the automated design of perceptually augmented simulations, and we describe a user-study of the training effectiveness of a search-task simulator designed using our approach vs. an un-augmented simulator. The results indicate that our decomposition approach allows existing psychophysical findings to be leveraged in the design of haptic simulators that effectively impart skill by targeting perceptually significant aspects of the interaction.","","978-1-4244-3858-7","10.1109/WHC.2009.4810890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810890","","Haptic interfaces;Engines;Shape;Analytical models;Surface texture;Hardware;Surgery;Jamming;Virtual environment;Teleoperators","augmented reality;haptic interfaces;rendering (computer graphics)","perceptually augmented simulator design;haptic simulators augmentation;rendering hardware;search-task simulator","","1","","14","","3 Apr 2009","","","IEEE","IEEE Conferences"
"Interactive physical robot guidance through advanced 3D dynamic simulation-based robot control — A new eRobotics approach","E. G. Kaigom; J. Roßmann","Institute for Man-Machine Interaction RWTH - Aachen University, Germany; Institute for Man-Machine Interaction RWTH - Aachen University, Germany","2014 IEEE International Conference on Industrial Technology (ICIT)","11 Sep 2014","2014","","","676","681","Manual robot guidance is an intuitive and flexible approach to teach robots. It is particularly useful for manufacturers because of the low programming efforts. However, this method often requires compliance control that is generally not available in conventional position-controlled industrial robots. Addressing this issue from the perspective of simulation-driven engineering, we introduce in this contribution a novel approach for interactive physical robot guidance based upon simulated adaptable joint admittance control. The developed simulation-based controller is driven in real-time with real external joint torques estimated during interaction with a physical robot. Since the simulator closely replicates the dynamic behavior of the real robot, it enriches and enhances the robot guidance by providing unique and reliable information on the robot that is useful to the operator. The simulated compliant joint trajectories are fed back into the real robot controller to enable full-body guidance. By opening new and practical perspectives in assisted physical guidance of position-controlled robots, this approach hightlights the effectiveness of control-by-3D-simulation [1] as pursued by eRobotics [2] to address challenging issues in robotics and automation. Simulation and experimental case studies conducted on a physical 7 DoF KUKA LWR 4+ robot manipulator are provided to illustrate the performance of the approach.","","978-1-4799-3939-8","10.1109/ICIT.2014.6894912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894912","","Joints;Admittance;Solid modeling;Robot kinematics;Service robots;Trajectory","compliant mechanisms;electric admittance;human-robot interaction;industrial manipulators;intelligent robots;manipulator dynamics;position control;robot programming;torque;virtual reality","interactive physical robot guidance;3D dynamic simulation-based robot control;e-robotics approach;robot teaching;compliance control;simulation-driven engineering;simulated adaptable joint admittance control;real-external joint torque estimation;physical robot dynamic behavior;simulated compliant joint trajectories;assisted physical guidance;position-controlled robots;control-by-3D-simulation;physical 7-DoF KUKA LWR 4+ robot manipulator","","2","","15","","11 Sep 2014","","","IEEE","IEEE Conferences"
"Novel, Robust, and Efficient Guidewire Modeling for PCI Surgery Simulator Based on Heterogeneous and Integrated Chain-Mails","W. Wang; S. Li; H. Qin; A. Hao",NA; NA; NA; NA,"2015 14th International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics)","9 Apr 2016","2015","","","105","112","Despite the long R&D history of interactive minimally-invasive surgery and therapy simulations, the guide wire/catheter behavior modeling remains challenging in Percutaneous Coronary Intervention (PCI) surgery simulators. This is primarily due to the heterogeneous heart physiological structures and complex intravascular inter-dynamic procedures. To ameliorate, this paper advocates a novel, robust, and efficient guide wire/catheter modeling method based on heterogeneous and integrated chain-mails, that can afford medical practitioners and trainees the unique opportunity to experience the entire guide wire-dominant PCI procedures in virtual environments as our model aims to mimic what occurs in clinical settings. Our approach's originality is primarily founded upon this new method's unconditional stability, real time performance, flexibility, and high-fidelity realism for guide wire/catheter simulation. Considering the front end of the guide wire has different stiffness with its conjunctive slender body and the guide wire length is adaptive to the surrounding environment, we propose to model the spatially-varying six-degree of freedom behaviors by solely resorting to the generalized 3D chain-mails. Meanwhile, to effectively accommodate the motion constraints caused by the beating vessels and flowing blood, we integrate heterogeneous volumetric chain mails to streamline guide wire modeling and its interaction with surrounding substances. By dynamically coupling guide wire chain-mails with the surrounding media via virtual links, we are capable of efficiently simulating the collision-involved interdynamic behaviors of the guide wire. Finally, we showcase a PCI prototype simulator equipped with hap tic feedback for mimicing the guide wire intervention therapy, including pushing, pulling, and twisting operations, where the built-in high-fidelity, real-time efficiency, and stableness show great promise for its practical applications in clinical training and surgery rehearsal fields.","","978-1-4673-8020-1","10.1109/CADGRAPHICS.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450404","guidewire simulation;heterogeneous chain-mails;guidewire-vessel interaction;guidewire-blood interaction;PCI simulator;haptic feedback","Solid modeling;Adaptation models;Three-dimensional displays;Blood;Couplings;Computational modeling;Surgery","catheters;haptic interfaces;medical computing;surgery","guidewire modeling;integrated chain-mails;interactive minimally-invasive surgery;therapy simulations;guide wire/catheter behavior modeling method;percutaneous coronary intervention surgery simulators;PCI surgery simulators;heterogeneous heart physiological structures;complex intravascular inter-dynamic procedures;guide wire-dominant PCI procedures;virtual environments;unconditional stability;high-fidelity realism;guide wire/catheter simulation;conjunctive slender body;guide wire length;generalized 3D chain-mails;beating vessels;flowing blood;heterogeneous volumetric chain mails;streamline guide wire modeling;guide wire chain-mails;virtual links;collision-involved interdynamic behaviors;PCI prototype simulator;haptic feedback;guide wire intervention therapy;surgery rehearsal fields","","1","","23","","9 Apr 2016","","","IEEE","IEEE Conferences"
"iScreen: A Merged Screen of Local System with Remote Applications in a Mobile Cloud Environment","J. Li; Q. Song; W. Yu; C. Hu; J. Kang","Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China","2013 IEEE Seventh International Symposium on Service-Oriented System Engineering","10 Jun 2013","2013","","","509","517","With the convergence of cloud computing and mobile computing, mobile devices can access remote applications in a cloud environment. However, existing research work mostly focused on leveraging cloud capabilities to enhance mobile clients. Particularly, in order to access different cloud platforms and applications, specific version of clients such as Web portal, Remote Desktop, are generally required. The original display and interaction experience on the client local system are changed. This paper presents an approach named screen which keeps a consistent display and interaction experience between local and remote applications in a mobile cloud computing environment. screen consists of a three-factor merging framework including windows merging, meta-info merging and interaction merging for applications. We developed a prototype of screen for Windows applications to allow thin clients to seamlessly access remote cloud windows applications. Experimental studies show that screen can effectively merge local desktop with remote display, and mobile clients can achieve 20 frames per second when running a remote video playback application. The bandwidth usage of screen is reduced by about 10% compared to Ultra VNC. It performs better especially under high-motion scenarios.","","978-0-7695-4944-6","10.1109/SOSE.2013.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525569","cloud computing;remote screen;merged screen;tranmission protocol","Merging;Servers;Bandwidth;Mice;Mobile communication;Switches;Decoding","client-server systems;cloud computing;human computer interaction;mobile computing;user interfaces","iScreen;mobile device;cloud capability;mobile client;cloud platform;Web portal;Remote Desktop;interaction experience;client local system;mobile cloud computing environment;three-factor merging framework;window merging;metainfo merging;interaction merging;Windows application;thin client;remote cloud windows application access;local desktop;remote display;remote video playback application;bandwidth usage;Ultra VNC;high-motion scenario","","","4","21","","10 Jun 2013","","","IEEE","IEEE Conferences"
"Scrutinizing pseudo haptic feedback of surface roughness in virtual environments","G. Hannig; B. Deml","Institut für Arbeitswissenschaft, Universität der Bundeswehr München, Werner-Heisenberg-Weg 39, 85577 Neubiberg, Germany; Institut für Arbeitswissenschaft, Universität der Bundeswehr München, Werner-Heisenberg-Weg 39, 85577 Neubiberg, Germany","2008 IEEE Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems","8 Aug 2008","2008","","","1","4","Operators in virtual environments have to mostly rely on visual feedback when exploring virtual scenes and objects. If no additional haptic actuators are available then information about surface roughness will not be communicable and thus the degree of immersion will suffer. To compensate for this shortcome the usage of pseudo haptic feedback (Control/ Display ratio = visual cues encoding haptic object properties) is discussed and two psychophysical experiments are conducted. The first experiment deals with the perceptual scaling of different Control/ Display ratios while the second experiment examines the way, how human subjects assign suitable Control/ Display ratios to different real surfaces. It turns out that human operators can reliably perceive differences in Control/ Display ratios and a psychophysic function is established. The allocation of specific Control/ Display values to real materials does not give any generalizable results; only a global trend is observable.","1944-9410","978-1-4244-1927-2","10.1109/VECIMS.2008.4592742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4592742","Pseudo haptic feedback;Control-Display ratio;psychophysical scaling;surface roughness;multi modal feedback;virtual environment;human-machine interface;Blender","Haptic interfaces;Surface roughness;Rough surfaces;Materials;Virtual environment;Visualization;Tracking","haptic interfaces;human computer interaction;surface roughness;virtual reality","pseudohaptic feedback;surface roughness;virtual environment;visual feedback;virtual scenes;virtual objects;haptic actuators;psychophysical experiment","","1","","6","","8 Aug 2008","","","IEEE","IEEE Conferences"
"AR Petite Theater: Augmented reality storybook for supporting children's empathy behavior","Kyungwon Gil; Jimin Rhim; T. Ha; Young Yim Doh; W. Woo","Graduate School of Culture Technology, KAIST, S.Korea; Graduate School of Culture Technology, KAIST, S.Korea; Graduate School of Culture Technology, KAIST, S.Korea; Graduate School of Culture Technology, KAIST, S.Korea; Graduate School of Culture Technology, KAIST, S.Korea","2014 IEEE International Symposium on Mixed and Augmented Reality - Media, Art, Social Science, Humanities and Design (ISMAR-MASH'D)","27 Oct 2014","2014","","","13","20","In this paper, we present an AR Petite Theater, a story book that enables role-play using augmented reality (AR) technology. It provides an opportunity for children to learn the ability of empathy through interactive reading experience by thinking and speaking in accordance with the character's role of the story. In general, empathy is one of most important elements for children to make friends at school and to expand their social relations. In particular, it is crucial for early school-age children who have difficulties in getting along with friends due to their egocentric perspective. Through the experiment with 24 six-year-old children, we measured children's role-playing participation and perspective taking state. As a result, more empathic behaviors were revealed in the AR group. Children in the AR condition were more actively involved in role-playing and showed less unrelated perspectives than children in the non-AR condition. Therefore, we verified that AR Petite Theater had the potential of expanding children's ability to empathize with others.","2381-8360","978-1-4799-6887-9","10.1109/ISMAR-AMH.2014.6935433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935433","Augmented Reality;Empathy;Early School-age Children;Human-Computer Interaction;Role-playing","Abstracts;Magnetic resonance imaging;Atmospheric measurements;Particle measurements;Sun","augmented reality;behavioural sciences;multimedia computing;social aspects of automation","augmented reality storybook;children empathy behavior support;children role-playing;augmented reality technology;interactive reading experience;school-age children;egocentric perspective","","","","49","","27 Oct 2014","","","IEEE","IEEE Conferences"
"Virtual frog dissection for anatomical learning","T. Yamada; B. Tsagaan; H. Nakatani","Department of Computer Science, Shizuoka University, Hamamatsu 432-8011, Japan; Department of Computer Science, Shizuoka University, Hamamatsu 432-8011, Japan; Department of Computer Science, Shizuoka University, Hamamatsu 432-8011, Japan","The First Asian Conference on Pattern Recognition","12 Mar 2012","2011","","","135","138","Frog dissection practice used to be a student's laboratory work in Japanese elementary schools. It was an effective approach for learning animal's anatomy. However this practice has been stopped due to animal protection issues and implemental cost. In this study, we aimed to simulate interactive dissecting practice in the virtual space. The presented virtual frog dissection system consists of virtual reality software in conjunction with head-mounted stereoscopic crystal glasses and a haptic device that allows users to touch and manipulate virtual objects. The developed system was evaluated by examining learning ability of students. Participants were divided in four groups of different learning styles, took a written test on the same questions and their resultant scores were compared. Two findings were obtained from this evaluation exam. First, students who have experienced virtual frog dissection were more correct on the appearance related questions, while students who have used paper materials were more concrete on the function-related questions in general. This general tendency may be conducted in accordance with their habitual learning style. Second, in the aspect of convenience and preparing time of each learning style, students who have used conventional paper materials demonstrated more balanced learning than did students of virtual paper materials. These findings suggest that our virtual frog dissection system can show potential benefits in the structural anatomical learning of students; and in contrast, a habitual learning style is likely to produce better outcomes if the new technique provides only the same amount of information.","0730-6512","978-1-4577-0121-4","10.1109/ACPR.2011.6166690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166690","virtual frog dissection;virtual reality;3D vision;haptic device;multi-modal interaction","Materials;Three dimensional displays;Solid modeling;Haptic interfaces;Biological systems;Virtual reality;Educational institutions","biological organs;biology computing;computer aided instruction;haptic interfaces;stereo image processing;student experiments;virtual reality","student laboratory work;Japanese elementary school;animal anatomy learning;animal protection issues;interactive dissecting practice;virtual space;virtual frog dissection system;virtual reality software;head-mounted stereoscopic crystal glass;haptic device;virtual object touching;virtual object manipulation;student learning ability;habitual learning style;virtual paper material;structural anatomical learning","","","","13","","12 Mar 2012","","","IEEE","IEEE Conferences"
"Multitouch Vibrotactile Feedback on a Tactile Screen by the Inverse Filter Technique: Vibration Amplitude and Spatial Resolution","L. Pantera; C. Hudin","CEA, LIST, Gif-sur-Yvette, France; CEA, LIST, Gif-sur-Yvette, France","IEEE Transactions on Haptics","28 Aug 2020","2020","13","3","493","503","Nowadays, tactile surfaces, such as smartphones, provide haptic feedback to signify that a task has been performed correctly or more generally to enrich the interaction. However, this haptic feedback induces vibrations in the surface that propagate to the whole surface, reverberate and attenuate, thus making multi-finger interaction, with different feedbacks, difficult. Recently, the Inverse Filter Method has been proposed to control the propagation of these vibrations, and thus enable to product localized multitouch on a glass surface. This way, a user can put several fingers on a tactile surface and yet feel stimuli independently on his/her different fingers. This article continues this work and demonstrates that a localized multitouch haptic feedback can be delivered in real time using a capacitive screen. To achieve this, this article presents the two necessary steps: a calibration step and an interpolation calculation in order to save calculation and learning time. Furthermore, the paper describes the performance of the device through a study on the behaviour of the screen subjected to the Inverse Filter Method, indicating the movement of the whole screen and the voltage requirement for any haptic feedback.","2329-4051","","10.1109/TOH.2020.2981307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040287","Haptic feedback;multitouch;localised vibration;interpolation;capacitive screen","Calibration;Actuators;Haptic interfaces;Vibrations;Interpolation;Sensitivity;Frequency-domain analysis","calibration;haptic interfaces;interpolation;touch sensitive screens;vibrations","multifinger interaction;glass surface;tactile surface;localized multitouch haptic feedback;capacitive screen;tactile screen;inverse filter technique;vibration amplitude;spatial resolution;multitouch vibrotactile feedback;interpolation calculation;calibration;voltage requirement","","1","","27","IEEE","18 Mar 2020","","","IEEE","IEEE Journals"
"Assessing Knowledge Retention of an Immersive Serious Game vs. a Traditional Education Method in Aviation Safety","L. Chittaro; F. Buttussi","Department of Mathematics and Computer Science, Human-Computer Interaction Lab, Italy; Department of Mathematics and Computer Science, Human-Computer Interaction Lab, Italy","IEEE Transactions on Visualization and Computer Graphics","23 Mar 2015","2015","21","4","529","538","Thanks to the increasing availability of consumer head-mounted displays, educational applications of immersive VR could now reach to the general public, especially if they include gaming elements (immersive serious games). Safety education of citizens could be a particularly promising domain for immersive serious games, because people tend not to pay attention to and benefit from current safety materials. In this paper, we propose an HMD-based immersive game for educating passengers about aviation safety that allows players to experience a serious aircraft emergency with the goal of surviving it. We compare the proposed approach to a traditional aviation safety education method (the safety card) used by airlines. Unlike most studies of VR for safety knowledge acquisition, we do not focus only on assessing learning immediately after the experience but we extend our attention to knowledge retention over a longer time span. This is a fundamental requirement, because people need to retain safety procedures in order to apply them when faced with danger. A knowledge test administered before, immediately after and one week after the experimental condition showed that the immersive serious game was superior to the safety card. Moreover, subjective as well as physiological measurements employed in the study showed that the immersive serious game was more engaging and fear-arousing than the safety card, a factor that can contribute to explain the obtained superior retention, as we discuss in the paper.","1941-0506","","10.1109/TVCG.2015.2391853","Federal Aviation Administration (FAA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014255","Immersive VR;serious games;user evaluation;knowledge retention;physiological measurements;aviation safety;Immersive VR;serious games;user evaluation;knowledge retention;physiological measurements;aviation safety","Games;Safety;Aircraft;Education;Materials;Avatars;Engines","avionics;computer aided instruction;computer games;helmet mounted displays;virtual reality","knowledge retention;consumer head-mounted displays;educational applications;immersive VR;gaming elements;immersive serious games;safety materials;HMD-based immersive game;aircraft emergency;aviation safety education method;airlines;safety knowledge acquisition;safety procedures;knowledge test;safety card;physiological measurements;superior retention","Accidents, Aviation;Aviation;Computer Graphics;Humans;User-Computer Interface;Video Games","99","","59","","19 Jan 2015","","","IEEE","IEEE Journals"
"Communication with robots: evidence from a Web-based experiment on human-computer interaction","P. Gieselmann; P. Stenneken","Interactive Syst. Lab., Univ. of Karlsruhe, Karlsruhe; NA","2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","118","121","Natural human-computer interaction is an increasingly relevant topic, which, however, is rather difficult to investigate in an empirical fashion. Whereas recent studies have addressed the problems of adding human communicative abilities to robots, not much is known about communicative strategies humans use with non-human partners. The present study aimed to explore factors determining the communicative success in human-robot interaction, using an on-screen conversation agent. A more specific question regarded possible influences of user strategies on different objective and subjective measures of communicative success. Results were rather heterogeneous reflecting the complexity of the interaction processes involved. Even though the manipulation of the user strategy resulted in qualitative and quantitative changes in communicative behaviour, it did not seem to affect the communicative success. Rather, the general attitude of the user towards robots appeared to have a relatively stronger impact on subjective success measures.","","1-4244-0872-5","10.1109/SLT.2006.326831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123376","","Human robot interaction;Humanoid robots;System testing;Particle measurements;Switches;Prototypes;Interactive systems;Intelligent robots;Man machine systems;User interfaces","human computer interaction;human factors;Internet;man-machine systems;robots;user interfaces","human-computer interaction;Web-based experiment;human-robot interaction;on-screen conversation agent;user attitude;user interface","","1","","9","","19 Mar 2007","","","IEEE","IEEE Conferences"
"Towards a social virtual reality learning environment in high fidelity","C. Zizza; A. Starr; D. Hudson; S. S. Nuguri; P. Calyam; Z. He",Grinnell College; Pomona College; Truman State University; University of Missouri; University of Missouri; University of Missouri,"2018 15th IEEE Annual Consumer Communications & Networking Conference (CCNC)","19 Mar 2018","2018","","","1","4","Virtual Learning Environments (VLEs) are spaces designed to educate students remotely via online platforms. Although traditional VLEs such as iSocial have shown promise in educating students, they offer limited immersion that diminishes learning effectiveness. This paper outlines a virtual reality learning environment (VRLE) over a high-speed network, which promotes educational effectiveness and efficiency via our creation of flexible content and infrastructure which meet established VLE standards with improved immersion. This paper further describes our implementation of multiple learning modules developed in High Fidelity, a “social VR” platform. Our experiment results show that the VR mode of content delivery better stimulates the generalization of lessons to the real world than non-VR lessons and provides improved immersion when compared to an equivalent desktop version.","2331-9860","978-1-5386-4790-5","10.1109/CCNC.2018.8319187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319187","Virtual Reality;Virtual Learning Environment;High Fidelity;Multi-user Network Application","Avatars;Games;Servers;Web pages;Education;Cloud computing","computer aided instruction;human computer interaction;virtual reality","multiple learning modules;social VR platform;social virtual reality learning environment;online platforms;high-speed network;educational effectiveness;VRLE;VLE standards","","4","","11","","19 Mar 2018","","","IEEE","IEEE Conferences"
"The Use of Intelligent Agents to Improve a Web Interface Interaction and its Usability","B. C. A. Petroni","Institute of Sciences, Computer and Technology, University Paulista, Jundiaí, São Paulo, Brazil. cristiano@petroni.com.br","2007 11th International Conference on Computer Supported Cooperative Work in Design","30 Jul 2007","2007","","","372","376","This article presents a computational system which uses concepts of an artificial intelligence technique known as intelligent agents aggregated to human-computer interaction concepts. Its main objective is to help the improvement of a determined web interface interaction and usability -applied to the teaching at distance. The architecture was developed over a LGPL (Lesser General Public License) platform that includes intelligent agents, two bases of knowledge with information obtained through heuristic evaluations and, afterwards transformed in rules. In order to verify the co-operation of the computational system two types of evaluations were performed: heuristic, to verify the help referring to the usability improvement and cognitive trajectory to verify the user's learning to manipulate the interface. The result was the comparison of time and the verifications of some questions in the web interface screens that show the user's performance with or without the utilization of the developed computational system.","","1-4244-0962-4","10.1109/CSCWD.2007.4281464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281464","Artificial Intelligence;Intelligent Agents;Human-Computer Interaction;Heuristic Evaluation;Cognitive Trajectory;Interaction;Application","Intelligent agent;Usability;Computer interfaces;Artificial intelligence;Education;Collaborative work;Computer architecture;Licenses;Performance evaluation;Learning","computer aided instruction;distance learning;graphical user interfaces;human computer interaction;Internet;multi-agent systems;teaching","intelligent agent;Web interface interaction;Web interface usability;artificial intelligence technique;human-computer interaction;distance teaching;lesser general public license platform;heuristic evaluation;GUI","","","","16","","30 Jul 2007","","","IEEE","IEEE Conferences"
"Integrating Multi-Modal Interfaces to Command UAVs [Video Abstract]","V. Monajjemi; S. Pourmehr; S. A. Sadat; F. Zhan; J. Wawerla; G. Mori; R. Vaughan","School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada","2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","22 Nov 2018","2014","","","106","106","We present an integrated human-robot interaction system that enables a user to select and command a team of two Unmanned Aerial Vehicles (UAV) using voice, touch, face engagement and hand gestures. This system integrates multiple human [multi]-robot interaction interfaces as well as a navigation and mapping algorithm in a coherent semirealistic scenario. The task of the UAVs is to explore and map a simulated Mars environment. To initiate a mission, the user needs to select a robot. To do this, We used the“Touch-To-Name”selection and naming interface [3]. In this method, the user first announces the desired number of robot(s) (e.g “You” or “You Two”), then gently moves intended robot(s) iteratively. Robots compare their accelerometer readings over Wi-Fi to agree on which one is selected. Once selected, the user names the selected robot using verbal commands (e.g “You are Green”). These names are then used to command the robots (e.g., “Green Takeoff”) [4]. Here, we use this interface with maximum group size set to one. After taking off and while hovering, robot looks for human faces in its camera feed. When user's face is detected, the robot continuously controls its altitude and heading direction to face the user. A hand wave gesture (left or right) assigns an exploration task to the robot in the indicated direction. We used the method described in [2] for face tracking and gesture recognition. While exploring, each robot performs vision-based Simultaneous Localization and Mapping (SLAM) using their onboard monocular camera [1]. We used the“Feature-rich path planning algorithm” introduced in [5] to robustly navigate a UAV while exploring an unknown environment. To terminate the mission, the user commands each robot to come back home (e.g “Green come back”). To come back, robots use the same algorithm to plan a feature-rich path to their takeoff position. Finally, The user asks robots to land. (e.g., “Green land”).The system provides two types of feedback to the user during interaction sessions and mission execution. Robots change the color and blinking pattern of their LED lights to inform the user about their state (e.g., “tracking user's face”, “exploring” or “being idle”). In addition, a text-to-speech (TTS) engine provides verbal feedback to the user whenever a robot's state changes. As an example, when the Green robot is asked by the user to comeback, it acknowledges by saying“Green is coming back”. The TTS is embedded within a general purpose web-based robot monitoring dashboard.We used Parrot AR-Drone 2.0 quadrocopter as UAV platform in our system. All described software components run off-board on two commodity Intel Core i7 notebooks (one dedicated to each robot). The computers are connected to UAVs via Wi-Fi connection.The video shows a complete run-through of a two robot exploration mission in which the HRI worked perfectly.","2167-2121","978-1-4503-2658-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8542483","","Robots;Face;Navigation;Human-robot interaction;Unmanned aerial vehicles;Task analysis;Wireless fidelity","autonomous aerial vehicles;cameras;gesture recognition;human-robot interaction;interactive systems;mobile robots;path planning;remotely operated vehicles;robot vision;SLAM (robots)","face engagement;hand gestures;navigation;mapping algorithm;UAV;naming interface;verbal commands;human faces;face tracking;gesture recognition;Green robot;general purpose web-based robot monitoring dashboard;robot exploration mission;multimodal interfaces;human-robot interaction system;unmanned aerial vehicles;Touch-To-Name selection;multiple human-robot interaction;text-to-speech engine","","","","5","","22 Nov 2018","","","IEEE","IEEE Conferences"
"From Snappy App to Screens in the Wild: Gamifying an Attention Deficit Hyperactivity Disorder Continuous Performance Test for Public Engagement and Awareness","M. P. Craven; Z. Young; L. Simons; H. Schnädelbach; A. Gillott","NIHR MindTech Healthcare Technol. Cooperative Inst. of Mental Health, Univ. of Nottingham Innovation Park, Nottingham, UK; NIHR MindTech Healthcare Technol. Cooperative Inst. of Mental Health, Univ. of Nottingham Innovation Park, Nottingham, UK; NIHR MindTech Healthcare Technol. Cooperative Inst. of Mental Health, Univ. of Nottingham Innovation Park, Nottingham, UK; Mixed Reality Lab., Univ. of Nottingham, Nottingham, UK; Nottinghamshire Healthcare NHS Trust Intellectual & Dev. Disabilities Service Specialist Services Directorate, Highbury Hosp., Nottingham, UK","2014 International Conference on Interactive Technologies and Games","18 Dec 2014","2014","","","36","43","Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental condition that is characterised by three core behaviours: inattention, hyperactivity and impulsivity. It is typically thought that around 3-5% of school aged children have ADHD, with lifetime persistence for the majority. A psychometric Continuous Performance Test (CPT) had recently been incorporated into an interactive smartphone application (App), Snappy App, to allow the measurement of the three ADHD symptom domains. Snappy App presents a sequence of letters of the alphabet in a pseudo-random manner with responses via the device's touch screen. Following a pilot test in the general population where the CPT showed sensitivity to ADHD-related symptoms (self-reported impulsive behaviour related to CPT measures), a new project was begun to convert the App into a game Attention Grabber based on the functionality of the test, focussing on the attention and impulsivity domains. The Screens in the Wild (SITW) platform is in the process of being employed for public engagement in awareness about ADHD through interactive technology. SITW has deployed a network of four public touch-screens in urban places. Each of the four nodes has a large (46 inch) display, a camera, a microphone and a speaker. The Snappy App web-app was translated for presentation on to the SITW platform. The browser-based App was redesigned, with the input of a commercial graphics design company, based on an initial proof-of-concept whereby the original App was reprogrammed to present sequences of graphical objects (fruit) and to introduce further engagement features including animations. A shortened video about Adult ADHD and a brief questionnaire were incorporated to form a stand-alone edutainment package. The earlier design and user testing of Snappy App is briefly described and details are then provided of the process of gamification to produce Attention Grabber. An evaluation process is described whereby awareness of ADHD and its related symptoms are to be probed. In general, finding out whether and how people engage with interactive screen technology can help in the design of future public engagement and health promotion activities. Ethical considerations are discussed, since public access to this kind of game could potentially raise health anxiety related to self-interpretation of game performance. This risk is balanced with the need to provide health information.","","978-1-4799-6795-7","10.1109/iTAG.2014.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990188","healthcare;displays;gamification;m-Health;pervasive and ubiquitous computing","Games;Educational institutions;Communities;Films;Medical services;Sociology;Statistics","computer games;ethical aspects;human computer interaction;medical computing;medical disorders;mobile computing;smart phones;touch sensitive screens","snappy app;gamification;attention deficit hyperactivity disorder;psychometric continuous performance test;public engagement;public awareness;neurodevelopmental condition;core behaviours;inattention;school aged children;lifetime persistence;interactive smartphone application;ADHD symptom domains measurement;device touch screen;self-reported impulsive behaviour;CPT measures;Attention Grabber game;test functionality;attention domains;impulsivity domains;Screens in the Wild;SITW platform;public touch-screens;camera;microphone;speaker;browser-based App;commercial graphics design company;graphical objects;adult ADHD;user testing;interactive screen technology;health promotion activities;ethical considerations;public access;health anxiety;self-interpretation;game performance;health information","","3","","40","","18 Dec 2014","","","IEEE","IEEE Conferences"
"Enhancing the digital heritage experience from field to museum: User-centered system design of an augmented reality tablet application for cultural heritage","J. M. Darling; D. J. Vanoni; T. E. Levy; F. Kuester","Center of Interdisciplinary Science for Art, Architecture, and Archaeology (CISA3), Qualcomm Institute, Calit2, University of California, San Diego La Jolla, California, USA; Center of Interdisciplinary Science for Art, Architecture, and Archaeology (CISA3), Qualcomm Institute, Calit2, University of California, San Diego La Jolla, California, USA; Center of Interdisciplinary Science for Art, Architecture, and Archaeology (CISA3), Qualcomm Institute, Calit2, University of California, San Diego La Jolla, California, USA; Center of Interdisciplinary Science for Art, Architecture, and Archaeology (CISA3), Qualcomm Institute, Calit2, University of California, San Diego La Jolla, California, USA","2013 Digital Heritage International Congress (DigitalHeritage)","20 Feb 2014","2013","1","","453","453","As smartphones and tablets continue to pervade our daily lives, museums have turned to these devices as a new platform for engaging visitors with cultural heritage. However, the needs of the museum visitor are often overlooked when designing these applications. Researchers from CISA3 are beginning to address this problem by performing studies to gage users' wants in both engaging with the interfaces of mobile applications and the affordances associated with their environment of use. CISA3 has developed its own augmented reality tablet application, ARtifact, which intends to put a wealth of collected information directly into the hands of both researchers and the general public in a variety of contexts [1]. With ARtifact, users can examine metadata in museums as well as in the field at archaeological sites [2]; users are able to view both multispectral images of artifacts and annotated information pertaining to them in real time using the tablet's video seethrough interface. Wishing to deliver an optimal experience that engages the user and enhances the discovery and learning process, we followed the methodology of cognitive design to refine ARtifact's utility, based on contextual interviews for data collection and affinity diagrams for qualitative data organization and interpretation. Interviews were conducted in three stages. The first round of interviews took place at the Timken Museum in Balboa Park, San Diego, shadowing the art museum's testing of a new in-house developed mobile app. For this initial round, twenty-three visitors were interviewed and observed as they made their way around the museum. The second round of interviews took place with two directors from Balboa Park looking into enhancing the use of technology throughout the park, providing access to findings on extensive prior visitor research that the ","","978-1-4799-3170-5","10.1109/DigitalHeritage.2013.6743782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743782","","","archaeology;art;augmented reality;history;human computer interaction;mobile computing;museums;notebook computers","cultural context;iconography;iconology;science, technology, engineering, arts, and mathematics;STEAM;free-form exploration mode;diverse data analysis;mobile app;art museum testing;San Diego;Balboa park;Timken museum;qualitative data interpretation;affinity diagrams;data collection;qualitative data organization;ARtifact utility;cognitive design;learning process;tablet video see through interface;multispectral artifact images;archaeological sites;metadata;augmented reality tablet application;CISA3;museum visitor;cultural heritage;smart phones;digital heritage experience","","1","","3","","20 Feb 2014","","","IEEE","IEEE Conferences"
"Enabling behavior reuse in development of virtual environment applications","H. Liu; M. Bowman; W. A. Hunt; A. M. Duffy","Intel Labs, 2111 NE 25th Ave, JF2-04, Hillsboro, OR USA; Intel Labs, 2111 NE 25th Ave, JF2-04, Hillsboro, OR USA; Intel Labs, 2111 NE 25th Ave, JF2-04, Hillsboro, OR USA; Utah State University, 5305 Old Main Hill, Logan, UT USA","Proceedings of the 2012 Winter Simulation Conference (WSC)","21 Feb 2013","2012","","","1","12","Virtual environments (VEs) provide simulated 3D spaces in which users can interact, collaborate, and visualize in real time. Accordingly, virtual environments have the potential to transform education, creating classrooms that ignore geographic boundaries and immerse students in experiences that would be difficult or impossible to arrange in the real world. A major impediment to the widespread adoption of educational VEs is the high cost of developing VE applications. We believe application development must become tractable for non-expert users in the same way that Web development is no longer the exclusive purview of professional programmers. In this position paper, we describe our experiences in enabling behavior reuse across VE applications. Our approach replaces, whenever possible, application-specific behaviors with general purpose, reusable simulation modules. These modules bootstrap one another until a rich ecosystem develops; thus, VE application development is reduced to compositing content and behaviors instead of developing them from scratch.","1558-4305","978-1-4673-4782-2","10.1109/WSC.2012.6465246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465246","","Virtual environments;Engines;Planets;Games;Sociology;Statistics;Sorting","computer aided instruction;digital simulation;virtual reality","behavior reuse;virtual environment applications development;simulated 3D spaces;user interaction;user collaboration;educational transformation;educational VE;Web development;professional programmers;application-specific behaviors;reusable simulation modules;ecosystem","","2","","31","","21 Feb 2013","","","IEEE","IEEE Conferences"
"The effect of visual display properties and gain presentation mode on the perceived naturalness of virtual walking speeds","N. C. Nilsson; S. Serafin; R. Nordahl",Aalborg University; Aalborg University; Aalborg University,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","81","88","Individuals tend to find realistic walking speeds too slow when relying on treadmill walking or Walking-In-Place (WIP) techniques for virtual travel. This paper details three studies investigating the effects of visual display properties and gain presentation mode on the perceived naturalness of virtual walking speeds: The first study compared three different degrees of peripheral occlusion; the second study compared three different degrees of perceptual distortion produced by varying the geometric field of view (GFOV); and the third study compared three different ways of presenting visual gains. All three studies compared treadmill walking and WIP locomotion. The first study revealed no significant main effects of peripheral occlusion. The second study revealed a significant main effect of GFOV, suggesting that the GFOV size may be inversely proportional to the degree of underestimation of the visual speed. The third study found a significant main effect of gain presentation mode. Allowing participants to interactively adjust the gain led to a smaller range of perceptually natural gains and this approach was significantly faster. However, the efficiency may come at the expense of confidence. Generally the lower and upper bounds of the perceptually natural speeds were higher for treadmill walking than WIP. However, not all differences were statistically significant.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223328","H.1.2 [Information Systems]: User/Machine Systems — Human factors;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism — Virtual Reality","Legged locomotion;Visualization;Optical distortion;Analysis of variance;Virtual environments;Distortion;Adaptive optics","computer displays;gait analysis;human computer interaction;virtual reality","visual display properties;gain presentation mode;perceived naturalness;virtual walking speeds;realistic walking speeds;treadmill walking;walking-in-place technique;virtual travel;peripheral occlusion degrees;perceptual distortion;geometric field of view;GFOV;visual gains;WIP locomotion;visual speed underestimation degree;perceptually natural gains;lower bounds;upper bounds","","4","","33","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Scalable context-aware development infrastructure for interactive systems in smart environments","T. Eichler; S. Draheim; C. Grecos; Q. Wang; K. von Luck","Department of Computer Science, Hamburg University of Applied Sciences, Hamburg, Germany; Department of Computer Science, Hamburg University of Applied Sciences, Hamburg, Germany; Department of Computer Science, Central Washington University, Ellensburg, WA, USA; School of Engineering & Computing, University of the West of Scotland, Paisley, UK; Department of Computer Science, Hamburg University of Applied Sciences, Hamburg, Germany","2017 IEEE 13th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)","23 Nov 2017","2017","","","147","150","Context-aware systems for smart environments can be very complex and demanding for developers especially in distributed computing and communication environments. We propose a new development infrastructure, that targets this challenge by improving the general system's scalability and traceability. The infrastructure has been developed for and tested in two research labs for smart environments and human computer interaction. First measurements show that the platform has high scalability and low message latency that is perfectly suitable for interactive projects and virtual reality experiments.","","978-1-5386-3839-2","10.1109/WiMOB.2017.8115848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115848","context awareness;scalability;middleware;multi-agent systems;smart environments","Middleware;Runtime environment;Sensors;Monitoring;Scalability;Libraries","human computer interaction;ubiquitous computing;virtual reality","scalable context-aware development infrastructure;interactive systems;smart environments;context-aware systems;distributed computing;communication environments;human computer interaction;general system scalability;general system traceability;interactive projects;virtual reality experiments","","","","9","","23 Nov 2017","","","IEEE","IEEE Conferences"
"A cognitive theory for affective user modelling in a virtual reality educational game","G. Katsionis; M. Virvou","Dept. of Informatics, Piraeus Univ., Greece; Dept. of Informatics, Piraeus Univ., Greece","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)","7 Mar 2005","2004","2","","1209","1213 vol.2","The educational community in general considers computer-assisted learning to be very beneficial. As a result, numerous new educational software applications are being developed. An educational application can become very effective if it is adaptive and individualised to the student. However, one important aspect of students that has been overlooked so far, and should be included in such individualisation models, is students' behaviour and emotional state that affects their learning. This paper describes how system observations of students' behavioural characteristics, during their interaction with an educational application, may provide important evidence about students' emotions while they learn. The information collected from these observations mainly concerns students' behaviour while using the application, combined with students' reactions and responds to questions depending on the correctness of their answers. The system's inferences about students' emotions are used to adapt interaction to each individual student's needs taking into account their character and mood.","1062-922X","0-7803-8566-7","10.1109/ICSMC.2004.1399789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1399789","","Virtual reality;Game theory;Application software;Intelligent systems;Informatics;Decision making;Mood;Logic;Testing;Educational institutions","cognition;virtual reality;computer games;computer aided instruction;behavioural sciences computing","cognitive theory;affective user modelling;virtual reality educational game;computer-assisted learning;educational software applications","","6","","16","","7 Mar 2005","","","IEEE","IEEE Conferences"
"Effective simulation of the radial thickness of helix for broad band, practical TWT's","M. V. Kartikeyan; A. K. Sinha; H. N. Bandopadhyay; D. S. Venkateswarlu","Central Electron. Eng. Inst., Pilani, India; NA; NA; NA","IEEE Transactions on Plasma Science","6 Aug 2002","1999","27","4","1115","1123","A generalized analytical model has been developed to simulate the radial thickness of the helix employed in practical traveling wave tubes. With this proposed model, the exact physical dimensions of the helix radial thickness can be incorporated in the design equations. Relevant dispersion relations, characteristic and interaction impedances, and equivalent circuit parameters are derived by making use of both the field theory and equivalent analysis methods. Besides, in the field theory approach, a novel technique has been introduced to reduce the complexity in the derivation of the design formula. The results obtained using the present model are critically assessed, for four different practical slow-wave structures, over a wide range of experimental frequencies and found to be better than those obtained using earlier models.","1939-9375","","10.1109/27.782291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=782291","","Analytical models;Equivalent circuits;Circuit simulation;Dispersion;Circuit analysis;Dielectrics;Impedance;Frequency;Equations;Bandwidth","travelling wave tubes;digital simulation;dispersion relations","effective simulation;radial thickness;helix;broad band travelling wave tubes;generalized analytical model;helix radial thickness;design equations;dispersion relations;characteristic impedances;interaction impedances;equivalent circuit parameters;field theory approach;design formula;slow-wave structures","","11","","24","","6 Aug 2002","","","IEEE","IEEE Journals"
"Machine-induced coordination behavior in human-machine interaction","G. C. de Guzman; E. Tognoli; J. A. S. Kelso","Human Brain and Behavior Lab, Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, FL 33138 USA; Human Brain and Behavior Lab, Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, FL 33138 USA; Human Brain and Behavior Lab, Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton, FL 33138 USA","2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)","25 Feb 2010","2009","","","510","515","The dynamics of human-machine interactions can vary from simple to complex. It spans a range that go from basic rule-based serial stimulus-response exchanges to more realistic bi-directional interactions that evolve in real-time according to (possibly) complex laws. Increasingly, a subset of such systems, those between humans and humanoid robots has been drawing more scrutiny because of their potential as surrogate systems for live multi-agent social interaction. Such systems may provide insights into the mechanisms of human cooperation and competition as well as uncover novel behaviors with judicious extension of the parameter space in which to investigate them. We draw on a recently proposed human-machine interaction system, the virtual partner interaction (VPI) to focus on the fundamental issue of machine-induced coordination behavior on a human partner. This study uses coordination dynamics, an empirical/theoretical framework that has found numerous applications in studies of coordination at the levels of both brain and behavior. Specifically, we study VPI inducing a coordination behavior opposite to that which a human is required to execute during a coordination task. Its importance and scalability to more general situations is discussed.","","978-1-4244-4774-9","10.1109/ROBIO.2009.5420699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420699","coordination;dynamic clamp;virtual partner","Man machine systems;Clamps;Computational modeling;Humanoid robots;Virtual reality;Frequency;Robot kinematics;Human robot interaction;Biomimetics;Bidirectional control","control engineering computing;humanoid robots;human-robot interaction;multi-agent systems;robot dynamics;virtual reality","machine-induced coordination behavior;human-machine interaction;rule-based serial stimulus-response;realistic bidirectional interactions;humanoid robots;live multiagent social interaction;virtual partner interaction","","1","","38","","25 Feb 2010","","","IEEE","IEEE Conferences"
"Leveraging Mixed Reality and the Three Apprenticeships Model to Facilitate & Assess Authentic Learning Experiences for Civil Engineering Students","L. Perry; J. London; W. Wu; S. Ayer; K. Smith; K. Patil","Virginia Tech,Department of Engineering Education,Blacksburg,USA; Virginia Tech,Department of Engineering Education,Blacksburg,USA; California State University, Fresno,Department of Construction Management,Fresno,USA; Arizona State University,School of Sustainable Engineering and the Built Environment,Tempe,USA; Arizona State University,School of Sustainable Engineering and the Built Environment,Tempe,USA; Arizona State University,School of Sustainable Engineering and the Built Environment,Tempe,USA","2020 IEEE Frontiers in Education Conference (FIE)","4 Dec 2020","2020","","","1","4","This work-in-progress paper presents highlights from a multi-year study aiming to develop and assess the impact of a mixed reality experience that sufficiently replicates the learning civil engineering students experience during a physical design and construction task. Human Centered Design principles and tenets of the Carnegie Foundation's Three Apprenticeships Model (i.e., learning related to ""Head"", ""Hand"", and ""Heart"") inform the project design, development, and assessments. The development of heart-focused assessments is one focus during the second year in this three-year project. This paper includes a brief overview of the project progress, in general, along with preliminary findings regarding the instrument development. It summarizes the results of a pilot study, including an item analysis of the survey responses. These findings offer preliminary evidence for the content validity and substantive validity of the instrument. Next steps and implications for the engineering education community are also discussed.","2377-634X","978-1-7281-8961-1","10.1109/FIE44824.2020.9274289","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274289","mixed reality;civil engineering;instrument development","Instruments;Industries;Heart;Safety;Engineering students;Particle measurements;Civil engineering","civil engineering;computer aided instruction;engineering education;human computer interaction;user centred design","instrument development;engineering education community;authentic learning experiences;work-in-progress paper presents highlights;multiyear study;mixed reality experience;construction task;project design;Carnegie foundation;apprenticeships model;civil engineering students experience;human centered design","","","","11","","4 Dec 2020","","","IEEE","IEEE Conferences"
"Virtual Reality for Pain Management in Cancer: A Comprehensive Review","M. Pittara; M. Matsangidou; K. Stylianides; N. Petkov; C. S. Pattichis","Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligent, University of Groningen, CP Groningen, The Netherlands; Research Centre on Interactive Media, Smart Systems and Emerging Technologies RISE, Nicosia, Cyprus; Cyprus Association of Cancer Patients and Friends, PASYKAF, Nicosia, Cyprus; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligent, University of Groningen, CP Groningen, The Netherlands; Research Centre on Interactive Media, Smart Systems and Emerging Technologies RISE, Nicosia, Cyprus","IEEE Access","24 Dec 2020","2020","8","","225475","225489","Virtual Reality is a computer-simulated 3-Dimensional technology in which the user interacts via different senses: visual, auditory, tactile, and/or olfactory. In the past decades, it has been argued that Virtual Reality as a technique could be applied in the clinical environment to successfully manage pain. This article provides a systematic review of research on Virtual Reality and pain management for patients who are suffering from cancer. More specifically, this article focuses on all types of Virtual Reality technologies (Non-Immersive, Semi-Immersive, Fully-Immersive) which has been developed and released to manage the pain which evokes from the treatment of cancer. An exhaustive search identified 23 relevant studies from 2010 to 2020. Overall, the identified studies indicated that Virtual Reality can improve the experience of pain for patients who are suffering from cancer. It was also found that, if Virtual Reality is appropriately designed, the pain which is arising from cancer treatments can be reduced. Even though some positive outcomes have been reported, overall, the results are inconclusive and studies that examine specifically the treatment of pain in cancer patients are limited. Further research needs to be conducted, to articulate clearly, under what circumstances Virtual Reality is an effective tool for cancer patients, and under what factors Virtual Reality can be the solution to the pain patients are experiencing.","2169-3536","","10.1109/ACCESS.2020.3044233","European Union’s Horizon 2020 Research and Innovation Programme; Government of the Republic of Cyprus through the Directorate-General for European Programmes, Coordination, and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291419","Virtual reality;cancer;pain;interactive devices","Cancer;Pain;Virtual reality;Libraries;Breast;Lung;Licenses","cancer;cultural aspects;medical computing;reviews;virtual reality","pain management;cancer patients;virtual reality technologies;computer-simulated 3-dimensional technology;user interaction;tactile senses;olfactory senses;visual senses;auditory senses;clinical environment;fully-immersive technologies;non-immersive technologies;semi-immersive technologies;cancer treatment","","","","66","CCBYNCND","11 Dec 2020","","","IEEE","IEEE Journals"
"Anno 2010-remembering our future. Challenges and frontiers of visualization for Human-Media Technology as the kernel for Human-Centered Computing","J. L. Encarnacao","Fraunhofer Inst. of Comput. Graphics, Tech. Univ. of Darmstadt, Germany","XII Brazilian Symposium on Computer Graphics and Image Processing (Cat. No.PR00481)","6 Aug 2002","1999","","","3","","Summary form only given. Human-Media-Technology is one of the ""technology's top 10 challenges and opportunities"" for the year 2010. The subject hereby is: ""humans live, science finds out how technology conforms."" The goal is to develop environments that allow users to cooperate in the most efficient and natural way. Human-centered systems will have to incorporate people as an explicit design component. The article addresses the main goals in developing such systems based on their general characteristics and the corresponding enabling technologies: visualization; virtual and augmented reality; and multimedia. Examples from applications and case studies support the clarification of ideas and goals. So far, the human-centered interfaces to accommodate human perception and human response capabilities and limitations have been presented and discussed. These interfaces allow us to integrate the desired amount of immersion and cooperation (CSCW). Based on this, some ""hands-on"" live demos are shown to discuss the state of the art of these technologies, like Virtual Tables (responsive workbenches), special I/O technologies, etc. Some of these demos are stand-alone demos; others show the potential of telecommunication for collaboration by connecting the floor with other locations to demonstrate CSCW based visual tele-applications. A brief survey is given on the most important research and application challenges, with the objective of having technology conforming to users and not vice versa.","","0-7695-0481-7","10.1109/SIBGRA.1999.805591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=805591","","Kernel;Human computer interaction;Computer graphics;Technology forecasting;Data visualization;Augmented reality;Multimedia systems;Videos;Collaborative work;Joining processes","technological forecasting;user centred design;user interfaces;virtual reality;multimedia systems;groupware;data visualisation","visualization;Human-Media Technology;Human-Centered Computing;2010;explicit design component;enabling technologies;augmented reality;virtual reality;multimedia;case studies;human-centered interfaces;human perception;human response capabilities;Virtual Tables;responsive workbenchs;special I/O technologies;CSCW based visual tele-applications","","","","","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A distributed collaborative simulation environment for orthopedic surgical training","J. Cecil; A. Gupta; P. Ramanathan; M. Pirela-Cruz","Center for Cyber Physical Systems, Computer Science, Oklahoma State University, Stillwater, Ok 74078; Center for Cyber Physical Systems, Computer Science, Oklahoma State University, Stillwater, OK 74078; Electrical and Computer Engineering, University of Wisconsin-Madison; Orthopedic Surgery, Paul Foster School of Medicine, TTHSC, El Paso, TX","2017 Annual IEEE International Systems Conference (SysCon)","29 May 2017","2017","","","1","8","The use of Virtual Reality (VR) simulators has increased rapidly in the field of medical surgery for training purposes. In this paper, the design and development of a Virtual Surgical Environment (VSE) for training residents in an orthopaedic surgical process called Less Invasive Stabilization System (LISS) surgery is discussed; LISS plating surgery is a process used to address fractures of the femur bone. The development of such virtual environments for educational and training purposes will accelerate and supplement existing training approaches enabling medical residents to be better prepared to serve the surgical needs of the general public. One of the important aspects of the VSE is that it is a network based simulator. Our approach explores the potential of emerging Next Generation Internet frameworks and technologies to support such distributed interaction contexts. A discussion of the validation activities is also presented, which highlights the effectiveness of the VSE for teaching medical residents and students.","2472-9647","978-1-5090-4623-2","10.1109/SYSCON.2017.7934721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934721","virtual reality;orthopaedic simulator;LISS plating","Surgery;Training;Solid modeling;Haptic interfaces;Plating;Software","biomedical education;groupware;medical computing;orthopaedics;surgery;virtual reality","distributed collaborative simulation environment;orthopedic surgical training;virtual reality;VR simulators;medical surgery;virtual surgical environment;VSE;orthopaedic surgical process;less invasive stabilization system;LISS plating surgery;fractures;femur bone;virtual environments;medical residents;surgical needs;next generation Internet frameworks;distributed interaction contexts;students","","7","","40","","29 May 2017","","","IEEE","IEEE Conferences"
"A multi-modal interface for an interactive simulated vascular reconstruction system","E. V. Zudilova; P. M. A. Sloot; R. G. Belleman","Sect. of Computational Sci., Amsterdam Univ., Netherlands; Sect. of Computational Sci., Amsterdam Univ., Netherlands; Sect. of Computational Sci., Amsterdam Univ., Netherlands","Proceedings. Fourth IEEE International Conference on Multimodal Interfaces","22 Jan 2003","2002","","","313","318","This paper is devoted to multi-modal interface design and implementation of a simulated vascular reconstruction system. It provides multi-modal interaction methods such as speech recognition, hand gestures, direct manipulation of virtual 3D objects and measurement tools. The main challenge is that no general interface scenario in existence today can satisfy all the users of the system (radiologists, vascular surgeons, medical students, etc.). The potential users of the system can vary by their skills, expertise level, habits and psycho-motional characteristics. To make a multimodal interface user-friendly is a crucial issue. In this paper we introduce an approach to develop such an efficient, user-friendly multi-modal interaction system. We focus on adaptive interaction as a possible solution to address the variety of end-users. Based on a user model, the adaptive user interface identifies each individual by means of a set of criteria and generates a customized exploration environment.","","0-7695-1834-6","10.1109/ICMI.2002.1167013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167013","","Computational modeling;Surgery;Aneurysm;Speech recognition;Arteries;Diseases;Computer simulation;Virtual environment;Stress;Blood flow","digital simulation;speech recognition;speech-based user interfaces;gesture recognition;cardiovascular system;user modelling;surgery;virtual reality;medical computing","multi-modal interface design;interactive simulated vascular reconstruction system;speech recognition;hand gestures;direct virtual 3D object manipulation;measurement tools;radiologists;vascular surgeons;medical students;psycho-motional characteristics;expertise level;user-friendly multi-modal interaction system;adaptive interaction;end-users;user model;customized exploration environment","","8","1","21","","22 Jan 2003","","","IEEE","IEEE Conferences"
"Towards a taxonomy of virtual reality user interfaces","M. K. D. Coomans; H. J. P. Timmermans","Fac. of Archit., Building & Planning, Eindhoven Univ. of Technol., Netherlands; NA","Proceedings. 1997 IEEE Conference on Information Visualization (Cat. No.97TB100165)","6 Aug 2002","1997","","","279","284","Virtual reality based user interfaces (VRUIs) are expected to bring about a revolution in computing. VR can potentially communicate large amounts of data in an easily understandable format. VR looks very promising, but it is still a very new interface technology for which very little application oriented knowledge is available. As a basis for such a future VRUI design theory, a taxonomy of VRUIs is required. A general model of human computer communication is formulated. This model constitutes a frame for the integration of partial taxonomies of human computer interaction that are found in the literature. The whole model constitutes a general user interface taxonomy. The field of VRUIs is described and delimited with respect to this taxonomy.","1093-9547","0-8186-8076-8","10.1109/IV.1997.626531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=626531","","Taxonomy;Virtual reality;User interfaces;Computer interfaces;Visualization;Computer architecture;Buildings;Technology planning;Postal services;Humans","graphical user interfaces;virtual reality;interactive systems;human factors","virtual reality user interfaces;VRUIs;VR;understandable format;interface technology;application oriented knowledge;future VRUI design theory;human computer communication;partial taxonomies;human computer interaction;general user interface taxonomy","","12","3","9","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Balancing Algorithm and Active Shutter Improved ROV Monitoring and Control Platform Using Mobile Phone","Z. Tao; J. Qiao; L. Jiang; L. Han","School of Information Science and Engineering, Harbin Institute of Technology,Weihai,China; Weihai Beiyang Optoelectronic Info-Tech Co., Ltd,Weihai,China; Weihai Oceanic Fishery Monitoring and Hazard Mitigation Center,Weihai,China; School of Information Science and Engineering, Harbin Institute of Technology,Weihai,China","2020 IEEE 6th International Conference on Computer and Communications (ICCC)","12 Feb 2021","2020","","","2340","2344","In this paper, we propose a novel and convenient Remote Operated Vehicle (ROV) monitoring and control platform based on Virtual Reality (VR) system and virtual rocker control system. In general, conventional monitoring and control platforms for ROV is bulky and lack of real-world interaction. To overcome these problems, we design a portable monitor and control platform that offers 3D visualization and control interface based on mobile phone. Besides, to improve the immersion perception of mobile phone VR platform, an algorithm based on Kalman filter is proposed to improve the accuracy of controlling data. Active shutter technology is also applied to enhance the image resolution in VR visualization.","","978-1-7281-8635-1","10.1109/ICCC51575.2020.9344986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344986","VR;monitoring and control platform;ROV;mobile phone;virtual rocker;Kalman filter;active shutter","Three-dimensional displays;Data visualization;Virtual reality;Resists;Mobile handsets;Kalman filters;Monitoring","data visualisation;image resolution;Kalman filters;remotely operated vehicles;underwater vehicles;virtual reality","active shutter technology;mobile phone VR platform;control interface;portable monitor;control platforms;conventional monitoring;virtual rocker control system;Virtual Reality system;control platform;active shutter improved ROV monitoring;balancing algorithm","","","","18","","12 Feb 2021","","","IEEE","IEEE Conferences"
"Poster: Improving motor rehabilitation process through a natural interaction based system using Kinect sensor","A. Da Gama; T. Chaves; L. Figueiredo; V. Teichrieb",Voxar Labs at Center of Informatics of the Federal University of Pemambuco; Voxar Labs at Center of Informatics of the Federal University of Pemambuco; Voxar Labs at Center of Informatics of the Federal University of Pemambuco; Voxar Labs at Center of Informatics of the Federal University of Pemambuco,"2012 IEEE Symposium on 3D User Interfaces (3DUI)","19 Apr 2012","2012","","","145","146","In general, the motor rehabilitation process can take advantage of natural interaction based systems, including measurements from patient performance to track its evolution during time and therapy direction. Thus, the aim of this research is the analysis of the use of Kinect sensor as interaction support tool for rehabilitation systems. The Kinect sensor gives three-dimensional information about the user body, recognizing skeleton and joint positions, however does not provide the detection of the body specific movements. This way, the correct description of a rehabilitation movement (shoulder abduction, for instance) was implemented in a system prototype. A scoring mechanism was also developed in order to measure the patient performance, as well as to stimulate his improvement by displaying a positive feedback.","","978-1-4673-1205-9","10.1109/3DUI.2012.6184203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184203","Rehabilitation;Natural Interaction;Body Tracking;Kinect","Prototypes;Vectors;Medical treatment;Elbow;Shoulder;Virtual reality;Usability","human computer interaction;patient rehabilitation;user interfaces","motor rehabilitation process;natural interaction based system;patient performance;therapy direction;Kinect sensor;skeleton positions;joint positions;system prototype;scoring mechanism","","30","","5","","19 Apr 2012","","","IEEE","IEEE Conferences"
"SonifEye: Sonification of Visual Information Using Physical Modeling Sound Synthesis","H. Roodaki; N. Navab; A. Eslami; C. Stapleton; N. Navab","Chair for Computer Aided Medical Procedures, Technische Universitäat Müunchen, Munich, Germany; Topological Media Lab, Concordia University, Montreal, Canada; Carl Zeiss Meditec AG, Munich, Germany; Simiosys Real World Laboratory, Oviedo, FL, USA; Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Visualization and Computer Graphics","29 Sep 2017","2017","23","11","2366","2371","Sonic interaction as a technique for conveying information has advantages over conventional visual augmented reality methods specially when augmenting the visual field with extra information brings distraction. Sonification of knowledge extracted by applying computational methods to sensory data is a well-established concept. However, some aspects of sonic interaction design such as aesthetics, the cognitive effort required for perceiving information, and avoiding alarm fatigue are not well studied in literature. In this work, we present a sonification scheme based on employment of physical modeling sound synthesis which targets focus demanding tasks requiring extreme precision. Proposed mapping techniques are designed to require minimum training for users to adapt to and minimum mental effort to interpret the conveyed information. Two experiments are conducted to assess the feasibility of the proposed method and compare it against visual augmented reality in high precision tasks. The observed quantitative results suggest that utilizing sound patches generated by physical modeling achieve the desired goal of improving the user experience and general task performance with minimal training.","1941-0506","","10.1109/TVCG.2017.2734327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007327","Aural augmented reality;sonification;sonic interaction;auditory feedback","Visualization;Mathematical model;Augmented reality;Load modeling;Auditory displays;Acceleration;Computational modeling","augmented reality;data visualisation","SonifEye;visual information;sound patches;conveyed information;minimum mental effort;mapping techniques;sonification scheme;cognitive effort;sonic interaction design;sensory data;computational methods;visual field;conventional visual augmented reality methods;physical modeling sound synthesis","Computer Graphics;Feedback, Sensory;Humans;Models, Neurological;Psychomotor Performance;Software;Virtual Reality","5","","13","Traditional","10 Aug 2017","","","IEEE","IEEE Journals"
"A Mobile Augmented Reality Framework Based on Reusable Components","R. I. Barraza Castillo; O. O. Vergara Villegas; V. G. Cruz Sanchez","Univ. Autonoma de Ciudad Juarez, Ciudad Juárez, Mexico; Univ. Autonoma de Ciudad Juarez, Ciudad Juárez, Mexico; Univ. Autonoma de Ciudad Juarez, Ciudad Juárez, Mexico","IEEE Latin America Transactions","26 Mar 2015","2015","13","3","713","720","In recent years, Augmented Reality (AR) has experienced a great growth in the interest of researchers and general public. Commercial applications such as AR browsers, games, and marketing campaigns are gradually becoming more available; however, the need for a flexible authoring solution still remains. The aim of this paper is to present a novel framework to create marker based augmented reality applications for mobile devices. The framework is composed of five main subsystems: a) presentation: with a game engine as the core component, b) tracking: which performs all the computer vision algorithms, c) interaction: composed of a custom visual editor which processes all the inputs to the framework, d) world model: used to store and provide access to the digital representation of the world, and e) context: used to provide status information of the entire system. All the subsystems were used to provide for users a comprehensive way to develop and share prefabricated objects that allow reusability across applications. A particular case, of an interactive “pARabola plotting” application, is presented to demonstrate the efficiency of the framework. The application was tested in four mobile devices running Android OS. The experimental results demonstrate the ability of the framework to create usable mobile AR applications, observing better visual and interaction results on the higher quality mobile devices.","1548-0992","","10.1109/TLA.2015.7069096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069096","Android operating system;Augmented reality;Framework;Mobile devices;Reusable components","Random access memory;Augmented reality;Mobile communication;Mobile handsets;Visualization;Silicon compounds;Three-dimensional displays","augmented reality;computer vision;mobile computing","mobile augmented reality;reusable component;flexible authoring solution;marker based augmented reality;mobile device;game engine;computer vision algorithm;custom visual editor;interactive pARabola plotting;Android OS","","1","","","","26 Mar 2015","","","IEEE","IEEE Journals"
"The mental simulation of a human-robot interaction: Positive effects on attitudes and anxiety toward robots","D. Kuchenbrandt; F. Eyssel","Center of Excellence in Cognitive Interaction Technology, University of Bielefeld, Germany; Center of Excellence in Cognitive Interaction Technology, University of Bielefeld, Germany","2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication","10 Nov 2012","2012","","","463","468","In the present study, we experimentally investigated whether the mental simulation of an interaction with the robot NAO would improve human-robot interaction (HRI) acceptance, and would reduce negative attitudes and anxiety toward robots in general. Participants were introduced to the robot NAO and were then instructed to imagine for two minutes either a cooperative or a competitive interaction with the robot or a neutral scenario that did not include an HRI. Our results showed that, independent of its content, imagining contact with NAO improved HRI acceptance. Moreover, after imagining contact with the robot, participants indicated less negative attitudes and less anxiety toward robots in general. These effects were strongest for cooperative imagined contact. Theoretical and practical implications for research on HRI will be discussed.","1944-9437","978-1-4673-4606-1","10.1109/ROMAN.2012.6343795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6343795","","Robots;Humans;Psychology;Computers;Atmospheric measurements;Particle measurements;Educational institutions","humanoid robots;human-robot interaction","mental simulation;human-robot interaction;positive effects;anxiety;robot NAO;negative attitude reduction;HRI acceptance","","11","","32","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Design and evaluation of a haptically enable virtual environment for object assembly training","D. Jia; A. Bhatti; S. Nahavandi","Centre for Intelligent Systems Research (CISR), Deakin University, Geelong, Australia; Centre for Intelligent Systems Research (CISR), Deakin University, Geelong, Australia; Centre for Intelligent Systems Research (CISR), Deakin University, Geelong, Australia","2009 IEEE International Workshop on Haptic Audio visual Environments and Games","18 Dec 2009","2009","","","75","80","Virtual training systems are attracting paramount attention from the manufacturing industries due to their potential advantages over the conventional training practices. Significant cost savings can be realized due to the shorter times for the development of different training-scenarios as well as reuse of existing designed engineering (math) models. This paper presents a newly developed virtual environment (VE) for training of procedure tasks i.e. object assembly. Unlike existing VE systems, the presented idea tries to imitate real physical training scenarios by providing comprehensive user interaction, constrained within the physical limitations of the real world. These physical constrains are imposed by the haptics devices in the virtual environment. As a result, in contrast to the existing VE systems that are capable of providing knowledge generally about assembly sequences only, the proposed system helps in cognitive learning and procedural skill development due to its high physically interactive nature. In addition a novel evaluation framework has also been proposed to evaluate system efficacy through a large scale of user-testing, which is often been neglected by design experts in the field of VEs. Results confirm the practical significance of evaluating a VE design by involving sample of real and representative users through the effective discovery of critical usability problems and system deficiencies. Results also indicate benefits of collecting multimodal information for accurate and comprehensive assessment of system efficacy. Evaluation results and improvement of existing design are also presented.","","978-1-4244-4217-1","10.1109/HAVE.2009.5356130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356130","Virtual environment;Training;User-ecntred evaluation;Performance;Perception;Memory","Virtual environment;Assembly systems;Industrial training;Haptic interfaces;Manufacturing industries;Costs;Usability;Computer aided manufacturing;Feedback;Intelligent manufacturing systems","assembling;computer based training;haptic interfaces;human factors;industrial training;manufacturing industries;production engineering computing;virtual reality","virtual environment;object assembly training;virtual training systems;manufacturing industry;user interaction;haptics devices;user-testing;user-centered evaluation;cognitive learning;procedural skill development","","7","","16","","18 Dec 2009","","","IEEE","IEEE Conferences"
"Making simulated faces come alive","M. Sagar","Laboratory for Animate Technologies, Auckland Bioengineering Institute, The University of Auckland, New Zealand","2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","14 Apr 2016","2016","","","1","1","Our behaviour emerges as the result of many systems interacting at different scales, from low level biology to high level social interaction. Is it possible to create naturalistic explanatory models which can integrate these factors? We describe the general approach and design of a framework to create autonomous expressive embodied models of behaviour based on affective and cognitive neuroscience theories, sharing similar goals with current research in developmental robotics and embodied cognition, but with emphasis on computer graphics and face to face interaction. We aim to create a “large functioning sketch” of several fundamental aspects of behaviour affecting the face to explore emergence from interaction of low level and high level systems in a top-down bottom up approach.","2167-2148","978-1-4673-8370-7","10.1109/HRI.2016.7451726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451726","","Computational modeling;Biological system modeling;Visualization;Facial animation;Neuroscience;Cognition","computer graphics;graphical user interfaces;human computer interaction;human-robot interaction","low level biology;high level social interaction;autonomous expressive embodied models;cognitive neuroscience theory;developmental robotics;computer graphics;large functioning sketch;top-down bottom up approach","","","","3","","14 Apr 2016","","","IEEE","IEEE Conferences"
"Generalized average modelling of FACTS for real time simulation in ARENE","I. Etxeberria-Otadui; V. Manzo; S. Bacha; F. Baltes","IKERLAN, Mondragon, Spain; NA; NA; NA","IEEE 2002 28th Annual Conference of the Industrial Electronics Society. IECON 02","20 Mar 2003","2002","2","","864","869 vol.2","This paper studies two cutting edge technologies applied to the power system: FACTS devices and real time (RT) simulation tools. On the one hand, the complexity of the power system and its components, makes power system simulators very useful and even necessary. Amongst all the existing simulators, real time digital simulators permit the test of power system components in conditions that are quite close to reality. On the other hand power electronic devices such as FACTS, are becoming more and more frequent on the power system, and it is necessary to evaluate their impact on the performance of the system. For all these reasons it is very important to develop FACTS models which can be suitable for real time simulation in order to test their interaction with other power system components. This paper presents the generalized average model of two FACTS devices the static VAr compensator (SVC) and the static VAr generator, which have been then introduced in ARENE for the realization of real time simulations.","","0-7803-7474-6","10.1109/IECON.2002.1185385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1185385","","Power system simulation;Power system modeling;Power system analysis computing;Power system dynamics;Real time systems;Power system planning;Static VAr compensators;Analytical models;System testing;Power electronics","flexible AC transmission systems;power system simulation;software packages;digital simulation;real-time systems;static VAr compensators","generalized average modelling;FACTS;real time simulation;ARENE;cutting edge technologies;power system;real time simulation tools;power system simulators;real time digital simulators;power system components testing;hand power electronic devices;static VAr compensator;static VAr generator","","16","","9","","20 Mar 2003","","","IEEE","IEEE Conferences"
"Interactive mediated reality","R. Grasset; J. -. Gascuel; Schmalstieg","iMAGIS/GRAVIR, INRIA Rhone-Alpes, Montbonnot, France; iMAGIS/GRAVIR, INRIA Rhone-Alpes, Montbonnot, France; NA","The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.","27 Oct 2003","2003","","","302","303","Mediated reality describes the concept of filtering or vision of reality, typically using a head-worn video mixing display. In this paper, we propose a generalized concept and new tools for interactively mediated reality. We present also our first prototype system for painting, grabbing and gluing together real and virtual elements.","","0-7695-2006-5","10.1109/ISMAR.2003.1240731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240731","","Painting;Augmented reality;Computer graphics;Geometry;Brushes;Video sharing;Displays;Virtual prototyping;Information filtering;Information filters","augmented reality;computer graphics;helmet mounted displays;human computer interaction;graphical user interfaces","interactive mediated reality;reality filtering;reality vision;head-worn display;video mixing display;prototype system;computer graphics;painting tool;grab tool;glue tool","","4","","4","","27 Oct 2003","","","IEEE","IEEE Conferences"
"Virtual cooperating manipulators as a virtual reality haptic interface","G. R. Luecke; J. C. Edwards","Dept. of Mech. Eng., Iowa State Univ., Ames, IA, USA; NA","Proceedings Third Annual Symposium on Human Interaction with Complex Systems. HICS'96","6 Aug 2002","1996","","","133","140","One central element in the focus on research into human-machine interfaces is the capability to interact physically with the computer model. The sense of touch and feel is vital for realistic manipulation and control of virtual objects. The research described here is the development and implementation of a new dynamic control strategy using a standard six-degree-of-freedom robot manipulator as a force interface to virtual reality systems. The haptic element of VR interfacing is currently the subject of abundant research, some addressing the stability and control of interactive systems but with much of the focus on the development of new hardware systems to support stable interaction between humans and VR graphics displays. However, general six-degree-of-freedom manipulators are well understood today and are known to have the capability for generating the general force and motion constraints necessary for the design interaction described in the story. This approach to haptic feedback capability is based on the concept of describing a virtual manipulator model that mimics the motion constraints imposed by a virtual surface. This virtual manipulator is conceptually linked to an actual manipulator to form a closed kinematic chain system. The closed chain system equations are used to define a set of constraints that control the actual robot manipulator so as to allow motion only in the free directions of the virtual manipulator. These free directions are also the free motion directions allowed by the virtual surfaces one tremendous advantage of the approach is that the control algorithm is formulated using local error feedback schemes at the robot level providing effective, stable, and simple control of the robotic hardware. Using the proposed control scheme will allow any six-degree-of-freedom manipulator to be used as a haptic interface device.","","0-8186-7493-8","10.1109/HUICS.1996.549503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549503","","Virtual reality;Haptic interfaces;Control systems;Motion control;Manipulator dynamics;Hardware;Feedback;Robot control;Man machine systems;Computer interfaces","virtual reality","virtual cooperating manipulators;virtual reality haptic interface;human-machine interfaces;virtual objects;standard six-degree-of-freedom robot manipulator;hardware systems;haptic feedback capability;local error feedback schemes","","5","1","19","","6 Aug 2002","","","IEEE","IEEE Conferences"
