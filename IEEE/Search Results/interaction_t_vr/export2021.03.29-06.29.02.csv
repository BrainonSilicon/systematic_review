"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"AR-based product design in automobile industry","J. Fruend; C. Matysczok; R. Radkowski","Heinz Nixdorf Inst., Paderborn Univ., Germany; Heinz Nixdorf Inst., Paderborn Univ., Germany; Heinz Nixdorf Inst., Paderborn Univ., Germany","The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","2 pp.","","This paper describes different scenarios for the use of augmented reality in the early design phase for new cars. The prototype uses different tracking systems like Ar-Toolkit, Polhemus Fastrack and Pinch Gloves for user interaction.","","0-7803-7680-3","10.1109/ART.2002.1106981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106981","","Product design;Prototypes;Testing;Ergonomics;Graphics;Technological innovation;Strategic planning;Automobile manufacture;Process planning;Product development","augmented reality;automobile industry;CAD","augmented reality;cars;early design;tracking systems;AR-ToolKit;Polhemus Fastrack;Pinch Gloves;user interaction;product design;automobile industry","","4","1","4","","6 Jan 2003","","","IEEE","IEEE Conferences"
"Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications","D. Kurz",Metaio GmbH,"2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","6 Nov 2014","2014","","","9","16","We present an approach that makes any real object a true touch interface for mobile Augmented Reality applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on visual object tracking using a visible light camera. Finally the 3D position of the touch is used by human machine interfaces for Augmented Reality providing natural means to interact with real and virtual objects. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. Based on tests with a variety of different materials and different users, we show that our method enables intuitive interaction for mobile Augmented Reality with most common objects.","","978-1-4799-6184-9","10.1109/ISMAR.2014.6948403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948403","H.5.2 [User Interfaces]: Input devices and strategies — Graphical user interfaces;H.5.1 [Multimedia Information Systems];Artificial, augmented, virtual realities — Evaluation/methodology","Cameras;Three-dimensional displays;Temperature measurement;Materials;Augmented reality;User interfaces;Heating","augmented reality;mobile computing;object tracking;user interfaces","thermal touch;thermography-enabled everywhere touch interfaces;mobile augmented reality application;infrared thermography;residual heat detection;warm fingertip;heat transfer;visual object tracking;visible light camera;wearable computers;head-mounted displays;voice control;touchpads","","4","5","18","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Skill transfer in a simulated underactuated dynamic task","M. K. O'Malley; A. Gupta","Mech. Eng. & Material Sci., Rice Univ., Houston, TX, USA; Mech. Eng. & Material Sci., Rice Univ., Houston, TX, USA","The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.","19 Dec 2003","2003","","","315","320","Machine-mediated teaching of dynamic task completion is typically implemented with passive intervention via virtual fixtures or active assist by means of record and replay strategies. During interaction with a real dynamic system however, the user relies on both visual and haptic feedback in order to elicit desired motions. This work investigates skill transfer from assisted to unassisted modes for a Fitts' type targeting task with an underactuated dynamic system. Performance, in terms of between target tap times, is measured during an unassisted baseline session and during various types of assisted training sessions. It is hypothesized that passive and active assist modes that are implemented during training of a dynamic task could improve skill transfer to a real environment or unassisted simulation of the task. Results indicate that transfer of skill is slight but significant for the assisted training modes.","","0-7803-8136-X","10.1109/ROMAN.2003.1251864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251864","","Haptic interfaces;Education;Displays;Fixtures;Humans;Virtual reality;Force feedback;Shape control;Control systems;Mechanical engineering","haptic interfaces;computer based training;virtual reality;learning (artificial intelligence)","underactuated dynamic system;Fitts' type;skill transfer;haptic feedback","","3","","22","","19 Dec 2003","","","IEEE","IEEE Conferences"
"What User Interface to Use for Virtual Reality? 2D, 3D or Speech–A User Study","Y. Weiß; D. Hepperle; A. Sieß; M. Wölfel","Fac. of Math., Ludwig Maximilian Univ., Munich, Germany; Fac. of Comput. Sci. & Bus. Inf. Syst., Karlsruhe Univ. of Appl. Sci., Karlsruhe, Germany; Fac. of Comput. Sci. & Bus. Inf. Syst., Karlsruhe Univ. of Appl. Sci., Karlsruhe, Germany; Fac. of Comput. Sci. & Bus. Inf. Syst., Karlsruhe Univ. of Appl. Sci., Karlsruhe, Germany","2018 International Conference on Cyberworlds (CW)","27 Dec 2018","2018","","","50","57","In virtual reality different demands on the user interface have to be addressed than on classic screen applications. That's why established strategies from other digital media cannot be transferred unreflected and at least adaptation is required. So one of the leading questions is: which form of interface is preferable for virtual reality? Are 2D interfaces-that are mostly used in combination with mouse or touch interactions- the means of choice, although they do not use the medium's full capabilities? What about 3D interfaces that can be naturally integrated into the virtual space? And last but not least: are speech interfaces, the fastest and most natural form of human interaction/communication, which have recently established themselves in other areas (e.g. digital assistants), ready to conquer the world of virtual reality? To answer these question this work compares these three approaches based on a quantitative user study and highlights advantages and disadvantages of the respective interfaces for virtual reality applications.","","978-1-5386-7315-7","10.1109/CW.2018.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8590016","virtual reality;comparison of user interfaces;input modalities;2D interface;3D interface;Speech interface","Three-dimensional displays;Two dimensional displays;User interfaces;Speech recognition;Image color analysis;Virtual reality;Task analysis","user interfaces;virtual reality","virtual space;speech interfaces;quantitative user study;virtual reality applications;user interface;classic screen applications;digital media","","1","","17","","27 Dec 2018","","","IEEE","IEEE Conferences"
"Kinect based interactive music application for disabled children","Y. Kadakal; H. Kivrak; H. Kose","Bilgisayar Mühendisliği Bölümü, İstanbul Teknik Üniversitesi, İstanbul, TÜRKİYE; Bilgisayar Mühendisliği Bölümü, İstanbul Teknik Üniversitesi, İstanbul, TÜRKİYE; Bilgisayar Mühendisliği Bölümü, İstanbul Teknik Üniversitesi, İstanbul, TÜRKİYE","2014 22nd Signal Processing and Communications Applications Conference (SIU)","12 Jun 2014","2014","","","453","456","This work is part of a project for imitation based turn-taking and interaction games for children with communication impairments. The paper focuses on a music based interaction game mainly for children with autism. Autism Spectrum Disorder (ASD) involves communication impairments, limited social interaction, and limited imagination. Music can facilitate social interaction between the child and teacher/parent. In this work, the child is expected to imitate the predefined drumming actions in order, and a Kinect camera based system is used to recognize the signs and convert the actions to drum beats. The system simulates an e-drum which maps 6 different arm actions to different drum beats. A visual interface uses the input from the Kinect based system and gives visual and audio feedback to the participants based on the performance. In the pilot studies with adult participants, positive feedback about the system interface is taken, as well as the high success rate in the recognition of the actions and transferring these actions to music by the Kinect based system.","2165-0608","978-1-4799-4874-1","10.1109/SIU.2014.6830263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830263","Human-robot interaction;autism;imitation games;sign language;Kinect;gesture recognition","Autism;Signal processing;Conferences;Games;Variable speed drives;Visualization;Gesture recognition","audio-visual systems;cameras;computer games;handicapped aids;human computer interaction;image motion analysis;interactive systems;medical disorders;music;sign language recognition","action recognition;audio feedback;visual feedback;visual interface;arm actions;e-drum;drum beats;sign recognition;Kinect camera based system;drumming action;child-parent social interaction;child-teacher social interaction;limited imagination;limited social interaction;ASD;autism spectrum disorder;music based interaction game;communication impairment;imitation based turn-taking game;disabled children;Kinect based interactive music application","","1","","10","","12 Jun 2014","","","IEEE","IEEE Conferences"
"Investigating Body Transfer Illusion from Human to Monkey Body","T. Javorský; F. Škola; S. Sylaiou; J. Martins; F. Liarokapis","HCILab Masaryk University, Brno, Czech Republic; HCILab Masaryk University, Brno, Czech Republic; School of Social Sciences, Hellenic Open University, Patra, Greece; Faculty of Sciences and Technology, Universidade Tova de Lisboa, Lisbon, Portugal; HCILab Masaryk University, Brno, Czech Republic","2018 International Conference on Intelligent Systems (IS)","9 May 2019","2018","","","549","556","This paper presents a virtual reality study examining the magnitude of embodiment into a human and nonhuman avatar. It examines the user experience of inhabiting the body of animals in immersive virtual environments. Participants embodied in a human-like virtual avatar experienced body transfer illusion into a body of a monkey. The experiment consisted of two variants. In the first variant, participants did not have the ability to control the hands inside the Monkey avatar, they were instructed to just look over the scene from their fixed point of view. In the second variant, the ability to move arms and hands of the Monkey avatar was enabled, and this fact was articulated to the test subjects. Results suggest that the body transfer illusion is indeed possible. The study also indicates that the actual shape or visual representation of the body matters less than the amount and diversity of stimuli, and possibilities of controlling the avatar's body. Results of this study can be leveraged in the design of e-learning, health-care, and affective computing platforms, where amplification of the human-oriented design using malleable virtual avatars can bring additional feedback channel to the users.","1541-1672","978-1-5386-7097-2","10.1109/IS.2018.8710499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710499","virtual reality;body transfer illusion;sensors","Hafnium;Erbium;Hidden Markov models","avatars;human computer interaction;user interfaces","virtual reality study;human avatar;nonhuman avatar;user experience;immersive virtual environments;human-oriented design;malleable virtual avatars;monkey body;human body;human-like virtual avatar experienced body transfer illusion;visual representation;monkey avatar","","2","","41","","9 May 2019","","","IEEE","IEEE Conferences"
"Spatial sound rendering for dynamic virtual environments","B. Cowan; B. Kapralos","Faculty of Business and Information Technology, University of Ontario Institute of Technology, Oshawa, Canada; Faculty of Business and Information Technology, University of Ontario Institute of Technology, Oshawa, Canada","2013 18th International Conference on Digital Signal Processing (DSP)","10 Oct 2013","2013","","","1","6","We present the details of a virtual sound rendering engine (VSRE) that is being developed for virtual environments and serious games. The VSRE incorporates innovative graphics processing unit-based methods to allow for the approximation of acoustical occlusion/diffraction and reverberation effects at interactive rates. In addition, the VSRE includes a GPU-based method that performs the one-dimensional convolution allowing for the incorporation of head-related transfer functions also at interactive rates. The VSRE is being developed as a research tool for examining multi-modal (audio-visual) interactions through the simple manipulation of the acoustic environment and audio parameters (sound quality), that will, through a series of human-based experiments, allow for the testing of the effect of varying these parameters may have on immersion, engagement, and visual fidelity perception within a virtual environment. Finally, we also provide a running time comparison of several one-dimensional convolution implementations.","2165-3577","978-1-4673-5807-1","10.1109/ICDSP.2013.6622815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622815","spatial sound;3D sound;graphics processing unit (GPU);virtual environment;serious games","Graphics processing units;Convolution;Rendering (computer graphics);Virtual environments;Games;Reverberation;Reflection","acoustic signal processing;audio signal processing;convolution;graphics processing units;virtual reality","one-dimensional convolution;visual fidelity perception;audio parameter;acoustic environment;audio-visual interaction;multimodal interaction;head related transfer function;one dimensional convolution;GPU based method;reverberation effect;diffraction effect;acoustical occlusion;graphic processing unit based method;serious game;VSRE;virtual sound rendering engine;dynamic virtual environment;spatial sound rendering","","","","23","","10 Oct 2013","","","IEEE","IEEE Conferences"
"Visualization by Proxy: A Novel Framework for Deferred Interaction with Volume Data","A. Tikhonova; C. D. Correa; K. Ma","Univ. of California, Davis, CA, USA; Univ. of California, Davis, CA, USA; Univ. of California, Davis, CA, USA","IEEE Transactions on Visualization and Computer Graphics","28 Oct 2010","2010","16","6","1551","1559","Interactivity is key to exploration of volume data. Interactivity may be hindered due to many factors, e.g. large data size,high resolution or complexity of a data set, or an expensive rendering algorithm. We present a novel framework for visualizing volumedata that enables interactive exploration using proxy images, without accessing the original 3D data. Data exploration using directvolume rendering requires multiple (often redundant) accesses to possibly large amounts of data. The notion of visualization by proxyrelies on the ability to defer operations traditionally used for exploring 3D data to a more suitable intermediate representation forinteraction - proxy images. Such operations include view changes, transfer function exploration, and relighting. While previous workhas addressed specific interaction needs, we provide a complete solution that enables real-time interaction with large data sets andhas low hardware and storage requirements.","1941-0506","","10.1109/TVCG.2010.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613497","volume visualization;deferred interaction;image-based rendering;volume distortion camera","Attenuation;Rendering (computer graphics);Cameras;Three dimensional displays;Image color analysis;Transfer functions;Lighting","computational complexity;data visualisation;interactive systems;rendering (computer graphics);transfer functions","visualization;deferred interaction;volume data;interactivity;rendering algorithm;data set complexity;proxy images;data exploration;transfer function exploration","Computer Graphics;Computer Simulation;Head;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Models, Anatomic;Tomography, X-Ray Computed","32","2","34","","28 Oct 2010","","","IEEE","IEEE Journals"
"Creating tangible cultural learning opportunities for indigenous dance with motion detecting technologies","M. Khan; P. de Byl","School of Communication, Northwestern University in Qatar; Multimedia and Games, School of Communication & Media, Bond University, Australia","2012 IEEE International Games Innovation Conference","15 Oct 2012","2012","","","1","3","Witnessing and imitating a dance instructor facilitates improved learning opportunities compared with textual, auditory or video reproductions. Learning a complex dance from a verbal description is difficult, as spoken words are slow and cannot encapsulate all the details of a precise maneuver. Unfortunately, in a museum environment, visitors are not afforded the opportunity to become fully immersed in ancient and endangered forms of dance with a live instructor because having instructions inside the museum is not logistically feasible. Instead prerecorded video or paper-based images and notes are presented for perusal. This neither assists in the communication of the importance of this type of cultural heritage nor preserves the performances by passing on skills to future generations. The lack of knowledge transfer in this domain means each year many indigenous dances and their particular movements are lost in time as they become irrelevant and no longer practiced. This is a disgrace as cultural dance represents an expression, social interaction and sometime spiritual representation of a feeling or even an historical narrative in some cultures. It is an important part in understanding a culture, as much as the tangible that remains so highly soughtout by archeologists, yet dance, like other Intangible Cultural Heritage (ICH), without human practice, cannot be preserved in the same manner. To address this issue, we propose herein a system using motion-sensing and gaming technology that can assist in the preservation and knowledge transfer of indigenous dances. GLIDE is an application based on motion detecting technology that targets children in a heritage-related environment in order to create awareness about indigenous dance movements. It will offer kinesthetic clarity and playfulness in a domain very much dominated by flat, non-interactive video content.","2166-675X","978-1-4673-1358-2","10.1109/IGIC.2012.6329834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329834","Heritage;Serious Gaming;Museum Technologie;Augmented Reality;Motion detecting Technologies","Cultural differences;Games;Prototypes;Educational institutions;Knowledge transfer;Humans;Motion segmentation","augmented reality;computer aided instruction;computer games;humanities","tangible cultural learning;indigenous dance;motion detecting technology;museum environment;cultural heritage;knowledge transfer;cultural dance;social interaction;intangible cultural heritage;ICH;motion-sensing technology;gaming technology;GLIDE;heritage-related environment","","3","","15","","15 Oct 2012","","","IEEE","IEEE Conferences"
"Dynamic modeling and control simulation of a haptic device","B. Behzadpour; M. Moghaddam; M. Arbabtafti","Mechatronics Lab in Mechanical Engineering Department, Tarbiat Modares University, Tehran, Iran; Mechanical Engineering Department, Tarbiat Modares University, Tehran, Iran; Mechanical Engineering Department, Shahid Rajaee Teacher Training University, Tehran, Iran","2011 IEEE International Conference on Robotics and Biomimetics","12 Apr 2012","2011","","","2993","2998","In this paper dynamic modeling, simulation and control of a six-DOF haptic interface are discussed, And two control strategies (the admittance control method and the impedance Control with Force Feedback method) are used for the applications of human/virtual environment, interaction. The simulated results of the admittance control are compered with the impedance control with force feedback. In this paper the Adams/Controls, part of the MD Adams software is used for modeling the dynamic behavior of the haptic device. The Adams model of the haptic interface is transferred directly to the MATLAB/Simulink environment. Then the simulation of the virtual environment and the controller design are performed in the MATLAB Simulink environment. In fact a real-time connection between the dynamic model and the controller is made by connection of the two software. Feedback linereazation method is used to control of position in the admitance control method. Then, dynamic equations of the haptic interface is derived, and integrated with the feedback linearization controller.","","978-1-4577-2138-0","10.1109/ROBIO.2011.6181483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181483","Haptic Interface;Impedance Control;Admittance Control;Real-Time;Feedback Linearization","Haptic interfaces;Impedance;Admittance;Mathematical model;Software;Equations;Force","control engineering computing;control system synthesis;force feedback;haptic interfaces;human computer interaction;position control;virtual reality","dynamic modeling;control simulation;haptic device;six-DOF haptic interface;control strategy;admittance control method;impedance control;force feedback method;human-virtual environment;Adams-controls;MD Adams software;dynamic behavior;MATLAB environment;Simulink environment;controller design;real-time connection;feedback linereazation method;position control;dynamic equations;feedback linearization controller","","4","","8","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Surgical strike: interface design across task domains","L. J. Hettinger; R. S. Tannen; E. E. Geiselman; B. J. Brickman; B. W. Moroney; M. W. Haas","Logicon Tech. Services Inc., Dayton, OH, USA; NA; NA; NA; NA; NA","Proceedings Fourth Annual Symposium on Human Interaction with Complex Systems","6 Aug 2002","1998","","","131","136","To what extent can similarities in the general demands placed on human performance in apparently disparate task domains be used to enhance human-machine interface design in each? How can the ""lessons learned"" in one area of human endeavour be systematically and successfully applied to another? What is the nature of the information that permits such transfer to occur? These are important questions currently being encountered in the realm of virtual environment (VE) systems design. Applications of VE technology originally developed for use in advanced tactical aviation settings are currently being used as models for the development of neurosurgical interface concepts. While at first glance the connection between air combat and surgery may seem a distant one at best, there are many important similarities between them. For instance, both task domains share at least ten common, important human performance attributes, each of which may provide key insights into how to import knowledge gained from one domain into the other. Each of these areas have important implications for designing successful technical systems to support human performance. This paper explores the notion that general principles of interface design, derived from a concern with optimizing the above aspects of task performance, can be derived from the study of apparently disparate task domains. To illustrate this general idea, we elaborate how cross-fertilization between neurosurgery and air combat can be used to successfully advance the design of interfaces to support both.","","0-8186-8341-4","10.1109/HUICS.1998.659968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=659968","","Surgery;Biomedical imaging;Medical diagnostic imaging;Humans;Educational technology;Medical treatment;Laboratories;Virtual environment;Neurosurgery;Medical simulation","user interfaces;technology transfer;task analysis;surgery;aerospace computing;medical computing;human factors;virtual reality;military computing","human-machine interface design;task domains;human performance;virtual environment systems design;advanced tactical aviation;neurosurgical interface concepts;air combat;surgery;technical systems;task performance optimization","","1","","5","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Policy Adaptation for Deep Reinforcement Learning-Based Dialogue Management","L. Chen; C. Chang; Z. Chen; B. Tan; M. Gašić; K. Yu","Department of Computer Science and Engineering, Brain Science and Technology Research, Center Shanghai Jiao Tong University, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Speech Lab, Shanghai, China; Department of Computer Science and Engineering, Brain Science and Technology Research, Center Shanghai Jiao Tong University, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Speech Lab, Shanghai, China; Department of Computer Science and Engineering, Brain Science and Technology Research, Center Shanghai Jiao Tong University, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Speech Lab, Shanghai, China; Department of Computer Science and Engineering, Brain Science and Technology Research, Center Shanghai Jiao Tong University, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Speech Lab, Shanghai, China; Engineering Department, University of Cambridge, Cambridge, UK; Department of Computer Science and Engineering, Brain Science and Technology Research, Center Shanghai Jiao Tong University, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Speech Lab, Shanghai, China","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","6074","6078","Policy optimization is the core part of statistical dialogue management. Deep reinforcement learning has been successfully used for dialogue policy optimization for a static pre-defined domain. However, when the domain changes dynamically, e.g. a new previously unseen concept (or slot) which can be then used as a database search constraint is added, or the policy for one domain is transferred to another domain, the dialogue state space and action sets both will change. Therefore, the model structures for different domains have to be different. This makes dialogue policy adaptation/transfer challenging. Here a multi -agent dialogue policy (MADP) is proposed to tackle these problems. MADP consists of some slot-dependent agents (S-Agents) and a slot-independent agent (G-Agent). S-Agents have shared parameters in addition to private parameters for each one. During policy transfer, the shared parameters in S-Agents and the parameters in G-Agent can be directly transferred to the agents in extended/new domain. Simulation experiments showed that MADP can significantly speed up the policy learning and facilitate policy adaptation.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462272","dialogue policy;deep reinforcement learning;adaptation;multi -agent","Adaptation models;Databases;Machine learning;Task analysis;Optimization;Ontologies;Semantics","interactive systems;learning (artificial intelligence);multi-agent systems","dialogue policy adaptation;dialogue policy transfer;multiagent dialogue policy;deep reinforcement learning-based dialogue management;policy learning;G-Agent;policy transfer;slot-independent agent;S-Agents;slot-dependent agents;MADP;dialogue state space;database search constraint;dialogue policy optimization;statistical dialogue management","","2","","22","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Movement Patterns and Trajectories in Three-Dimensional Software Visualization","S. Marcel; K. Rainer; M. Rudel",University of Bremen; University of Bremen; University of Bremen,"2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM)","12 Dec 2019","2019","","","163","174","Software visualization is a growing field of research, in which developers are assisted in understanding and analyzing complex applications by mapping different aspects of a software system onto visual attributes. Under the assumption that virtual reality, due to the higher degree of immersion, may enhance user experience, researchers have begun to port existing visualization techniques to this environment. Oftentimes, layout algorithms and user interaction methods are more or less transferred one-to-one, though little is known about the effect of virtual reality in visual analytics and program comprehension. Moreover, little research on the behavior of developers in different three-dimensional visualization environments has been done yet. This paper extends the results of a previous controlled experiment, in which the EvoStreets visualization technique was compared in different two-and three-dimensional environments. In the original experiment, we could not find evidence that any of the environments, namely, 2D, 2.5D, and virtual reality, effects the time required to find an answer or the correctness of the given answer. However, we found indications that movement patterns differ between the 2.5D and the virtual reality environments. For this paper, we analyzed and refined the movement trajectories that have been recorded in the previous experiment. We found significant differences for some of the tasks that had to be solved by the participants. In particular, we found evidence that the path length, average speed, and occupied volume differ. Though we could find significant correlations between these metrics and correctness, we found indications that there is a correlation with time, which, in turn, differs significantly between the 2.5D and the VR environments for many tasks. These findings may have implications on the design of visualizations, interactions, and recommendation systems for these different environments.","2470-6892","978-1-7281-4937-0","10.1109/SCAM.2019.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930890","Software Visualization;2.5D Environment;Virtual Reality Environment;Movement patterns;Trajectories","Conferences","data visualisation;software engineering;virtual reality","three-dimensional software visualization;complex applications;software system;visual attributes;user experience;layout algorithms;user interaction methods;visual analytics;three-dimensional visualization environments;EvoStreets visualization technique;movement patterns;virtual reality environments;movement trajectories;VR environments;2.5D environments","","1","","59","","12 Dec 2019","","","IEEE","IEEE Conferences"
"Generic and systematic evaluation of haptic interfaces based on testbeds","Evren Samur; Fei Wang; U. Spaelter; H. Bleuler","Laboratory of Robotic Systems, Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Laboratory of Robotic Systems, Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Laboratory of Robotic Systems, Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Laboratory of Robotic Systems, Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland","2007 IEEE/RSJ International Conference on Intelligent Robots and Systems","10 Dec 2007","2007","","","2113","2119","The purpose of evaluation procedures is to achieve both qualitative and quantitative statements on haptic rendering realism and performance. Since a haptic interface provides an interaction between a user and a virtual environment, fidelity of a haptic interface directly affects the performance. To our knowledge, a standard, generic and reusable validation method which comprehensively addresses all the attributes of haptic feedback has not been realized yet. Despite the large number of human factor studies, only few of them have been proposed as well for haptic interface performance measurements. For this reason, we review validation procedures for haptic rendering and propose an evaluation method based on testbeds to obtain a systematic haptic interface assessment. We integrated the approaches of human factor studies into the testbeds to obtain a simple and yet complete measure of human-machine interaction performance. The testbeds were tested on a haptic interface, the IHP of Xitact SA, and performance results are presented. In the testbeds, performance metrics for generic haptic interaction tasks are expressed in terms of information transfer (bits) and sensory thresholds which are indeed device specific benchmark metrics. Thus, the suitability of a haptic interface for a defined task can be verified, device comparisons become possible and the obtained information can be used to identify possible improvements.","2153-0866","978-1-4244-0911-2","10.1109/IROS.2007.4399522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4399522","","Haptic interfaces;System testing;Rendering (computer graphics);Human factors;Measurement;Virtual environment;Feedback;Anthropometry;Man machine systems;Benchmark testing","haptic interfaces;human factors;rendering (computer graphics);virtual reality","haptic interface;haptic rendering;virtual environment;haptic feedback;human factor;human-machine interaction;generic haptic interaction;information transfer;sensory threshold","","7","","36","","10 Dec 2007","","","IEEE","IEEE Conferences"
"Socializing with robots: Human-robot interactions within a virtual environment","A. Richert; M. A. Shehadeh; S. L. Müller; S. Schröder; S. Jeschke","ZLW of the RWTH Aachen University, Germany; IMA/ZLW & IfU of the RWTH Aachen University, Germany; IMA/ZLW & IfU of the RWTH Aachen University, Germany; IMA/ZLW & IfU of the RWTH Aachen University, Germany; IMA/ZLW & IfU of the RWTH Aachen University, Germany","2016 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)","7 Nov 2016","2016","","","49","54","Robots are already physically supporting humans within multiple processes, but as a step further, the robots will be able to identify and adapt to any individual strengths and become the flawless co-workers needed. One of the questions is whether in other fields of social robotics, e.g. in ergonomics, existing knowledge about human teams can be transferred into the design of hybrid teams and the shaping of human-computer interactions. These developments serve the appearance of Industry 4.0, which is directly correlated to the modernization of productions that are directly involved in defining the concept of hybrid human-robot-teams. This is a concept paper that describes the investigation whether the appearance of the robot and its accuracy while fulfilling the task influence the stress level of the human, his cooperation behavior and trust towards the robot, and as an overall result the performance of the cooperative work in a complete virtual environment. The participants will be given a task to accomplish with the aid of their virtual robot partner and rely on their features and abilities, they have to act as efficiently as possible, which gives room to investigate the teamwork over various team development stages, stress, trust, and performance. The study will take place in August 2016, where the results will be published shortly afterwards.","2162-7576","978-1-5090-4079-7","10.1109/ARSO.2016.7736255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736255","","Robot kinematics;Service robots;Teamwork;Stress;Human-robot interaction;Facsimile","control engineering computing;factory automation;human-robot interaction;industrial robots;production engineering computing","robot socialization;human-robot interactions;virtual environment;social robotics;hybrid team design;human-computer interactions;Industry 4.0;hybrid human-robot-teams;cooperation behavior;trust;cooperative work;virtual robot partner;teamwork;team development stages","","3","","22","","7 Nov 2016","","","IEEE","IEEE Conferences"
"Bypassing the cloud: Peer-assisted event dissemination for augmented reality games","B. Richerzhagen; D. Stingl; R. Hans; C. Gross; R. Steinmetz","Multimedia Communications Lab, Technische Universität Darmstadt; Multimedia Communications Lab, Technische Universität Darmstadt; Multimedia Communications Lab, Technische Universität Darmstadt; Multimedia Communications Lab, Technische Universität Darmstadt; Multimedia Communications Lab, Technische Universität Darmstadt","14-th IEEE International Conference on Peer-to-Peer Computing","23 Oct 2014","2014","","","1","10","The rising number of mobile devices and their increasing computational capabilities enable new interactive context-sensitive applications. Popular examples are augmented reality games such as Google's Ingress, where users interact with each other in the real world while being part of the game at the same time. This local interaction pattern in the real world as well as in the game is not reflected in the underlying communication pattern. Every locally generated game event is first transferred to a backend server via a cellular connection, from where it is then further disseminated to all players within the given area of interest. This communiation pattern introduces significant delays and limits the interactivity of the game. In this work, we propose an event dissemination system that exploits the locality characteristics of mobile augmented reality games to (i) enable and configure local peer-to-peer dissemination of events when appropriate and (ii) reconfigure or replace the utilized peer-to-peer protocol to adapt to a wide range of requirements. Through extensive evaluation we show that the proposed system decreases the delivery delay by a factor of eight compared to the existing communication pattern, leading to significantly increased information accuracy.","2161-3567","978-1-4799-6201-3","10.1109/P2P.2014.6934296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934296","","Games;Peer-to-peer computing;Protocols;Mobile communication;Augmented reality;Ad hoc networks;Engines","augmented reality;computer games;mobile computing;peer-to-peer computing","peer-assisted event dissemination system;mobile augmented reality games;games locality characteristics;peer-to-peer events dissemination;peer-to-peer protocol;delivery delay reduction;communication pattern","","14","","17","","23 Oct 2014","","","IEEE","IEEE Conferences"
"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?","A. Kadian; J. Truong; A. Gokaslan; A. Clegg; E. Wijmans; S. Lee; M. Savva; S. Chernova; D. Batra","Facebook AI Research, Menlo Park, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Facebook AI Research, Menlo Park, CA, USA; Facebook AI Research, Menlo Park, CA, USA; Facebook AI Research, Menlo Park, CA, USA; Oregon State University, Corvallis, OR, USA; Facebook AI Research, Menlo Park, CA, USA; Facebook AI Research, Menlo Park, CA, USA; Facebook AI Research, Menlo Park, CA, USA","IEEE Robotics and Automation Letters","26 Aug 2020","2020","5","4","6670","6677","Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation - developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots - transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections - abusing collision dynamics to `slide' along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCCSucc from 0.18 to 0.844) - increasing confidence that in-simulation comparisons will translate to deployed systems in reality.","2377-3766","","10.1109/LRA.2020.3013848","NSF; Air Force Research Laboratory; Defense Advanced Research Projects Agency; ONR YIPs; ARO PECASE; Amazon Catalyst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158349","Visual-based navigation;reinforcement learning","Robots;Navigation;Task analysis;Predictive models;Correlation;Measurement","artificial intelligence;collision avoidance;computer simulation;human-robot interaction;learning (artificial intelligence);mobile robots;multi-agent systems;virtual reality","one-line code change;simulation predict Real-world performance;embodied PointGoal navigation;Habitat-PyRobot Bridge;robots-transferring simulation-trained agents;LoCoBot platform;Sim-vs-Real Correlation Coefficient;Habitat-Sim M. Savva et al;Sim2Real Predictivity;virtualized replica;SRCC;AI agents learning;HaPy;agent simulation","","3","","37","IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"Perceptually Augmented Simulator Design","T. Edmunds; D. K. Pai","University of British Columbia, Vancouver; University of British Columbia, Vancouver","IEEE Transactions on Haptics","9 Mar 2012","2012","5","1","66","76","Training simulators have proven their worth in a variety of fields, from piloting to air-traffic control to nuclear power station monitoring. Designing surgical simulators, however, poses the challenge of creating trainers that effectively instill not only high-level understanding of the steps to be taken in a given situation, but also the low-level “muscle-memory” needed to perform delicate surgical procedures. It is often impossible to build an ideal simulator that perfectly mimics the haptic experience of a surgical procedure, but by focussing on the aspects of the experience that are perceptually salient we can build simulators that effectively instill learning. We propose a general method for the design of surgical simulators that augment the perceptually salient aspects of an interaction. Using this method, we can increase skill-transfer rates without requiring expensive improvements in the capability of the rendering hardware or the computational complexity of the simulation. In this paper, we present our decomposition-based method for surgical simulator design, and describe a user-study comparing the training effectiveness of a haptic-search-task simulator designed using our method versus an unaugmented simulator. The results show that perception-based task decomposition can be used to improve the design of surgical simulators that effectively impart skill by targeting perceptually significant aspects of the interaction.","2329-4051","","10.1109/TOH.2011.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5975145","Haptic I/O;artificial;augmented;and virtual realities;life and medical sciences;surgical simulation.","Haptic interfaces;Surgery;Training;Needles;Rendering (computer graphics);Surface roughness;Rough surfaces","augmented reality;computer based training;haptic interfaces;medical computing;surgery","perceptually augmented simulator design;training simulator;air-traffic control;piloting;nuclear power station monitoring;surgical simulator;low-level muscle-memory;surgical procedure;haptic experience;salient interaction aspect;skill-transfer rate;rendering hardware;computational complexity;decomposition-based method;user study;haptic-search-task simulator;unaugmented simulator;perception-based task decomposition","","2","","21","","4 Aug 2011","","","IEEE","IEEE Journals"
"Conceptualization of an ICU Infrastructure for Simulation Based Education in Medical Engineering & eHealth","M. Forjan; V. David; M. Wagner; L. Dolesch; M. Lechner; S. Sauermann","Department Life Sciences, University of Applied Sciences Technikum Wien, Vienna, Austria; Department Life Sciences, University of Applied Sciences Technikum Wien, Vienna, Austria; Division of Neonatology, Pediatric Intensive Care and Neuropediatrics, Medical University of Vienna, Vienna, Austria; gsm Gesellschaft für Sicherheit in der Medizintechnik GmbH, Vienna, Austria; gsm Gesellschaft für Sicherheit in der Medizintechnik GmbH, Vienna, Austria; Department Life Sciences, University of Applied Sciences Technikum Wien, Vienna, Austria","2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","7 Oct 2019","2019","","","4186","4189","The use of simulation-based training is gaining importance in medical as well as engineering related education. The complex environment of an intensive care unit is characterized by a high need of interaction between clinical as well as technical components and views. These diverse interactions and the connected requirements are the focus for the presented simulation infrastructure, enabling research, education and training. The presented concept of a modular and flexible intensive care environment provides a high degree of interoperability and flexibility for individual research questions and full support of connectivity for typical clinical workflows. The presented simulation and testing bed will allow both, education for engineering and medical students using patient simulation and simultaneous data transfer as well as research on medical workflows, infrastructural demands and connectivity conformance questions.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8856949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8856949","","Training;Solid modeling;Medical devices;Interoperability;Laboratories;Biomedical imaging","biomedical education;computer based training;health care;medical computing;patient care;patient monitoring","medical students;patient simulation;medical workflows;infrastructural demands;connectivity conformance questions;ICU infrastructure;simulation based education;medical engineering;simulation-based training;engineering related education;intensive care unit;diverse interactions;connected requirements;simulation infrastructure;modular care environment;flexible intensive care environment;interoperability;clinical workflows;testing bed;eHealth","Critical Care;Education, Medical;Humans;Intensive Care Units;Patient Simulation;Simulation Training;Telemedicine","","","12","","7 Oct 2019","","","IEEE","IEEE Conferences"
"Training of high-skilled workers using exercisers and simulators","L. Steshina; I. Petukhov; I. Tanryerdiev; P. Kurasov; A. Glazyrin","Volga State University of Technology,Department of Radio Technical,Yoshkar-Ola,Russia; Volga State University of Technology,Department of Radio Technical,Yoshkar-Ola,Russia; Volga State University of Technology,Department of Radio Technical,Yoshkar-Ola,Russia; Volga State University of Technology,Department of Radio Technical,Yoshkar-Ola,Russia; Volga State University of Technology,Department of Radio Technical,Yoshkar-Ola,Russia","2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)","20 Nov 2020","2019","","","134","139","The paper considers a new method of personalized training of logging machine operators. It deals with factors affecting efficiency of the logging process as well as with an influence of experience and training level of an human-operator on efficiency of the production process. Mathematical representation of an operator in form of transfer functions is presented. The paper presents a model of interaction between a human-operator and production equipment in the transfer functions and offers an evaluation method for a person's rate of making an operating decision. The method for evaluation of operator's professionally important qualities is presented. The key idea using the tests for the measuring of motor and sensory layers. The tests describe typical model of reaction for operator and activate motor and sensory layers as in real professional case. The imitation model shows that during four seconds of the control action the beam moved by 1.281 m and a delay of the beam movement start after the control signal start for 0.578 sec. Therefore quality of operator's control is definitely determined by the following parameters: a time constant that means a response time of a neuromuscular system and human adaptive abilities and describes a rate of control action development; a ratio of human internal feedback and describes accuracy of the control action development; and an operator's response delay time.","","978-1-7281-6109-9","10.1109/EECS49779.2019.00036","NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257542","Machine;Operator;Performance;Harvester;Simulator","Mathematical model;Training;Process control;Tools;Vegetation;Transfer functions;Production equipment","computer based training;human computer interaction;production engineering computing;production equipment;transfer functions;well logging","personalized training;logging process;production process;operator response delay time;neuromuscular system;human-operator training level;logging machine operators;simulators;high-skilled worker training;exercisers;human internal feedback;control action development;human adaptive abilities;control signal start;beam movement start;imitation model;sensory layers;evaluation method;production equipment;transfer functions;mathematical representation;size 1.281 m;time 0.578 s","","","","23","","20 Nov 2020","","","IEEE","IEEE Conferences"
"Interaction between Sunlight and the Sky Colour with 3D Objects in the Outdoor Virtual Environment","S. M. Halawani; M. S. Sunar","Fac. of Comput. & Inf. Technol., King Abdulaziz Univ., Rabigh, Saudi Arabia; Fac. of Comput. Sci. & Inf. Syst., Univ. Teknol. Malaysia, Skudai, Malaysia","2010 Fourth Asia International Conference on Mathematical/Analytical Modelling and Computer Simulation","21 Jun 2010","2010","","","470","475","The sky has always been the crucial element in modeling the background of an outdoor scene. The position of the sun during the day gives a different impact on the sky colour. The sky colour indirectly affects the colour of the objects which were exposed to the lighting, such as the orangish red colour of the clouds seen during sunsets. Consequently, this study will emphasize on how to produce illuminated 3D objects based upon the effects of interaction between the sunlight and sky. A two-part program was developed for this study. The first part of the program concentrates on producing the correct sky colour depending on the position of the sun using Perez's function. The sky colour will be plotted on the sky dome which in turn will become a closed environment for the clouds. The interaction will occur in the second part of the program where the energy transfer in the dome environment with color of the sky as the main source illumination, resulting in the colour bleeding effect when using the radiosity approach. The result from this study is applicable to daylight modeling of building by showing the lighting effects from the sun and the sky.","2376-1172","978-1-4244-7197-3","10.1109/AMS.2010.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489133","3D objects;Sun;Sky;Perez function;Radiosity","Virtual environment;Lighting;Sun;Color;Computer graphics;Hardware;Earth;Clouds;Physics;Asia","colour graphics;lighting;object recognition;sky brightness;sunlight;virtual reality","sunlight colour;sky colour;3D object illumination;Perez function;lighting effects;outdoor virtual environment","","2","","18","","21 Jun 2010","","","IEEE","IEEE Conferences"
"A Fundamental Linear Systems Conflict Between Performance and Passivity in Haptic Rendering","P. G. Griffiths; R. B. Gillespie; J. S. Freudenberg","Department of Mechanical Engineering, Johns Hopkins University, Baltimore, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","IEEE Transactions on Robotics","4 Feb 2011","2011","27","1","75","88","This paper analyzes inherent conflicts between model-matching goals of haptic rendering and passivity requirements for coupled stability. We apply results from complex analysis to prove that certain linear passive virtual environments cannot be rendered passively with a desired level of accuracy and over a given finite bandwidth. One practical consequence is that, under appropriate hypotheses, passivity will be violated when accurately rendering inertia in a virtual environment that is less than the inertia of the uncompensated hardware dynamics. In a related result, we show that there exists a waterbed-type tradeoff between performance and phase lag in the rendered dynamics. Both design constraints arise from feedback-bandwidth limitations and not sampled-data effects, quantization, or nonlinearities. The key to our analysis is an interpretation of a Bode gain-phase integral relationship that relates magnitude at low frequencies to phase at high frequencies. The performance limitation and the waterbed tradeoff are illustrated through an experimental study.","1941-0468","","10.1109/TRO.2010.2088751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648722","Bode gain–phase integral;haptics and haptic interfaces;passivity;physical human–robot interaction","Haptic interfaces;Rendering (computer graphics);Transfer functions;Stability criteria;Virtual environment;Hardware","Bode diagrams;haptic interfaces;human-robot interaction;rendering (computer graphics);virtual reality","linear system conflict;haptic rendering;coupled stability;linear passive virtual environment;Bode gain-phase integral relationship;human-robot interaction","","21","","28","","3 Dec 2010","","","IEEE","IEEE Journals"
"Motion-based virtual reality cognitive training targeting executive functions in acquired brain injury community-dwelling individuals: A feasibility and initial efficacy pilot","G. Shochat; S. Maoz; A. Stark-Inbar; B. Blumenfeld; D. Rand; S. Preminger; Y. Sacher","Department of Traumatic Brain Injury, Loewenstein Rehabilitation Center, Ra'anana, Israel; Intendu Ltd., Herzliya, Israel; Intendu Ltd., Herzliya, Israel; Intendu Ltd., Herzliya, Israel; Department of Occupational Therapy, Tel Aviv University, Israel; Intendu Ltd., Herzliya, Israel; Department of Traumatic Brain Injury, Loewenstein Rehabilitation Center, Ra'anana, Israel","2017 International Conference on Virtual Rehabilitation (ICVR)","14 Aug 2017","2017","","","1","8","Acquired brain injury (ABI) is a leading cause of long-term cognitive disability, often involving deficits in executive functions (EF). ABI patients usually stop receiving cognitive treatment when leaving the rehabilitation facility or shortly thereafter, due to the high cost of therapy sessions and the mobility requirement to access therapy. Software solutions offer a promising tool for accessible and affordable cognitive rehabilitation in the home environment. However, research provides limited evidence for effective transfer of benefits from computerized cognitive training to real-life functions. Virtual reality (VR) exergames using motion-interaction offer a more realistic and natural training environment, and are therefore expected to facilitate a more effective transfer. Although commercial exergames may bring about some cognitive gains, they usually do not target cognitive functions directly. Here we describe a novel exergames platform, the Active Brain Trainer (ABT), designed to directly target EF, using games in multiple realistic contexts. The software adapts in real-time to the patient's behavior, providing feedback and rewards, and hence may enhance usability and compliance. The primary goal of the current study is to assess the feasibly and acceptability of this platform for community-dwelling ABI patients during the chronic phase. A secondary goal is to assess the initial efficacy on EF and functional benefits from program training. Participants were instructed to use the games for 15-20 sessions. Neuropsychological assessments of EF and daily life functions were performed before and after training. Participants also filled a satisfaction questionnaire following training. All training and assessments were conducted in the participants' homes. Game performance was recorded throughout training sessions. Preliminary results from the six ABI patients who successfully completed the program so far show no adverse effects. Participants reported enjoyment and satisfaction from training. Participants performed increasingly more challenging EF tasks within game environments. Initial results show improvements in functional tasks and most executive neuropsychological assessments following training. Additional participants are currently being trained to increase the power of the results. These preliminary findings support the feasibility and potential efficacy of the motion-based cognitive training of EF for community-dwelling individuals with ABI.","2331-9569","978-1-5090-3053-8","10.1109/ICVR.2017.8007530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007530","cognitive training;exergames;motion-based;acquired brain injury;community-dwelling;executive functions","Games;Training;Avatars;Radiation detectors;Brain injuries;Medical treatment;Atmospheric measurements","brain;cognition;computer games;injuries;medical computing;neurophysiology;patient rehabilitation;virtual reality","motion-based virtual reality cognitive training;executive functions;acquired brain injury community-dwelling individuals;ABI;long-term cognitive disability;rehabilitation facility;therapy sessions;mobility requirement;home environment;computerized cognitive training;VR exergames;motion interaction;active brain trainer;ABT;chronic phase;neuropsychological assessments;daily life functions;game environments;functional tasks","","","","69","","14 Aug 2017","","","IEEE","IEEE Conferences"
"Transferring an Interactive Display Service to the Virtual Reality","J. Naber; C. Krupitzer; C. Becker","Univ. of Mannheim, Mannheim, Germany; Univ. of Mannheim, Mannheim, Germany; Univ. of Mannheim, Mannheim, Germany","2017 IEEE International Conference on Smart Computing (SMARTCOMP)","15 Jun 2017","2017","","","1","8","In today's world, smart devices - such as laptops, smartphones, or smart watches - may substitute the need for physical presence of people. By offering a virtual representation, these devices support social interactions, e.g., telecommuting, remote studying through e-learning, or long-distance relationships. With audio and video streaming platforms like Skype, Apple FaceTime, or Google Hangouts it is possible to stay in contact with friends and colleagues. However, these platforms only allow communication between two peers or at most in very small groups. Additionally, they are very inflexible regarding content sharing and support only limited, defined use cases. In this paper, we present an extension to our pervasive display service for a remote virtual environment. This allows an immersive participation at remote activities in larger groups, while retaining the flexibility regarding joining devices of a pervasive middleware. Our contributions are threefold. First, we present a reusable design for integrating an interactive display service into virtual environments. Second, we implement our approach using the BASE middleware and the Unity 3D engine. Third, we evaluate our approach in a smart meeting room scenario.","","978-1-5090-6517-2","10.1109/SMARTCOMP.2017.7947054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947054","","Virtual environments;Middleware;Smart phones;Access control;Companies","middleware;ubiquitous computing;virtual reality","interactive display service;virtual reality;smart devices;virtual representation;social interactions;audio streaming;video streaming;pervasive display service;remote virtual environment;immersive participation;pervasive middleware;reusable design;BASE middleware;Unity 3D engine;smart meeting room","","2","","10","","15 Jun 2017","","","IEEE","IEEE Conferences"
"Driver's gaze zone estimation by transfer learning","I. R. Tayibnapis; M. Choi; S. Kwon","DGIST, Daegu, Republic of Korea; DGIST, Daegu, Republic of Korea; DGIST, Daegu, Republic of Korea","2018 IEEE International Conference on Consumer Electronics (ICCE)","29 Mar 2018","2018","","","1","5","Estimating driver's gaze zone has very important role to support advanced driver assistant system (ADAS). The gaze estimation can monitor the driver focus and indirectly control the user interface/user experience (UI/UX) on a windshield using augmented reality-head up display (AR-HUD). However, to train gaze zone estimator as a classification task, someone pays huge costs to gather a large amount of annotated dataset. To reduce the labor work, we used a transfer-learning method using pre-trained CNN model to project the gaze estimation task by regression on mobile devices that have large and reliable dataset into new classification task to overcome lack of annotated dataset for gaze zone estimation. We tested the proposed method to our own building simulation test bed. The result is shown in validation accuracy around 99.01 % and test accuracy with unseen driver around 60.25 % for estimating 10 gaze zones in-vehicle.","2158-4001","978-1-5386-3025-9","10.1109/ICCE.2018.8326308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326308","","Estimation;Tablet computers;Support vector machines;Automobiles;Mobile handsets;Feature extraction","augmented reality;driver information systems;gaze tracking;head-up displays;human computer interaction;learning (artificial intelligence);mobile computing;motion estimation;neural nets;pattern classification;regression analysis;traffic engineering computing;user interfaces","transfer learning;advanced driver assistant system;augmented reality-head up display;gaze zone estimator;classification task;user interface;user experience;driver gaze zone estimation;pre-trained CNN model;regression;mobile devices","","","","16","","29 Mar 2018","","","IEEE","IEEE Conferences"
"Improved haptic interaction control with force filter compensator","J. Podobnik; M. Munih","Fac. of Electr. Eng., Ljubljana Univ., Slovenia; Fac. of Electr. Eng., Ljubljana Univ., Slovenia","9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005.","29 Aug 2005","2005","","","160","163","Stability of a haptic interface is essential for safe and quality haptic interaction. This paper addresses the contact instability of admittance control haptic interface in free space. Experiments with special dedicated system for measuring grasp force have been performed to explore conditions of contact instability. Baseline experimental results are here compared to simulations from a model of haptic interaction. The model serves also as a basis for stability and performance improvements with a special compensator filter for force filtering. Experimental and simulation results both confirm stability improvements.","1945-7901","0-7803-9003-2","10.1109/ICORR.2005.1501075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501075","","Force control;Haptic interfaces;Filters;Humans;Virtual environment;Admittance;Stability analysis;Impedance;Transfer functions;Force measurement","haptic interfaces;force measurement;stability;compensation;robot dynamics","force filter compensator;haptic interaction control;haptic interface stability;admittance control haptic interface;grasp force measurement","","2","","11","","29 Aug 2005","","","IEEE","IEEE Conferences"
"Learning Hand Movement Interaction Control Using RNNs: From HHI to HRI","O. S. Oguz; B. M. Pfirrmann; M. Guo; D. Wollherr","Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany","IEEE Robotics and Automation Letters","13 Aug 2018","2018","3","4","4100","4107","A key problem in robotics is enabling an autonomous agent to perform human-like arm movements in close proximity to another human. However, modeling the human decision and control process of the movement during dyadic interaction presents a challenge. Although, most prior approaches rely on multicomponent robot motion planning architectures, we use data of two humans performing interfering arm reaching movements to extract and transfer interaction behavior control skill to a robotic agent. A recurrent neural network-based framework is constructed to learn a policy that computes control signals for a robot end effector in order to replace one human. The learned policy is benchmarked against unseen interaction data and a state-of-the-art learning from demonstration framework in simulated scenarios. We compare several architectures and investigate a new activation function of three stacked tanh(). The results show that the proposed framework successfully learns a policy to imitate human movement behavior control during dyadic interaction. The policy is transferred to a real robot and its feasibility for close-proximity human-robot interaction is shown.","2377-3766","","10.1109/LRA.2018.2862923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424874","Human-robot interaction;human-in-the-loop;learning from demonstration;recurrent neural networks","Recurrent neural networks;Task analysis;Manipulators;Computer architecture;Hidden Markov models;Convergence","end effectors;human-robot interaction;learning by example;mobile robots;motion control;neurocontrollers;path planning;recurrent neural nets;transfer functions","hand movement interaction control;autonomous agent;control process;dyadic interaction;multicomponent robot motion;interaction behavior control skill;robotic agent;recurrent neural network-based framework;robot end effector;human movement behavior control;close-proximity human-robot interaction;human-human interaction;HRI;HHI;activation function;learning from demonstration framework","","1","","26","","3 Aug 2018","","","IEEE","IEEE Journals"
"Effects of mobile AR-enabled interactions on retention and transfer for learning in art museum contexts","Weiquan Lu; Linh-Chi Nguyen; Teong Leong Chuah; Ellen Yi-Luen Do","Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore; Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore; Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore; Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore","2014 IEEE International Symposium on Mixed and Augmented Reality - Media, Art, Social Science, Humanities and Design (ISMAR-MASH'D)","27 Oct 2014","2014","","","3","11","In this paper, we describe an experiment to study the effect of mobile Augmented Reality (AR) on learning in art museum contexts. We created six original paintings and placed them in a mini art museum. We then created an AR application on the iPad to enable the artist to visually augment each painting by introducing animation. We then measured the ability of the visitors to remember the appearance of the paintings after 24 hours, as well as their ability to objectify the paintings. Experiment results show that while AR does improve retention and transfer of such art information, the benefits of AR are mediated by other factors such as interference from other elements of the exhibition, as well as subjects' own prior art experience and training. The use of AR may also produce unexpected benefits, such as providing users with a new perspective of the artwork, as well as increasing their curiosity and encouraging them to experiment with the technology. Such benefits may potentially improve the chances for learning and analytical activities to take place.","2381-8360","978-1-4799-6887-9","10.1109/ISMAR-AMH.2014.6935432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935432","Evaluation;Learning;Museums;Software","Painting;Art;Tablet computers;Animation;Protocols;Context;Training","augmented reality;computer aided instruction;computer animation;mobile computing;museums","mobile AR enabled interaction;augmented reality;art museum context;learning;mini art museum;iPad;animation;art experience;art training","","2","","19","","27 Oct 2014","","","IEEE","IEEE Conferences"
"Empirical Study of Non-Reversing Magic Mirrors for Augmented Reality Anatomy Learning","F. Bork; R. Barmaki; U. Eck; K. Yu; C. Sandor; N. Navab","Tech. Univ. Munchen, Munich, Germany; Johns Hopkins Univ., Baltimore, MD, USA; Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany; Nara Inst. of Technol., Nara, Japan; Tech. Univ. Munchen, Munich, Germany","2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","23 Nov 2017","2017","","","169","176","Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)? In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.","","978-1-5386-2943-7","10.1109/ISMAR.2017.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115415","","Mirrors;Education;Augmented reality;Medical diagnostic imaging;Games","augmented reality;biomedical education;computer aided instruction;data visualisation;medical computing;psychology;teaching","NRMM;RMM;future MM systems;augmented reality anatomy learning;impeded ability;patient;traditional anatomy learning resources;Augmented Reality Magic Mirrors;anatomy teaching resources;virtual anatomy;MM setups;real-world physical mirrors;intriguing perceptual questions;medicine;virtual organs;knowledge transfer;nonreversing MM","","4","","57","","23 Nov 2017","","","IEEE","IEEE Conferences"
"A framework for human-computer interaction using dynamic time warping and neural network","D. K. Vishwakarma; S. Ansari","Department of Electronics and Communication Engineering, Delhi Technological University, Delhi, India; Department of Electronics and Communication Engineering, Delhi Technological University, Delhi, India","2017 International Conference on Inventive Computing and Informatics (ICICI)","28 May 2018","2017","","","242","246","Due to incapability in speaking and hearing, there always exist a group of people in the community who face difficulties in communication. These people use some symbols and gestures to convey their messages and receive their messages, this form of communication is known as Sign Language. We have provided a solution based on dynamic time warping (DTW) for the first module and software based solution for the second module by exploiting the latest technology of Microsoft Kinect depth camera which tracks the 20 joint location of human beings. In sign to speech/text conversion block, the actor performs some valid gestures within the Kinect field of view. The gestures are taken up by the Kinect sensor and then interpreted by comparing it with already stored trained gestures in the dictionary. After the sign is recognized, it is copied to the respective word which is transferred to the speech conversion and text conversion module to produce the output. In the second block, which is a speech to sign/gesture conversion, the person speaks in Kinect field of view which is taken by the Kinect, and the system converts speech into text, and corresponding word is mapped into predefined gesture which is played on the screen. This way a disabled person can visualize the spoken word. The accuracy of sign to speech module is found to be 87%, and that of speech to gesture module is 91.203%.","","978-1-5386-4031-9","10.1109/ICICI.2017.8365346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365346","Sign language descriptor;Dynamic time wrapping;Kinect;gesture","Assistive technology;Gesture recognition;Conferences;Torso;Informatics;Auditory system;Cameras","cameras;handicapped aids;human computer interaction;neural nets;sign language recognition;speech recognition;text analysis","speech module;gesture module;human-computer interaction;dynamic time warping;neural network;speaking hearing;Sign Language;software based solution;Microsoft Kinect depth camera;speech/text conversion block;valid gestures;Kinect sensor;stored trained gestures;speech conversion;sign/gesture conversion;predefined gesture","","","","11","","28 May 2018","","","IEEE","IEEE Conferences"
"3D binaural sound reproduction using a virtual ambisonic approach","M. Noisternig; T. Musil; A. Sontacchi; R. Holdrich","Inst. of Electron. Music & Acoust., Univ. of Music & Dramatic Arts, Graz, Austria; Inst. of Electron. Music & Acoust., Univ. of Music & Dramatic Arts, Graz, Austria; Inst. of Electron. Music & Acoust., Univ. of Music & Dramatic Arts, Graz, Austria; Inst. of Electron. Music & Acoust., Univ. of Music & Dramatic Arts, Graz, Austria","IEEE International Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems, 2003. VECIMS '03. 2003","4 Sep 2003","2003","","","174","178","Convincing binaural sound reproduction via headphones requires filtering the virtual sound source signals with head related transfer functions (HRTFs). Furthermore, humans are able to improve their localization capabilities by small unconscious head movements. Therefore it is important to incorporate head-tracking. This yields the problem of high-quality, time-varying interpolation between different HRTFs. A further improvement of human localization accuracy can be done by considering room simulation yielding a huge amount of virtual sound sources. To increase the computational efficiency of the proposed system, a virtual ambisonic approach is used, that result in a bank of time-invariant HRTF filter independent of the number of sources to encode.","","0-7803-7785-0","10.1109/VECIMS.2003.1227050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227050","","Loudspeakers;Integral equations;Acoustic noise;Interpolation;Filters;Transfer functions;Humans;Holography;Partial differential equations;Electronic music","sound reproduction;transfer functions;audio signal processing;virtual reality;headphones;time-varying filters","3D binaural sound reproduction;virtual ambisonic approach;headphone;virtual sound source signal;head related transfer function;human localization capability;head movement;head-tracking;high-quality interpolation;time-varying interpolation;human localization accuracy;room simulation;computational efficiency;time-invariant head related transfer function filter;source number independence","","17","2","15","","4 Sep 2003","","","IEEE","IEEE Conferences"
"Importance of binaural cues of depth in low-resolution audio-visual 3D scene reproductions","D. Salvati; C. Drioli; F. Fontana; G. L. Foresti","Department of Mathematics, Computer Science and Physics, University of Udine, Italy; Department of Mathematics, Computer Science and Physics, University of Udine, Italy; Department of Mathematics, Computer Science and Physics, University of Udine, Italy; Department of Mathematics, Computer Science and Physics, University of Udine, Italy","2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","16 Dec 2018","2018","","","1","6","The following topics are dealt with: acoustic signal processing; virtual reality; audio signal processing; hearing; music; transfer functions; headphones; architectural acoustics; rendering (computer graphics); human computer interaction.","","978-1-5386-5713-3","10.1109/SIVE.2018.8577121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8577121","3D Sound;Stereoscopic 3D Video;Depth Perception;Stereo Dipole;Low-Resolution","Three-dimensional displays;Visualization;Stereo image processing;Loudspeakers;Ear;Games;Virtual environments","acoustic signal processing;audio signal processing;hearing;music;virtual reality","acoustic signal processing;virtual reality;audio signal processing;hearing;music;transfer functions;headphones;architectural acoustics;rendering (computer graphics);human computer interaction","","","","28","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Semiotic Differences of Macintosh Os X & Microsoft Windows 7 Based on Metaphors and Interpretation","R. S. Abdulhassan; M. Masood; A. Hosam","Center for IT & Multimedia, Univ. Sains Malaysia, Minden, Malaysia; Center for IT & Multimedia, Univ. Sains Malaysia, Minden, Malaysia; Center for IT & Multimedia, Univ. Sains Malaysia, Minden, Malaysia","2011 Second International Conference on Intelligent Systems, Modelling and Simulation","14 Mar 2011","2011","","","209","213","This study utilized the Souza's semiotic engineering theory of HCI to user interface metaphors. The main aim of using Metaphors is the ability to determine the potential meaning in ongoing cognitive activities. The data were collected through the “Icon Recognition Test” and the interview questions. In addition, the users' navigation through Mac and Win7 operating systems include certain categories of metaphors. The examination result indicated that 90% of the “structure metaphors” appeared for the desktop icons and some of the Power Point environment. The findings of the study indicated that Win7 and Mac users did not recognize their system metaphors, which reflects how users can transfer their skills and experience through and across the two platforms after one week of practice across the systems.","2166-0670","978-1-4244-9809-3","10.1109/ISMS.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730348","Semiotic;Semiotic Engineering Theory;Metaphor","Semiotics;Human computer interaction;Biological system modeling;Navigation;Computers;Multimedia communication","human computer interaction;operating systems (computers);user interfaces","Macintosh Os X;Microsoft Windows 7;semiotic engineering theory;HCI;user interface metaphors;semiotic differences","","1","","6","","14 Mar 2011","","","IEEE","IEEE Conferences"
"Development of Quantitative Tactile Display Device to Provide Both Pin- Array-Type Tactile Feedback and Thermal Feedback","G. Yang; K. Kyung; M. A. Srinivasan; D. Kwon","Human-Robot Interaction Res. Center, Korea Adv. Inst. of Sci. & Technol., Daejeon; POST PC Research Group, ETRI, Korea; Touch Lab., MIT, USA; Human-Robot Interaction Research Center, KAIST, Korea","Second Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (WHC'07)","2 Apr 2007","2007","","","578","579","This paper proposes a tactile display device that can provide both pin-array type tactile feedback and thermal feedback. The pin-array type tactile display is composed of a 6times5 pin-array that is individually actuated by 30 piezoelectric bimorphs. Micro shape and vibrotactile feedback can be generated by the device, and various planar distributed patterns can be displayed as can Braille cell patterns. The thermal feedback device is composed of a thin film resistance temperature detector (RTD), a thermal pad as pin guide made of copper, 3 Peltier thermoelectric heat pumps and a water cooling jacket. Thermal feedback device allows that users can discriminate among different materials by considering the temperature variation that can be sensed as they touch an object's surface with stimulating heat transfer phenomena between a fingertip and an object","","0-7695-2738-8","10.1109/WHC.2007.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145245","","Feedback;Displays;Thermal resistance;Temperature sensors;Shape;Piezoelectric films;Thin film devices;Detectors;Copper;Thermoelectricity","cooling;copper;display devices;haptic interfaces;heat pumps;piezoelectric devices;temperature scales;thermoelectric devices;thin film resistors","quantitative tactile display device;pin-array-type tactile feedback;pin-array type tactile feedback;piezoelectric bimorphs;micro shape feedback;vibrotactile feedback;planar distributed patterns;Braille cell patterns;thin film resistance temperature detector;copper;Peltier thermoelectric heat pumps;water cooling jacket;thermal feedback device;heat transfer phenomena;Cu","","15","","6","","2 Apr 2007","","","IEEE","IEEE Conferences"
"Gradient Octrees: A New Scheme for Remote Interactive Exploration of Volume Models","L. Campoalegre; I. Navazo; P. B. Crosa","Res. Group on Modelling, Visualization, Interaction & Virtual Reality, Univ. Politec. de Catalunya Barcelona, Barcelona, Spain; Res. Group on Modelling, Visualization, Interaction & Virtual Reality, Univ. Politec. de Catalunya Barcelona, Barcelona, Spain; NA","2013 International Conference on Computer-Aided Design and Computer Graphics","15 May 2014","2013","","","306","313","Remote exploration of medical volume models is nowadays a challenging problem. Interactive visualization algorithms must be able to send and render in real-time the volume model at the maximum possible visual quality, while adapting to network bandwidth limitations and to hardware constraints in the client device. In this paper we present a transfer function-aware scheme for the remote interactive inspection of volume models in client-server architectures with the objectives of supporting multi-resolution, avoiding gradient computations in the client device and sending a very limited amount of information through the network. Gradient Octrees can be progressively transmitted to the clients in a strongly compact way while achieving a minimum loss of visual quality as compared to state of the art ray-casting renderings. Visual volume understanding can be complemented by showing 2D sections of the original volume data on demand.","","978-1-4799-2576-6","10.1109/CADGraphics.2013.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815010","Volume Visualization;Volume Compression;Progressive Transmission;Octrees","Octrees;Indexes;Solid modeling;Graphics processing units;Materials;Arrays;Rendering (computer graphics)","client-server systems;data visualisation;interactive systems;octrees;rendering (computer graphics)","gradient octrees;remote interactive exploration;medical volume models;interactive visualization algorithms;real-time the volume model;visual quality;transfer function-aware scheme;remote interactive inspection;client-server architectures;gradient computations;client device;ray-casting renderings;visual volume understanding","","","","20","","15 May 2014","","","IEEE","IEEE Conferences"
"Perceptual Rendering for Learning Haptic Skills","T. Edmunds; D. K. Pai","Rutgers University, e-mail: tedmunds@cs.rutgers.edu; University of British Columbia, Rutgers University, e-mail: pai@cs.ubc.ca","2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","31 Mar 2008","2008","","","225","230","We approach the problem of creating haptic simulators that effectively impart skill without requiring high-fidelity devices by identifying perceptually salient events that signal transitions in the interaction. By augmenting these events, we seek to overcome deficiencies in the fidelity of the rendering hardware. We present an extension of event-based haptic rendering to non- collision events, and we describe a user-study of the training effectiveness of passive force-field haptic simulation vs. active event- augmented simulation in a tool-manipulation task. The results indicate that active augmentation improves skill transfer without requiring an increase in the quality of the rendering device.","2324-7355","978-1-4244-2005-6","10.1109/HAPTICS.2008.4479948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479948","","Haptic interfaces;Discrete event simulation;Bones;Hardware;Humans;Feedback;Surgery;Surges;Signal processing;Shape","computer graphic equipment;haptic interfaces;learning (artificial intelligence);manipulators;rendering (computer graphics);virtual reality","perceptual rendering hardware;haptic skill learning;signal transitions;event-based haptic rendering;noncollision events;training;passive force-field haptic simulation;event-augmented simulation;tool-manipulation task","","9","","18","","31 Mar 2008","","","IEEE","IEEE Conferences"
"Immersive Group-to-Group Telepresence","S. Beck; A. Kunert; A. Kulik; B. Froehlich",Virtual Reality Systems Group at Bauhaus-UniversitÃ¤t Weimar; Virtual Reality Systems Group at Bauhaus-Universität Weimar; Virtual Reality Systems Group at Bauhaus-Universität Weimar; Virtual Reality Systems Group at Bauhaus-Universität Weimar,"IEEE Transactions on Visualization and Computer Graphics","13 Mar 2013","2013","19","4","616","625","We present a novel immersive telepresence system that allows distributed groups of users to meet in a shared virtual 3D world. Our approach is based on two coupled projection-based multi-user setups, each providing multiple users with perspectively correct stereoscopic images. At each site the users and their local interaction space are continuously captured using a cluster of registered depth and color cameras. The captured 3D information is transferred to the respective other location, where the remote participants are virtually reconstructed. We explore the use of these virtual user representations in various interaction scenarios in which local and remote users are face-to-face, side-by-side or decoupled. Initial experiments with distributed user groups indicate the mutual understanding of pointing and tracing gestures independent of whether they were performed by local or remote participants. Our users were excited about the new possibilities of jointly exploring a virtual city, where they relied on a world-in-miniature metaphor for mutual awareness of their respective locations.","1941-0506","","10.1109/TVCG.2013.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479190","Multi-user virtual reality;telepresence;3D capture.","Calibration;Cameras;Servers;Streaming media;Image reconstruction;Image color analysis;Virtual reality","image colour analysis;image sensors;solid modelling;stereo image processing;virtual reality","immersive group-to-group telepresence;shared virtual 3D world;coupled projection-based multiuser setups;stereoscopic images;local interaction space;color cameras;registered depth cameras;captured 3D information;virtual user representations;virtual city;world-in-miniature metaphor","Computer Graphics;Computer Simulation;Group Processes;Humans;Imaging, Three-Dimensional;Models, Biological;Social Behavior;Telecommunications;User-Computer Interface","98","","47","","13 Mar 2013","","","IEEE","IEEE Journals"
"The enhancement of students learning through COMSOL simulation projects","Y. Ngabonziza; H. Delcham","Department of Math, Engineering and Computer Science, LAGCC of the City University of New York, Long Island City, NY 11101 USA; Department of Math, Engineering and Computer Science, LAGCC of the City University of New York, Long Island City, NY 11101 USA","Proceedings of the 2014 Zone 1 Conference of the American Society for Engineering Education","26 May 2014","2014","","","1","6","Research has shown that student participation in research activities increases faculty/student interactions outside the classroom, increases student involvement in their learning, addresses different learning styles, provides opportunities to see the entire academic pipeline, and makes difficult coursework more relevant. As educators, we are always looking for ways students can enhance their learning; one possibility is to link the theoretical knowledge we provide to them in the classroom with, not only real life applications, but also with job market requirements they will face once they graduate. This paper describes how the use of computer simulation was introduced in Thermodynamics class to supplement and help students visualize theoretical knowledge they learned in the classroom. Two energy related projects to be simulated using COMSOL software were assigned to students; post-class reports and survey highlighted how the inclusion of the computer simulation projects in the class enhanced students' learning.","","978-1-4799-5233-5","10.1109/ASEEZone1.2014.6820674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820674","Computer simulation;Comsol;learning;thermodynamics","Computer simulation;Software;Thermodynamics;Heat transfer;Computational modeling;Coatings;Visualization","computer aided instruction;finite element analysis;physics computing;thermodynamics","COMSOL simulation projects;student learning style;student participation;computer simulation;Thermodynamics class;student involvement;finite element analysis software","","2","","19","","26 May 2014","","","IEEE","IEEE Conferences"
"How EvoStreets Are Observed in Three-Dimensional and Virtual Reality Environments","M. Steinbeck; R. Koschke; M. O. Rüdel","University of Bremen,Germany; University of Bremen,Germany; University of Bremen,Germany","2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)","2 Apr 2020","2020","","","332","343","When analyzing software systems, a large amount of data accumulates. In order to assist developers in the preparation, evaluation, and understanding of findings, different visualization techniques have been developed. Due to recent progress in immersive virtual reality, existing visualization tools were ported to this environment. However, three-dimensional and virtual reality environments have different advantages and disadvantages, and by transferring concepts, such as layout algorithms and user interaction mechanisms, more or less one-to-one, the characteristics of these environments are neglected. In order to develop techniques adapting to the circumstance of a particular environment, more research in this field is necessary. In previously conducted case studies, we compared EvoStreets deployed in three different environments: 2D, 2.5D, and virtual reality. We found evidence that movement patterns-path length, average speed, and occupied volume-differ significantly between the 2.5D and virtual reality environments for some of the tasks that had to be solved by 34 participants in a controlled experiment. In this paper, we analyze the results of this experiment in more details, to study if not only movement is affected by these environments, but also the way how EvoStreets are observed. Although we could not find enough evidence that the number of viewpoints and their duration differ significantly, we found indications that in virtual reality viewpoints are located closer to the EvoStreets and that the distance between viewpoints is shorter. Based on our previous results and the findings of this paper, we present visualization and user interaction concepts specific to the kind of environment.","1534-5351","978-1-7281-5143-4","10.1109/SANER48275.2020.9054802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054802","","","program visualisation;virtual reality","movement patterns;visualization tools;virtual reality environments;three-dimensional environments;user interaction;EvoStreets;immersive virtual reality;visualization techniques","","","","43","","2 Apr 2020","","","IEEE","IEEE Conferences"
"TelePort: Virtual touring of Dun-Huang with a mobile device","Shen-Chi Chen; Chia-Wei Hsu; Da-Yuan Huang; Shih-Yao Lin; Yi-Ping Hung","Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","3 Oct 2013","2013","","","1","6","Some of our daily encountering is the objects of pictures, antiques, sculptures or wall-paintings captured or moved from the source space to another media platform. For example, the Direct Mail advertising of house sale, the historical exhibit in the museum. In fact, few figures or text descriptions will never be able to provide us the rewarding experience unless people can actually visit the source space in person. Inspired by the Boo's doors in the Disney's cartoon of Monsters Inc. which are used as a pathway to different spaces, we propose the “TelePort” as a gateway providing a virtual touring to the source space and achieve the prototype using a mobile device. To begin the TelePort, users first aim the object onto the mobile's screen. Subsequently, the screen view will immediately transfer to the view of the source space seamlessly as if users look into the space from the entrance gate. After transferring to the space, users can explore the source space and interact with the contents belonged to the space in reality through panorama visualization. To support the first person navigation, the viewpoint direction within the space can be detected by the mobile pose. This novel navigation interface can deliver to users an interesting and realistic experience as teleportation. Our contribution is to demonstrate an innovative experience for a virtual tour of the Mogao Caves using a mobile device. The “Teleport” featuring the virtual gateway which brings users to the Cave not only attracts attention but also enhances the experience of space exploring.","","978-1-4799-1604-7","10.1109/ICMEW.2013.6618406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618406","Touring;navigation;HCI;mobile;interaction design","Logic gates;Navigation;Space exploration;Mobile handsets;Magnetic sensors;Mobile communication","data visualisation;human computer interaction;internetworking;mobile computing;mobile handsets;museums;navigation;painting;travel industry;virtual reality","virtual gateway;Mogao Caves;virtual reality;navigation interface;mobile pose;panorama visualization;mobile screen;TelePort;source space;museum;wall painting;sculpture;antiques;picture object;mobile device;virtual tour","","","","18","","3 Oct 2013","","","IEEE","IEEE Conferences"
"Intuitive Exploration of Volumetric Data Using Dynamic Galleries","D. Jönsson; M. Falk; A. Ynnerman","Linköping University, Sweden; Linköping University, Sweden; Linköping University, Sweden","IEEE Transactions on Visualization and Computer Graphics","27 Oct 2015","2016","22","1","896","905","In this work we present a volume exploration method designed to be used by novice users and visitors to science centers and museums. The volumetric digitalization of artifacts in museums is of rapidly increasing interest as enhanced user experience through interactive data visualization can be achieved. This is, however, a challenging task since the vast majority of visitors are not familiar with the concepts commonly used in data exploration, such as mapping of visual properties from values in the data domain using transfer functions. Interacting in the data domain is an effective way to filter away undesired information but it is difficult to predict where the values lie in the spatial domain. In this work we make extensive use of dynamic previews instantly generated as the user explores the data domain. The previews allow the user to predict what effect changes in the data domain will have on the rendered image without being aware that visual parameters are set in the data domain. Each preview represents a subrange of the data domain where overview and details are given on demand through zooming and panning. The method has been designed with touch interfaces as the target platform for interaction. We provide a qualitative evaluation performed with visitors to a science center to show the utility of the approach.","1941-0506","","10.1109/TVCG.2015.2467294","Swedish Research Council, VR; Excellence Center at Linköping and Lund in Information Technology (ELLIIT); Linnaeus Environment CADICS, and the Swedish e-Science Research Centre (SeRC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192682","Transfer function;scalar fields;volume rendering;rendering;touch interaction;visualization;user interfaces;Transfer function;scalar fields;volume rendering;touch interaction;visualization;user interfaces","Image color analysis;Data visualization;Transfer functions;Rendering (computer graphics);Color;Cameras;Navigation","data visualisation;museums;user interfaces","volumetric data exploration;dynamic galleries;volume exploration method;science centers;museums;artifacts digitalization;user experience;interactive data visualization;visual property mapping;visual parameters;zooming;panning;touch interfaces;qualitative evaluation","Animals;Computer Graphics;Humans;Imaging, Three-Dimensional;Mice;Museums;Touch;User-Computer Interface","12","1","34","","12 Aug 2015","","","IEEE","IEEE Journals"
"Automatic filter design for synthesis of haptic textures from recorded acceleration data","J. M. Romano; T. Yoshioka; K. J. Kuchenbecker","Haptics Group, GRASP Laboratory, University of Pennsylvania, USA; Zanvyl Krieger Mind/Brain Institute, Johns Hopkins University, USA; Haptics Group, GRASP Laboratory, University of Pennsylvania, USA","2010 IEEE International Conference on Robotics and Automation","15 Jul 2010","2010","","","1815","1821","Sliding a probe over a textured surface generates a rich collection of vibrations that one can easily use to create a mental model of the surface. Haptic virtual environments attempt to mimic these real interactions, but common haptic rendering techniques typically fail to reproduce the sensations that are encountered during texture exploration. Past approaches have focused on building a representation of textures using a priori ideas about surface properties. Instead, this paper describes a process of synthesizing probe-surface interactions from data recorded from real interactions. We explain how to apply the mathematical principles of Linear Predictive Coding (LPC) to develop a discrete transfer function that represents the acceleration response under specific probe-surface interaction conditions. We then use this predictive transfer function to generate unique acceleration signals of arbitrary length. In order to move between transfer functions from different probe-surface interaction conditions, we develop a method for interpolating the variables involved in the texture synthesis process. Finally, we compare the results of this process with real recorded acceleration signals, and we show that the two correlate strongly in the frequency domain.","1050-4729","978-1-4244-5038-1","10.1109/ROBOT.2010.5509853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509853","","Filters;Haptic interfaces;Acceleration;Surface texture;Transfer functions;Signal synthesis;Linear predictive coding;Probes;Cognitive science;Virtual environment","data recording;filtering theory;haptic interfaces;image texture;linear predictive coding;rendering (computer graphics);transfer functions","automatic filter design;haptic texture synthesis;recorded acceleration data;haptic virtual environments;haptic rendering;probe surface interaction synthesis;linear predictive coding;discrete transfer function;predictive transfer function","","45","2","20","","15 Jul 2010","","","IEEE","IEEE Conferences"
"Networked tank gunnery skill training based on haptic interaction","G. Liu; K. Lu","State Key Laboratory of Virtual Reality Technology and Systems, Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, Robotics Institute, School of Mechanical Engineering and Automation, Beihang University, Beijing, 100191, China","2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)","12 Dec 2011","2011","4","","2220","2224","This paper introduces networked tank gunnery skill training based on haptic interaction. “Hand in hand” training mode is implemented by constructing a networked haptic display system. A teacher and a trainee can respectively manipulate his own haptic device to realize skill transferring. In this process, the trainee can experience the teacher's motor skill and can be corrected by the teacher. A local network is built to connect two haptic devices and computers. Two modes for tank gunnery skill training are designed in this system: haptic guidance and real-time correction. A prototype system is established to testify the effectiveness of such a new type “Hand in hand” training system.","1948-2922","978-1-4244-9352-4","10.1109/BMEI.2011.6098650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6098650","networked skill training;hand in hand;haptic display;haptic guidance;real-time correction","Haptic interfaces;Training;Real time systems;Computers;Virtual environments;Fixtures;Torque","computer based training;haptic interfaces;local area networks;military computing","networked tank gunnery skill training;haptic interaction;hand in hand training mode;networked haptic display system;teacher;trainee;skill transferring;motor skill;local network;haptic guidance;realtime correction","","2","","11","","12 Dec 2011","","","IEEE","IEEE Conferences"
"Toward memory-based human motion simulation: development and validation of a motion modification algorithm","Woojin Park; D. B. Chaffin; B. J. Martin","Dept. of Mech., Univ. of Cincinnati, OH, USA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","19 Apr 2004","2004","34","3","376","386","Computer simulation of human motions helps test hypotheses on human motion planning and fosters timely and high-quality human-machine/environment interaction design. The current study introduces a novel simulation approach termed memory-based motion simulation (MBMS), and presents its key element ""motion modification"" (MoM) algorithm. The proposed approach implements a computational model inspired by the generalized motor program (GMP) theory. Operationally, when a novel motion scenario is submitted to the MBMS system, its motion database is searched to find relevant existing motions. The selected motions, referred to as ""root motions"", most likely do not meet exactly the novel motion scenario, and therefore, they need to be modified by the MoM algorithm. This algorithm derives a parametric representation of possible variants of a root motion in a GMP-like manner, and adjusts the parameter values such that the new modified motion satisfies the novel motion scenario, while retaining the root motion's overall angular movement pattern and inter-joint coordination. An evaluation of the prediction capability of the algorithm, using both seated upper body reaching and whole-body load-transfer motions, indicated that the algorithm can accurately predict various human motions with errors comparable to the inherent variability in human motions when repeated under identical task conditions.","1558-2426","","10.1109/TSMCA.2003.822965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288349","","Humans;Biological system modeling;Ergonomics;Computational modeling;Predictive models;Testing;Design automation;Man machine systems;Algorithm design and analysis;Prototypes","human computer interaction;ergonomics;digital simulation;motion estimation;CAD","memory-based human motion simulation;motion modification algorithm;computer simulation;human motion planning;human machine interaction;generalized motor program theory;motion database;parametric representation;root motion;angular movement pattern;interjoint coordination;load transfer motions;human motions","","49","2","51","","19 Apr 2004","","","IEEE","IEEE Journals"
"Human Adaptation to Interaction Forces in Visuo-Motor Coordination","F. C. Huang; R. B. Gillespie; A. D. Kuo","Dept. of Mech. Eng., Michigan Univ., Ann Arbor, MI; NA; NA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","18 Sep 2006","2006","14","3","390","397","We tested whether humans can learn to sense and compensate for interaction forces in contact tasks. Many tasks, such as use of hand tools, involve significant interaction forces between hand and environment. One control strategy would be to use high hand impedance to reduce sensitivity to these forces. But an alternative would be to learn feedback compensation for the extrinsic dynamics and associated interaction forces, with the potential for lower control effort. We observed subjects as they learned control of a ball-and-beam system, a visuo-motor task where the goal was to quickly position a ball rolling atop a rotating beam, through manual rotation of the beam alone. We devised a ball-and-beam apparatus that could be operated in a real mode, where a physical ball was present; or in a virtual training mode, where the ball's dynamics were simulated in real time. The apparatus presented the same visual feedback in all cases, and optionally produced haptic feedback of the interaction forces associated with the ball's motion. Two healthy adult subject groups, vision-only and vision-haptics (each n=10), both trained for 80 trials on the simulated system, and then were evaluated on the real system to test for skill transfer effects. If humans incorporate interaction forces in their learning, the vision-haptics group would be expected to exhibit a smoother transfer, as quantified by changes in completion time of a ball-positioning task. During training, both groups adapted well to the task, with reductions of 64%-70% in completion time. At skill transfer to the real system, the vision-only group had a significant 35% increase in completion time (p<0.05). There was no significant change in the vision-haptics group, indicating that subjects had learned to compensate for interaction forces. These forces could potentially be incorporated in virtual environments to assist with motor training or rehabilitation","1558-0210","","10.1109/TNSRE.2006.881533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703571","Feedback control;haptic interface;motor adaptation;motor control;object manipulation;upper extremity","Humans;Force feedback;Haptic interfaces;Neurofeedback;Muscles;Force control;Virtual environment;Sensor arrays;Testing;Impedance","biomechanics;feedback;haptic interfaces;medical control systems;patient rehabilitation;virtual reality;vision","human adaptation;interaction forces;visuo-motor coordination;contact tasks;hand tools;high hand impedance;feedback compensation;ball-and-beam system;visual feedback;haptic feedback;ball-positioning task;motor training;rehabilitation","Adaptation, Physiological;Feedback;Humans;Motor Skills;Movement;Muscle, Skeletal;Musculoskeletal Equilibrium;Stress, Mechanical;Task Performance and Analysis;Touch;Upper Extremity;Visual Perception","15","","20","","18 Sep 2006","","","IEEE","IEEE Journals"
"Is talking to a simulated robot like talking to a child?","K. Fischer; K. Foth; K. Rohlfing; B. Wrede","University of Southern Denmark, IFKI, Alsion2, DK-6400 Sonderborg, Denmark; University of Hamburg, Department of Informatics, Vogt-Koelln-Str. 30, D-22527, Germany; Bielefeld University, Universitätsstr. 23-25, D-33615, Germany; Bielefeld University, Universitätsstr. 23-25, D-33615, Germany","2011 IEEE International Conference on Development and Learning (ICDL)","10 Oct 2011","2011","2","","1","6","Previous research has found people to transfer behaviors from social interaction among humans to interactions with computers or robots. These findings suggest that people will talk to a robot which looks like a child in a similar way as people talking to a child. However, in a previous study in which we compared speech to a simulated robot with speech to preverbal, 10 months old infants, we did not find the expected similarities. One possibility is that people were targeting an older child than a 10 months old. In the current study, we address the similarities and differences between speech to four different age groups of children and a simulated robot. The results shed light on how people talk to robots in general.","2161-9476","978-1-61284-990-4","10.1109/DEVLRN.2011.6037320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037320","child-directed speech;human-robot interaction;mindless transfer;fine-tuning","","human computer interaction;human-robot interaction","simulated robot;social interaction;human computer interaction;human robot interaction","","8","","27","","10 Oct 2011","","","IEEE","IEEE Conferences"
"A New Design of ATM Interface for Banking Services in Thailand","N. Cooharojananone; K. Taohai; S. Phimoltares","Dept. of Math., Chulalongkorn Univ., Bangkok, Thailand; Dept. of Math., Chulalongkorn Univ., Bangkok, Thailand; Dept. of Math., Chulalongkorn Univ., Bangkok, Thailand","2010 10th IEEE/IPSJ International Symposium on Applications and the Internet","7 Oct 2010","2010","","","312","315","A new design of ATM interface for banking services in Thailand with a new hierarchical menu structure based on the seven most frequently used tasks is presented. A total, of 105 participants from five different work occupations were used to test the design. Participants were asked to use the new design and a well known existing design. Simulators of the new interface were adapted for testing in a laboratory environment. The HCI principles were considered for designing and testing the new interface using usability criteria as the evaluation. The experimental results showed that a new ATM design reduced the error rate as well as increased effectiveness, efficiency and satisfaction.","","978-1-4244-7527-8","10.1109/SAINT.2010.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598053","usability evaluation;usability testing;navigation menu design;laboratory testing;Human-Computer Interaction (HCI);Automatic Teller Machine (ATM)","Asynchronous transfer mode;Usability;Testing;Human computer interaction;Credit cards;Banking;ISO standards","automatic teller machines;banking;human computer interaction","ATM interface;banking services;Thailand;hierarchical menu structure;laboratory environment;HCI principle;human-computer interaction;automatic teller machine","","8","","12","","7 Oct 2010","","","IEEE","IEEE Conferences"
"Matrix Transfer Protocol a Unified Communication Framework for Distributed Modules in Cognitive Robotics","G. Giefing","Dept. of Electr. Eng. & Inf. Technol., Lab. of Inf. Technol. Tech. Fachhochschule Georg Agricola, Bochum, Germany","2013 IEEE International Conference on Systems, Man, and Cybernetics","27 Jan 2014","2013","","","2759","2764","This paper introduces the Matrix Transfer Protocol (MTP) as a communication framework for distributed cognitive modules and systems suitable for cognitive robotics applications. It uses a unified intra-module information representation considering findings in neuroscience, robotics, computer networks communication and results in automation. The protocol data unit contains a matrix style data object and supports distributed system states contributing to the global workspace. A demonstrator is presented proving the applicability of the proposed protocol. This demonstrator incorporates augmented reality techniques with a focus on 3D stereo visual computing. Cognitive modules intended for a humanoid service robot head are task-driven integrated. A basic set of tasks has been implemented and tested.","1062-922X","978-1-4799-0652-9","10.1109/SMC.2013.471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722224","Cognitive robotics;matrix transfer protocol;information representation;communication framework;unified message format;human-machine system;augmented reality;3D stereo visual computing","Protocols;Robot kinematics;Head;Cameras;Robot vision systems","augmented reality;cognitive systems;computer networks;control engineering computing;distributed control;humanoid robots;human-robot interaction;robot vision;service robots;stereo image processing;transport protocols","communication framework;distributed modules;matrix transfer protocol;MTP;distributed cognitive modules;distributed cognitive systems;cognitive robotics applications;intra-module information representation;neuroscience;computer networks communication;automation;protocol data unit;matrix style data object;distributed system states;global workspace;augmented reality;3D stereo visual computing;humanoid service robot head","","1","","21","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Intelligibility Catchers for Self-Managed Knowledge Transfer","C. Stary","University of Linz, Austria","Seventh IEEE International Conference on Advanced Learning Technologies (ICALT 2007)","30 Jul 2007","2007","","","517","521","Intelligibility catchers (ICs) are novel e-learning components. Designed for self-managed learning and transfer of knowledge they encapsulate didactic and learner-centered concepts in an effective way. ICs are based on the context-aware coupling of individualized content elements to communication entries in e-learning environments. Due to their generic structure they can be used for any domain and on different levels of granularity. A first evaluation of IC utility reveals appreciation in terms of facilitated capacity building and focused peer-to-peer interaction in the course of learning and knowledge transfer.","2161-377X","0-7695-2916-X","10.1109/ICALT.2007.168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4281082","","Knowledge transfer;Electronic learning;Context;Streaming media;Content management;Knowledge management;Buildings;Peer to peer computing;Collaborative work;Virtual environment","computer aided instruction","intelligibility catchers;self-managed knowledge transfer;e-learning components;self-managed learning;learner-centered concepts;context-aware coupling;individualized content elements;peer-to-peer interaction","","6","","12","","30 Jul 2007","","","IEEE","IEEE Conferences"
"Exercise simulation platform based on mathematical thermal model","X. Chen; L. Yu; F. Zhou; N. Jia; R. Wang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2017 IEEE International Conference on Information and Automation (ICIA)","23 Oct 2017","2017","","","959","964","With the development of social economy, increasing importance has been attached to people's health consciousness. Exercise is widely regarded as the first step for health maintenance. For reducing accidents like syncope and dehydration in exercise, researchers and consumers urgently need a user-friendly and reliable tool to assisted analysis the risks in exercise. Aiming at the prediction on the changes of physiological parameters during exercise, a user-friendly exercise simulation prediction platform which stems from the George Fu's transient and three-dimensional thermal theoretical framework is reported. This platform was divided into the preprocessing, numerical simulation and post-processing modules as well as employs client/server architecture. In this platform, the changes of the human body physiological indicators under different environment conditions, at different activity levels and with various clothing can be presented intuitively. This platform has more flexible operability by the means of user-friendly graphical user interface and can be used to simulate a variety of exercise scenes which can make up for the specify scenes cannot be measured experimentally in reality.","","978-1-5386-3154-6","10.1109/ICInfA.2017.8079041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079041","exercise simulation;heat and moisture transfer;client/server;user-friendly","Biological system modeling;Mathematical model;Clothing;Heat transfer;Moisture;Physiology;Skin","biology computing;client-server systems;graphical user interfaces;health care;human computer interaction;physiology","exercise simulation platform;mathematical thermal model;social economy;health maintenance;user-friendly exercise simulation prediction platform;human body physiological indicators;user-friendly graphical user interface;people health consciousness;exercise dehydration;exercise risks;threedimensional thermal theoretical framework;exercise scene simulation;client/server architecture","","","","15","","23 Oct 2017","","","IEEE","IEEE Conferences"
"Assistive VR Gym: Interactions with Real People to Improve Virtual Assistive Robots","Z. Erickson; Y. Gu; C. C. Kemp","Georgia Institute of Technology,Health-care Robotics Lab,Atlanta,GA,USA; Georgia Institute of Technology,Health-care Robotics Lab,Atlanta,GA,USA; Georgia Institute of Technology,Health-care Robotics Lab,Atlanta,GA,USA","2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","14 Oct 2020","2020","","","299","306","Versatile robotic caregivers could benefit millions of people worldwide, including older adults and people with disabilities. Recent work has explored how robotic caregivers can learn to interact with people through physics simulations, yet transferring what has been learned to real robots remains challenging. Virtual reality (VR) has the potential to help bridge the gap between simulations and the real world. We present Assistive VR Gym (AVR Gym), which enables real people to interact with virtual assistive robots. We also provide evidence that AVR Gym can help researchers improve the performance of simulation-trained assistive robots with real people. Prior to AVR Gym, we trained robot control policies (Original Policies) solely in simulation for four robotic caregiving tasks (robot-assisted feeding, drinking, itch scratching, and bed bathing) with two simulated robots (PR2 from Willow Garage and Jaco from Kinova). With AVR Gym, we developed Revised Policies based on insights gained from testing the Original policies with real people. Through a formal study with eight participants in AVR Gym, we found that the Original policies performed poorly, the Revised policies performed significantly better, and that improvements to the biomechanical models used to train the Revised policies resulted in simulated people that better match real participants. Notably, participants significantly dis-agreed that the Original policies were successful at assistance, but significantly agreed that the Revised policies were successful at assistance. Overall, our results suggest that VR can be used to improve the performance of simulation-trained control policies with real people without putting people at risk, thereby serving as a valuable stepping stone to real robotic assistance.","1944-9437","978-1-7281-6075-7","10.1109/RO-MAN47096.2020.9223609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223609","","","biomechanics;geriatrics;handicapped aids;humanoid robots;learning (artificial intelligence);medical computing;medical robotics;mobile robots;virtual reality","simulated robots;simulated people;simulation-trained control policies;robotic assistance;virtual assistive robots;versatile robotic caregivers;physics simulations;simulation-trained assistive robots;robot control policies;robotic caregiving tasks;drinking;assistive VR gym;revised policies;AVR gym","","","","48","","14 Oct 2020","","","IEEE","IEEE Conferences"
"An Adaptive Multi-node Downloading Partitioning Algorithm of Distributed Virtual Environment Based on Grid Computing","Y. Wang; H. Li; J. Jia","De Montfort University, UK; Zhuhai College of Jilin University, China; Siping Vocational College, China; Zhuhai College of Jilin University, China","Fourth International Conference on Image and Graphics (ICIG 2007)","4 Sep 2007","2007","","","1026","1032","When users are connected to a Large Scale distributed virtual environment system on internet, downloading speed is a key point to support a life-like world and real time interactions for a large number of avatars in a consistent fashion. We proposed an adaptive multi-node downloading partitioning algorithm to balance the bottleneck problem on multi- node downloading architecture we designed. It facilitates a flexible and scalable downloading large scale DVE. Our method has five major steps: (1) to build up a database by partitioning the ground of a DIE into two-dimensional square regions covered with three-dimensional objects uniformly; (2) to determine statically a region where an avatar's area of interest located according to its spatial coherence; (3) to group the grid nodes located on same region in DVE; (4) to build a downloading servers group within each region and adapt it dynamically; (5) to balance the downloading workload dynamically. A specific multi-node downloading component is devised for supporting remote multi-thread downloading DVE based Globus platform. The downloading speed is proportional to the number of grid nodes participated in the downloading task in each region. Initial experimental results show the feasibility and effectiveness of our approach.","","978-0-7695-2929-5","10.1109/ICIG.2007.182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297229","VRML;Distributed Virtual Environment;(DVE);Grid Computing;Multi-thread Downloading;AOI (Area of Interest);Reliable File Transfer service;(RFT);monitoring and discovery (MDS)","Partitioning algorithms;Virtual environment;Grid computing;Large-scale systems;Internet;Real time systems;Avatars;Algorithm design and analysis;Spatial databases;Spatial coherence","grid computing;virtual reality","adaptive multinode downloading partitioning;grid computing;large scale distributed virtual environment system;Internet;bottleneck problem balance;multinode downloading architecture","","","","19","","4 Sep 2007","","","IEEE","IEEE Conferences"
"Development visual literacy in digital environment","M. Buzaši Marganic","University of Novi sad, Faculty of Education, Somboru, Podgorička u. 4, 25000, Serbia","The 33rd International Convention MIPRO","29 Jul 2010","2010","","","1096","1098","The purpose of this work is to point out the importance of qualifying the students to be visually literate in the modern world of visual culture, in which a lot of information of various contents are transferred by images. Virtual environments redefined the preferential competences of students. In virtual environments the questions of receiving, interpreting and selecting visual messages are important, because it has been proven that visual analphabetism leads to disconnection. Pictures are always the medium of information, art pictures always represent certain quality, because they show the artists' relationship with the real world. Developing visual literacy is efficient during socializing with artworks of high quality. The author in her work pays special attention to creative games and how through them the attention of students is focused on artworks.","","978-9-5323-3050-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533615","visual literacy;digital Environment;art;game","Virtual environment;Art;Visual communication;Visual perception;Snow;Postal services;Sparks;Photography;Painting;Information resources","art;computer aided instruction;human computer interaction;virtual reality","visual literacy;digital environment;visual culture;virtual environments;visual analphabetism;creative games","","","","7","","29 Jul 2010","","","IEEE","IEEE Conferences"
"Digital representation of skills for human-robot interaction","C. A. Avizzano; E. Ruffaldi; M. Bergamasco","PERCRO, Center of Excellence for Information and Communication Engineering, Scuola Superiore Sant'Anna, Pisa, Italy; PERCRO, Center of Excellence for Information and Communication Engineering, Scuola Superiore Sant'Anna, Pisa, Italy; PERCRO, Center of Excellence for Information and Communication Engineering, Scuola Superiore Sant'Anna, Pisa, Italy","RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication","10 Nov 2009","2009","","","769","774","The present paper deals with a system architecture and a digital format to support the acquisition, storage and transfer of human skills. Virtual environments and haptic interfaces will be addressed as target platforms for the capturing and rendering of skills. There are several methodologies for approaching definition and modelling of skills, and the present work will focus on a specific approach that merges evidences from human sciences with present approaches in intelligent robotics and machine learning. This work presents a supporting tool that enables researchers to model, analyse and control the skill transfer process. In addition this work will provide an overview of a skill transfer framework, and information related to the models of skill representation that are being employed.","1944-9437","978-1-4244-5081-7","10.1109/ROMAN.2009.5326295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5326295","","Computer aided instruction;Virtual environment;Haptic interfaces;Intelligent robots;Machine learning;Robot sensing systems;Human robot interaction;Rendering (computer graphics);Learning systems;Application software","haptic interfaces;human-robot interaction;rendering (computer graphics)","digital skill representation;human-robot interaction;digital format;virtual environments;haptic interfaces;skill rendering;intelligent robotics;machine learning;skill transfer process","","1","","23","","10 Nov 2009","","","IEEE","IEEE Conferences"
"Persistent issues in the application of virtual environment systems to training","J. K. Caird","Dept. of Psychol., Calgary Univ., Alta., Canada","Proceedings Third Annual Symposium on Human Interaction with Complex Systems. HICS'96","6 Aug 2002","1996","","","124","132","The flexible constellation of technologies that comprise virtual environment (VE) systems provide many new opportunities for training. However, the lessons learned from decades of research in flight and driving simulation seem largely forgotten. This review recalls a set of long-term issues that will constrain the application of VE systems to training. In particular, the issues of cost effectiveness, interface usability, transfer of training, transfer theory, and training feedback are addressed. Training and research decisions that do not consider these issues are likely to result in programs and recommendations that do little to efficiently improve skills and knowledge.","","0-8186-7493-8","10.1109/HUICS.1996.549502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=549502","","Virtual environment;Costs;Computer graphics;Force feedback;Psychology;Aerospace simulation;Usability;Computational modeling;Image generation;Physics computing","virtual reality","virtual environment systems;training;long-term issues;cost effectiveness;interface usability;transfer theory;training feedback","","8","31","79","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Effect of interface type in the VR-based acquisition of pedestrian skills in persons with ASD","M. Saiano; E. Garbarino; S. Lumachi; S. Solari; V. Sanguineti","Dept of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via all'Opera Pia 13, 16145, Italy; Dept. of Primary Care, ASL3 Genovese, Genoa, Italy; Philos Counseling Academy, Genoa, Italy; Dept. of Education Sciences, University of Genoa, Corso A. Podestá 2, Italy; Dept of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via all'Opera Pia 13, 16145, Italy","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","5 Nov 2015","2015","","","5728","5731","Possession of `social' skills is crucial for persons with autism spectrum disorders (ASD) to maintain a certain independence and a better quality of life, and interaction with virtual environments seems an effective learning aid. In a previous study, we reported that in adults with ASD interaction with a virtual environment (a virtual city) is beneficial to the acquisition of pedestrian skills (street crossing and street navigation). Interaction was based on a gesture-based interface (Microsoft Kinect). Here we compare the learning performance when the same virtual environment is operated by a gamepad interface. We used exactly the same training protocol and data analysis than the original study. We found that both interface types are effective in the acquisition of street crossing and city navigation skills. The gamepad interface seems easier to use (thus leading to faster interaction), but gesture-based interfaces are superior in terms of transfer of the learned skills to real road environments (as reported by parents and caregivers).","1558-4615","978-1-4244-9271-8","10.1109/EMBC.2015.7319693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319693","","Nickel;Virtual environments;Training;Navigation;Autism;Protocols","computer games;data analysis;learning (artificial intelligence);medical diagnostic computing;medical disorders;virtual reality","interface type effect;VR-based acquisition;pedestrian skills;autism spectrum disorders;quality-of-life;virtual environments;effective learning aid;ASD interaction;street crossing;street navigation;gesture-based interface;Microsoft Kinect;learning performance;gamepad interface;training protocol;data analysis","Accidents, Traffic;Autism Spectrum Disorder;Humans;Pedestrians;Quality of Life;User-Computer Interface","3","","11","","5 Nov 2015","","","IEEE","IEEE Conferences"
"Interactions Between Threat and Executive Control in a Virtual Reality Stroop Task","T. D. Parsons; C. G. Courtney","University of North Texas, Denton, TX; University of Southern California, Los Angeles 501, CA","IEEE Transactions on Affective Computing","27 Feb 2018","2018","9","1","66","75","Understanding the ways in which persons rapidly transfer attention between tasks while still retaining ability to perform these tasks is an important area of study. Everyday activities commonly come in the form of emotional distractors. A recently developed Virtual Reality Stroop Task (VRST) allows for assessing neurocognitive and psychophysiological responding while traveling through simulated safe and ambush desert environments as Stroop stimuli appear on the windshield. We evaluated differences in psychophysiological response patterns associated with completion of an affective task alone versus completion of an affective task that also included a Stroop task. The VRST elicited increased heart rate, respiration rate, skin conductance level, and number of spontaneous fluctuations in electrodermal activity. Increased cognitive workload was found to be associated with the more cognitively challenging Stroop conditions which led to an increase in response level. This expands on previous findings and indicates that allocating attention away from the environment and toward Stroop stimuli likely requires greater inhibitory control. This is corroborated by behavioral findings from previous investigations with the VRST. The VRST revealed that the increased difficulty found in tasks like the Stroop interference task directly evoke autonomic changes in psychophysiological arousal beyond the threatening stimuli themselves.","1949-3045","","10.1109/TAFFC.2016.2569086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473917","Affective computing;psychology;arousal classification;affect recognition;virtual reality;Stroop task","Hidden Markov models;Skin;Automotive components;Heart rate;Atmospheric measurements;Particle measurements;Virtual reality","cognition;neurophysiology;psychology;skin;virtual reality","heart rate;respiration rate;skin conductance level;electrodermal activity;Stroop conditions;response level;Stroop stimuli;VRST;Stroop interference task;executive control;emotional distractors;neurocognitive responding;psychophysiological responding;simulated safe desert environments;ambush desert environments;psychophysiological response patterns;affective task;cognitive workload;inhibitory control;Virtual Reality Stroop Task","","1","","79","","19 May 2016","","","IEEE","IEEE Journals"
"Enhancing engineering education through distributed virtual reality","T. Sulbaran; N. C. Baker","Sch. of Civil & Environ. Eng., Georgia Inst. of Technol., Atlanta, GA, USA; NA","30th Annual Frontiers in Education Conference. Building on A Century of Progress in Engineering Education. Conference Proceedings (IEEE Cat. No.00CH37135)","6 Aug 2002","2000","2","","S1D/13","S1D/18 vol.2","Visual interaction can greatly change the way engineering is performed both in learning new materials and in terms of explaining activities between practicing professionals. The paper presents a study to develop and assess distributed virtual reality (DVR) for engineering education. Strategic key criteria are presented followed with the development and assessment of a DVR tool that allows the evaluation of learner issues such as: reading/following instructions, navigation, interaction, responsiveness, knowledge transfer, engagement and acceptance. The results suggest that DVR has the potential for enhancing engineering education. More extensive DVR development and assessment and international distributed deployment will further our understanding of more precise interactions.","0190-5848","0-7803-6424-4","10.1109/FIE.2000.896621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=896621","","Engineering education;Virtual reality;Internet;IEC standards;ISO standards;Educational institutions;Distributed computing;Engineering students;Navigation;Delay","courseware;teaching;engineering education;engineering computing;virtual reality;distributed processing","engineering education;distributed virtual reality;visual interaction;practicing professionals;strategic key criteria;DVR tool;learner issues;reading;instruction following;navigation;interaction;responsiveness;knowledge transfer;DVR development;international distributed deployment","","4","","15","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Development of Virtual Reality Serious Game for Underground Rock-Related Hazards Safety Training","Z. Liang; K. Zhou; K. Gao","School of Resources and Safety Engineering, Central South University, Changsha, China; School of Resources and Safety Engineering, Central South University, Changsha, China; School of Resources and Safety Engineering, Central South University, Changsha, China","IEEE Access","30 Aug 2019","2019","7","","118639","118649","Traditional safety training media to transfer safety knowledge specific to the rock-related hazards in underground mines are mainly video or manuals, which are inefficient and bring a poor training experience. In this paper, we designed and developed a serious game based on virtual reality (VR) technology in order to efficiently transfer safety knowledge and enable enhanced interactive safety training. For different training purposes and users, we designed two modes, one for professional scaling training suitable for novice scalers, the other for rock-related hazards perception training suitable for other miners. Our game is built based on game engine-Unity3D and equipped with HTC VIVE to improve immersion. The game pipeline is to have trainees basically understand safety knowledge through guided interaction and then make a self-adaptive practice to fully master it. We evaluated the effectiveness of our game, and the results of the comparative experiment show that our game is more efficient than the instructional video in both training modes. The application of our game is proven to have the potential to change the safety situation of underground mines and evaluate the level of safety awareness and risk aversion of the miners in the future.","2169-3536","","10.1109/ACCESS.2019.2934990","Central South University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795446","Safety training;underground mining;virtual reality;rock-related hazards;serious game","Games;Training;Rocks;Hazards;Virtual reality;Accidents","computer based training;design engineering;health hazards;industrial training;mining;mining industry;occupational safety;risk management;rocks;serious games (computing);virtual reality","underground mines;professional scaling training;game engine-Unity3D;risk aversion;virtual reality serious game design;hazards perception training;underground rock;HTC VIVE equipment","","3","","48","CCBY","13 Aug 2019","","","IEEE","IEEE Journals"
"Arch-Explore: A natural user interface for immersive architectural walkthroughs","G. Bruder; F. Steinicke; K. H. Hinrichs","Visualization and Computer Graphics (VisCG) Research Group, Department of Computer Science, University of Münster, Einsteinstr. 62, 48149, Germany; Visualization and Computer Graphics (VisCG) Research Group, Department of Computer Science, University of Münster, Einsteinstr. 62, 48149, Germany; Visualization and Computer Graphics (VisCG) Research Group, Department of Computer Science, University of Münster, Einsteinstr. 62, 48149, Germany","2009 IEEE Symposium on 3D User Interfaces","7 Apr 2009","2009","","","75","82","In this paper we propose the Arch-Explore user interface, which supports natural exploration of architectural 3D models at different scales in a real walking virtual reality (VR) environment such as head-mounted display (HMD) or CAVE setups. We discuss in detail how user movements can be transferred to the virtual world to enable walking through virtual indoor environments. To overcome the limited interaction space in small VR laboratory setups, we have implemented redirected walking techniques to support natural exploration of comparably large-scale virtual models. Furthermore, the concept of virtual portals provides a means to cover long distances intuitively within architectural models. We describe the software and hardware setup and discuss benefits of Arch-Explore.","","978-1-4244-3965-2","10.1109/3DUI.2009.4811208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811208","3D user interfaces;virtual environments;locomotion;architectural walkthroughs;redirected walking;passive haptic feedback","User interfaces;Legged locomotion;Virtual reality;Computer graphics;Indoor environments;Laboratories;Hardware;Virtual environment;Haptic interfaces;Feedback","helmet mounted displays;solid modelling;user interfaces;virtual reality","immersive architectural walkthroughs;Arch-Explore user interface;natural exploration;architectural 3D models;virtual reality environment;head-mounted display;CAVE setups;virtual indoor environments;VR laboratory setups;virtual models;virtual portals;architectural models","","52","","34","","7 Apr 2009","","","IEEE","IEEE Conferences"
"Measuring Anthropometric Data for HRTF Personalization","M. Rothbucher; T. Habigt; J. Habigt; T. Riedmaier; K. Diepold","Inst. for Data Process., Tech. Univ. Munchen, Munich, Germany; Inst. for Data Process., Tech. Univ. Munchen, Munich, Germany; Inst. for Data Process., Tech. Univ. Munchen, Munich, Germany; Inst. for Data Process., Tech. Univ. Munchen, Munich, Germany; Inst. for Data Process., Tech. Univ. Munchen, Munich, Germany","2010 Sixth International Conference on Signal-Image Technology and Internet Based Systems","17 Feb 2011","2010","","","102","106","Nowadays, multimodal human-like sensing, e.g. vision, haptics and audition seeks to improve interaction between an operator (human) and a teleoperator (robot) in human centered robotic systems. Head Related Transfer Function (HRTF) based sound rendering techniques, which seek to create a realistic virtual auditory space for listeners, have become a prominent concept in human robot interaction. Applications that demand high quality 3D sound synthesis are usually based on measured HRTFs of listeners. Recently, researchers propose to construct a set of personalized HRTFs using multiple linear regression models between anthropometric data and measured HRTFs, which implies the existence of a training HRTF dataset together with the corresponding anthropometric data. This paper focuses on the measurement of Head-Related transfer Functions (HRTFs) and the corresponding anthropometric data of a listener. Several state-of-the-art techniques of measuring the HRTFs are described. For measuring the anthropometric data, we develop a low budget approach, which enables us to measure the anthropometry of a person within short time at a high accuracy, whereas the hardware costs for the scanning system are significantly reduced.","","978-1-4244-9527-6","10.1109/SITIS.2010.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714537","","Three dimensional displays;Measurement by laser beam;Solid modeling;Anthropometry;Transfer functions;Lasers;Ear","acoustic signal processing;acoustic waves;anthropometry;ear;humanoid robots;human-robot interaction;regression analysis;rendering (computer graphics);solid modelling;transfer functions;virtual reality","anthropometric data measurement;HRTF personalization;multimodal human-like sensing;human centered robotic system;head related transfer function;sound rendering technique;virtual auditory space;human robot interaction;high quality 3D sound synthesis;multiple linear regression model;HRTF dataset;state-of-the-art technique;hardware cost","","3","9","15","","17 Feb 2011","","","IEEE","IEEE Conferences"
"SkyTools and DigiStrips: from the technology to the European operational context","S. Carlier; G. Gawinowski; L. Guichard; H. Hering","Innovative Res. Unit, Eurocontrol Exp. Centre, Bretigny, France; NA; NA; NA","20th DASC. 20th Digital Avionics Systems Conference (Cat. No.01CH37219)","6 Aug 2002","2001","2","","7E1/1","7E1/7 vol.2","For four years, CENA has been developing DigiStrips (Cena'00), a prototype that offers human machine interaction solutions based on large flat, touch-input screen, animation, graphical design, feedback and gesture recognition. DigiStrips offers an alternative solution to the display screen and mouse-based interaction. The Eurocontrol Experimental Centre (EEC) in its neutral role to identify, promote and integrate, at the European level, innovative ideas from European ATC R&D institutes, has considered DigiStrips as being able to provide benefits for the controller working position. Assumptions were made that this technology, when embedded in an operational context could make the ATCO work more comfortable and safe. This paper describes how we carried out the design and the validation of an air traffic controller working position that uses this technology. The challenge of this project is to make an ecological interface that respects controllers' skills and know-how, while introducing new technologies-new at least in the ATC domain. First, we summarize the technology proposed by CENA. Then we present the operational context and the result of the demonstrations given to several hundreds of European controllers. We also describe the validation approach chosen for DigiStrips.","","0-7803-7034-1","10.1109/DASC.2001.964203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=964203","","Humans;Air traffic control;Feedback;Virtual reality;Animation;Large screen displays;Paper technology;Auditory system;Prototypes;Research and development","air traffic control;interactive systems;touch sensitive screens;gesture recognition;human factors;computer graphics;aerospace computing;technology transfer","DigiStrips;human machine interaction;large flat screen;touch-input screen;animation;gesture recognition;graphical design;feedback;SkyTools project;European operational context;European ATC;air traffic controller working position;CENA;technology transfer","","","1","5","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Modeling an AGV based facility logistics system to measure and visualize performance availability in a VR environment","K. Eilers; J. Rossmann","RIF Institute for Research and Transfer, Joseph-von-Fraunhofer-Strasse 20, 44227 Dortmund, GERMANY; Institute for Man-Machine Interaction, RWTH Aachen, Ahornstrasse 55, 52074, GERMANY","Proceedings of the Winter Simulation Conference 2014","26 Jan 2015","2014","","","367","375","Performance availability is an approach to rate the performance of material flow systems. Since the data necessary to determine the performance availability can only be obtained by observing the system in operation, planning towards a certain performance availability is a challenging task. In this paper, we present an approach to model an AGV (Automated Guided Vehicle) based logistics facility to ultimately measure and visualize the performance availability of the system within VR environments. We employed 3-D laser scans to create a visual representation of the facility and modelled the mechanical components using the simulation system's kinematic mechanisms. An interface to the real system's control architecture makes it possible to incorporate real world data and scenarios. Data not readily visible or not visible at all such as vehicle health, waiting times, and running times is surveyed and presented in a comprehensive VR environment for evaluating overall system performance and performance availability.","1558-4305","978-1-4799-7486-3","10.1109/WSC.2014.7019903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019903","","Availability;Logistics;Vehicles;Three-dimensional displays;Floors;Data models;Real-time systems","automatic guided vehicles;robot kinematics;virtual reality","AGV based facility logistics system;VR environment;material flow system;automated guided vehicle;3D laser scans;kinematic mechanism","","3","","19","","26 Jan 2015","","","IEEE","IEEE Conferences"
"Point, shoot, and paste: Direct photo pasting from a digital still camera","T. Takashina; H. Sasaki; Y. Kokumai; Y. Iwasaki","Nikon Corporation, 6-3 Nishioi 1-chome, Shinagawaku, Tokyo 140-8601 Japan; Nikon Corporation, 6-3 Nishioi 1-chome, Shinagawaku, Tokyo 140-8601 Japan; Nikon Corporation, 6-3 Nishioi 1-chome, Shinagawaku, Tokyo 140-8601 Japan; Nikon Corporation, 6-3 Nishioi 1-chome, Shinagawaku, Tokyo 140-8601 Japan","2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)","14 Nov 2013","2013","","","98","99","We propose a novel interaction method “point-shoot-and-paste” for transferring a photo into the specified position of a PC document directly and intuitively from a DSC; a user aims a DSC at the pasting position in the screen of the PC, presses the shutter button, and then the photo is transferred and pasted in the position. This method is advantageous because it is intuitive and doesn't need any special sensors to specify the location of pasting. Our middleware will also enable generic applications to utilize “point-shoot-and-paste”.","2378-8143","978-1-4799-0892-9","10.1109/GCCE.2013.6664939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664939","","Middleware;Sensors;Prototypes;User interfaces;Cameras;Presses;Standards","document image processing;electronic data interchange;human computer interaction;image sensors;screens (display);user interfaces","PC screen;digital devices;data exchange;pasting location specification;shutter button;pasting position;PC document;interaction method;digital still camera;direct photo pasting;point-shoot-and-paste","","2","","3","","14 Nov 2013","","","IEEE","IEEE Conferences"
"WAVE: Interactive Wave-based Sound Propagation for Virtual Environments","R. Mehra; A. Rungta; A. Golas; M. Lin; D. Manocha",UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill,"IEEE Transactions on Visualization and Computer Graphics","20 Mar 2015","2015","21","4","434","442","We present an interactive wave-based sound propagation system that generates accurate, realistic sound in virtual environments for dynamic (moving) sources and listeners. We propose a novel algorithm to accurately solve the wave equation for dynamic sources and listeners using a combination of precomputation techniques and GPU-based runtime evaluation. Our system can handle large environments typically used in VR applications, compute spatial sound corresponding to listener's motion (including head tracking) and handle both omnidirectional and directional sources, all at interactive rates. As compared to prior wave-based techniques applied to large scenes with moving sources, we observe significant improvement in runtime memory. The overall sound-propagation and rendering system has been integrated with the Half-Life 2 game engine, Oculus-Rift head-mounted display, and the Xbox game controller to enable users to experience high-quality acoustic effects (e.g., amplification, diffraction low-passing, high-order scattering) and spatial audio, based on their interactions in the VR application. We provide the results of preliminary user evaluations, conducted to study the impact of wave-based acoustic effects and spatial audio on users' navigation performance in virtual environments.","1941-0506","","10.1109/TVCG.2015.2391858","Advanced Simulation and Training; ARO Contracts; National Science Foundation; Impulsonic Inc.; Acoustect SDK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014276","Sound propagation;dynamic sources;spatial sound;Sound propagation;dynamic sources;directivity;spatial sound;Helmholtz equation","Runtime;Acoustics;Vectors;Transfer functions;Virtual environments;Linear systems;Navigation","acoustic wave propagation;graphics processing units;rendering (computer graphics);virtual reality;wave equations","WAVE;interactive wave-based sound propagation;virtual environments;dynamic sources;wave equation;GPU-based runtime evaluation;VR applications;omnidirectional sources;rendering system;Half-Life 2 game engine;Oculus-Rift head-mounted display;Xbox game controller;high-quality acoustic effects;spatial audio;wave-based acoustic effects;user navigation performance","Acoustic Stimulation;Adult;Algorithms;Computer Graphics;Female;Humans;Male;Sound;User-Computer Interface;Video Games;Young Adult","20","","43","","19 Jan 2015","","","IEEE","IEEE Journals"
"Qualitative analysis of a multimodal interface system using speech/gesture","M. Z. Baig; M. Kavakli","Virtual and Interactive Simulations of Reality (VISOR) Research Group, Department of Computing, Faculty of Science and Engineering Macquarie University, Sydney, Australia; Virtual and Interactive Simulations of Reality (VISOR) Research Group, Department of Computing, Faculty of Science and Engineering Macquarie University, Sydney, Australia","2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)","28 Jun 2018","2018","","","2811","2816","In this paper, we present an upgraded version of the 3D modelling system, De-SIGN v3 [1]. The system uses speech and gesture recognition technology to collect information from the user in real-time. These inputs are then transferred to the main program to carry out required 3D object creation and manipulation operations. The aim of the system is to analyse the designer behaviour and quality of interaction, in a virtual reality environment. The system has the basic functionality for 3D object modelling. The users have performed two sets of experiments. In the first experiment, the participants had to draw 3D objects using keyboard and mouse. In the second experiment, speech and gesture inputs have been used for 3D modelling. The evaluation has been done with the help of questionnaires and task completion ratings. The results showed that with speech, it is easy to draw the objects but sometimes system detects the numbers incorrectly. With gestures, it is difficult to stabilize the hand at one position. The completion rate was above 90% with the upgraded system but the precision is low depending on participants.","2158-2297","978-1-5386-3758-6","10.1109/ICIEA.2018.8398188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8398188","Speech;Gesture;MMIS;3D Modelling;CAD;Object Manipulation","Three-dimensional displays;Speech recognition;Cameras;Sensors;Solid modeling;Task analysis;Human computer interaction","gesture recognition;solid modelling;speech recognition;speech-based user interfaces;virtual reality","multimodal interface system;3D modelling system;manipulation operations;virtual reality environment;3D object modelling;speech recognition;De-SIGN v3;gesture recognition;3D object creation;designer behaviour","","2","","28","","28 Jun 2018","","","IEEE","IEEE Conferences"
"Exploration of Computer Emotion Decision Based on Artificial Intelligence","Y. Shi; C. Li","Fac. of Inf. Eng., Wuhan Univ. of Sci. & Technol, Wuhan, China; Fac. of Inf. Eng., Wuhan Univ. of Sci. & Technol, Wuhan, China","2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)","11 Nov 2018","2018","","","293","295","To carry out the discussion of computer emotion decision based on artificial intelligence, first of all, based on the psychological experiment paradigm of children's game task, and the test process of the artificial emotion generating engine was completed on the emotional spontaneous transfer and the stimulus transfer model. Secondly, the reasoning method and analytic hierarchy process (AHP) were introduced into the multi system, and a kind of multi emotion decision model based on the emotion reasoning was constructed. The hierarchical structure method was used to solve the complex decision problem of the humanoid robot in the intelligent home environment. Then, based on the emotion energy theory, a mood state regulation algorithm based on the combination of HMM-based spontaneous transfer and stimulus transfer was established. In addition, on this basis, the design and implementation of humanoid robot associative memory model was realized. Finally, the theory and algorithm were integrated into the interactive platform of human-computer expression, and the validity of the model was analysed and verified. The results showed that, on this robot platform, the process of human-computer interaction and cooperation which integrated emotion evaluation, emotional decision, associative memory and emotion regulation was realized. As a result, the computer emotion decision based on artificial intelligence can be well applied in many fields.","","978-1-5386-8031-5","10.1109/ICVRIS.2018.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531405","Humanoid robot;human-computer interaction;emotion","Humanoid robots;Artificial intelligence;Hidden Markov models;Psychology;Cognition;Associative memory","analytic hierarchy process;control engineering computing;data mining;emotion recognition;hidden Markov models;human computer interaction;humanoid robots;psychology","computer emotion decision;artificial intelligence;artificial emotion generating engine;emotional spontaneous transfer;stimulus transfer model;multiemotion decision model;emotion reasoning;complex decision problem;intelligent home environment;emotion energy theory;humanoid robot associative memory model;human-computer interaction;integrated emotion evaluation;emotional decision;emotion regulation;psychological experiment;childrens game task;analytic hierarchy process;hierarchical structure method;mood state regulation algorithm;HMM-based spontaneous transfer;human-computer expression","","1","","9","","11 Nov 2018","","","IEEE","IEEE Conferences"
"Interactive near-field illumination for photorealistic augmented reality on mobile devices","K. Rohmer; W. Büschel; R. Dachselt; T. Grosch","Computational Visualistics, University of Magdeburg; Interactive Media Lab, Technical University of Dresden; Interactive Media Lab, Technical University of Dresden; Computational Visualistics, University of Magdeburg","2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","6 Nov 2014","2014","","","29","38","Mobile devices become more and more important today, especially for augmented reality (AR) applications in which the camera of the mobile device acts like a window into the mixed reality world. Up to now, no photorealistic augmentation is possible since the computational power of the mobile devices is still too weak. Even a streaming solution from a stationary PC would cause a latency that affects user interactions considerably. Therefore, we introduce a differential illumination method that allows for a consistent illumination of the inserted virtual objects on mobile devices, avoiding a delay. The necessary computation effort is shared between a stationary PC and the mobile devices to make use of the capacities available on both sides. The method is designed such that only a minimum amount of data has to be transferred asynchronously between the stationary PC and one or multiple mobile devices. This allows for an interactive illumination of virtual objects with a consistent appearance under both temporally and spatially varying real illumination conditions. To describe the complex near-field illumination in an indoor scenario, multiple HDR video cameras are used to capture the illumination from multiple directions. In this way, sources of illumination can be considered that are not directly visible to the mobile device because of occlusions and the limited field of view of built-in cameras.","","978-1-4799-6184-9","10.1109/ISMAR.2014.6948406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948406","I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism H.5.1 [Information Interfaces and Representation];Artificial;Augmented and Virtual Realities","Lighting;Cameras;Mobile handsets;Rendering (computer graphics);Image reconstruction;Light sources;Streaming media","augmented reality;lighting;mobile computing","interactive near-field illumination;photorealistic augmented reality;mobile device;AR application;mixed reality;differential illumination method;virtual objects;necessary computation effort;illumination condition;illumination source","","19","","46","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Improved feature detection over large force ranges using history dependent transfer functions","P. B. Persson; M. D. Cooper; G. E. Host; L. A. E. Tibell; A. Ynnerman","Department of Science and Technology, Sweden; Department of Science and Technology, Sweden; Department of Science and Technology, Sweden; Department of Clinical and Experimental Medicine, Sweden; Department of Science and Technology, Linköping University, Sweden","World Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","3 Apr 2009","2009","","","476","481","In this paper we present a history dependent transfer function (HDTF) as a possible approach to enable improved haptic feature detection in high dynamic range (HDR) volume data. The HDTF is a multi-dimensional transfer function that uses the recent force history as a selection criterion to switch between transfer functions, thereby adapting to the explored force range. The HDTF has been evaluated using artificial test data and in a realistic application example, with the HDTF applied to haptic protein-ligand docking. Biochemistry experts performed docking tests, and expressed that the HDTF delivers the expected feedback across a large force magnitude range, conveying both weak attractive and strong repulsive protein-ligand interaction forces. Feature detection tests have been performed with positive results, indicating that the HDTF improves the ability of feature detection in HDR volume data as compared to a static transfer function covering the same range.","","978-1-4244-3858-7","10.1109/WHC.2009.4810843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810843","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Haptic I/O;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Virtual reality;J.3 [Life and Medical Sciences]: Biology and Genetics;K.3.1 [Computers and Education]: Computer Uses in Education—Computer-assisted instruction (CAI)","Computer vision;History;Transfer functions;Testing;Haptic interfaces;Switches;Proteins;Performance evaluation;Force feedback;Dynamic range","biology computing;feature extraction;haptic interfaces;molecular biophysics;proteins;rendering (computer graphics);transfer functions","haptic feature detection;large force range;history dependent transfer function;high dynamic range volume data;protein molecule binding;haptic volume rendering;haptic protein-ligand docking","","1","","25","","3 Apr 2009","","","IEEE","IEEE Conferences"
"Construction of virtual 3D life-like human hand for real-time human-computer interaction","S. Ren; H. Wu; Y. Wan","Institute for Signals and Information Processing, Lanzhou University, China; Institute for Signals and Information Processing, Lanzhou University, China; Institute for Signals and Information Processing, Lanzhou University, China","2015 Seventh International Conference on Advanced Computational Intelligence (ICACI)","13 Aug 2015","2015","","","345","350","Virtual hand plays an important role in many human-computer interaction applications. However, modeling a life-like 3D virtual hand is not a trivial task, especially the creation of flexible skin textures. In this paper, we propose a new method to execute texture mapping for the 3D virtual hand model automatically and apply it for human-computer interaction in real-time. Specifically, We first adjust the palm image using the mirror of the dorsal image. Then color transfer is performed for the palm image to ensure its similar color as the dorsal image. Finally interpolation is used to further adjust the direct texture mapping considering the virtual seams in fingers and thumb side. The proposed method is very efficient and experimental results show that the generated virtual hand model is more realistic than conventional virtual hands and can yield a stronger sense of immersion in the virtual world.","","978-1-4799-7259-3","10.1109/ICACI.2015.7184727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184727","","Computational modeling;Thumb;Indexes;Joints","human computer interaction;interpolation;solid modelling;virtual reality","virtual 3D life like human hand;real-time human-computer interaction;flexible skin textures;texture mapping;3D virtual hand model;palm image;dorsal image;interpolation;virtual seams;virtual world","","","","23","","13 Aug 2015","","","IEEE","IEEE Conferences"
"Task planning for human robot interactive processes","N. Wantia; M. Esen; A. Hengstebeck; F. Heinze; J. Rossmann; J. Deuse; B. Kuhlenkoetter","Dept. Robot Technology, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Production Automation, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Industrial Engineering, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Robot Technology, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Robot Technology, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Industrial Engineering, Institute for Research and Transfer, RIF e.V., Dortmund, Germany; Dept. Production Automation, Institute for Research and Transfer, RIF e.V., Dortmund, Germany","2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)","7 Nov 2016","2016","","","1","8","One of the next steps in factory automation might not exclusively be an automation issue, but instead combining robot and human skills to further improve industrial work processes. For various reasons, there is still a low dissemination of hybrid work processes characterized by direct human robot interaction. For instance, it is very difficult to decide which manual work processes are eligible for a transformation to a human-robot interactive process. Thus, the research project MANUSERV delivers a tool to support this decision process. Here, the central concept is a task planning system capable of generating automated as well as hybrid human-robot solutions. Therefore, a structured description of manual work processes forms the input to the planning system. Subsequently, a simulation system verifies and evaluates the proposed solutions and generates the necessary information for a transformation of the planning results to a real application scenario.","","978-1-5090-1314-2","10.1109/ETFA.2016.7733523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733523","task planning;manual process;service robot;simulation;process description","Planning;Service robots;Manuals;Human-robot interaction;Automation;Collaboration","factory automation;human-robot interaction;industrial robots;planning","human robot interactive process;factory automation;industrial work;MANUSERV;task planning system","","4","","24","","7 Nov 2016","","","IEEE","IEEE Conferences"
"Keyboard control method for virtual reality micro-robotic cell injection training","S. Faroque; B. Horan; M. Joordens","School of Engineering, Deakin University Geelong, VIC, Australia; School of Engineering, Deakin University Geelong, VIC, Australia; School of Engineering, Deakin University Geelong, VIC, Australia","2015 10th System of Systems Engineering Conference (SoSE)","9 Jul 2015","2015","","","416","421","The rapid development of virtual reality offers significant potential for skills training applications. Our ongoing work proposes virtual reality operator training for the micro-robotic cell injection procedure. The interface between the operator and the system can be achieved in many different ways. The computer keyboard is ubiquitous in its use for everyday computing applications and also commonly utilized in virtual reality systems. Based on the premise that most people have experience in using a computer keyboard, as opposed to more sophisticated input devices, this paper considers the feasibility of using a keyboard to control the micro-robot for cell injection. In this study, thirteen participants underwent the experimental evaluation. The participants were asked to perform three simulated trial sessions in a virtual micro-robotic cell injection environment. Each session consisted of ten cell injection trials and relevant data for each trial were recorded and analyzed. Results showed participants' performance improvement after the three sessions. It was also observed that participants intuitively controlled multiple axes of the micro-robot simultaneously despite the absence of instruction on how to do so. This continued throughout the experiments and suggests skills transfer from other keyboard based interactions. Based on the results provided, it is suggested that keyboard control is a feasible, simple and low-cost control method for the virtual micro-robot.","","978-1-4799-7611-9","10.1109/SYSOSE.2015.7151946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7151946","Cell injection;virtual reality;micromanipulation;micro-robot;skills training","Keyboards;Haptic interfaces;Training;Systems engineering and theory;Computers;Virtual environments","biomedical education;cellular biophysics;computer based training;keyboards;microrobots;virtual reality","keyboard control method;virtual reality microrobotic cell injection training;skill training applications;virtual reality operator training;microrobotic cell injection procedure;ubiquitous computer keyboard;input devices;keyboard based interactions;low-cost control method","","1","","15","","9 Jul 2015","","","IEEE","IEEE Conferences"
"Tangible hypermedia using the ARToolKit","P. Sinclair; K. Martinez","Dept. of Electron. & Comput. Sci., Southampton Univ., UK; Dept. of Electron. & Comput. Sci., Southampton Univ., UK","The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","2 pp.","","We propose a demonstration of the use of hypermedia In augmented reality. The system explores some possible modes of interaction that embody the functionality of open hypermedia and contextual linking using commonplace and easily understandable real world metaphors.","","0-7803-7680-3","10.1109/ART.2002.1106990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106990","","Augmented reality;Kirk field collapse effect;Intelligent agent;Computer science;Joining processes;Tiles;Layout;Context modeling","hypermedia;augmented reality;interactive systems;computer interfaces;software tools","hypermedia;augmented reality;interaction;open hypermedia;functionality;contextual linking;real world metaphors;tangible interfaces","","1","","5","","6 Jan 2003","","","IEEE","IEEE Conferences"
"The application of complex research simulation models in education; A generic approach","W. Dassen; T. Arts; P. M. van Dam; N. H. L. Kuijpers; E. Hermeling; E. M. van Dam; T. Delhaas","Dept. Cardiology, Maastricht University, Maastricht, the Netherlands; Dept. Biomedical Engineering, Maastricht University, Maastricht, the Netherlands; Dept. Cognitive NeuroScience, Radboud University Medical Center, Nijmegen, the Netherlands; Dept. Biomedical Engineering, Maastricht University, Maastricht, the Netherlands; Dept. Biomedical Engineering, Maastricht University, Maastricht, the Netherlands; Peacs, Arnhem, the Netherlands; Dept. Biomedical Engineering, Maastricht University, Maastricht, the Netherlands","2011 Computing in Cardiology","9 Mar 2012","2011","","","465","468","Computer models are frequently used for scientific research. Since scientific models are usually targeted at a specific application, using them in an educational setting is not straightforward. At Maastricht University, the CircAdapt model of heart and circulation has been developed as a scientific research tool. The model describes hemodynamic interaction between the left and right ventricle as well as mechanical interaction of three wall segments. Pressure-volume relations are obtained by simulating the pulmonary and systemic circulations. To transfer this model to a user-friendly educational tool, an approach was chosen allowing stepwise definition of educational cases (e.g., exercise, hypovolemic shock, and cardio-genic shock) in teacher mode. In student mode, interactive simulation is possible by changing model parameters defined in teacher mode. Consistent use of the same graphical user interface throughout the curriculum familiarizes students with computer simulations.","2325-8853","978-1-4577-0611-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164603","","Computational modeling;Adaptation models;Biological system modeling;Heart rate;Electric shock;Blood","biology computing;computer aided instruction;educational institutions;graphical user interfaces;haemodynamics;human computer interaction","complex research simulation models;computer models;scientific research tool;scientific models;educational setting;Maastricht University;CircAdapt model;hemodynamic interaction;mechanical interaction;pressure-volume relations;pulmonary circulation simulation;systemic circulation simulation;educational tool;educational cases;interactive simulation;teacher mode;student mode;graphical user interface","","1","","3","","9 Mar 2012","","","IEEE","IEEE Conferences"
"Mapping distributed interactive simulation network requirements onto broadband networks and services","T. L. Gehl","IBM, Federal Syst. Co., USA","Proceedings of ICC/SUPERCOMM'94 - 1994 International Conference on Communications","6 Aug 2002","1994","","","154","159 vol.1","Just imagine, being able to immerse yourself into a virtual world where you could see the results of your actions with respect to other realistic entities (i.e. people, machines, environments) in a particular scenario. Then, imagine the expansion of this virtual world through the interaction of real-time distributed simulations. This interaction of real-time distributed simulations could greatly enhance education and training, operational verification of design, and mission rehearsal functions needed to support the defense and commercial industries. Through the advancements in technologies (i.e. visual systems, host processors, databases, and networks), we can now realize the expectations of this virtual world. This paper addresses the network requirements for a federal government initiative called Distributed Interactive Simulation (DIS). Through our discussion of the DIS environment, we will identify the services and performance capabilities required of advanced network architectures, and relate these requirements to the proposed capabilities of the standard-based broadband technologies of asynchronous transfer mode (ATM) and synchronous optical network (SONET). We map the DIS network requirements to the capabilities inherent to these broadband technologies, and specify the access of those capabilities from the DIS application through the broadband network and services.<>","","0-7803-1825-0","10.1109/ICC.1994.369004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=369004","","Broadband communication;Computational modeling;Computer simulation;Protocols;Distributed computing;Application software;Industrial training;Visual databases;Asynchronous transfer mode;Space technology","B-ISDN;asynchronous transfer mode;SONET;digital simulation;telecommunication computing;interactive systems","distributed interactive simulation;broadband networks;broadband services;real-time distributed simulations;federal government initiative;performance;network architectures;asynchronous transfer mode;synchronous optical network;ATM;SONET;B-ISDN","","","","2","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Robot Patient Design to Simulate Various Patients for Transfer Training","Z. Huang; C. Lin; M. Kanai-Pak; J. Maeda; Y. Kitajima; M. Nakamura; N. Kuwahara; T. Ogata; J. Ota","School of Automation, Guangdong University of Technology, Guangzhou, China; Research into Artifacts, Center for Engineering, University of Tokyo, Chiba, Japan; Kanto Gakuin University, Yokohama, Japan; Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, Japan; Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, Japan; Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, Japan; Department of Advanced Fibro-Science, Kyoto Institute of Technology, Kyoto, Japan; Research into Artifacts, Center for Engineering, University of Tokyo, Chiba, Japan; Research into Artifacts, Center for Engineering, University of Tokyo, Chiba, Japan","IEEE/ASME Transactions on Mechatronics","13 Oct 2017","2017","22","5","2079","2090","To improve the patient transfer skill of nursing education students, we developed a robot patient that can simulate three categories of patients: 1) patients whose movements are affected by paralysis; 2) patients whose movements are sensitive to pain with painful expression; and 3) patients whose movements are constrained by medical devices. By practicing with the robot patient, nursing students can learn the skills required for interacting with various patients. To simulate trunk movements of these different patients, novel waist and hip joints with hardware-inherent compliance and force-sensing capability were proposed. In addition, control methods were developed and the parameters were tuned based on actual patient videos. To evaluate the developed robot, nursing teachers performed trials of transferring the robot patient as they would transfer an actual patient. The nursing teachers scored the robot patients based on a checklist. Moreover, subjective evaluations of a questionnaire were performed by the nursing teachers. The results showed that the nursing teachers performed most of the required skills of the checklist and agreed regarding the learning effectiveness of the robot. They recommended training nursing students using the robot patient in the questionnaire. Finally, hugging speed comparison showed that the nurses slow down the speed when dealing with a robot patient with painful expression.","1941-014X","","10.1109/TMECH.2017.2730848","Japan Society for the Promotion of Science KAKENHI; National Science Foundation of China; Natural Science Foundation of Guangdong Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990192","Compliant joint;patient transfer;robot patient;various patients","Training;Medical services;Robot sensing systems;Hip;Force","biomedical education;computer aided instruction;control system synthesis;educational robots;human-robot interaction;medical computing;medical robotics;patient care","nursing teachers;robot patient design;patient transfer skill;nursing education students;force-sensing capability;control parameter tuning;transfer training","","","","48","Traditional","24 Jul 2017","","","IEEE","IEEE Journals"
"IMA-VR: A multimodal virtual training system for skills transfer in Industrial Maintenance and Assembly tasks","T. Gutiérrez; J. Rodríguez; Y. Vélaz; S. Casado; A. Suescun; E. J. Sánchez","LABEIN-TECNALIA, Spain; Department of Applied Mechanics, CEIT and TECNUN, Spain; Department of Applied Mechanics, CEIT and TECNUN, Spain; LABEIN-TECNALIA, Spain; Department of Applied Mechanics, CEIT and TECNUN, Spain; Department of Applied Mechanics, CEIT and TECNUN, Spain","19th International Symposium in Robot and Human Interactive Communication","11 Oct 2010","2010","","","428","433","Industrial Maintenance and Assembly is a very complex task involving both cognitive skills (procedural skills) and motor skills (fine motor control and bi-manual coordination skills). This paper presents a controlled multimodal training system, for transferring the motor and cognitive skills involved in these tasks. The new platform provides different multimodal aids and learning strategies that help and guide the users during their training process. One of the main features of this system is its flexibility to adapt itself to the task demands and to the users' preferences and needs supporting different configurations. To address bi-manual operations the platform offers different alternatives, one of them is a set-up composed of a haptic device to track the motion of the operator's dominant hand and simulate the physical interaction within the virtual environment, together with a marker-less motion capture system to track the motion of the other hand in real time.","1944-9437","978-1-4244-7990-0","10.1109/ROMAN.2010.5598643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598643","","Haptic interfaces;Training;Assembly;Rendering (computer graphics);Visualization;Force;Message systems","assembling;computer based training;haptic interfaces;maintenance engineering;production engineering computing;virtual reality","multimodal virtual training system;skills transfer;industrial maintenance task;industrial assembly task;multimodal aids;learning strategy;haptic device;marker-less motion capture system;IMA-VR training system;virtual reality","","20","","18","","11 Oct 2010","","","IEEE","IEEE Conferences"
"Research on key technology of machining simulation based on Web","T. Yu; W. Liang; P. Guan; W. Wang","School of Mechanical Engineering and Automation, Northeastern University, Shenyang, P.R. China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, P.R. China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, P.R. China; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, P.R. China","2010 IEEE International Conference on Industrial Technology","27 May 2010","2010","","","1071","1076","According to the single release form of product information on the network in machine manufacturing enterprises and the high price of the conventional NC simulation software, a machining simulation system based on Web is developed in this paper. Making use of the superiority of Java and JavaScript on Web and the interaction of VRML technology, 3D browsing, virtual assembly and machining simulation are realized on the network. Based on the authenticity and real-time of virtual reality, a network distributed virtual simulation system is designed. This method of NC machining simulation is not dependent on the expensive CAD/CAM software, and for the size of the system files is small, it can be transferred on the network conveniently. Using the browser with the plug as the client, the system has free installation, authenticity, interaction, low cost, portability, low requirements for the client, etc.","","978-1-4244-5697-0","10.1109/ICIT.2010.5472567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5472567","","Machining;Java;Pulp manufacturing;Virtual manufacturing;Assembly;Real time systems;Virtual reality;Design automation;Computer aided manufacturing;CADCAM","CAD/CAM;Internet;Java;machining;numerical control;virtual reality","World Wide Web;product information;machine manufacturing enterprises;NC simulation software;JavaScript;VRML technology;3D browsing;virtual assembly;virtual reality;network distributed virtual simulation system;NC machining simulation;CAD/CAM software","","","","8","","27 May 2010","","","IEEE","IEEE Conferences"
"Perceptually augmented simulator design through decomposition","T. Edmunds; D. K. Pai","Rutgers University, USA; University of British Columbia, USA","World Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","3 Apr 2009","2009","","","505","510","We approach the problem of determining a general method for augmenting haptic simulators to amplify the perceptually salient aspects of the interaction that induce effective skill transfer. Using such a method, we seek to simplify the design of haptic simulators that can improve training effectiveness without requiring expensive improvements in the capability of the rendering hardware. We present a decomposition approach to the automated design of perceptually augmented simulations, and we describe a user-study of the training effectiveness of a search-task simulator designed using our approach vs. an un-augmented simulator. The results indicate that our decomposition approach allows existing psychophysical findings to be leveraged in the design of haptic simulators that effectively impart skill by targeting perceptually significant aspects of the interaction.","","978-1-4244-3858-7","10.1109/WHC.2009.4810890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4810890","","Haptic interfaces;Engines;Shape;Analytical models;Surface texture;Hardware;Surgery;Jamming;Virtual environment;Teleoperators","augmented reality;haptic interfaces;rendering (computer graphics)","perceptually augmented simulator design;haptic simulators augmentation;rendering hardware;search-task simulator","","1","","14","","3 Apr 2009","","","IEEE","IEEE Conferences"
"Human performance evaluation of manipulation schemes in virtual environments","S. Zhai; P. Milgram","Dept. of Ind. Eng., Toronto Univ., Ont., Canada; Dept. of Ind. Eng., Toronto Univ., Ont., Canada","Proceedings of IEEE Virtual Reality Annual International Symposium","6 Aug 2002","1993","","","155","161","Presents the results of one of the first experiments in a research program aimed at systematically investigating manipulation schemes for spatial input, from a human factors point of view. A 3D design space model is proposed as a framework for such investigations, and four options within this model are tested in a six degree-of-freedom target acquisition task within a virtual environment. Experimental results indicate strong performance advantages for isometric sensing combined with rate control and for isotonic sensing combined with position control. A strong interaction between sensing mode and mapping function is found. The findings are discussed in relation to the literature on spatial manipulation.<>","","0-7803-1363-1","10.1109/VRAIS.1993.380784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=380784","","Taxonomy;Virtual environment;Telerobotics;Human factors;Position control;Virtual reality;Systematics;Transfer functions;Ergonomics;Control systems","virtual reality;human factors;position control;performance evaluation;man-machine systems","human performance evaluation;manipulation schemes;virtual environments;spatial input;human factors;3D design space model;target acquisition task;isometric sensing;rate control;isotonic sensing;position control;sensing mode;mapping function;spatial manipulation","","34","14","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"BReA: Potentials of combining reality and virtual communications using a blended reality agent","Y. Kanai; H. Osawa; M. Imai","Faculty of Science and Technology, Keio University, Japan; Faculty of Science and Technology, Keio University, Japan; Faculty of Science and Technology, Keio University, Japan","2013 IEEE RO-MAN","15 Oct 2013","2013","","","604","609","This paper proposes a blended reality agent, called BReA, whose body presents and transfers itself between real and virtual environments. BReA can communicate with people through not only real-world communication but also virtual communication. Recent research studies have shown the merits and demerits of communication with robotic agents under a real-world environment, and on-screen agents in a virtual world. BReA, whose body can be located in both real and virtual worlds, possesses the merits of both robotic and on-screen agents. In addition, the feature through which BReA communicates with people in real and virtual environments allows users to acknowledge real-world objects based on its references from a virtual environment. We conducted a field test in a supermarket to confirm whether customers can engage themselves in communication with BReA. An analysis of the consumer reactions confirmed that customers definitely recognized that the directions of the pointing gestures performed by BReA in a virtual environment were oriented outside of the display. Moreover, we observed some customers approaching closer to BReA as it transferred from a real environment into a virtual world. These results demonstrated that BReA succeeds in immersing customers in its presentation.","1944-9437","978-1-4799-0509-6","10.1109/ROMAN.2013.6628546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628546","","Virtual environments;Robot kinematics;Educational institutions;Buildings;Speech;Games","gesture recognition;human computer interaction;human-robot interaction;virtual reality","BReA;virtual communication;blended reality agent;reality communication;virtual environments;robotic agents;real-world environment;virtual worlds;on-screen agents;consumer reactions;gesture pointing direction recognition","","1","","18","","15 Oct 2013","","","IEEE","IEEE Conferences"
"Reproducing Interaction Contingency Toward Open-Ended Development of Social Actions: Case Study on Joint Attention","H. Sumioka; Y. Yoshikawa; M. Asada","Graduate School of Eng., Osaka Univ., Suita, Japan; JST ERATO Asada Synergistic Intelligence Project; Graduate School of Eng., Osaka Univ., Suita, Japan","IEEE Transactions on Autonomous Mental Development","11 Mar 2010","2010","2","1","40","50","How can human infants gradually socialize through interaction with their caregivers? This paper presents a learning mechanism that incrementally acquires social actions by finding and reproducing the contingency in interaction with a caregiver. A contingency measure based on transfer entropy is used to select the appropriate pairs of variables to be associated to acquire social actions from the set of all possible pairs. Joint attention behavior is tested to examine the development of social actions caused by responding to changes in caregiver behavior due to reproducing the found contingency. The results of computer simulations of human-robot interaction indicate that a robot acquires a series of actions related to joint attention such as gaze following and alternation in an order that almost matches the infant development of joint attention found in developmental psychology. The difference in the order between them is discussed based on the analysis of robot behavior, and then future issues are given.","1943-0612","","10.1109/TAMD.2010.2042167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406083","Contingency chain;joint attention;sequential acquisition of social behavior;transfer entropy","Robot sensing systems;Electroencephalography;Human robot interaction;Learning systems;Entropy;Psychology;Enterprise resource planning;Face;Testing;Computer simulation","behavioural sciences;human-robot interaction","reproducing interaction contingency;open ended development;social actions;joint attention behavior;learning mechanism;transfer entropy;human robot interaction;robot behavior","","18","","44","","5 Feb 2010","","","IEEE","IEEE Journals"
"Effect of perspective change in body ownership transfer to teleoperated android robot","K. Ogawa; K. Taura; S. Nishio; H. Ishiguro","Advanced Telecommunications Research Institute International (ATR), 2-2 Hikaridai, Keihanna Science City, Kyoto 6190288, Japan; Advanced Telecommunications Research Institute International (ATR), 2-2 Hikaridai, Keihanna Science City, Kyoto 6190288, Japan; Advanced Telecommunications Research Institute International (ATR), 2-2 Hikaridai, Keihanna Science City, Kyoto 6190288, Japan; Advanced Telecommunications Research Institute International (ATR), 2-2 Hikaridai, Keihanna Science City, Kyoto 6190288, Japan","2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication","10 Nov 2012","2012","","","1072","1077","We previously investigated body ownership transfer to a teleoperated android body caused by motion synchronization between the robot and its operator. Although visual feedback is the only information provided from the robot, due to body ownership transfer, some operators feel as if they were touched when the robot's body was touched. This illusion can help operators transfer their presence to the robotic body during teleoperation. By enhancing this phenomenon, we can improve our communication interface and the quality of the interaction between operator and interlocutor. In this paper, we examined how the change in the operator's perspective affects the body ownership transfer during teleoperation. Based on past studies on the rubber hand illusion, we hypothesized that the perspective change will suppress the body owner transfer. Our results, however, showed that in any perspective condition, the participants felt the body ownership transfer. This shows that its generation process may differ in the body ownership transfer for teleoperated androids and the rubber hand illusion.","1944-9437","978-1-4673-4606-1","10.1109/ROMAN.2012.6343891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6343891","","Synchronization;Thyristors;Rubber;Mirrors;Robots;Visualization;Delay","cognition;humanoid robots;human-robot interaction;Linux;synchronisation;telerobotics;user interfaces;virtual reality","body ownership transfer;teleoperated android robot;motion synchronization;robot operator;visual feedback;communication interface improvement;interaction quality improvement;interlocutor;operator perspective change;perspective condition;rubber hand illusion","","11","","14","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Using the Distributed Co-Simulation Protocol for a Mixed Real-Virtual Prototype","P. Baumann; M. Krammer; M. Driussi; L. Mikelsons; J. Zehetner; W. Mair; D. Schramm","Robert Bosch GmbH, Simulation and Requirements Engineering, Renningen, Germany; VIRTUAL VEHICLE Research Center, Co-Simulation and Software Group, Graz, Austria; VIRTUAL VEHICLE Research Center, Co-Simulation and Software Group, Graz, Austria; Chair for Mechatronics, University of Augsburg, Augsburg, Germany; AVL List GmbH, Integrated Open Development Platform, Graz, Austria; Spath Micro Electronic Design GmbH, Graz, Austria; Chair for Mechatronics, University of Duisburg-Essen, Duisburg, Germany","2019 IEEE International Conference on Mechatronics (ICM)","27 May 2019","2019","1","","440","445","Future automotive technologies become more and more autonomous and connected. This trend requires a rethinking of validation processes due to the amount of test kilometers needed. To be able to test automated and connected functions in many different traffic scenarios, virtual and mixed real-virtual prototypes will be used. Moreover, due to the complexity of such systems, cross-company cooperation is increasing and demands for common prototypes. Spatially distributed prototypes simplify and enhance cross-company collaboration due to faster provisioning of models and better IP protection. However, the setup of such prototypes is very time consuming due to the high integration effort. Here it is shown that the integration effort of spatially distributed prototypes can be massively reduced by using the Distributed Co-Simulation Protocol (DCP). A demonstrator consisting of a small scale test bed located in Graz and a co-simulation containing Bosch driving functions located in Renningen is presented. The demonstrated integration workflow as well as an analysis of the communication challenges of the coupling can be transferred to any other coupling of this kind.","","978-1-5386-6959-4","10.1109/ICMECH.2019.8722844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8722844","DCP;FMI;XiL;co-simulation;real-time","Real-time systems;Couplings;Prototypes;Software;Hardware;Protocols;Numerical models","computer simulation;driver information systems;human computer interaction;virtual prototyping","real-virtual prototype;cross-company cooperation;cross-company collaboration;high integration effort;spatially distributed prototypes;co-simulation containing Bosch driving functions;validation processes;test kilometers;distributed co-simulation protocol;automotive technologies;traffic scenarios","","3","","19","","27 May 2019","","","IEEE","IEEE Conferences"
"Design of the Interactive Sharing Transfer Protocol","R. Waters; D. B. Anderson; D. L. Schwenke","Mitsubishi Electr. Res. Lab., Cambridge, MA, USA; NA; NA","Proceedings of IEEE 6th Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises","6 Aug 2002","1997","","","140","147","The Interactive Sharing Transfer Protocol (ISTP) supports the sharing of information about a virtual world among a group of processes. The key advantages of ISTP are that it supports: near real-time interaction among users; the communication of every kind of information required in a virtual world; and scalability to large numbers of users and large virtual worlds. ISTP was developed in the context of the Spline platform for distributed virtual environments (DVEs).","","0-8186-7967-0","10.1109/ENABL.1997.630805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630805","","Scalability;Virtual environment;Multicast protocols;Bandwidth;Context;Spline;Computer simulation;Personal communication networks;Internet;Modems","protocols;real-time systems;user interfaces;distributed processing;virtual reality;interactive systems","Interactive Sharing Transfer Protocol;ISTP;information sharing;virtual world;real-time interaction;scalability;Spline platform;distributed virtual environments;multicast;HTTP","","9","8","6","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Simulation-based Design of Transfer Support Robot and Experimental Verification","Y. Imamura; Y. Endo; E. Yoshida","UM13218/RL, Intelligent Systems Research Institute, National Institute of Advanced Industrial Science and Technology, CNRS-AIST Joint Robotics Laboratory, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8560, Japan; UM13218/RL, Intelligent Systems Research Institute, National Institute of Advanced Industrial Science and Technology, CNRS-AIST Joint Robotics Laboratory, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8560, Japan; UM13218/RL, Intelligent Systems Research Institute, National Institute of Advanced Industrial Science and Technology, CNRS-AIST Joint Robotics Laboratory, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8560, Japan","2019 2nd IEEE International Conference on Soft Robotics (RoboSoft)","27 May 2019","2019","","","754","761","The need for robotic care devices that support the movements of the elderly is increasing with demographic changes in modern societies. Such devices should be designed and controlled while considering their physical effects on users, since the devices make direct contact with the users and move their body. However, human physical burdens are difficult to evaluate for machines that undergo complex interactions with humans, and little research has focused on the care robots' effects on the human body. We have proposed a simulation-based optimization method of the design parameters, which uses a digital model of the human body. The user is represented as a link model, and the joint torques and contact forces on this model are analyzed. The motion trajectories of the device were then designed according to the simulation results. To verify this design method, we then performed an experiment with human subjects and measured the contact forces between human and device using a custom mockup of the transfer aid robot.","","978-1-5386-9260-8","10.1109/ROBOSOFT.2019.8722807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8722807","Soft Robot Applications;Physically Assistive Devices;Product Design;Physical Human-Robot Interaction;Digital human","Robots;Trajectory;Analytical models;Force;Biological system modeling;Design methodology;Ergonomics","design engineering;geriatrics;human-robot interaction;medical robotics;mobile robots;optimisation;patient care;service robots","digital model;human body;link model;contact forces;transfer aid robot;simulation-based design;transfer support robot;experimental verification;robotic care devices;human physical burdens;simulation-based optimization method;elderly movements;physical human-robot interaction","","","","19","","27 May 2019","","","IEEE","IEEE Conferences"
"Control-theoretic model of haptic human-human interaction in a pursuit tracking task","D. Feth; R. Groten; A. Peer; M. Buss","Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany; Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany; Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany; Institute of Automatic Control Engineering, Technische Universität München, Munich, Germany","RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication","10 Nov 2009","2009","","","1106","1111","Achieving natural and intuitive interaction is one of the main challenges in physical human-robot interaction. We approach this challenge by modeling haptic human-human interaction with the final goal of transferring found relationships to human-robot interaction. The focus of this paper is on two human operators performing collaboratively a joint object manipulation, i.e. a pursuit tracking task. McRuer's crossover model is a well established method to describe the behavior of one human operator performing such a task. In this paper, we extent McRuer's approach to two human operators performing the task collaboratively. Results based on experimetally gained data show that the interacting partners adapt their behavior to each other and to the task in such a way that the crossover model can still be applied to the interacting dyad. It is also shown that the individual's behavior changes when interacting with a partner in contrast to performing the task alone.","1944-9437","978-1-4244-5081-7","10.1109/ROMAN.2009.5326316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5326316","","Haptic interfaces;Hidden Markov models;Human robot interaction;Robotics and automation;Feedback;Collaboration;Speech analysis;Communication system control;Virtual reality;Avatars","groupware;haptic interfaces;human-robot interaction;manipulators","haptic human-human interaction;pursuit tracking task;control-theoretic model;human-robot interaction;collaborative work;joint object manipulation;McRuers crossover model;interacting dyad","","8","","18","","10 Nov 2009","","","IEEE","IEEE Conferences"
"Binocular Phase-Coded Visual Stimuli for SSVEP-Based BCI","I. Kramberger; Z. kačič; G. Donaj","Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia","IEEE Access","22 Apr 2019","2019","7","","48912","48922","This paper presents a method of binocular visual stimulation for brain-computer interfaces (BCIs) based on steady-state visual evoked potentials (SSVEPs) using phase-coded symbols. The proposed method's emphasis is on a binocular phase-coded visual stimulus, which is based on the phase differences between the left- and right-eye stimuli, and a symbol detection and recognition procedure based on SSVEP response of the left and right occipital lobes of the user's scalp, where the SSVEP response is obtained as electroencephalography (EEG) signaling. The symbols are coded as phase differences and maintain the same frequency of the sine wave-modulated light provided to the user's left and right eyes as a binocular visual stimulation. Based on this method, a basic system setup is presented to explore the possibilities of binocular phase-coded visual stimuli for virtual or augmented reality applications, where the binocular visual stimulation was achieved by the specially designed head-mounted displays. Multiple visually coded targets are realized as eight different phase-coded binocular symbols and further evaluated as a random sequence of single targets, thus representing the situations in virtual or augmented reality, where multiple visually coded targets are present but not visualized to the user simultaneously within the same field of view. The offline results obtained from ten healthy subjects revealed that an average symbol recognition accuracy of 90.63% and an information transfer rate (ITR) of 70.55 bits/min were achieved for a symbol stimulation time of 2 s. The results of this paper demonstrate the feasibility of using binocular visual stimuli for SSVEP-based BCIs, where reasonable ITR is achieved using single-frequency binocular phase-coded symbols. The proposed method indicates the possibility of combining it with 3D wearable visualization technologies, such as binocular head-mounted displays (HMDs), in order to improve the intuitiveness of the interaction with more immersive user experience using BCI modalities.","2169-3536","","10.1109/ACCESS.2019.2910737","Javna Agencija za Raziskovalno Dejavnost RS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688386","Brain computer interfaces;brain stimulation;electroencephalography;data acquisition;human computer interaction;phase measurement;steady state visually evoked potential;three-dimensional displays;visualization;virtual reality;wearable sensors","Visualization;Electroencephalography;Encoding;Frequency modulation;Light emitting diodes;Liquid crystal displays;Phase measurement","augmented reality;brain-computer interfaces;electroencephalography;eye;helmet mounted displays;medical signal detection;medical signal processing;neurophysiology;visual evoked potentials","right-eye stimuli;symbol detection;recognition procedure;SSVEP response;phase differences;binocular visual stimulation;binocular phase-coded visual stimuli;multiple visually coded targets;symbol stimulation time;SSVEP-based BCIs;single-frequency binocular phase-coded symbols;3D wearable visualization technologies;binocular head-mounted displays;left- eye stimuli;brain-computer interfaces;steady-state visual evoked potentials;electroencephalography signaling;sine wave-modulated light;augmented reality applications;virtual reality applications;symbol recognition accuracy;information transfer rate;time 2.0 s","","","","38","","11 Apr 2019","","","IEEE","IEEE Journals"
"Towards understanding the capability of spatial audio feedback in virtual environments for people with visual impairments","M. Dong; R. Guo",Kennesaw State University; Kennesaw State University,"2016 IEEE 2nd Workshop on Everyday Virtual Reality (WEVR)","23 Feb 2017","2016","","","15","20","This research analyzes if and how the Head Related Transfer Function (HRTF) can be used to support effective Human-Computer Interaction when people in a Virtual Environment (VE) without visual feedback. If sounds can be located in a VE by using HRTF only, designing and developing considerably safer but diversified training environments might greatly benefit individuals with visual impairments. To investigate this, we ran 2 usability studies: 1) to ascertain whether the HRTF could provide sufficient position information in VEs; 2) to learn whether the HRTF could provide sufficient distance and direction information in VEs. The results showed that a continuous audio feedback could help navigate in a VE without vision feedback.","","978-1-5090-0840-7","10.1109/WEVR.2016.7859538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859538","Assistive technology; HRTF; 3D Audio; user study","Visualization;Training;Three-dimensional displays;Virtual environments;Navigation;Headphones;Haptic interfaces","feedback;human computer interaction;transfer functions;virtual reality;vision defects","spatial audio feedback;virtual environments;visual impairments;head related transfer function;HRTF;human-computer interaction;VE;diversified training environments might;position information;direction information","","1","","19","","23 Feb 2017","","","IEEE","IEEE Conferences"
"Towards understanding the differences of using 3D auditory feedback in virtual environments between people with and without visual impairments","M. Dong; H. Wang; R. Guo","Kennesaw State University; Georgia State University, GA, USA; Kennesaw State Univ., Kennesaw, GA, USA","2017 IEEE 3rd VR Workshop on Sonic Interactions for Virtual Environments (SIVE)","17 Apr 2017","2017","","","1","5","The research objective was to exploration to determine any differences of using a generic Head Related Transfer Function (HRTF) to simulate 3D auditory feedback in Virtual Environments (VEs) without visual feedback between people with and without Visual Impairments (VI). To investigate this, we designed and developed a usability study which included the two tasks of recognizing the positions and distinguishing the moving directions of 3D sound sources in a VE. We conducted this study with 20 participants using VI (7 males and 13 females), and 18 participants without VI (7 males and 11 females). The results indicated different experiences in the VE between people with VI and without VI.","","978-1-5386-0459-5","10.1109/SIVE.2017.7901608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7901608","Assistive technology;HRTF;3D Audio;user study;Human-center computing;Interaction devices;Accessibility;Accessibility technologies","Three-dimensional displays;Visualization;Virtual environments;Training;Headphones;Estimation;Haptic interfaces","handicapped aids;virtual reality;vision defects","3D auditory feedback;virtual environments;people with visual impairments;head related transfer function;HRTF;3D auditory feedback simulate;VE;visual feedback;VI;3D sound sources;assistive technology","","1","","18","","17 Apr 2017","","","IEEE","IEEE Conferences"
"Augmented-Reality-Based Visualization of Navigation Data of Mobile Robots on the Microsoft Hololens - Possibilities and Limitations","L. Kästner; J. Lambrecht","Technical University of Berlin,Chair Industry Grade Networks and Clouds Department,Berlin,Germany; Technical University of Berlin,Chair Industry Grade Networks and Clouds Department,Berlin,Germany","2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","19 May 2020","2019","","","344","349","The demand for mobile robots has rapidly increased in recent years due to the flexibility and high variety of application fields comparing to static robots. To deal with complex tasks such as navigation, they work with high amounts of different sensor data making it difficult to operate with for non-experts. To enhance user understanding and human robot interaction, we propose an approach to visualize the navigation stack within a cutting edge 3D Augmented Reality device -the Microsoft Hololens. Therefore, relevant navigation stack data including laser scan, environment map and path planing data are visualized in 3D within the head mounted device. Based on that prototype, we evaluate the Hololens in terms of computational capabilities and limitations for dealing with huge amount of real-time data. Results show that the Hololens is capable of a proper visualization of huge amounts of sensor data. We demonstrate a proper visualization of navigation stack data in 3D within the Hololens. However, there are limitations when transferring and displaying different kinds of data simultaneously.","2326-8239","978-1-7281-3458-1","10.1109/CIS-RAM47153.2019.9095836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095836","","Data visualization;Robot sensing systems;Robot kinematics;Navigation;Three-dimensional displays;Mobile robots","augmented reality;control engineering computing;data visualisation;human-robot interaction;mobile robots;path planning","navigation data;mobile robots;Microsoft Hololens;high variety;static robots;sensor data;user understanding;human robot interaction;environment map;path planing data;real-time data;Augmented-Reality-based visualization;relevant navigation stack data;laser scan;cutting edge 3D augmented reality device","","2","","26","","19 May 2020","","","IEEE","IEEE Conferences"
"A visual interface for scripting virtual behaviors","Mansoo Kim; EaTaek Lee","Visual Inf. Lab., Electron. & Telecommun. Res. Inst., Taejeon, South Korea; NA","Proceedings. 3rd Asia Pacific Computer Human Interaction (Cat. No.98EX110)","6 Aug 2002","1998","","","165","168","The storyboarding stage in filmmaking is necessary for communicating overall scenarios rapidly through approximate visual drawings. In contrast, most 3D animation systems produce final products through repeated precise modeling. The interaction suggested in the paper promotes the maximum transfer of the users' knowledge of physical human actions into virtual environments, so the users can rapidly generate virtual human behaviors such as running, walking, grasping, and other motions. To achieve this, users visually describe the behaviors of a virtual human, then the interface parses the visual scripting and finally achieves the semantics. In this approach, users draw only curves, which describe synchronization among motions as well as geometric motion paths of the human, in the same three dimensional space using 3D devices such as a spaceball.","","0-8186-8347-3","10.1109/APCHI.1998.704187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=704187","","Animation;Humans;Electrical capacitance tomography;Graphics;Laboratories;Grasping;Identity-based encryption;Application software;Virtual environment;Layout","graphical user interfaces;computer animation;virtual reality;interactive systems;human factors;grammars","visual interface;virtual behavior scripting;storyboarding stage;filmmaking;3D animation systems;repeated precise modeling;user knowledge transfer;physical human actions;virtual environments;virtual human behaviors;semantics;parsing;geometric motion paths;three dimensional space;3D devices;spaceball","","","","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multimodal interface for temporal pattern based interactive large volumetric visualization","P. Kumar; A. Agrawal; S. Prasad","Indian Institute of Information Technology, Allahabad, India; Indian Institute of Information Technology, Allahabad, India; Nanyang Technical University, Singapore","TENCON 2017 - 2017 IEEE Region 10 Conference","21 Dec 2017","2017","","","1239","1244","Scientific data visualization is a prominent area of research in the development of Virtual Reality Applications in order to make it more interactive and robotic. But the efficient interaction with the large size of medical data is a challenging task to diagnose virtual surgerical environment learning for a Physician. In this paper, we proposed a multimodal interface for GPU-accelerated interactive large scale volumetric data rendering to overcome this limitation. The large data has been pre-processed by octree method. An improved raycasting algorithm is used in association with a transfer function classification method for the effective rendering. The temporal data is used for defining gestures, retrieving in a pattern from the wearable device for providing multimodality with the large rendered data. A gesture vocabulary has been defined by these patterns for the navigation in visualizing the large scale medical data, which consists of five complex interactive postures used for Normal, Picking, Rotation, Dragging, and Zooming gestures. These gesture vocabularies have been categorized by kNN classification method of pattern recognition. Experimental results of the proposed approach are analyzed with the help of various ANOVA and T-testing graphs using SPSS 20 version tool and confidence interval of interaction with hand gestures vocabulary. The results of proposed approach are further compared with the existing approaches in which Microsoft Kinect and P5 dataglove have been used. The proposed system has been navigated by the DG5 VHand 2.0 Bluetooth version hand dataglove as wearable assistive device to achieve an effective interaction. The system has been tested on 10 different sizes of volume datasets ranging from 10MB to 3.15 GB. The scope of this paper is basically to develop system training with robotic arm in medical domain.","2159-3450","978-1-5090-1134-6","10.1109/TENCON.2017.8228047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8228047","Hand DataGlove;Volume Rendering;Human-Computer Interaction;Multimodal Inteface;Medical Dataset;Large scale","Data visualization;Three-dimensional displays;Virtual reality;Rendering (computer graphics);Thumb;Vocabulary;Biomedical imaging","Bluetooth;data gloves;data visualisation;feature extraction;gesture recognition;human computer interaction;interactive systems;medical computing;octrees;pattern classification;rendering (computer graphics);surgery;virtual reality","multimodal interface;temporal pattern;interactive large volumetric visualization;scientific data visualization;interactive large scale volumetric data rendering;octree method;improved raycasting algorithm;transfer function classification method;wearable device;multimodality;rendered data;gesture vocabulary;complex interactive postures;kNN classification method;pattern recognition;DG5 VHand 2.0 Bluetooth version hand dataglove;wearable assistive device;virtual reality applications;zooming gesture;normal gesture;picking gesture;rotation gesture;dragging gesture;hand gesture vocabulary;virtual surgical environment","","1","","17","","21 Dec 2017","","","IEEE","IEEE Conferences"
"Recalibration of Perceived Distance in Virtual Environments Occurs Rapidly and Transfers Asymmetrically Across Scale","J. W. Kelly; W. W. Hammel; Z. D. Siegel; L. A. Sjolund",Iowa State University; Iowa State University; Iowa State University; Iowa State University,"IEEE Transactions on Visualization and Computer Graphics","24 Mar 2014","2014","20","4","588","595","Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance.","1941-0506","","10.1109/TVCG.2014.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777445","Distance perception; virtual reality; recalibration","Legged locomotion;Virtual environments;Visualization;Educational institutions;Atmospheric measurements;Particle measurements","calibration;distance measurement;virtual reality","perceived distance recalibration;virtual environments;visual feedback;walking interaction;distance perception;immersive virtual reality","","29","","53","","24 Mar 2014","","","IEEE","IEEE Journals"
"Information transfer techniques for mobile devices by ""toss"" and ""swing"" actions","K. Yatani; K. Tamura; M. Sugimoto; H. Hashizume","Graduate Sch. of Frontier Sci., Tokyo Univ., Japan; Graduate Sch. of Frontier Sci., Tokyo Univ., Japan; Graduate Sch. of Frontier Sci., Tokyo Univ., Japan; NA","Sixth IEEE Workshop on Mobile Computing Systems and Applications","17 Jan 2005","2004","","","144","151","In recent years, mobile devices have rapidly penetrated into our daily lives. Several drawbacks of mobile devices have been mentioned so far, such as their limited computational capability, small screen real estate, and, so on, as compared with notebook or desktop computers. However, by fully utilizing the most notable feature of mobile devices, that is, mobility, we can explore possibilities for a new user interface of the devices. In this paper, we use PDAs and propose intuitive information transfer techniques for them, which could not be achieved with notebook or desktop computers. By using the system called Toss-It, a user can send information from the user's PDA to other electronic devices with a ""toss"" or ""swing"" action, as the user would toss a ball or deals cards to others. We have developed a circuit board designed to be attached to a PDA and algorithms for recognizing ""toss"" and ""swing"" actions. Preliminary user studies of Toss-It indicated that it could correctly identify receivers of information by ""toss"" or ""swing"" actions. Our research project is in progress, and this paper describes the current status of the project by focusing on issues related to HCI (human computer interaction). We discuss about several critical issues to be investigated in our future studies.","1550-6193","0-7695-2258-0","10.1109/MCSA.2004.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1377323","","Personal digital assistants;Printers;Mobile computing;User interfaces;Human computer interaction;Computer interfaces;Printed circuits;Algorithm design and analysis;Informatics;Optical fiber communication","mobile computing;notebook computers;electronic data interchange;human computer interaction;user interfaces","mobile devices;toss action;swing action;notebook computers;desktop computers;user interface;PDA;intuitive information transfer;Toss-It system;electronic devices;human computer interaction","","6","7","13","","17 Jan 2005","","","IEEE","IEEE Conferences"
"A multimodal ubiquitous interface system using smart phone for human-robot interaction","Young-Hoon Jeon; Hyunsik Ahn","Department of Robot System Engineering, Tongmyong University, Busan, 608-711, Korea; Department of Robot System Engineering, Tongmyong University, Busan, 608-711, Korea","2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","6 Feb 2012","2011","","","764","767","Smart phones have enhanced computing power and various interface modes which are able to be used as ubiquitous devices. A multimodal ubiquitous interface system using a smart phone for human-robot interaction (HRI) is proposed. In the interface system, the host computer of a robot acquires images for the front view and the state of motion and generates voice as a server, and transmits them to a smart phone. The smart phone as a client using Android as a mobile OS displays the received information on the screen and to speaker, and transfers control signals activated by button and voice events. For providing various media, the interface system provides remote moving mode, remote operation mode, and voice control mode which utilize the little space of the screen and the limited computing power. Experimental results show the implementation of the proposed interface system and demonstrate its feasibility to control service robots.","","978-1-4577-0723-0","10.1109/URAI.2011.6146007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146007","Android;Smart Phone;Ubiquitous Control;Service Robot;Multimodal Operation","Servers;Smart phones;Service robots;Sockets;Computers;Programming","control engineering computing;human-robot interaction;mobile computing;operating systems (computers);robot vision;service robots;smart phones;speech-based user interfaces;telerobotics","multimodal ubiquitous interface system;smart phones;human-robot interaction;ubiquitous devices;voice generation;image acquisition;Android;mobile OS;remote moving mode;remote operation mode;voice control mode;service robot","","2","","6","","6 Feb 2012","","","IEEE","IEEE Conferences"
"Estimating Cognitive Processes Related to Haptic Interaction within Virtual Environments","S. Tarng; D. Wang; Y. Hu","University of Calgary,Department of Electrical and Computer Engineering,Calgary, AB,CANADA; University of Calgary,Department of Electrical and Computer Engineering,Calgary, AB,CANADA; University of Calgary,Department of Electrical and Computer Engineering,Calgary, AB,CANADA","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)","28 Nov 2019","2019","","","2823","2828","Efforts exist to combine a brain-machine interface (BMI) into a 3D virtual environment (VE) for visual tasks. User interaction via haptic stimuli within the VE is still unexplored for developing the BMI however, due to little understanding of cognitive processes related to such haptic interaction. Hence, we investigated a feasibility of estimating cognitive processes related to haptic interaction. Involved in the investigation, human participants undertook a task via different haptic stimuli (e.g., force and vibration) within a 3D VE . Their brain activities evoked by the stimuli were acquired as electroencephalography signals. Patterns of event-related potential and power spectral density were extracted from the signals, indicating activation in certain brain areas. The estimation of connectivity among these areas used directed transfer function, emphasizing on the middle of the β band (1/030 Hz) in the signals. The emphasis was due to the band's association with active focus and thinking. We found that, while behavioral differences were unapparent, all vibration-related stimuli yielded distinct active brain areas and connectivity to form certain cognitive processes. The finding implied a potential of localizing the processes for BMI-based haptic interaction.","2577-1655","978-1-7281-4569-3","10.1109/SMC.2019.8913853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913853","electroencephalography (EEG);brain-machine interface (BMI);haptic interaction;virtual environment (VE)","Electroencephalography;Conferences;Cybernetics;Brain-computer interfaces;Haptic interfaces;Virtual environments;Data collection","bioelectric potentials;brain-computer interfaces;cognition;electroencephalography;haptic interfaces;medical signal detection;medical signal processing;neurophysiology;virtual reality","active brain areas;vibration-related stimuli;directed transfer function;electroencephalography signals;brain activity;haptic stimuli;user interaction;visual tasks;3D virtual environment;brain-machine interface;BMI-based haptic interaction;cognitive process;power spectral density;event-related potential;frequency 1.0 Hz;frequency 30.0 Hz","","","","24","","28 Nov 2019","","","IEEE","IEEE Conferences"
"A Physics-based Virtual Reality Environment to Quantify Functional Performance of Upper-limb Prostheses","K. Odette; Q. Fu","Modeling and Simulation Program at University of Central Florida, Orlando, FL, 32827; NeuroMechanical Systems Laboratory, Mechanical and Aerospace Engineering, Biionix (Bionic Materials, Implants & Interfaces) Cluster, University of Central Florida, Orlando, FL, 32827","2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","7 Oct 2019","2019","","","3807","3810","Usability of upper-limb prostheses remains to be a challenge due to the complexity of hand-object interactions in activities of daily living. Functional evaluation is critical for the optimization of prosthesis performance during device design and parameter tuning phase. Therefore, we implemented a low-cost physics-based virtual reality environment (VRE) capable of simulating wide range of object grasping and manipulation tasks to enable human-in-the-loop optimization. Importantly, our novel VRE can assess user performance quantitatively using movement kinematics and interaction forces. We present a preliminary experiment to validate our VRE. Four able-bodied subjects performed object transfer tasks with a simulated myoelectric one DoF soft-synergy prosthetic hand, while wearing braces to restrain different levels of wrist motion. We found that the task completion time was similar across conditions, however limited wrist pronation led to more shoulder compensatory motion whereas challenging object orientation caused more torso compensatory motion.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857850","","","biomechanics;electromyography;medical signal processing;prosthetics;virtual reality","parameter tuning phase;object grasping;human-in-the-loop optimization;interaction forces;simulated myoelectric one DoF soft-synergy prosthetic hand;object orientation;quantify functional performance;upper-limb prostheses;hand-object interactions;daily living;low-cost physics-based virtual reality environment;torso compensatory motion","Activities of Daily Living;Artificial Limbs;Biomechanical Phenomena;Humans;Physical Functional Performance;Physics;Prosthesis Design;Upper Extremity;Virtual Reality","","","25","","7 Oct 2019","","","IEEE","IEEE Conferences"
"Keynote 1: Internet of Things(IoT) and augmented reality for e-learning","N. S. C. Babu",C-DAC Bangalore,"2017 5th National Conference on E-Learning & E-Learning Technologies (ELELTECH)","19 Oct 2017","2017","","","1","10","With the advent of the Internet of Things (IOT) technology, we will be expecting all the smart devices around us to provide various services in real-time. When this IoT technology is combined with Augmented Reality (AR), it will provide end users with real time as well as real world information. One of the prime advantages of AR combining with IoT is the bridging of the gap between the real and digital worlds around us. The ease of downloading features and scope of dynamic usage which AR brings to the smart devices and is expected to bring to the IoT is beyond human imagination! The education sector is expected to gain significantly using these technologies. The goal of technology in education is to make, learning easier, faster and more engaging for the students and to equip the instructor with powerful tools to provide more relevant courseware to their students. With introducing AR in education, we shall be able to provide contextually aware, more relevant, engaging and interactive courseware to the students, which would keep them interested in subject. Also, through IoT, the physical devices can provide context that the AR ecosystem would use for contextually aware rendering of multimedia content. Medical education domain is expected to gain significantly using IOT and AR. It is also observed that the training in the areas of equipment maintenance could gain significantly by applying these technologies judiciously. For eg. The conventional software virtual lab solution does not provide a real feel of performing an experiment. However, using IOT and AR technology, a sensor enabled Hardware based real lab environment can be created for performing the volumetric analysis experiments in chemistry by using WATER instead of Chemicals. C-DAC has developed few AR products in education domain - AR Board, AR Book and AR Game. AR Board is a virtual writing board on which a user can write with a laser pointer. It uses projection based AR Technology by creating a virtual environment for contextual information presentation, and facilitates real-time interaction with the content. AR Book is a standard book that has been instrumented with AR markers printed, which when scanned would show augmented, 3D interactive content, videos, audio and animations contextually, on a printed book, thus adding new dimensionality to it. AR Game is a game based learning mobile application that can be used by students for learning through playing, and for evaluating their knowledge. The technology of these products has been transferred to industry for their further development and proliferation to various educational institutes.","","978-1-5386-1922-3","10.1109/ELELTECH.2017.8074987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8074987","","","augmented reality;biomedical education;computer aided instruction;computer games;Internet of Things;mobile computing","augmented reality;smart devices;IoT technology;education sector;medical education domain;Internet of Things technology;IoT;e-learning;AR technology;AR board;AR book;AR game;game based learning mobile application","","1","","","","19 Oct 2017","","","IEEE","IEEE Conferences"
"Force Feedback Simulation Design for Fruit Hardness","H. Üstünel; E. Serdar Güner; M. O. Özcan; B. Aslan","University Kirklareli,Software Engineering Dept. Kirklareli,Turkey; University Kirklareli,Software Engineering Dept. Kirklareli,Turkey; University Kirklareli,Software Engineering Dept. Kirklareli,Turkey; University Kirklareli,Software Engineering Dept. Kirklareli,Turkey","2020 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)","30 Jul 2020","2020","","","1","4","Hardness value is one of the decisive criteria for fruit harvest period. It is important for harvest yield that the harvesting personnel knows the differences of this value with respect to the fruits. Especially in industrial harvesting, the use of simulations in personnel training and student education in agricultural faculties will be efficient in terms of saving time. Within the scope of this study, the hardness values of the selected fruits (apple and tomato) were measured using a penetrometer. Data from the penetrometer were transferred to objects (fruit) modeled in high resolution. User interface design has been realized by using various graphic libraries. In the interaction (collision) of the objects displayed in the graphic interface provided by the software developed within the scope of the research with the haptic cursor, a force feedback was transmitted to the user via the haptic device. In this way, a simulation design has been made that allows users to conceptualize and distinguish the hardness values of certain fruits by touching them. This study can be extended in the next steps by adding fruits with different hardness values to the simulation.","","978-1-7281-9352-6","10.1109/HORA49412.2020.9152906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152906","Simulation;Fruit hardness;Force feedback;Haptic device;3D modeling","Force feedback;Force measurement;Force;Solid modeling;Graphics;Three-dimensional displays","agricultural products;agriculture;computer graphics;force feedback;graphical user interfaces;haptic interfaces;human computer interaction;production engineering computing","student education;hardness value;penetrometer;user interface design;graphic interface;force feedback simulation design;fruit hardness;decisive criteria;fruit harvest;harvest yield;industrial harvesting;personnel training;software development","","","","12","","30 Jul 2020","","","IEEE","IEEE Conferences"
