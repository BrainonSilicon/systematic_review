Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
"Kourtesis P., Collina S., Doumas L.A.A., MacPherson S.E.","57210959726;35309731500;12345301900;57212061035;","Validation of the Virtual Reality Everyday Assessment Lab (VR-EAL): An Immersive Virtual Reality Neuropsychological Battery with Enhanced Ecological Validity",2021,"Journal of the International Neuropsychological Society","27","2",,"181","196",,2,"10.1017/S1355617720000764","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095727298&doi=10.1017%2fS1355617720000764&partnerID=40&md5=682a1271572f8544a09deaa59f15dad1","Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy; Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy","Kourtesis, P., Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy, Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy; Collina, S., Lab of Experimental Psychology, Suor Orsola Benincasa University of Naples, Naples, Italy, Interdepartmental Centre for Planning and Research Scienza Nuova, Suor Orsola Benincasa University of Naples, Naples, Italy; Doumas, L.A.A., Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom; MacPherson, S.E., Human Cognitive Neuroscience, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom, Department of Psychology, University of Edinburgh, Edinburgh, United Kingdom","The assessment of cognitive functions such as prospective memory, episodic memory, attention, and executive functions benefits from an ecologically valid approach to better understand how performance outcomes generalize to everyday life. Immersive virtual reality (VR) is considered capable of simulating real-life situations to enhance ecological validity. The present study attempted to validate the Virtual Reality Everyday Assessment Lab (VR-EAL), an immersive VR neuropsychological battery, against an extensive paper-and-pencil neuropsychological battery. Methods: Forty-one participants (21 females) were recruited: 18 gamers and 23 non-gamers who attended both an immersive VR and a paper-and-pencil testing session. Bayesian Pearson's correlation analyses were conducted to assess construct and convergent validity of the VR-EAL. Bayesian t-tests were performed to compare VR and paper-and-pencil testing in terms of administration time, similarity to real-life tasks (i.e., ecological validity), and pleasantness. Results: VR-EAL scores were significantly correlated with their equivalent scores on the paper-and-pencil tests. The participants' reports indicated that the VR-EAL tasks were significantly more ecologically valid and pleasant than the paper-and-pencil neuropsychological battery. The VR-EAL battery also had a shorter administration time. Conclusion: The VR-EAL appears as an effective neuropsychological tool for the assessment of everyday cognitive functions, which has enhanced ecological validity, a highly pleasant testing experience, and does not induce cybersickness. © 2020 INS. Published by Cambridge University Press.","Attention; Episodic memory; Everyday functioning; Executive function; Prospective memory; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85095727298
"Zhu Y., Zhai G., Min X., Zhou J.","57189598758;15847120000;56030205300;55938080200;","Learning a Deep Agent to Predict Head Movement in 360-Degree Images",2021,"ACM Transactions on Multimedia Computing, Communications and Applications","16","4", 3410455,"","",,,"10.1145/3410455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100305639&doi=10.1145%2f3410455&partnerID=40&md5=50236f32dd2ed174b5431f405e66566a","Shanghai Jiao Tong University, Shanghai, China; University of Macau, Macau","Zhu, Y., Shanghai Jiao Tong University, Shanghai, China; Zhai, G., Shanghai Jiao Tong University, Shanghai, China; Min, X., Shanghai Jiao Tong University, Shanghai, China; Zhou, J., University of Macau, Macau","Virtual reality adequately stimulates senses to trick users into accepting the virtual environment. To create a sense of immersion, high-resolution images are required to satisfy human visual system, and low latency is essential for smooth operations, which put great demands on data processing and transmission. Actually, when exploring in the virtual environment, viewers only perceive the content in the current field of view. Therefore, if we can predict the head movements that are important behaviors of viewers, more processing resources can be allocated to the active field of view. In this article, we propose a model to predict the trajectory of head movement. Deep reinforcement learning is employed to mimic the decision making. In our framework, to characterize each state, features for viewport images are extracted by convolutional neural networks. In addition, the spherical coordinate maps and visited maps are generated for each viewport image, which facilitate the multiple dimensions of the state information by considering the impact of historical head movement and position information. To ensure the accurate simulation of visual behaviors during the watching of panoramas, we stipulate that the model imitates the behaviors of human demonstrators. To allow the model to generalize to more conditions, the intrinsic motivation is employed to guide the agent's action toward reducing uncertainty, which can enhance robustness during the exploration. The experimental results demonstrate the effectiveness of the proposed stepwise head movement predictor. © 2020 ACM.","360 degree; deep reinforcement learning (DRL); head movement prediction; omnidirectional; panoramic; saliency; VR","Behavioral research; Convolutional neural networks; Data handling; Decision making; Forecasting; Image processing; Reinforcement learning; Data processing and transmission; High resolution image; Human Visual System; Intrinsic motivation; Multiple dimensions; Position information; Processing resources; Spherical coordinates; Deep learning",Article,"Final","",Scopus,2-s2.0-85100305639
"Monteiro D., Liang H.-N., Wang J., Chen H., Baghaei N.","57144011000;8636386200;57207048907;57221155309;14020983900;","An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284718,"713","724",,,"10.1109/ISMAR50242.2020.00102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099294395&doi=10.1109%2fISMAR50242.2020.00102&partnerID=40&md5=9a9ee4dfccc97f04f5650a4b63a6eea1","Xi'an Jiaotong-Liverpool University, China; Massey University, New Zealand","Monteiro, D., Xi'an Jiaotong-Liverpool University, China; Liang, H.-N., Xi'an Jiaotong-Liverpool University, China; Wang, J., Xi'an Jiaotong-Liverpool University, China; Chen, H., Xi'an Jiaotong-Liverpool University, China; Baghaei, N., Massey University, New Zealand","The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness. © 2020 IEEE.","2D/3D Views; Controller types; First Person Shooter; Gaming; Head-Mounted Displays; Virtual Reality","Augmented reality; Controllers; Diseases; Helmet mounted displays; Contributing factor; First person shooter games; Head mounted displays; Input methods; Input techniques; Simulator sickness; Visual techniques; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099294395
"Martin N., Mathieu N., Pallamin N., Ragot M., Diverrez J.-M.","57188745225;57221495248;13405769400;57073515600;56928312300;","Virtual reality sickness detection: An approach based on physiological signals and machine learning",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284654,"387","399",,,"10.1109/ISMAR50242.2020.00065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099321186&doi=10.1109%2fISMAR50242.2020.00065&partnerID=40&md5=eef96cc31598451773f458ec1c3fa5c9","Irt B<com, Cesson-Sevigne, France; Ubisoft, Montreuil, France","Martin, N., Irt B<com, Cesson-Sevigne, France; Mathieu, N., Ubisoft, Montreuil, France; Pallamin, N., Irt B<com, Cesson-Sevigne, France; Ragot, M., Irt B<com, Cesson-Sevigne, France; Diverrez, J.-M., Irt B<com, Cesson-Sevigne, France","Virtual Reality (VR) is spreading to the general public but still has a major issue: VR sickness. To take it into consideration and minimize its occurrence, evaluation methods are required. The current methods are mainly based on subjective measurements and therefore have several drawbacks (e.g., non-continuous, intrusive). Physiological signals combined with Machine Learning (ML) methods seem an interesting approach to go beyond these limits. In this paper, we present a large-scale experimentation (103 participants) where physiological data (cardiac and electrodermal activities) and subjective data (perceived VR sickness) were gathered during 30-minute VR video game sessions. Using ML methods, models were trained to predict VR sickness level (based on the physiological data labeled with the subjective data). Results showed an explained variance up to 75% (in a regression approach) and an accuracy up to 91% (in a classification approach). Despite generalization issues, this method seems promising and valuable for a real time, automatic and continuous evaluation of VR sickness, based on physiological signals and ML models. © 2020 IEEE.","Ergonomics; H.1.2 [Models and principles]: User/Machine Systems; Human factors; I.3.6 [Computer graphics]: Methodology and Techniques","Augmented reality; Diseases; E-learning; Machine learning; Physiology; Virtual reality; Classification approach; Electrodermal activity; Evaluation methods; General publics; Large-scale experimentations; Physiological data; Physiological signals; Subjective measurements; Physiological models",Conference Paper,"Final","",Scopus,2-s2.0-85099321186
"Lampen E., Lehwald J., Pfeiffer T.","57209683199;57202847650;14027435500;","Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095834964&doi=10.1145%2f3385956.3418974&partnerID=40&md5=6990b67981dda33559c0b31456034299","EvoBus GmbH, Neu-Ulm, Germany; Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Lampen, E., EvoBus GmbH, Neu-Ulm, Germany; Lehwald, J., EvoBus GmbH, Neu-Ulm, Germany; Pfeiffer, T., Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al. [21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks. © 2020 ACM.","Augmented Reality; Expert-Based Learning; Virtual Human","E-learning; Educational settings; Implementation cost; Industrial use case; Learning by observation; Learning scenarios; Sports and rehabilitations; State of the art; Threedimensional (3-d); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095834964
"Armstrong M., Tsuchiya K., Liang F., Kunze K., Pai Y.S.","57219866273;57194083287;57191504624;21743317500;56267209600;","Multiplex Vision: Understanding Information Transfer and F-Formation with Extended 2-Way FOV",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095814394&doi=10.1145%2f3385956.3418954&partnerID=40&md5=89c416380a230628f82185e756df2444","Keio University, Graduate School of Media Design, Tokyo, Japan; INNOCC Ignition Point Inc., Tokyo, Japan; University of Auckland, Auckland, New Zealand","Armstrong, M., Keio University, Graduate School of Media Design, Tokyo, Japan; Tsuchiya, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Liang, F., INNOCC Ignition Point Inc., Tokyo, Japan; Kunze, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Pai, Y.S., University of Auckland, Auckland, New Zealand","Research in sociology shows that effective conversation relates to people's spatial and orientational relationship, namely the proxemics (distance, eye contact, synchrony) and the F-formation (orientation and arrangement). In this work, we introduce novel conversational paradigms that effects conventional F-formation by introducing the concept of multi-directional conversation. Multiplex Vision is a head-mounted device capable of providing a 360° field-of-view (FOV) and facilitating multi-user interaction multi-directionally, thereby providing novel methods on how people can interact with each other. We propose 3 possible new forms of interactions from our prototype: one-to-one, one-to-many, and many-to-many. To facilitate them, we manipulate 2 key variables, which are the viewing parameter and the display parameter. To gather feedback for our system, we conducted a study to understand information transfer between various modes, as well as a user study on how different proposed paradigms effect conversation. Finally, we discuss present and future use cases that can benefit from our system. © 2020 ACM.","360 field-of-view; conversation; F-formation; vision augmentation","Sociology; Display parameters; Field of views; Information transfers; Key variables; Multi-user interaction; Novel methods; Orientational relationship; Viewing parameters; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095814394
"Petersen G.B., Klingenberg S., Mayer R.E., Makransky G.","57205735672;57214077924;7403065717;50361371800;","The virtual field trip: Investigating how to optimize immersive virtual learning in climate change education",2020,"British Journal of Educational Technology","51","6",,"2098","2114",,6,"10.1111/bjet.12991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087505916&doi=10.1111%2fbjet.12991&partnerID=40&md5=95f409f7b19bb124fd571c95d712bfb7","Department of Psychology, University of Copenhagen, Denmark; University of Copenhagen, Denmark; University of California, Santa Barbara, United States; Department of Psychology at the University of Copenhagen, Denmark","Petersen, G.B., Department of Psychology, University of Copenhagen, Denmark; Klingenberg, S., University of Copenhagen, Denmark; Mayer, R.E., University of California, Santa Barbara, United States; Makransky, G., Department of Psychology at the University of Copenhagen, Denmark","Immersive Virtual Reality (IVR) is being used for educational virtual field trips (VFTs) involving scenarios that may be too difficult, dangerous or expensive to experience in real life. We implemented an immersive VFT within the investigation phase of an inquiry-based learning (IBL) climate change intervention. Students investigated the consequences of climate change by virtually traveling to Greenland and exploring albedo and greenhouse effects first hand. A total of 102 seventh and eighth grade students were randomly assigned to one of two instructional conditions: (1) narrated pretraining followed by IVR exploration or (2) the same narrated training material integrated within the IVR exploration. Students in both conditions showed significant increases in declarative knowledge, self-efficacy, interest, STEM intentions, outcome expectations and intentions to change behavior from the pre- to post-assessment. However, there was a significant difference between conditions favoring the pretraining group on a transfer test consisting of an oral presentation to a fictitious UN panel. The findings suggest that educators can choose to present important prerequisite learning content before or during a VFT. However, adding pretraining may lead to better transfer test performance, presumably because it helps reduce cognitive load while learning in IVR. © 2020 British Educational Research Association",,"Climate change; Students; Virtual reality; Declarative knowledge; Immersive virtual reality; Inquiry based learning (IBL); Learning contents; Oral presentations; Training material; Virtual field trips; Virtual learning; E-learning",Article,"Final","",Scopus,2-s2.0-85087505916
"Du J., Yu F.R., Lu G., Wang J., Jiang J., Chu X.","57188706756;57213980384;7403460635;13613533800;57198571713;8536386700;","MEC-Assisted Immersive VR Video Streaming over Terahertz Wireless Networks: A Deep Reinforcement Learning Approach",2020,"IEEE Internet of Things Journal","7","10", 9120235,"9517","9529",,10,"10.1109/JIOT.2020.3003449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709939&doi=10.1109%2fJIOT.2020.3003449&partnerID=40&md5=9d415f1ee32084295aa918f217856727","Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON  K1S 5B6, Canada; Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom","Du, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Yu, F.R., Department of Systems and Computer Engineering, Carleton University, Ottawa, ON  K1S 5B6, Canada; Lu, G., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Wang, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Jiang, J., Shaanxi Key Laboratory of Information Communication Network and Security, School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, 710121, China; Chu, X., Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, S1 3JD, United Kingdom","Immersive virtual reality (VR) video is becoming increasingly popular owing to its enhanced immersive experience. To enjoy ultrahigh resolution immersive VR video with wireless user equipments, such as head-mounted displays (HMDs), ultralow-latency viewport rendering, and data transmission are the core prerequisites, which could not be achieved without a huge bandwidth and superior processing capabilities. Besides, potentially very high energy consumption at the HMD may impede the rapid development of wireless panoramic VR video. Multiaccess edge computing (MEC) has emerged as a promising technology to reduce both the task processing latency and the energy consumption for HMD, while bandwidth-rich terahertz (THz) communication is expected to enable ultrahigh-speed wireless data transmission. In this article, we propose to minimize the long-term energy consumption of a THz wireless access-based MEC system for high quality immersive VR video services support by jointly optimizing the viewport rendering offloading and downlink transmit power control. Considering the time-varying nature of wireless channel conditions, we propose a deep reinforcement learning-based approach to learn the optimal viewport rendering offloading and transmit power control policies and an asynchronous advantage actor-critic (A3C)-based joint optimization algorithm is proposed. The simulation results demonstrate that the proposed algorithm converges fast under different learning rates, and outperforms existing algorithms in terms of minimized energy consumption and maximized reward. © 2014 IEEE.","Asynchronous advantage actor-critic (A3C); computation offloading; deep reinforcement learning (DRL); terahertz (THz) communication; virtual reality (VR)","Bandwidth; Data communication equipment; Data transfer; Deep learning; Energy utilization; Green computing; Helmet mounted displays; Learning algorithms; Power control; Quality control; Reinforcement learning; Rendering (computer graphics); Video streaming; Wave transmission; Wireless networks; Head mounted displays; Immersive virtual reality; Processing capability; Reinforcement learning approach; Terahertz(THz) communications; Transmit power control; Wireless channel condition; Wireless data transmission; Virtual reality",Article,"Final","",Scopus,2-s2.0-85092709939
"Pinardi D., Ebri L., Belicchi C., Farina A., Binelli M.","57195467991;56770178100;57219547851;7202992441;55354577100;","Direction Specific Analysis of Psychoacoustics Parameters inside Car Cockpit: A Novel Tool for NVH and Sound Quality",2020,"SAE Technical Papers",,"2020",,"","",,,"10.4271/2020-01-1547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093822780&doi=10.4271%2f2020-01-1547&partnerID=40&md5=c96cd017a73014350d0726971591bd1f","University of Parma, Italy; University of Parma, Ask Industries SpA, Italy","Pinardi, D., University of Parma, Italy; Ebri, L., University of Parma, Ask Industries SpA, Italy; Belicchi, C., University of Parma, Italy; Farina, A., University of Parma, Italy; Binelli, M., University of Parma, Italy","Psychoacoustics parameters are widely employed in automotive field for objective evaluation of Sound Quality (SQ) of vehicle cabins and their components. The standard approach relies on binaural recordings from which numerical values and curves are calculated. In addition, head-locked binaural listening playback can be performed. The Virtual Reality (VR) technology recently started to diffuse also in automotive field, bringing new possibilities for enhanced and immersive listening sessions, thanks to the usage of massive microphone arrays instead of binaural microphones. In this paper, we combine both solutions: the principal SQ parameters are derived from multichannel recordings. This allows computing a map of direction-dependent values of SQ parameters. The acquisition system consists in a spherical microphone array with 32 capsules and a multiple-lens camera for capturing a panoramic equirectangular background image. The audio recording is encoded into High Order Ambisonics (HOA) format for being compared with a classic omnidirectional microphone and into Spatial PCM Sampling (SPS) format for producing 360° equirectangular color maps. The SPS encoding is used to plot over the background image the distribution of SPL values in dB (A) and of the SQ parameters: by adding to them the directional information, it results into a novel 360° diagnostic tool for localizing the most annoying sources. Furthermore, the playback of the HOA soundtrack can be performed both on a loudspeaker rig inside an Ambisonics listening room or on binaural headphones attached to a Head Mounted Display (HMD), benefiting from head-tracking and personalized Head Related Transfer Functions (HRTFs), allowing to make quick subjective evaluations with a degree of realism unattainable with the older static binaural approach. © 2020SAE International. All Rights Reserved.",,"Acoustic noise; Acoustic variables measurement; Helmet mounted displays; Loudspeakers; Microphones; Parameter estimation; Quality control; Virtual reality; Binaural recordings; Directional information; Head mounted displays; Head related transfer function; Multi-channel recording; Objective evaluation; Spherical microphone array; Subjective evaluations; Audio recordings",Conference Paper,"Final","",Scopus,2-s2.0-85093822780
"Delamarre A., Lisetti C., Buche C.","57195671754;6602670860;8349259000;","A Cross-Platform Classroom Training Simulator: Interaction Design and EvaluationA Cross-Platform Classroom Training Simulator: Interaction Design and Evaluation",2020,"Proceedings - 2020 International Conference on Cyberworlds, CW 2020",,, 9240533,"86","93",,,"10.1109/CW49994.2020.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099535199&doi=10.1109%2fCW49994.2020.00020&partnerID=40&md5=0afab14bfbc31f31c90b991bafa5ee8d","Florida International University, Visage Lab, Scis, Miami, United States; LAB-STICC Cnrs, Enib, Brest, France","Delamarre, A., Florida International University, Visage Lab, Scis, Miami, United States; Lisetti, C., Florida International University, Visage Lab, Scis, Miami, United States; Buche, C., LAB-STICC Cnrs, Enib, Brest, France","Virtual training environments experienced with different immersive technologies can accommodate users' preferences, proficiency, and platform availability. Whereas research comparing the effects of immersive technologies can provide important insights about their impact on users' experience (e.g. engagement, transfer of learning), current studies do not address how to design the user interface (UI) to ensure sound comparisons across platforms. For effective comparisons, however, the UI designs must be adapted for the platform used to provide comparable usability. In this article we describe our UI design methodology for the development of an effective and usable virtual classroom training simulator built for three technologies: (1) desktop; (2) Head-Mounted Display (HMD); and (3) Cave Automatic Virtual Environment (CAVE). Usability and other user experience factors were evaluated for each platform with concurrent think-aloud protocol and semi-structured interviews indicating that all three UIs were easy to use and to learn. We discuss insights for future development of cross-platform VTEs. © 2020 IEEE.","Design; Human Computer Interaction; Immersive Virtual Environment; User Study; Virtual Reality","Availability; Caves; Computer aided instruction; Design; E-learning; Helmet mounted displays; Simulators; Transfer learning; User experience; User interfaces; Cave automatic virtual environments; Head mounted displays; Immersive technologies; Platform availabilities; Semi structured interviews; Think-aloud protocol; Transfer of learning; Virtual training environments; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099535199
"Tsuchiya K., Koizumi N.","57205476444;36117906000;","An Optical Design for Avatar-User Co-axial Viewpoint Telepresence",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,, 9089599,"108","116",,1,"10.1109/VR46266.2020.1581039456803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085502301&doi=10.1109%2fVR46266.2020.1581039456803&partnerID=40&md5=c03cd4ec5a2d9c8038db72edce4b185e","University of Electro-Communications, Japan; University of Electro-Communications, JST PRESTO, Japan","Tsuchiya, K., University of Electro-Communications, Japan; Koizumi, N., University of Electro-Communications, JST PRESTO, Japan","We propose a mid-air image system for telepresence. Virtual reality (VR) social networks enable users to interact with each other through CG avatars and choose their appearances freely. However, this is only possible in VR space. We propose a system that takes the avatar from VR space to real space with the help of mid-air imaging technology. In this system, the micro-mirror array plates (MMAPs) display the mid-air image and optically transfer the camera viewpoint to capture users from the mid-air image position. Luminance measurement and modulation transfer function (MTF) measurement were performed to evaluate the image capturing performance of this system. As a result, we found that the MMAPs ccause a decrease in brightness and an increase in blur. In addition, the stray light generated by the MMAPs was in the captured video. We also confirmed that face detection works correctly on the captured video by adjusting the ISO sensitivity of the camera. Furthermore, we designed an application for telepresence called Levitar, which uses a dual camera to output the captured video to the HMD and controls the camera gaze direction. © 2020 IEEE.","Displays and imagers; Human computer interaction (HCI); Human-centered computing; Interaction devices","Cameras; Face recognition; Imaging techniques; Luminance; Optical design; Stray light; User interfaces; Visual communication; Gaze direction; Image capturing; Image position; Image systems; Imaging technology; Luminance measurements; Micromirror array; Modulation transfer function measurements; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085502301
"Rothe S., Schmidt A., Montagud M., Buschek D., Hußmann H.","57199996760;57219418508;35868074700;55850134500;23389275800;","Social viewing in cinematic virtual reality: a design space for social movie applications",2020,"Virtual Reality",,,,"","",,1,"10.1007/s10055-020-00472-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092676924&doi=10.1007%2fs10055-020-00472-4&partnerID=40&md5=def8d44ae1a903490a6dd20957473a4d","Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Universitat de València & i2CAT Foundation, Valencia, Spain; Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany","Rothe, S., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Schmidt, A., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany; Montagud, M., Universitat de València & i2CAT Foundation, Valencia, Spain; Buschek, D., Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany; Hußmann, H., Institute of Informatics, Ludwig-Maximilians-University Munich, Munich, Germany","Since watching movies is a social experience for most people, it is important to know how an application should be designed for enabling shared cinematic virtual reality (CVR) experiences via head-mounted displays (HMDs). Viewers can feel isolated when watching omnidirectional movies with HMDs. Even if they are watching the movie simultaneously, they do not automatically see the same field of view, since they can freely choose their viewing direction. Our goal is to explore interaction techniques to efficiently support social viewing and to improve social movie experiences in CVR. Based on the literature review and insights from earlier work, we identify seven challenges that need to be addressed: communication, field-of-view (FoV) awareness, togetherness, accessibility, interaction techniques, synchronization, and multiuser environments. We investigate four aspects (voice chat, sending emotion states, FoV indication, and video chat) to address some of the challenges and report the results of four user studies. Finally, we present and discuss a design space for CVR social movie applications and highlight directions for future work. © 2020, The Author(s).","360° video; Cinematic virtual reality; Interactive TV; Omnidirectional video; Social viewing","Helmet mounted displays; Motion pictures; Technology transfer; Design spaces; Field of views; Head mounted displays; Interaction techniques; Literature reviews; Multiuser environments; User study; Viewing directions; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85092676924
"Dhiman H., Buttner S., Rocker C., Reisch R.","57202890221;56954752800;55919922900;57214365751;","Handling work complexity with ar/deep learning",2019,"ACM International Conference Proceeding Series",,,,"518","522",,1,"10.1145/3369457.3370919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078696444&doi=10.1145%2f3369457.3370919&partnerID=40&md5=197e2b1fe56134c8ada58b36cf5d8ccf","Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Fraunhofer IOSB-INA, Lemgo, Germany; Resolto Informatik GmbH, Herford, Germany","Dhiman, H., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Buttner, S., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Rocker, C., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Fraunhofer IOSB-INA, Lemgo, Germany; Reisch, R., Resolto Informatik GmbH, Herford, Germany","Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair. © 2019 Association for Computing Machinery.","Augmented Reality; Deep Learning","Augmented reality; Human computer interaction; Object recognition; Product design; Repair; Assistance system; Digital design techniques; Hardware/software; Learning approach; Product assembly; Work complexity; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85078696444
"Piccione J., Collett J., De Foe A.","57211538030;57206464821;55555885000;","Virtual skills training: the role of presence and agency",2019,"Heliyon","5","11", e02583,"","",,6,"10.1016/j.heliyon.2019.e02583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074370179&doi=10.1016%2fj.heliyon.2019.e02583&partnerID=40&md5=41c139affc0f49a955d0d2674602e6aa","RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Piccione, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; Collett, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; De Foe, A., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Virtual reality (VR) simulations provide increased feelings of presence and agency that could allow increased skill improvement during VR training. Direct relationships between active agency in VR and skill improvement have previously not been investigated. This study examined the relationship between (a) presence and agency, and (b) presence and skills improvement, via active and passive VR simulations and through measuring real-world golf-putting skill. Participants (n = 23) completed baseline putting skill assessment before using an Oculus Rift VR head-mounted display to complete active (putting with a virtual golf club) and passive (watching a game of golf) VR simulations. Measures of presence and agency were administered after each simulation, followed by a final putting skill assessment. The active simulation induced higher feelings of general presence and agency. However, no relationship was identified between presence and either agency or skill improvement. No skill improvement was evident in either the active or passive simulations, potentially due to the short training period applied, as well as a lack of realism in the VR simulations inhibiting a transfer of skills to a real environment. These findings reinforce previous literature that shows active VR to increase feelings of presence and agency. This study generates a number of fruitful research questions about the relationship between presence and skills training. © 2019 The AuthorsPsychology; Virtual reality; Presence; Human factors; Sport psychology © 2019 The Authors","Human factors; Presence; Psychology; Sport psychology; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85074370179
"Tome D., Peluse P., Agapito L., Badino H.","57201376011;57215781039;12752515600;8981371600;","XR-EgoPose: Egocentric 3D human pose from an HMD camera",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",, 9010983,"7727","7737",,7,"10.1109/ICCV.2019.00782","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081916660&doi=10.1109%2fICCV.2019.00782&partnerID=40&md5=e9ba98391986b4166ef8f90ed97da3b2","University College London, United Kingdom; Facebook Reality Lab","Tome, D., University College London, United Kingdom, Facebook Reality Lab; Peluse, P., Facebook Reality Lab; Agapito, L., University College London, United Kingdom; Badino, H., Facebook Reality Lab","We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm.∼away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint. © 2019 IEEE.",,"Benchmarking; Cameras; Decoding; Large dataset; Virtual reality; Body pose estimation; Encoder-decoder architecture; Head mounted virtual reality; Lighting conditions; Perspective distortion; Quantitative evaluation; Real-world datasets; Visual appearance; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85081916660
"He Q., McNamara T.P., Bodenheimer B., Klippel A.","56675107000;7004768772;56133948600;8438953000;","Acquisition and transfer of spatial knowledge during wayfinding",2019,"Journal of Experimental Psychology: Learning Memory and Cognition","45","8",,"1364","1386",,11,"10.1037/xlm0000654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051931514&doi=10.1037%2fxlm0000654&partnerID=40&md5=a0330f54db114282d221d66a78e37eb9","School of Psychology, Georgia Institute of Technology, United States; Department of Psychology, Vanderbilt University, United States; Department of Electrical Engineering and Computer Science, Vanderbilt University, United States; Department of Geography, The Pennsylvania State University, United States","He, Q., School of Psychology, Georgia Institute of Technology, United States; McNamara, T.P., Department of Psychology, Vanderbilt University, United States; Bodenheimer, B., Department of Electrical Engineering and Computer Science, Vanderbilt University, United States; Klippel, A., Department of Geography, The Pennsylvania State University, United States","In the current study, we investigated the ways in which the acquisition and transfer of spatial knowledge were affected by (a) the type of spatial relations predominately experienced during learning (routes determined by walkways vs. straight-line paths between locations); (b) environmental complexity; and (c) the availability of rotational body-based information. Participants learned the layout of a virtual shopping mall by repeatedly searching for target storefronts located in 1 of the buildings. We created 2 novel learning conditions to encourage participants to use either route knowledge (paths on walkways between buildings) or survey knowledge (straight-line distances and directions from storefront to storefront) to find the target, and measured the development of route and survey knowledge in both learning conditions. Environmental complexity was manipulated by varying the alignment of the buildings with the enclosure, and the visibility within space. Body-based information was manipulated by having participants perform the experiment in front of a computer monitor or using a head-mounted display. After navigation, participants pointed to various storefronts from a fixed position and orientation. Results showed that the frequently used spatial knowledge could be developed similarly across environments with different complexities, but the infrequently used spatial knowledge was less developed in the complex environment. Furthermore, rotational body-based information facilitated spatial learning under certain conditions. Our results suggest that path integration may play an important role in spatial knowledge transfer, both from route to survey knowledge (cognitive map construction), and from survey to route knowledge (using cognitive map to guide wayfinding). © 2018 American Psychological Association.","Body-based information; Environmental complexity; Path integration; Spatial knowledge; Spatial navigation","adult; association; attention; distance perception; female; human; male; orientation; problem solving; social environment; spatial orientation; virtual reality; young adult; Adult; Attention; Cues; Distance Perception; Female; Humans; Male; Orientation; Problem Solving; Social Environment; Spatial Navigation; Transfer, Psychology; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85051931514
"Juliano J.M., Saldana D., Schmiesing A., Liew S.-L.","57209396794;57194044260;57194041188;36992162200;","Experience with head-mounted virtual reality (HMD-VR) predicts transfer of HMD-VR motor skills",2019,"International Conference on Virtual Rehabilitation, ICVR","2019-July",, 8994345,"","",,,"10.1109/ICVR46560.2019.8994345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080146106&doi=10.1109%2fICVR46560.2019.8994345&partnerID=40&md5=e52cf9a653b976ff6d3a6891f629dc26","University of Southern California (USC), Neuroscience Graduate Program, Los Angeles, CA, United States; University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States","Juliano, J.M., University of Southern California (USC), Neuroscience Graduate Program, Los Angeles, CA, United States; Saldana, D., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States; Schmiesing, A., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States; Liew, S.-L., University of Southern California, Chan Division of Occupational, Los Angeles, CA, United States","Immersive, head-mounted virtual reality (HMD-VR) has the potential to be a useful tool for motor rehabilitation. However, when developing tools for rehabilitation, it is essential to design interventions that will be most effective for generalizing to the real world. Therefore, it is important to understand what factors facilitate transfer from HMD-VR to non-HMD-VR environments. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train healthy individuals in an HMD-VR environment. We examined whether learned motor skills transferred to a more conventional (non-HMD-VR) environment and what factors facilitated transfer. Our results suggest that on average, learned motor skills from this task transfer from an immersive virtual environment to a conventional environment; however, some individuals did not transfer the learned motor skills. We then examined individual differences between those that did show transfer and those that did not. We found that individuals who had previous exposure to HMD-VR were more likely to transfer their learned motor skills than those who did not. Individual differences in previous exposure to HMD-VR environments prior to training may serve as a predictor to whether learned motor skills will transfer out of HMD-VR. © 2019 IEEE.","head-mounted virtual reality; skilled motor learning; transfer","Helmet mounted displays; Virtual reality; Head mounted virtual reality; Healthy individuals; Immersive virtual environments; Individual Differences; Motor learning; Motor rehabilitation; Task transfer; transfer; Transfer learning",Conference Paper,"Final","",Scopus,2-s2.0-85080146106
"Vertemati M., Cassin S., Rizzetto F., Vanzulli A., Elli M., Sampogna G., Gallieni M.","6602085318;57205576019;57205575796;16026541900;7003507564;57193081915;7004243711;","A Virtual Reality Environment to Visualize Three-Dimensional Patient-Specific Models by a Mobile Head-Mounted Display",2019,"Surgical Innovation","26","3",,"359","370",,7,"10.1177/1553350618822860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060629868&doi=10.1177%2f1553350618822860&partnerID=40&md5=e7b6cc98e1f592ddf43393f6e8f6d519","Università degli Studi di Milano, Milan, Italy","Vertemati, M., Università degli Studi di Milano, Milan, Italy; Cassin, S., Università degli Studi di Milano, Milan, Italy; Rizzetto, F., Università degli Studi di Milano, Milan, Italy; Vanzulli, A., Università degli Studi di Milano, Milan, Italy; Elli, M., Università degli Studi di Milano, Milan, Italy; Sampogna, G., Università degli Studi di Milano, Milan, Italy; Gallieni, M., Università degli Studi di Milano, Milan, Italy","Introduction. With the availability of low-cost head-mounted displays (HMDs), virtual reality environments (VREs) are increasingly being used in medicine for teaching and clinical purposes. Our aim was to develop an interactive, user-friendly VRE for tridimensional visualization of patient-specific organs, establishing a workflow to transfer 3-dimensional (3D) models from imaging datasets to our immersive VRE. Materials and Methods. This original VRE model was built using open-source software and a mobile HMD, Samsung Gear VR. For its validation, we enrolled 33 volunteers: morphologists (n = 11), trainee surgeons (n = 15), and expert surgeons (n = 7). They tried our VRE and then filled in an original 5-point Likert-type scale 6-item questionnaire, considering the following parameters: ease of use, anatomy comprehension compared with 2D radiological imaging, explanation of anatomical variations, explanation of surgical procedures, preoperative planning, and experience of gastrointestinal/neurological disorders. Results in the 3 groups were statistically compared using analysis of variance. Results. Using cross-sectional medical imaging, the developed VRE allowed to visualize a 3D patient-specific abdominal scene in 1 hour. Overall, the 6 items were evaluated positively by all groups; only anatomy comprehension was statistically significant different among the 3 groups. Conclusions. Our approach, based on open-source software and mobile hardware, proved to be a valid and well-appreciated system to visualize 3D patient-specific models, paving the way for a potential new tool for teaching and preoperative planning. © The Author(s) 2019.","anatomy; head-mounted display; mobile; training; virtual reality","anatomy; Article; clinical practice; computer assisted tomography; human; image segmentation; medical education; nuclear magnetic resonance imaging; partial nephrectomy; questionnaire; radiography; surgeon; surgical technique; surgical training; three-dimensional imaging; training; treatment planning; virtual reality; computer assisted surgery; computer interface; devices; equipment design; software; three dimensional imaging; x-ray computed tomography; Equipment Design; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Software; Surgery, Computer-Assisted; Surveys and Questionnaires; Tomography, X-Ray Computed; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85060629868
"Gimeno H., Brown R.G., Lin J.-P., Cornelius V., Polatajko H.J.","54893114200;7406363771;50461740800;6603318530;7003926100;","Cognitive approach to rehabilitation in children with hyperkinetic movement disorders post-DBS",2019,"Neurology","92","11",,"E1212","E1224",,3,"10.1212/WNL.0000000000007092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062887142&doi=10.1212%2fWNL.0000000000007092&partnerID=40&md5=e8ab3c3f609d5465844897d30adc4db1","Complex Motor Disorders Service, Paediatric Neuroscience, Evelina London Children's Hospital, Guy's and St Thomas' NHS Foundation Trust, United Kingdom; Department of Psychology, Institute of Psychiatry, Psychology and Neurosciences, King's College London, United Kingdom; South London and Maudsley NHS Foundation Trust, United Kingdom; School of Public Health, Imperial Clinical Trials Unit, Imperial College London, London, United Kingdom; Department of Occupational Science and Occupational Therapy, University of Toronto, Canada","Gimeno, H., Complex Motor Disorders Service, Paediatric Neuroscience, Evelina London Children's Hospital, Guy's and St Thomas' NHS Foundation Trust, United Kingdom, Department of Psychology, Institute of Psychiatry, Psychology and Neurosciences, King's College London, United Kingdom; Brown, R.G., Department of Psychology, Institute of Psychiatry, Psychology and Neurosciences, King's College London, United Kingdom, South London and Maudsley NHS Foundation Trust, United Kingdom; Lin, J.-P., Complex Motor Disorders Service, Paediatric Neuroscience, Evelina London Children's Hospital, Guy's and St Thomas' NHS Foundation Trust, United Kingdom; Cornelius, V., School of Public Health, Imperial Clinical Trials Unit, Imperial College London, London, United Kingdom, Department of Occupational Science and Occupational Therapy, University of Toronto, Canada; Polatajko, H.J., Complex Motor Disorders Service, Paediatric Neuroscience, Evelina London Children's Hospital, Guy's and St Thomas' NHS Foundation Trust, United Kingdom","This proof-of-concept feasibility trial examined the potential of the Cognitive Orientation to daily Occupational Performance Approach (CO-OP) to augment deep brain stimulation (DBS) outcomes in childhood-onset hyperkinetic movement disorders (HMD) including dystonia and dyskinetic cerebral palsy.MethodsThis is a single case experimental design using multiple baseline as n-of-1 trial comprising 10 intervention sessions, with replications across participants (n = 10). Treatment focused on 3 participant-selected goals. Transfer was assessed on 2 additional untreated goals. Individuals enrolled were 6-21 years of age and had DBS in situ and sufficient manual ability. Primary outcome was functional performance change on the Performance Quality Rating Scale-Individualized (PQRS-i) measured before, during, and posttreatment, and at 3-month follow-up. Assessors of outcome were blinded to time of assessment, number of intervention session, and treatment allocation. To measure effect size, a nonoverlapping index, Tau-U, was used. Feasibility measures were captured.ResultsOne participant withdrew before baseline assessment. Effect sizes of at least 0.66 were seen at both posttreatment and follow-up with all participants showing improvements in at least one trained goal in PQRS-i. Six participants improved on all 3 goals and 2 improved on 2 trained goals. Two children showed deterioration in one trained goal each. Transfer to untrained goals was observed in 3 participants for a total of 5 goals. CO-OP was feasible and acceptable to all participants.ConclusionA cognitive-based, task-oriented approach to support performance of personally relevant functional skills enabling participation is acceptable in childhood-onset HMD post-DBS. Further, preliminary efficacy to improve outcomes and proof of concept with CO-OP has been established in this population.Classification of evidenceThis study provides Class IV evidence that for children with HMD who had undergone DBS, CO-OP improves performance of personally relevant functional skills. © 2019 The Author(s). Published by Wolters Kluwer Health, Inc.",,"adolescent; Article; brain depth stimulation; cerebral palsy; child; clinical assessment; clinical effectiveness; clinical evaluation; clinical trial; cognitive rehabilitation; disease duration; effect size; evidence based practice; feasibility study; female; follow up; human; hyperkinesia; major clinical study; male; outcome assessment; physical performance; priority journal; proof of concept; quantitative analysis; school child; study design; athetosis; cerebral palsy; chorea; daily life activity; dystonia; hyperkinesia; multimodality cancer therapy; myoclonus; occupational therapy; patient attitude; patient care planning; procedures; Activities of Daily Living; Adolescent; Athetosis; Cerebral Palsy; Child; Chorea; Combined Modality Therapy; Deep Brain Stimulation; Dystonia; Feasibility Studies; Female; Humans; Hyperkinesis; Male; Myoclonus; Occupational Therapy; Patient Acceptance of Health Care; Patient Care Planning; Physical Functional Performance; Proof of Concept Study",Article,"Final","",Scopus,2-s2.0-85062887142
"Day B., Ebrahimi E., Hartman L.S., Pagano C.C., Robb A.C., Babu S.V.","55578106200;55868302600;56369773000;7005950745;55211963300;9039004700;","Examining the effects of altered avatars on perception-action in virtual reality",2019,"Journal of Experimental Psychology: Applied","25","1",,"1","24",,10,"10.1037/xap0000192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055081790&doi=10.1037%2fxap0000192&partnerID=40&md5=d74d5faae21781732add1aad9ad044df","Department of Psychology, Butler University, United States; Department of Computer Science, University of North Carolina-Wilmington, United States; Department of Psychology, Clemson University, United States; School of Computing, Clemson University, United States","Day, B., Department of Psychology, Butler University, United States; Ebrahimi, E., Department of Computer Science, University of North Carolina-Wilmington, United States; Hartman, L.S., Department of Psychology, Clemson University, United States; Pagano, C.C., Department of Psychology, Clemson University, United States; Robb, A.C., School of Computing, Clemson University, United States; Babu, S.V., School of Computing, Clemson University, United States","In virtual reality (VR), avatars are graphical representations of people. Previous research highlights benefits of having a self-avatar when perceiving-acting while embedded in a virtual environment. We studied the effect that an altered avatar had on the perception of one's action capabilities. In Experiment 1, some participants acted with a normal, or faithful, avatar whereas another group of participants used an avatar with an extended arm, all in virtual reality. Experiment 2 utilized the same methodology and procedure as Experiment 1, except that only a calibration phase occurred in VR, whereas other phases were completed in the real world. All participants performed reaches to various distances presented visually. Results showed that calibration to altered dimensions of avatars is possible after receiving feedback while acting with the altered avatar. Calibration occurred more quickly when feedback was used to transition from a normal avatar to an altered avatar than when transitioning from the altered avatar back to the normal avatar without feedback. The implications of these findings for training in virtual reality simulations and for transfer to the real world are discussed, along with the implications for the concept of an embodied action schema. © 2018 American Psychological Association.","Action capabilities; Avatar; Calibration; Perception-action; Virtual reality","adolescent; computer interface; feedback system; female; human; male; movement (physiology); perception; physiology; virtual reality; Adolescent; Feedback; Female; Humans; Male; Movement; Perception; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85055081790
"Mertens G., Wagensveld P., Engelhard I.M.","56642504800;57204568011;6701593489;","Cue conditioning using a virtual spider discriminates between high and low spider fearful individuals",2019,"Computers in Human Behavior","91",,,"192","200",,4,"10.1016/j.chb.2018.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056172525&doi=10.1016%2fj.chb.2018.10.006&partnerID=40&md5=01119daeb39d2e59e0cf4f30d5d0e9d1","Department of Clinical Psychology, Utrecht University, Utrecht, Netherlands","Mertens, G., Department of Clinical Psychology, Utrecht University, Utrecht, Netherlands; Wagensveld, P., Department of Clinical Psychology, Utrecht University, Utrecht, Netherlands; Engelhard, I.M., Department of Clinical Psychology, Utrecht University, Utrecht, Netherlands","The fear conditioning paradigm is one of the most commonly used procedures to examine the etiology and treatment of anxiety disorders in laboratories. However, findings with this procedure often do not generalize to clinical settings. Virtual reality (VR) is a promising tool for improving the ecological and predictive validity of fear conditioning. The current study explored whether a classical differential cue conditioning paradigm with spider-fearful participants can be conducted in a VR-environment. Specifically, 25 spider-fearful and 25 non-fearful female students participated in a fear-conditioning experiment with a virtual spider as an unconditioned stimulus (US). The experiment took place in a virtual office in which participants viewed an avatar of themselves sitting at a desk. Conditioned stimuli (CS) were a blue (CS+; 100% reinforcement) and a green (CS-) light emitted by a desk lamp. Fear reactions were measured by fear ratings, skin conductance responses (SCR), and fear potentiated startle responses (FPS). Our results indicated stronger differential fear conditioning for spider-fearful participants than for non-fearful participants. Furthermore, we demonstrate that these results relate specifically to spider-fear, and not to general trait anxiety. We conclude that fear conditioning in VR is a promising tool to improve the validity of classical fear conditioning procedures. © 2018 Elsevier Ltd","Acquisition; Extinction; Fear conditioning; Spider-related fear; Virtual reality","Light extinction; Mobile computing; Acquisition; Clinical settings; Female students; Skin conductance response; Spider-related fear; Virtual office; Virtual reality; adult; anxiety; article; clinical article; controlled study; electrodermal response; fear conditioning test; female; human; nonhuman; reinforcement; spider; startle reflex; stimulus; student; validity; virtual reality",Article,"Final","",Scopus,2-s2.0-85056172525
"Geronazzo M., Sikstrom E., Kleimola J., Avanzini F., De Gotzen A., Serafin S.","36720522500;55354784700;24829233900;7005300654;24724148200;6603367536;","The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of Immersive Virtual Reality Scenarios",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613754,"90","97",,8,"10.1109/ISMAR.2018.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062179431&doi=10.1109%2fISMAR.2018.00034&partnerID=40&md5=5a0b7c6c5270986682096e1aad3ed439","Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Virsabi ApS, Denmark; Hefio Ltd, Denmark; Dept. of Computer Science, University of Milano, Italy","Geronazzo, M., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Sikstrom, E., Virsabi ApS, Denmark; Kleimola, J., Hefio Ltd, Denmark; Avanzini, F., Dept. of Computer Science, University of Milano, Italy; De Gotzen, A., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Serafin, S., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark","Achieving a full 3D auditory experience with head-related transfer functions (HRTFs) is still one of the main challenges of spatial audio rendering. HRTFs capture the listener's acoustic effects and personal perception, allowing immersion in virtual reality (VR) applications. This paper aims to investigate the connection between listener sensitivity in vertical localization cues and experienced presence, spatial audio quality, and attention. Two VR experiments with head-mounted display (HMD) and animated visual avatar are proposed: (i) a screening test aiming to evaluate the participants' localization performance with HRTFs for a non-visible spatialized audio source, and (ii) a 2 minute free exploration of a VR scene with five audiovisual sources in a both non-spatialized (2D stereo panning) and spatialized (free-field HRTF rendering) listening conditions. The screening test allows a distinction between good and bad localizers. The second one shows that no biases are introduced in the quality of the experience (QoE) due to different audio rendering methods; more interestingly, good localizers perceive a lower audio latency and they are less involved in the visual aspects. © 2018 IEEE.","Auditory feedback; Human-centered computing; Interaction devices; Interaction paradigms; Interaction techniques; Sound-based input / output Human-centered computing; Virtual reality; Human-centered computing","Augmented reality; Helmet mounted displays; Three dimensional computer graphics; Transfer functions; Virtual reality; Auditory feedback; Human-centered computing; Input/output; Interaction devices; Interaction paradigm; Interaction techniques; Sound reproduction",Conference Paper,"Final","",Scopus,2-s2.0-85062179431
"Werrlich S., Daniel A., Ginger A., Nguyen P.-A., Notni G.","57195069564;57201618829;57207031366;57209915720;7004204934;","Comparing HMD-Based and Paper-Based Training",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613759,"134","142",,22,"10.1109/ISMAR.2018.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062171885&doi=10.1109%2fISMAR.2018.00046&partnerID=40&md5=c206a2dd977504ba7c080ff7a8072a51","BMW Group, Germany; TU Ilmenau, Germany","Werrlich, S., BMW Group, Germany; Daniel, A., BMW Group, Germany; Ginger, A., BMW Group, Germany; Nguyen, P.-A., BMW Group, Germany; Notni, G., TU Ilmenau, Germany","Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ. © 2018 IEEE.","Augmented Reality; Evaluation; Head Mounted Displays; Training","Augmented reality; NASA; Personnel training; Surveys; Assembly trainings; Collaborative systems; Evaluation; Head mounted displays; Laboratory studies; Learning approach; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85062171885
"Szczurowski K., Smith M.","57202899868;57196078717;","'Woodlands'-A Virtual Reality Serious Game Supporting Learning of Practical Road Safety Skills",2018,"2018 IEEE Games, Entertainment, Media Conference, GEM 2018",,, 8516493,"427","435",,6,"10.1109/GEM.2018.8516493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057030782&doi=10.1109%2fGEM.2018.8516493&partnerID=40&md5=ad57ac1f8e24833e90e1f57db49254f1","Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland","Szczurowski, K., Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland; Smith, M., Department of Informatics, Institute of Technology Blanchardstown, Dublin, Ireland","In developed societies road safety skills are taught early and often practiced under the supervision of a parent, providing children with a combination of theoretical and practical knowledge. At some point children will attempt to cross a road unsupervised, at that point in time their safety depends on the effectiveness of their road safety education. To date, various attempts to supplement road safety education with technology were made. Most common approach focus on addressing declarative knowledge, by delivering road safety theory in an engaging fashion. Apart from expanding on text based resources to include instructional videos and animations, some stakeholders (e.g.: Irish Road Safety Authority) attempt to take advantage of game-based learning [1]. However, despite the high capacity for interaction being common in Virtual Environments, available game-based solutions to road safety education are currently limited to delivering and assessing declarative knowledge. With recent advancements in the field of Virtual Reality (VR) Head Mounted Displays, procedural knowledge might also be addressed in Virtual Environments. This paper describes the design and development process of a computer-supported learning system that attempts to address psycho-motor skills involved in crossing a road safely, changing learners' attitude towards road safety best practices, and enabling independent practice of transferable skills. By implementing game-based learning principles and following best practice for serious game design (such as making educational components essential to successful game-play, or instructional scaffolding) we hope to make it not only more effective, but also engaging, allowing us to rely on learners' intrinsic motivation [2], to increase their independent practice time and provide them with feedback that will help to condition safe behaviour and increase retention. Presence in Virtual Reality might evoke responses to Virtual Environment as if it was real (RAIR) [3] and enable learners to truly experience learning scenarios. In consequence leading to formation of autobiographical memories constructed from multisensory input, which should result in an increased knowledge retention and transfer [4]. © 2018 IEEE.","Experiential Learning; Game-Based Learning; Road Safety; Serious Game; Virtual Environment; Virtual Reality; VR","Accident prevention; Animation; E-learning; Education computing; Helmet mounted displays; Learning systems; Motor transportation; Roads and streets; Scaffolds; Virtual reality; Autobiographical memory; Computer supported learning systems; Declarative knowledge; Design and development process; Experiential learning; Game-based Learning; Head mounted displays; Road safety; Serious games",Conference Paper,"Final","",Scopus,2-s2.0-85057030782
"Brouwer A.-M., van der Waa J., Stokking H.","7102575587;57193858656;25032071700;","BCI to potentially enhance streaming images to a VR headset by predicting head rotation",2018,"Frontiers in Human Neuroscience","12",, 420,"","",,1,"10.3389/fnhum.2018.00420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056811876&doi=10.3389%2ffnhum.2018.00420&partnerID=40&md5=51b5275784df9bd4315f22469dfaa8f4","Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; Department of Media Networking, Netherlands Organization for Applied Scientific Research (TNO), Den Haag, Netherlands","Brouwer, A.-M., Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; van der Waa, J., Department of Perceptual and Cognitive Systems, Netherlands Organization for Applied Scientific Research (TNO), Soesterberg, Netherlands; Stokking, H., Department of Media Networking, Netherlands Organization for Applied Scientific Research (TNO), Den Haag, Netherlands","While numerous studies show that brain signals contain information about an individual’s current state that are potentially valuable for smoothing man–machine interfaces, this has not yet lead to the use of brain computer interfaces (BCI) in daily life. One of the main challenges is the common requirement of personal data that is correctly labeled concerning the state of interest in order to train a model, where this trained model is not guaranteed to generalize across time and context. Another challenge is the requirement to wear electrodes on the head. We here propose a BCI that can tackle these issues and may be a promising case for BCI research and application in everyday life. The BCI uses EEG signals to predict head rotation in order to improve images presented in a virtual reality (VR) headset. When presenting a 360° video to a headset, field-of-view approaches only stream the content that is in the current field of view and leave out the rest. When the user rotates the head, other content parts need to be made available soon enough to go unnoticed by the user, which is problematic given the available bandwidth. By predicting head rotation, the content parts adjacent to the currently viewed part could be retrieved in time for display when the rotation actually takes place. We here studied whether head rotations can be predicted on the basis of EEG sensor data and if so, whether application of such predictions could be applied to improve display of streaming images. Eleven participants generated left- and rightward head rotations while head movements were recorded using the headsets motion sensing system and EEG. We trained neural network models to distinguish EEG epochs preceding rightward, leftward, and no rotation. Applying these models to streaming EEG data that was withheld from the training showed that 400 ms before rotation onset, the probability “no rotation” started to decrease and the probabilities of an upcoming right- or leftward rotation started to diverge in the correct direction. In the proposed BCI scenario, users already wear a device on their head allowing for integrated EEG sensors. Moreover, it is possible to acquire accurately labeled training data on the fly, and continuously monitor and improve the model’s performance. The BCI can be harnessed if it will improve imagery and therewith enhance immersive experience. © 2018 Brouwer, van der Waa and Stokking.","Applied neuroscience; Brain computer interface; EEG; Head mounted display; Head rotation; Movement prediction; Neuroadaptive technology; Virtual reality","adult; Article; artificial neural network; brain computer interface; electroencephalography; head movement; human; human experiment; image display; image processing; imagery; normal human; prediction; probability; signal transduction; videorecording; virtual reality",Article,"Final","",Scopus,2-s2.0-85056811876
"Grubert J., Witzani L., Ofek E., Pahud M., Kranz M., Kristensson P.O.","35114754100;56024891600;10139546600;6602493330;12239425600;6507412583;","Text Entry in Immersive Head-Mounted Display-Based Virtual Reality Using Standard Keyboards",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,, 8446059,"159","166",,41,"10.1109/VR.2018.8446059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053830754&doi=10.1109%2fVR.2018.8446059&partnerID=40&md5=751e12a3f39023107cd2837e1c6effd0","Coburg University of Applied Sciences and Arts, Germany; University of Passau, Germany; Microsoft Research, Germany; University of Cambridge, United Kingdom","Grubert, J., Coburg University of Applied Sciences and Arts, Germany; Witzani, L., University of Passau, Germany; Ofek, E., Microsoft Research, Germany; Pahud, M., Coburg University of Applied Sciences and Arts, Germany; Kranz, M., Coburg University of Applied Sciences and Arts, Germany; Kristensson, P.O., University of Cambridge, United Kingdom","We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing. © 2018 IEEE.","H.5.2: [User Interfaces-Input devices and strategies.]","Economic and social effects; Helmet mounted displays; Typewriter keyboards; Virtual reality; Visual communication; Base-line performance; Head mounted displays; Input devices and strategies; Learning effects; Performance trade-off; Touch-screen keyboards; User experience; Visual feedback; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85053830754
"Ahmad N., Shariff Z.M., Mukhtar F., Lye M.-S.","57203285442;6506573125;25624932700;35973928500;","Family-based intervention using face-to-face sessions and social media to improve Malay primary school children's adiposity: A randomized controlled field trial of the Malaysian REDUCE programme",2018,"Nutrition Journal","17","1", 74,"","",,9,"10.1186/s12937-018-0379-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051107761&doi=10.1186%2fs12937-018-0379-1&partnerID=40&md5=ef1d94bfe4d4dd75206d7a937e305770","Department of Community Health, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, 43400 UPM, Malaysia; Department of Nutrition, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, Malaysia; Department of Psychiatry, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, Malaysia","Ahmad, N., Department of Community Health, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, 43400 UPM, Malaysia; Shariff, Z.M., Department of Nutrition, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, Malaysia; Mukhtar, F., Department of Psychiatry, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, Malaysia; Lye, M.-S., Department of Community Health, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang Selangor, 43400 UPM, Malaysia","Background: Social media may be an effective medium by which parents could be trained to promote healthy eating behaviour and physical activity for their children. This trial evaluates the effectiveness of a family-based intervention using social media in combination with face-to-face sessions - the REDUCE (REorganise Diet, Unnecessary sCreen time and Exercise) programme - on adiposity of Malay children. Methods: Five primary schools in an urban area in Selangor, Malaysia participated in this two-arm randomized controlled field trial. Participants were parents (n = 134) and their primary school-going children 8-11 years of age who were either overweight or obese. These parent-child dyads were randomly allocated to intervention and wait-list control groups and were blinded to group assignment. The intervention was a four-week training programme using two face-to-face sessions and two Facebook sessions followed by weekly booster sessions over a three-month period using WhatsApp. The primary outcome was body mass index (BMI) z-score. Height, body weight, waist circumference and percentage of body fat were measured by blinded assessors. Data were collected at baseline (T1), immediately post-training (T2) and at three- (T3) and six-month post training (T4) and were analysed using generalized linear mixed modelling adjusted for covariates to estimate the intervention effects. Subgroup analysis was conducted for overweight and obese children. Results: Ninety-one percent of parents completed the study, 64 in intervention group and 58 in wait-list group. At the sixth month post-training, BMI z-scores were significantly reduced in the intervention group compared to the wait-list group, for the all children (overweight and obese children) and within the obese subgroup ((F(6, 517) = 2.817, p = 0.010) and (F(6, 297) = 6.072, p < 0.001) respectively. For waist circumference percentile and body fat percentage, the intervention group experienced a significant reduction compared to the wait-list group, within the obese subgroup ((F(6, 297) = 3.998, p = 0.001) and within the overweight subgroup (F(6, 201) = 2.526, p = 0.022). Conclusions: The four-month REDUCE intervention programme was effective in reducing childhood adiposity. Further research using this approach needs to be conducted including cost-effectiveness studies before implementing it in a child obesity prevention programme. Trial registration: Australian New Zealand Clinical Trials Registry: ACTRN12617000844347 (7 June 2017 retrospectively registered). National Medical Research Register, Ministry of Health Malaysia: NMRR-14-685-21,874 (July 2014). © 2018 The Author(s).","Body fat percentage; Body mass index; Paediatric obesity; Parents intervention; Primary school children; Social media; Waist circumference","adolescent obesity; Article; body fat; body height; body mass; body weight; child; clinical effectiveness; clinical outcome; controlled study; family counseling; feeding behavior; female; food intake; health program; healthy lifestyle; human; major clinical study; Malaysian; male; outcome assessment; primary school; randomized controlled trial; school child; social interaction; social media; training; urban area; waist circumference; behavior therapy; body composition; child parent relation; childhood obesity; exercise; health promotion; healthy diet; Malaysia; obesity; pathophysiology; procedures; psychology; school; treatment outcome; urban population; Adiposity; Behavior Therapy; Body Composition; Body Mass Index; Body Weight; Child; Exercise; Female; Health Promotion; Healthy Diet; Humans; Malaysia; Male; Parents; Pediatric Obesity; Schools; Social Media; Treatment Outcome; Urban Population; Waist Circumference",Article,"Final","",Scopus,2-s2.0-85051107761
"Werrlich S., Nguyen P.-A., Notni G.","57195069564;57209915720;7004204934;","Evaluating the training transfer of Head-Mounted Display based training for assembly tasks",2018,"ACM International Conference Proceeding Series",,,,"297","302",,10,"10.1145/3197768.3201564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049917107&doi=10.1145%2f3197768.3201564&partnerID=40&md5=cf6d53b805f90786790e6809e4a4b063","BMW AG, Taunusstraße 41, München, 80807, Germany; Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","Werrlich, S., BMW AG, Taunusstraße 41, München, 80807, Germany; Nguyen, P.-A., BMW AG, Taunusstraße 41, München, 80807, Germany; Notni, G., Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","The automotive industry is growing constantly and more and more assembly workers are needed to negotiate the production volume. The training of new employees is essential to ensure premium quality products and processes. New technologies for training such as head-mounted displays (HMDs) receive a growing amount of attention by the scientific community, especially in the industrial domain. Due to its possibility to work hands-free while providing users with necessary augmented information, HMDs can enhance the quality and efficiency of assembly training tasks. However, comprehensive evaluations in industrial environments regarding the training transfer using augmented reality (AR) technologies are still very limited. In this paper, we aim to close this gap by conducting a user study with two groups and 30 participants, measuring the training transfer. We compare the effects of two slightly different HMD-based training applications. The first group complete a tutorial, beginner, intermediate and expert training level, while the second group received an additional quiz level. Results show that group two needed 17% more time to complete the training but made 79% less sequence mistakes compared to the first group. Additionally, we compare the user satisfaction by using the system usability scale (SUS) and the perceived workload by measuring the NASA-TLX. © 2018 Association for Computing Machinery.","Assembly; Augmented reality; Evaluation; Head-mounted display; Training","Assembly; Augmented reality; Automotive industry; NASA; Personnel training; Sensory perception; Street traffic control; Comprehensive evaluation; Evaluation; Head mounted displays; Industrial environments; Premium-quality products; Scientific community; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85049917107
"Jung J., Ahn Y.J.","57220995238;57201853639;","Effects of interface on procedural skill transfer in virtual training: Lifeboat launching operation study",2018,"Computer Animation and Virtual Worlds","29","3-4", e1812,"","",,7,"10.1002/cav.1812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046267906&doi=10.1002%2fcav.1812&partnerID=40&md5=0f4881ff1662eaf041a1adb3184611b7","Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","Jung, J., Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Ahn, Y.J., Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","A comparative study assessing the effect of interface type on procedural skill transfer during virtual training is presented. The aim of this research is to evaluate the transferability of two aspects of procedural skills, that is, procedural knowledge and technical skills. We established one group with a lecture and three virtual training groups with a combination of output and input devices: a monitor and keyboard/mouse, a head-mounted display (HMD) and joypad, and an HMD and wearable sensors. The task for assessment was a lifeboat launching operation that requires a participant to memorize a 10-step procedure utilizing 14 different pieces of equipment that should be manipulated in each step. Before and after training, we evaluated the participants' procedural knowledge and technical skill on a real lifeboat. The monitor and keyboard/mouse group showed the best performance in a procedural knowledge assessment that addressed visually induced recollections from the real lifeboat. Alternatively, in the assessment of technical skills that determined manipulation ability that requires word-based mnemonics, the HMD and wearable sensors group outperformed the other groups. Moreover, the results showed that the virtual training was a more efficient training format for short-term training than a lecture due to the freedom of observation viewpoint, despite simulator sickness. Copyright © 2018 John Wiley & Sons, Ltd.","interface; maritime safety; procedural skill transfer; virtual reality; virtual training","Helmet mounted displays; Interfaces (materials); Lifeboats; Typewriter keyboards; Virtual reality; Wearable sensors; Comparative studies; Head mounted displays; Lifeboat launching; Maritime safety; Procedural knowledge; Simulator sickness; Skill transfer; Virtual training; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85046267906
"Ip H.H.S., Wong S.W.L., Chan D.F.Y., Byrne J., Li C., Yuan V.S.N., Lau K.S.Y., Wong J.Y.W.","7005395690;35095800900;7402216810;55327177600;56180363500;57190301256;56014513500;57190292326;","Enhance emotional and social adaptation skills for children with autism spectrum disorder: A virtual reality enabled approach",2018,"Computers and Education","117",,,"1","15",,52,"10.1016/j.compedu.2017.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034031316&doi=10.1016%2fj.compedu.2017.09.010&partnerID=40&md5=5461287673669e8bfbf4e12a366653f0","Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Education Studies, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Paediatrics, Faculty of Medicine, The Chinese University of Hong Kong, Shatin, Hong Kong","Ip, H.H.S., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Wong, S.W.L., Department of Education Studies, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Chan, D.F.Y., Department of Paediatrics, Faculty of Medicine, The Chinese University of Hong Kong, Shatin, Hong Kong; Byrne, J., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Li, C., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Yuan, V.S.N., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Lau, K.S.Y., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong; Wong, J.Y.W., Centre for Innovative Applications of Internet and Multimedia Technologies, City University of Hong Kong, Kowloon Tong, Hong Kong","Deficits in social-emotional reciprocity, one of the diagnostic criteria of Autism Spectrum Disorder (ASD), greatly hinders children with ASD from responding appropriately and adapting themselves in various social situations. Although evidences have shown that virtual reality environment is a promising tool for emotional and social adaptation skills training on ASD population, there is a lack of large-scale trials with intensive evaluations to support such findings. This paper presents a virtual reality enabled program for enhancing emotional and social adaptation skills for children with ASD. Six unique learning scenarios, of which one focuses on emotion control and relaxation strategies, four that simulate various social situations, and one that facilitates consolidation and generalization, are designed and developed with corresponding psychoeducation procedures and protocols. The learning scenarios are presented to the children via a 4-side immersive virtual reality environment (a.k.a., half-CAVE) with non-intrusive motion tracking. A total number of 94 children between the ages of 6–12 with clinical diagnosis of ASD participated in the 28-session program that lasted for 14 weeks. By comparing pre- and post-assessments, results reported in this paper show significant improvements in the project's primary measures on children's emotion expression and regulation and social-emotional reciprocity but not on other secondary measures. © 2017 Elsevier Ltd","Autism spectrum disorders; Emotional skills; Situated learning; Social adaptation; Virtual reality","Diagnosis; Diseases; Motion tracking; Program diagnostics; Social aspects; Autism spectrum disorders; Children with autisms; Emotional skills; Immersive virtual reality; Relaxation strategies; Situated learning; Social adaptation; Virtual-reality environment; Virtual reality",Article,"Final","",Scopus,2-s2.0-85034031316
"Dehn L.B., Kater L., Piefke M., Botsch M., Driessen M., Beblo T.","55893825700;57192807227;6507173176;6602523499;56056374100;55978325200;","Training in a comprehensive everyday-like virtual reality environment compared to computerized cognitive training for patients with depression",2018,"Computers in Human Behavior","79",,,"40","52",,15,"10.1016/j.chb.2017.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007894&doi=10.1016%2fj.chb.2017.10.019&partnerID=40&md5=2353908903251aac94bfb95d545d8767","Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany; Clinic of Internal and Geriatric Medicine, Schildische Str. 99, Bielefeld, D-33611, Germany; Department of Psychology and Psychotherapy, Witten/Herdecke University, Alfred-Herrhausen-Str. 50, Witten, D-58448, Germany; Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany","Dehn, L.B., Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany; Kater, L., Clinic of Internal and Geriatric Medicine, Schildische Str. 99, Bielefeld, D-33611, Germany; Piefke, M., Department of Psychology and Psychotherapy, Witten/Herdecke University, Alfred-Herrhausen-Str. 50, Witten, D-58448, Germany; Botsch, M., Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany; Driessen, M., Computer Graphics and Geometry Processing, Bielefeld University, P.O. Box 10 01 31, Bielefeld, D-33501, Germany; Beblo, T., Clinic for Psychiatry and Psychotherapy Bethel, Remterweg 69-71, Bielefeld, D-33617, Germany","Neurocognitive impairments in patients with depression compromise everyday functioning. Thus, should neuropsychological therapy be designed as real-life-like as possible to maximize transfer effects? We investigated whether ecological validity of computerized cognitive training could be increased by a comprehensive everyday-life-simulating training device combining virtual reality, 360°-all-around visibility and autonomous navigation motions. In an eight days training program, patients exercised the learning and purchasing of shopping list products in a virtual supermarket using either the novel training device (n = 21) or a corresponding desktop application (n = 17). In a pre-post-design, effects of the two training conditions were compared regarding several outcome measures. Altogether, results did not prove a benefit of the more naturalistic training setting regarding different training performances (recognition, performance speed, spatial orientation), self-perceived daily cognitive impairments, a real-life shopping task as well as various neuropsychological capabilities. Findings are discussed in the context of general challenges in striving after ecological validity in neuropsychology. © 2017 Elsevier Ltd","Cognitive impairment; Cognitive remediation; Computerized training; Depression; Ecological validity; Virtual reality","Application programs; Ecology; Virtual reality; Autonomous navigation; Cognitive impairment; Depression; Desktop applications; Ecological validity; Spatial orientations; Training conditions; Virtual-reality environment; E-learning",Article,"Final","",Scopus,2-s2.0-85032007894
"Huber T., Paschold M., Hansen C., Wunderling T., Lang H., Kneist W.","18535462800;50361876100;55890379200;57193857398;7402486188;7005632003;","New dimensions in surgical training: immersive virtual reality laparoscopic simulation exhilarates surgical staff",2017,"Surgical Endoscopy","31","11",,"4472","4477",,56,"10.1007/s00464-017-5500-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017182338&doi=10.1007%2fs00464-017-5500-6&partnerID=40&md5=83d81dd35d910a73edf015e3e70d0f69","Department of General, Visceral and Transplant Surgery, University Medicine of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany; Department of Simulation and Graphics, Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Magdeburg, Germany","Huber, T., Department of General, Visceral and Transplant Surgery, University Medicine of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany; Paschold, M., Department of General, Visceral and Transplant Surgery, University Medicine of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany; Hansen, C., Department of Simulation and Graphics, Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Magdeburg, Germany; Wunderling, T., Department of Simulation and Graphics, Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Magdeburg, Germany; Lang, H., Department of General, Visceral and Transplant Surgery, University Medicine of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany; Kneist, W., Department of General, Visceral and Transplant Surgery, University Medicine of the Johannes Gutenberg-University Mainz, Langenbeckstraße 1, Mainz, 55131, Germany","Introduction: Virtual reality (VR) and head mount displays (HMDs) have been advanced for multimedia and information technologies but have scarcely been used in surgical training. Motion sickness and individual psychological changes have been associated with VR. The goal was to observe first experiences and performance scores using a new combined highly immersive virtual reality (IVR) laparoscopy setup. Methods: During the study, 10 members of the surgical department performed three tasks (fine dissection, peg transfer, and cholecystectomy) on a VR simulator. We then combined a VR HMD with the VR laparoscopic simulator and displayed the simulation on a 360° video of a laparoscopic operation to create an IVR laparoscopic simulation. The tasks were then repeated. Validated questionnaires on immersion and motion sickness were used for the study. Results: Participants’ times for fine dissection were significantly longer during the IVR session (regular: 86.51 s [62.57 s; 119.62 s] vs. IVR: 112.35 s [82.08 s; 179.40 s]; p = 0.022). The cholecystectomy task had higher error rates during IVR. Motion sickness did not occur at any time for any participant. Participants experienced a high level of exhilaration, rarely thought about others in the room, and had a high impression of presence in the generated IVR world. Conclusion: This is the first clinical and technical feasibility study using the full IVR laparoscopy setup combined with the latest laparoscopic simulator in a 360° surrounding. Participants were exhilarated by the high level of immersion. The setup enables a completely new generation of surgical training. © 2017, Springer Science+Business Media New York.","Abdominal surgery; Immersive virtual reality; Laparoscopy; Simulation; Training; Virtual surgery","abdominal surgery; Article; cholecystectomy; dissection; feasibility study; female; human; laparoscopic surgery; male; motion sickness; operating room personnel; priority journal; surgical training; time; virtual reality; clinical competence; education; health care personnel; laparoscopy; procedures; questionnaire; simulation training; statistics and numerical data; Clinical Competence; Feasibility Studies; Female; Health Personnel; Humans; Laparoscopy; Male; Simulation Training; Surveys and Questionnaires; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85017182338
"Dubovi I., Levy S.T., Dagan E.","57194381219;7402774725;6701806618;","Now I know how! The learning process of medication administration among nursing students with non-immersive desktop virtual reality simulation",2017,"Computers and Education","113",,,"16","27",,41,"10.1016/j.compedu.2017.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019934867&doi=10.1016%2fj.compedu.2017.05.009&partnerID=40&md5=919db5ba81874d7253ec797d5f546f84","Department of Learning, Instruction and Teacher Education, Faculty of Education, University of Haifa, Israel; The Cheryl Spencer Department of Nursing, Faculty of Social Welfare and Health Sciences, University of Haifa, Israel","Dubovi, I., Department of Learning, Instruction and Teacher Education, Faculty of Education, University of Haifa, Israel, The Cheryl Spencer Department of Nursing, Faculty of Social Welfare and Health Sciences, University of Haifa, Israel; Levy, S.T., Department of Learning, Instruction and Teacher Education, Faculty of Education, University of Haifa, Israel; Dagan, E., The Cheryl Spencer Department of Nursing, Faculty of Social Welfare and Health Sciences, University of Haifa, Israel","The purpose of this study was to create and explore an effective and accessible teaching method for the higher education of professionals requiring practical skills. We aimed to evaluate the effectiveness of our Pharmacology Inter-Leaved Learning Virtual Reality (PILL-VR) simulation when applied to nursing education, as a tool for learning medication administration procedures. A quasi-experimental pretest-intervention-posttest comparison group design was conducted based on quantitative analysis of questionnaires, video recordings and worksheets. Participants were nursing students who either learned medication administration processes with a PILL-VR simulation platform (experimental group; n = 82) or who learned with lecture-based curriculum (n = 47; comparison group). The results revealed significantly higher conceptual and procedural knowledge learning gains following activity with the PILL-VR simulation compared to studying via lecture-based curriculum. PILL-VR exposed the students to their own errors, allowing procedure rehearsal followed by constant feedback which is essential to skill acquisition. Although PILL-VR is based on a desktop system, it facilitated a strong sense of presence. A small positive correlation was found on questionnaire scores between the sense of presence, particularly the sense of control, and conceptual-procedural learning of medication administration. This indicates that by improving students' sense of control in the PILL-VR, the learning process can be improved. Hence, VR simulations may provide affordable and flexible access to practice necessary practical skills in higher education, which is crucial to developing students’ expertise. © 2017 Elsevier Ltd","Higher education; Nursing education; Simulation; Virtual reality","Curricula; E-learning; Learning systems; Nursing; Surveys; Technology transfer; Video recording; Virtual reality; Analysis of questionnaire; Desktop virtual reality; Higher education; Nursing education; Positive correlations; Procedural knowledge; Procedural learning; Simulation; Students",Article,"Final","",Scopus,2-s2.0-85019934867
"Siegel Z.D., Kelly J.W., Cherep L.A.","56094810300;55346243800;57193440989;","Rescaling of perceived space transfers across virtual environments",2017,"Journal of Experimental Psychology: Human Perception and Performance","43","10",,"1805","1814",,8,"10.1037/xhp0000401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027048373&doi=10.1037%2fxhp0000401&partnerID=40&md5=f4e76dd94dd38e9b80bcfb4f93c94273","Department of Psychology, Iowa State University, United States","Siegel, Z.D., Department of Psychology, Iowa State University, United States; Kelly, J.W., Department of Psychology, Iowa State University, United States; Cherep, L.A., Department of Psychology, Iowa State University, United States","Research over the past 20 years has consistently shown that egocentric distance is underperceived in virtual environments (VEs) compared with real environments. In 2 experiments, judgments of object distance (Experiment 1) and object size (Experiment 2) improved after a brief period of walking through the VE with continuous visual feedback. Whereas improvement of blind-walking distance judgments could be attributable to recalibration of walking, improvement in perceived size is considered evidence for rescaling of perceived space, whereby perceived size and distance increased after walking interaction. Furthermore, improvements in judged distance and size transferred to a new VE. Distance judgments, but not size judgments, continued to improve after additional walking interaction in the new VE. These results have theoretical implications regarding the effects of walking interaction on perceived space, and practical implications regarding methods of improving perceived distance in VEs. © 2017 American Psychological Association.",,"depth perception; human; psychological feedback; psychology; virtual reality; vision; walking; Feedback, Psychological; Humans; Space Perception; Virtual Reality; Visual Perception; Walking",Article,"Final","",Scopus,2-s2.0-85027048373
"Ragan E.D., Scerbo S., Bacim F., Bowman D.A.","26667185300;55210765300;16174310700;57203231782;","Amplified Head Rotation in Virtual Reality and the Effects on 3D Search, Training Transfer, and Spatial Orientation",2017,"IEEE Transactions on Visualization and Computer Graphics","23","8", 7547900,"1880","1895",,22,"10.1109/TVCG.2016.2601607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028407406&doi=10.1109%2fTVCG.2016.2601607&partnerID=40&md5=36f5ebc4f3b8d70469432fd11b448a6f","Texas A and M University, College Station, TX  77843, United States; Virginia Tech, Blacksburg, VA  24061, United States","Ragan, E.D., Texas A and M University, College Station, TX  77843, United States; Scerbo, S., Virginia Tech, Blacksburg, VA  24061, United States; Bacim, F., Virginia Tech, Blacksburg, VA  24061, United States; Bowman, D.A., Virginia Tech, Blacksburg, VA  24061, United States","Many types of virtual reality (VR) systems allow users to use natural, physical head movements to view a 3D environment. In some situations, such as when using systems that lack a fully surrounding display or when opting for convenient low-effort interaction, view control can be enabled through a combination of physical and virtual turns to view the environment, but the reduced realism could potentially interfere with the ability to maintain spatial orientation. One solution to this problem is to amplify head rotations such that smaller physical turns are mapped to larger virtual turns, allowing trainees to view the entire surrounding environment with small head movements. This solution is attractive because it allows semi-natural physical view control rather than requiring complete physical rotations or a fully-surrounding display. However, the effects of amplified head rotations on spatial orientation and many practical tasks are not well understood. In this paper, we present an experiment that evaluates the influence of amplified head rotation on 3D search, spatial orientation, and cybersickness. In the study, we varied the amount of amplification and also varied the type of display used (head-mounted display or surround-screen CAVE) for the VR search task. By evaluating participants first with amplification and then without, we were also able to study training transfer effects. The findings demonstrate the feasibility of using amplified head rotation to view 360 degrees of virtual space, but noticeable problems were identified when using high amplification with a head-mounted display. In addition, participants were able to more easily maintain a sense of spatial orientation when using the CAVE version of the application, which suggests that visibility of the user's body and awareness of the CAVE's physical environment may have contributed to the ability to use the amplification technique while keeping track of orientation. © 1995-2012 IEEE.","3D interaction; cybersickness; Rotation amplification; Search; Spatial orientation; Virtual reality","Caves; E-learning; Helmet mounted displays; Virtual reality; 3D interactions; Amplification technique; Cybersickness; Head mounted displays; Physical environments; Search; Spatial orientations; Surrounding environment; Rotation",Article,"Final","",Scopus,2-s2.0-85028407406
"Ferrer V., Perdomo A., Ali H.R., Fies C., Quarles J.","55949304700;55949402300;57195229030;6508009602;55868360300;","Virtual humans for temperature visualization in a tangible augmented reality educational game",2017,"2017 IEEE Virtual Reality Workshop on K-12 Embodied Learning through Virtual and Augmented Reality, KELVAR 2017",,, 7961559,"","",,7,"10.1109/KELVAR.2017.7961559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026413057&doi=10.1109%2fKELVAR.2017.7961559&partnerID=40&md5=d8f25c80f52a141d27d41e926cd27720","University of Texas at San Antonio, United States","Ferrer, V., University of Texas at San Antonio, United States; Perdomo, A., University of Texas at San Antonio, United States; Ali, H.R., University of Texas at San Antonio, United States; Fies, C., University of Texas at San Antonio, United States; Quarles, J., University of Texas at San Antonio, United States","Our primary objective is to enable effective game based learning approaches in tangible augmented reality. In game based learning there is often a tradeoff in motivation between the educational aspects and game aspects. For example, consider our previous work - a tangible augmented reality application for passive solar energy education (AR-SEE), in which users learn about the science behind architectural design by interacting with a tangible model house and an augmented reality-based visualization of energy transfer within the house. This research extends AR-SEE to begin to convert this educational simulation into an effective educational game by introducing gaming elements, such as interactive virtual humans. Although it is known that AR-SEE does enable learning, it is unknown how the addition of interactive virtual humans will affect user perception of temperature data and learning. In this paper, the goal was to compare user perception of two approaches to temperature data visualization in in tangible augmented reality on mobile phones: 1) the current particle-based visualization (i.e., based on the science of energy transfer) and 2) novel virtual human-based visualizations. The game was intended for high school students. However, as a preliminary study, we conducted a user study with 27 3rd and 4th year architecture students that compared these two visualization approaches and their impact on temperature estimation, motivation, and perceived learning effectiveness. In the future, we plan to integrate this game into high school curricula. © 2017 IEEE.","Augmented reality; education; visualization","Augmented reality; Data visualization; Education; Education computing; Energy transfer; Flow visualization; Motivation; Solar energy; Students; Virtual reality; Visualization; Augmented reality applications; Educational aspects; Educational simulations; Energy educations; Game-based Learning; High school students; Perceived learning; Temperature estimation; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85026413057
"Lessard L., Michalowski W., Fung-Kee-Fung M., Jones L., Grudniewicz A.","35078785900;7003487392;8607731700;57194612702;55544990500;","Architectural frameworks: Defining the structures for implementing learning health systems",2017,"Implementation Science","12","1", 78,"","",,19,"10.1186/s13012-017-0607-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021246061&doi=10.1186%2fs13012-017-0607-7&partnerID=40&md5=39cdcd052ddd8815af9a18d35b5c1a05","University of Ottawa, Telfer School of Management, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada; Institut du Savoir Montfort (ISM), 202-745A Montreal Road, Ottawa, ON  K1K 0T1, Canada; University of Ottawa, Departments of Obstetrics-Gynecology and Surgery, Faculty of Medicine, 451 Smyth Rd, Ottawa, ON  K1H 8M5, Canada; University of Ottawa/Ottawa Regional Cancer Centre, The Ottawa Hospital-General Campus, 501 Smyth Rd, Ottawa, ON  K1H 8L6, Canada; University of Ottawa, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada","Lessard, L., University of Ottawa, Telfer School of Management, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada, Institut du Savoir Montfort (ISM), 202-745A Montreal Road, Ottawa, ON  K1K 0T1, Canada; Michalowski, W., University of Ottawa, Telfer School of Management, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada, Institut du Savoir Montfort (ISM), 202-745A Montreal Road, Ottawa, ON  K1K 0T1, Canada; Fung-Kee-Fung, M., University of Ottawa, Departments of Obstetrics-Gynecology and Surgery, Faculty of Medicine, 451 Smyth Rd, Ottawa, ON  K1H 8M5, Canada, University of Ottawa/Ottawa Regional Cancer Centre, The Ottawa Hospital-General Campus, 501 Smyth Rd, Ottawa, ON  K1H 8L6, Canada; Jones, L., University of Ottawa, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada; Grudniewicz, A., University of Ottawa, Telfer School of Management, 55 Ave. Laurier E, Ottawa, ON  K1N 6N5, Canada","Background: The vision of transforming health systems into learning health systems (LHSs) that rapidly and continuously transform knowledge into improved health outcomes at lower cost is generating increased interest in government agencies, health organizations, and health research communities. While existing initiatives demonstrate that different approaches can succeed in making the LHS vision a reality, they are too varied in their goals, focus, and scale to be reproduced without undue effort. Indeed, the structures necessary to effectively design and implement LHSs on a larger scale are lacking. In this paper, we propose the use of architectural frameworks to develop LHSs that adhere to a recognized vision while being adapted to their specific organizational context. Architectural frameworks are high-level descriptions of an organization as a system; they capture the structure of its main components at varied levels, the interrelationships among these components, and the principles that guide their evolution. Because these frameworks support the analysis of LHSs and allow their outcomes to be simulated, they act as pre-implementation decision-support tools that identify potential barriers and enablers of system development. They thus increase the chances of successful LHS deployment. Discussion: We present an architectural framework for LHSs that incorporates five dimensions-goals, scientific, social, technical, and ethical-commonly found in the LHS literature. The proposed architectural framework is comprised of six decision layers that model these dimensions. The performance layer models goals, the scientific layer models the scientific dimension, the organizational layer models the social dimension, the data layer and information technology layer model the technical dimension, and the ethics and security layer models the ethical dimension. We describe the types of decisions that must be made within each layer and identify methods to support decision-making. Conclusion: In this paper, we outline a high-level architectural framework grounded in conceptual and empirical LHS literature. Applying this architectural framework can guide the development and implementation of new LHSs and the evolution of existing ones, as it allows for clear and critical understanding of the types of decisions that underlie LHS operations. Further research is required to assess and refine its generalizability and methods. © 2017 The Author(s).","Architectural framework; Decision-support tools; Learning health system; Pre-implementation","decision support system; ethics; human; information technology; learning; simulation; vision; decision making; health care delivery; health care planning; health care policy; learning; organization and management; procedures; Decision Making; Delivery of Health Care; Health Care Reform; Health Plan Implementation; Health Systems Plans; Humans; Learning",Article,"Final","",Scopus,2-s2.0-85021246061
"Bertrand J., Bhargava A., Madathil K.C., Gramopadhye A., Babu S.V.","55858839700;57194158382;37075253800;7005569103;9039004700;","The effects of presentation method and simulation fidelity on psychomotor education in a bimanual metrology training simulation",2017,"2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings",,, 7893318,"59","68",,7,"10.1109/3DUI.2017.7893318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019003187&doi=10.1109%2f3DUI.2017.7893318&partnerID=40&md5=01f06159d13212f502fbea389f779a5b","Clemson University, United States","Bertrand, J., Clemson University, United States; Bhargava, A., Clemson University, United States; Madathil, K.C., Clemson University, United States; Gramopadhye, A., Clemson University, United States; Babu, S.V., Clemson University, United States","In this study, we empirically evaluated the effects of presentation method and simulation fidelity on task performance and psychomotor skills acquisition in an immersive bimanual simulation towards precision metrology education. In a 2 × 2 experiment design, we investigated a large-screen immersive display (LSID) with a head-mounted display (HMD), and the presence versus absence of gravity. Advantages of the HMD include interacting with the simulation in a more natural manner as compared to using a large-screen immersive display due to the similarities between the interactions afforded in the virtual compared to the real-world task. Suspending the laws of physics may have an effect on usability and in turn could affect learning outcomes. Our dependent variables consisted of a pre and post cognition questionnaire, quantitative performance measures, perceived workload and system usefulness, and a psychomotor assessment to measure to what extent transfer of learning took place from the virtual to the real world. Results indicate that the HMD condition was preferable to the immersive display in several metrics while the no-gravity condition resulted in users adopting strategies that were not advantageous for task performance. © 2017 IEEE.","and virtual realities; augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial","Personnel training; Units of measurement; User interfaces; Virtual reality; augmented; Dependent variables; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; Head mounted displays; Large-screen immersive displays; Simulation fidelity; Training simulation; Transfer of learning; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85019003187
"Anglin J., Saldana D., Schmiesing A., Liew S.-L.","57193856710;57194044260;57194041188;36992162200;","Transfer of a skilled motor learning task between virtual and conventional environments",2017,"Proceedings - IEEE Virtual Reality",,, 7892346,"401","402",,10,"10.1109/VR.2017.7892346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018412711&doi=10.1109%2fVR.2017.7892346&partnerID=40&md5=65da755f34034bf37533b59e030ddbca","University of Southern California, Los Angeles, CA, United States","Anglin, J., University of Southern California, Los Angeles, CA, United States; Saldana, D., University of Southern California, Los Angeles, CA, United States; Schmiesing, A., University of Southern California, Los Angeles, CA, United States; Liew, S.-L., University of Southern California, Los Angeles, CA, United States","Immersive, head-mounted virtual reality (HMD-VR) can be a potentially useful tool for motor rehabilitation. However, it is unclear whether the motor skills learned in HMD-VR transfer to the non-virtual world and vice-versa. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train individuals in either an HMD-VR or conventional training (CT) environment. Participants were then tested in both environments. Our results show that participants who train in the CT environment have an improvement in motor performance when they transfer to the HMD-VR environment. In contrast, participants who train in the HMD-VR environment show a decrease in skill level when transferring to the CT environment. This has implications for how training in HMD-VR and CT may affect performance in different environments. © 2017 IEEE.","Skilled motor learning; Transfer; Virtual reality","E-learning; Helmet mounted displays; Virtual reality; Head mounted virtual reality; Motor learning; Motor performance; Motor rehabilitation; Motor skills; Skill levels; Transfer; Virtual worlds; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-85018412711
"Ng A.K.T.","57190281471;","Cognitive psychology and human factors engineering of virtual reality",2017,"Proceedings - IEEE Virtual Reality",,, 7892349,"407","408",,1,"10.1109/VR.2017.7892349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018458440&doi=10.1109%2fVR.2017.7892349&partnerID=40&md5=21c24a780861ae214df7983eca402d37","University of Hong Kong, Hong Kong, Hong Kong","Ng, A.K.T., University of Hong Kong, Hong Kong, Hong Kong","This position paper summarizes the author's research interest in Cognitive Psychology and Human-Computer Interaction in the imseCAVE, a CAVE-like system in the University of Hong Kong. Several areas of interest were explored while finding the thesis topic for the Ph.D. research. They include a perception research on distance estimation with proposed error correction mechanism, neurofeedback meditation with EEG in VR and the effect with audio and video, the study of training transfer in VR training, the comparison and research of cybersickness between HMD and the imseCAVE, and comparing VR gaming in TV, HMD, and the imseCAVE by performance, activity level and time perception. With a broad interest, the exact direction is still in the search and requires future exploration. © 2017 IEEE.","Cognitive psychology; HCI; Virtual environment","Error correction; Human computer interaction; Human engineering; Psychophysiology; Activity levels; Audio and video; Cognitive psychology; Correction mechanism; Distance estimation; Position papers; Research interests; Time perception; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85018458440
"Halabi O., El-Seoud S.A., Aljaam J.M., Alpona H., Al-Hemadi M., Al-Hassan D.","57203219290;6507058670;24528095900;56407207400;57194434607;57194429167;","Design of immersive virtual reality system to improve communication skills in individuals with autism",2017,"International Journal of Emerging Technologies in Learning","12","5",,"50","64",,10,"10.3991/ijet.v12i05.6766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020066717&doi=10.3991%2fijet.v12i05.6766&partnerID=40&md5=e2208a321c0b786aa1272f0ea478869e","Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt","Halabi, O., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; El-Seoud, S.A., Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt; Aljaam, J.M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Alpona, H., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hemadi, M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hassan, D., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar","Individuals with autism spectrum disorder (ASD) regularly experience situations in which they need to give answers but do not know how to respond; for example, questions related to everyday life activities that are asked by strangers. Research geared at utilizing technology to mend social and communication impairments in children with autism is actively underway. Immersive virtual reality (VR) is a relatively recent technology that has the potential of being an effective therapeutic tool for developing various skills in autistic children. This paper presents an interactive scenario-based VR system developed to improve the communications skills of autistic children. The system utilizes speech recognition to provide natural interaction and role-play and turntaking to evaluate and verify the effectiveness of the immersive environment on the social performance of autistic children. In experiments conducted, participants showed more improved performance with a computer augmented virtual environment (CAVE) than with a head mounted display (HMD) or a normal desktop. The results indicate that immersive VR could be more satisfactory and motivational than desktop for children with ASD.","Autism spectrum disorder; Communication skill; Immersion; Social performance; Virtual reality","Diseases; Helmet mounted displays; Speech recognition; Technology transfer; Virtual reality; Autism spectrum disorders; Children with autisms; Communication skills; Head mounted displays; Immersion; Immersive environment; Immersive virtual reality; Social performance; Education",Article,"Final","",Scopus,2-s2.0-85020066717
"Lee H., Cho Y.-S.","57204033467;57204041517;","Virtual and mixed reality for students: How to control human factors",2017,"ICCE 2017 - 25th International Conference on Computers in Education: Technology and Innovation: Computer-Based Educational Systems for the 21st Century, Workshop Proceedings",,,,"343","354",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054174464&partnerID=40&md5=489c0f8f593877d3aeca1f316d0aa874","Korea Education and Research Information Service, South Korea","Lee, H., Korea Education and Research Information Service, South Korea; Cho, Y.-S., Korea Education and Research Information Service, South Korea","Emerging technologies, such as virtual reality and mixed reality, help teachers as well as students understand educational contents more easily. But we need to consider more deeply. Because most of the devices and contents that released recently are targeted at the game and entertainment market, it may well be doubted whether they are proper for education. Some people have difficulty in health and social aspects after experience virtual or mixed reality. In this paper, we introduce the human factors issues in the virtual and mixed reality area. If we know how to control human factors, virtual and mixed reality could be used more safely in education. We gathered the usage guides, best practices and guidelines about those. We analyze and put the parts commonly mentioned together. But the consideration of hardware itself is excluded. As a result, we propose human factor guideline for users and contents creators using virtual and mixed reality in education. © 2017 Asia-Pacific Society for Computers in Education. All rights reserved.","Augmented reality; Education; Guidelines; Human factors; Virtual reality","Augmented reality; E-learning; Education; Educational technology; Engineering research; Human engineering; Social aspects; Students; Teaching; Technology transfer; Virtual reality; Best practices; Educational contents; Emerging technologies; Guidelines; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85054174464
"Chen W., Ladeveze N., Clavel C., Bourdot P.","55918917900;26537655000;57210674262;14051447300;","Refined experiment of the altered human joystick for user cohabitation in multi-stereocopic immersive CVEs",2016,"2016 IEEE 3rd VR International Workshop on Collaborative Virtual Environments, 3DCVE 2016",,, 7563558,"1","8",,1,"10.1109/3DCVE.2016.7563558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991287566&doi=10.1109%2f3DCVE.2016.7563558&partnerID=40&md5=bb8eb089f580f84d422dbe6a238fec7c","VENISE Group, CNRS-LIMSI, Orsay, France; CPU Group, CNRS-LIMSI, Orsay, France","Chen, W., VENISE Group, CNRS-LIMSI, Orsay, France; Ladeveze, N., VENISE Group, CNRS-LIMSI, Orsay, France; Clavel, C., CPU Group, CNRS-LIMSI, Orsay, France; Bourdot, P., VENISE Group, CNRS-LIMSI, Orsay, France","Immersive multi-user virtual environments give good support for closely-coupled collaboration between co-located users. More complex collaborative tasks may require individual user navigation to achieve loosely-coupled collaboration. We designed a navigation framework based on the human joystick metaphor with some alterations for cohabitation management. This model allows each user to navigate independently using a human joystick based control while avoiding physical obstacles and staying within the usable part of the system. We conducted a series of user studies to investigate the influence of each alteration by testing their combinations on various navigation tasks. The results show that modified transfer functions and adaptive neutral orientations improve users' cohabitation performance, while the impact of adaptive neutral positions need to be further studied. © 2016 IEEE.","I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality","Computer graphics; Navigation; Virtual reality; Co-located; Collaborative tasks; I.3.7 [computer graphics]: three-dimensional graphics and realism - virtual realities; Loosely coupled; Multi-user virtual environment; Navigation tasks; User navigation; User study; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84991287566
"Sankaranarayanan G., Li B., Manser K., Jones S.B., Jones D.B., Schwaitzberg S., Cao C.G.L., De S.","15623319200;57169023100;55996844500;15739783000;55387240300;7007036892;25957557800;7202304567;","Face and construct validation of a next generation virtual reality (Gen2-VR©) surgical simulator",2016,"Surgical Endoscopy","30","3",,"979","985",,13,"10.1007/s00464-015-4278-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959176014&doi=10.1007%2fs00464-015-4278-7&partnerID=40&md5=a5508327f4021456b0383555601182e6","Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States; School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; Cambridge Health Alliance, Cambridge, MA, United States; Beth Israel Deaconess Medical Center, Boston, MA, United States; Wright State University, Dayton, OH, United States; Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States","Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States; Li, B., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States, School of Mechanical Engineering and Automation, Northeastern University, Shenyang, China; Manser, K., Cambridge Health Alliance, Cambridge, MA, United States; Jones, S.B., Beth Israel Deaconess Medical Center, Boston, MA, United States; Jones, D.B., Beth Israel Deaconess Medical Center, Boston, MA, United States; Schwaitzberg, S., Cambridge Health Alliance, Cambridge, MA, United States; Cao, C.G.L., Wright State University, Dayton, OH, United States; De, S., Center for Modeling, Simulation and Imaging in Medicine (CeMSIM), Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States, Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110 8th Street, JEC 2049, Troy, NY  12180, United States","Introduction: Surgical performance is affected by distractors and interruptions to surgical workflow that exist in the operating room. However, traditional surgical simulators are used to train surgeons in a skills laboratory that does not recreate these conditions. To overcome this limitation, we have developed a novel, immersive virtual reality (Gen2-VR©) system to train surgeons in these environments. This study was to establish face and construct validity of our system. Methods and procedures: The study was a within-subjects design, with subjects repeating a virtual peg transfer task under three different conditions: Case I: traditional VR; Case II: Gen2-VR© with no distractions and Case III: Gen2-VR© with distractions and interruptions. In Case III, to simulate the effects of distractions and interruptions, music was played intermittently, the camera lens was fogged for 10 s and tools malfunctioned for 15 s at random points in time during the simulation. At the completion of the study subjects filled in a 5-point Likert scale feedback questionnaire. A total of sixteen subjects participated in this study. Results: Friedman test showed significant difference in scores between the three conditions (p &lt; 0.0001). Post hoc analysis using Wilcoxon signed-rank tests with Bonferroni correction further showed that all the three conditions were significantly different from each other (Case I, Case II, p &lt; 0.0001), (Case I, Case III, p &lt; 0.0001) and (Case II, Case III, p = 0.009). Subjects rated that fog (mean 4.18) and tool malfunction (median 4.56) significantly hindered their performance. Conclusion: The results showed that Gen2-VR© simulator has both face and construct validity and that it can accurately and realistically present distractions and interruptions in a simulated OR, in spite of limitations of the current HMD hardware technology. © 2015, Springer Science+Business Media New York.","Cognitive simulator; Face and construct validation; Gen2-VR©; Head-mounted display; Immersive virtual reality; Surgery simulator","accuracy; Article; Bonferroni correction; camera; computer program; construct validity; face validity; female; Friedman test; human; human experiment; Likert scale; male; monitor; music; operating room; post hoc analysis; priority journal; simulator; statistical model; surgeon; surgery simulator; surgical training; virtual reality; Wilcoxon signed ranks test; attention; computer interface; education; feedback system; laparoscopy; procedures; simulation training; validation study; Attention; Feedback; Female; Humans; Laparoscopy; Male; Simulation Training; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84959176014
"Vašl J., Oblak A., Peternelj T.T., Klett J., Martín-Santamaría S., Gioannini T.L., Weiss J.P., Jerala R.","6504445662;18437495800;41361811800;57207720595;6603039597;56774946900;7402740912;7004624486;","Molecular basis of the functional differences between soluble human versus murine MD-2: Role of Val135 in transfer of lipopolysaccharide from CD14 to MD-2",2016,"Journal of Immunology","196","5",,"2309","2318",,6,"10.4049/jimmunol.1502074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962016594&doi=10.4049%2fjimmunol.1502074&partnerID=40&md5=c3ad0129ed0851b0919b76b02f92499b","Department of Biotechnology, National Institute of Chemistry, Hajdrihova 19, P.O. Box 660, Ljubljana, SI-1000, Slovenia; Center for Biological Research, Superior Council for Scientific Research, Madrid, 28040, Spain; Department of Microbiology, Carver College of Medicine, University of Iowa, Iowa City, IA  52241, United States; Veterans Affairs Medical Center, Iowa City, IA  52246, United States; Excellent Nuclear Magnetic Resonance, Future Innovation for Sustainable Technologies Center of Excellence, Ljubljana, 1000, Slovenia","Vašl, J., Department of Biotechnology, National Institute of Chemistry, Hajdrihova 19, P.O. Box 660, Ljubljana, SI-1000, Slovenia; Oblak, A., Department of Biotechnology, National Institute of Chemistry, Hajdrihova 19, P.O. Box 660, Ljubljana, SI-1000, Slovenia; Peternelj, T.T., Department of Biotechnology, National Institute of Chemistry, Hajdrihova 19, P.O. Box 660, Ljubljana, SI-1000, Slovenia; Klett, J., Center for Biological Research, Superior Council for Scientific Research, Madrid, 28040, Spain; Martín-Santamaría, S., Center for Biological Research, Superior Council for Scientific Research, Madrid, 28040, Spain; Gioannini, T.L., Department of Microbiology, Carver College of Medicine, University of Iowa, Iowa City, IA  52241, United States, Veterans Affairs Medical Center, Iowa City, IA  52246, United States; Weiss, J.P., Department of Microbiology, Carver College of Medicine, University of Iowa, Iowa City, IA  52241, United States; Jerala, R., Department of Biotechnology, National Institute of Chemistry, Hajdrihova 19, P.O. Box 660, Ljubljana, SI-1000, Slovenia, Excellent Nuclear Magnetic Resonance, Future Innovation for Sustainable Technologies Center of Excellence, Ljubljana, 1000, Slovenia","Myeloid differentiation factor 2 (MD-2) is an extracellular protein, associated with the ectodomain of TLR4, that plays a critical role in the recognition of bacterial LPS. Despite high overall structural and functional similarity, human (h) and murine (m) MD-2 exhibit several species-related differences. hMD-2 is capable of binding LPS in the absence of TLR4, whereas mMD-2 supports LPS responsiveness only when mMD-2 and mTLR4 are coexpressed in the same cell. Previously, charged residues at the edge of the LPS binding pocket have been attributed to this difference. In this study, site-directed mutagenesis was used to explore the hydrophobic residues within the MD-2 binding pocket as the source of functional differences between hMD-2 and mMD-2. Whereas decreased hydrophobicity of residues 61 and 63 in the hMD-2 binding pocket retained the characteristics of wild-type hMD-2, a relatively minor change of valine to alanine at position 135 completely abolished the binding of LPS to the hMD-2 mutant. The mutant, however, retained the LPS binding in complex with TLR4 and also cell activation, resulting in a murine-like phenotype. ©2016 by The American Association of Immunologists, Inc.",,"alanine; biological factor; CD14 antigen; human myeloid differentiation factor 2; lipopolysaccharide; murine myeloid differentiation factor 2; toll like receptor 4; unclassified drug; valine; CD14 antigen; lipopolysaccharide; protein binding; protein MD 2; toll like receptor 4; amino acid substitution; Article; binding site; cell activation; controlled study; human; human cell; hydrophobicity; molecular biology; nonhuman; phenotype; priority journal; site directed mutagenesis; species difference; amino acid sequence; animal; cell line; chemical phenomena; chemical structure; chemistry; gene expression; genetics; metabolism; molecular genetics; mouse; protein conformation; protein domain; sequence alignment; structure activity relation; transport at the cellular level; Amino Acid Sequence; Animals; Antigens, CD14; Binding Sites; Biological Transport; Cell Line; Gene Expression; Humans; Hydrophobic and Hydrophilic Interactions; Lipopolysaccharides; Lymphocyte Antigen 96; Mice; Models, Molecular; Molecular Sequence Data; Mutagenesis, Site-Directed; Protein Binding; Protein Conformation; Protein Interaction Domains and Motifs; Sequence Alignment; Structure-Activity Relationship; Toll-Like Receptor 4",Article,"Final","",Scopus,2-s2.0-84962016594
"Turkan Y., Karabulut-Ilgu A., Radkowski R., Chen A., Behzadan A.H., Jahren C.T.","54390463700;57190805412;10043883300;57215560344;15073914400;6603866838;","Mobile augmented reality implementation for teaching structural analysis",2016,"23rd International Workshop of the European Group for Intelligent Computing in Engineering, EG-ICE 2016",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987741097&partnerID=40&md5=aca85a7504888958f54fe9be09b5c33a","Iowa State University, United States; Missouri State University, United States","Turkan, Y., Iowa State University, United States; Karabulut-Ilgu, A., Iowa State University, United States; Radkowski, R., Iowa State University, United States; Chen, A., Iowa State University, United States; Behzadan, A.H., Missouri State University, United States; Jahren, C.T., Iowa State University, United States","In this paper, augmented reality (AR) technology is used to design and launch a mobile application to visualize structural components, and their behavior under loading, which will help students transfer their abstract knowledge into real world situations. To validate results, a set of example problems are used that include questions concerning structural component behavior under loading. For these examples, 3D models are developed and overlaid on top of the book images to help students see the effect of loads on structures in action. For assessment, both control and experimental groups are deployed and students learning is measured in order to investigate the design characteristics of an effective pedagogy involving AR to teach structural analysis, how AR impacts students learning, the role of AR in creating an inclusive learning environment that caters to various learning styles, and enhancing students adaptive flexibility, and how AR impacts students engagement in the learning process.",,"Augmented reality; Computer aided instruction; Human computer interaction; Intelligent computing; Structural analysis; Students; Teaching; Design characteristics; Experimental groups; Learning environments; Mobile applications; Mobile augmented reality; Real world situations; Structural component; Students learning; Education",Conference Paper,"Final","",Scopus,2-s2.0-84987741097
"Alghamdi M., Regenbrecht H., Hoermann S., Langlotz T., Aldridge C.","36459710000;6603333462;54787745300;8250843500;57062294500;","Social presence and mode of video communication in a Collaborative Virtual Environment",2016,"Pacific Asia Conference on Information Systems, PACIS 2016 - Proceedings",,,,"","",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011117105&partnerID=40&md5=0235f7bf3ef9f4bdceecd4b42a98b91e","Department of Information Science, University of Otago, New Zealand; School of Electrical and Information Engineering, University of Sydney, Sydney, Australia","Alghamdi, M., Department of Information Science, University of Otago, New Zealand; Regenbrecht, H., Department of Information Science, University of Otago, New Zealand; Hoermann, S., School of Electrical and Information Engineering, University of Sydney, Sydney, Australia; Langlotz, T., Department of Information Science, University of Otago, New Zealand; Aldridge, C., Department of Information Science, University of Otago, New Zealand","Collaborative Virtual Environments (CVE) with co-located or remote video communication functionality require a continuous experience of social presence. If, at any stage during the experience the communication interrupts presence then the CVE experience as a whole is affected - spatial presence is then decoupled from social presence. We present a solution to this problem by introducing the concept of a virtualized version of Google Glass™ called Virtual Glass. Virtual Glass is integrated into the CVE as a real-world metaphor for a communication device, one particularly suited for collaborative instructor-performer systems. Together with domain experts we developed a prototype system based on an instructor-performer architecture. In two studies with a total number of 115 participants we showed that the concept of Virtual Glass is effective, that it supports a high level of social presence and that the social presence for the performers is rated significantly higher than a standard picture-in-picture videoconferencing approach used for the performers. We present our experimental system, our studies, and the generalizability of our approach towards future uses.","Human-computer interface; Videoconferencing; Virtual reality","Display devices; Glass; Human computer interaction; Information systems; Video conferencing; Collaborative virtual environment; Communication device; Domain experts; Experimental system; Human computer interfaces; Prototype system; Social presence; Video communications; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85011117105
"Katz B.F.G., Felinto D.Q., Touraine D., Poirier-Quinot D., Bourdot P.","35557015800;54788920100;6506895230;55613431100;14051447300;","BlenderVR: Open-source framework for interactive and immersive VR",2015,"2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings",,, 7223366,"203","204",,12,"10.1109/VR.2015.7223366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954548555&doi=10.1109%2fVR.2015.7223366&partnerID=40&md5=9cd4b67baa192a1ea65bdcc275d8639b","LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France","Katz, B.F.G., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Felinto, D.Q., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Touraine, D., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Poirier-Quinot, D., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Bourdot, P., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France","BlenderVR is an open-source project framework for interactive and immersive applications based on an extension of the Blender Game Engine to Virtual Reality applications. BlenderVR is a generalization of the BlenderCAVE project, accounting for alternate platforms (e.g., HMD, video-walls). The goal is to provide a flexible and easy to use framework for the creation of VR applications for various platforms, making use of the existing power of the BGE's graphics rendering and physics engine. Compatible with 3 major Operating Systems, BlenderVR has been developed by VR researchers with support from the Blender Community. BlenderVR currently handles multi-screen/multi-user tracked stereoscopic rendering through efficient low-level master/slave synchronization process with multimodal interactions via OSC and VRPN protocols. © 2015 IEEE.","H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities; I.3.2 [Graphics Systems]: Distributed/network graphics","Blending; Computer graphics; Stereo image processing; Three dimensional computer graphics; Virtual reality; Distributed/network graphics; H.5.1 [multimedia information systems]: artificial , augmented , and virtual realities; Immersive application; Multi-Modal Interactions; Open source frameworks; Open source projects; Stereoscopic renderings; Synchronization process; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-84954548555
"Andrews S., Vincent J.B., McCormick J.","57185000800;57184835300;57197554028;","Duet: Improvising spatial dialogues with an artificially intelligent agent",2015,"SUI 2015 - Proceedings of the 3rd ACM Symposium on Spatial User Interaction",,,,"57","60",,,"10.1145/2788940.2788952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961205751&doi=10.1145%2f2788940.2788952&partnerID=40&md5=217ef1efc54e66e4387fe530c4afaac3","Deakin University, 221 Burwood Highway, Burwood, VIC  3125, Australia","Andrews, S., Deakin University, 221 Burwood Highway, Burwood, VIC  3125, Australia; Vincent, J.B., Deakin University, 221 Burwood Highway, Burwood, VIC  3125, Australia; McCormick, J., Deakin University, 221 Burwood Highway, Burwood, VIC  3125, Australia","This paper presents an experimental framework for a virtual reality artwork, Duet, that employs a combination of live, full body motion capture and Oculus Rift HMD to construct an experience through which a human User can spatially interact with an artificially intelligent Agent. The project explores conceptual notions of embodied knowledge transfer, shared poetics of movement and distortions of the body schema. Within this context, both the User and the Agent become performers, constructing an intimate and spontaneously generated proximal space. The project generates a visualization of the relationship between the User and the Agent without the context of a fixed VR landscape or architecture. The Agent's ability to retain and accumulate movement knowledge in a way that mimics human learning transforms an interactive experience into a collaborative one. The virtual representation of both performers is distorted and amplified in a dynamic manner, enhancing the potential for creative dialogue between the Agent and the User. © 2015 ACM.","Aesthetic Interaction; Agents and Intelligent Systems; Embodiment; Performance; Virtual Reality","Intelligent agents; Intelligent systems; Knowledge management; Virtual reality; Aesthetic Interaction; Embodied knowledge; Embodiment; Full-body motions; Human learning; Performance; Proximal spaces; Virtual representations; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-84961205751
"Bleser G., Damen D., Behera A., Hendeby G., Mura K., Miezal M., Gee A., Petersen N., Maçães G., Domingues H., Gorecky D., Almeida L., Mayol-Cuevas W., Calway A., Cohn A.G., Hogg D.C., Stricker D.","22950167800;25654582700;8684934900;15925569800;55523862900;54788071500;24821358900;35318283600;50961471900;56765803700;36095668000;57206479891;6507899137;6602753391;7005699215;7103188000;6701489212;","Cognitive learning, monitoring and assistance of industrial workflows using egocentric sensor networks",2015,"PLoS ONE","10","6", e0127769,"","",,29,"10.1371/journal.pone.0127769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938704361&doi=10.1371%2fjournal.pone.0127769&partnerID=40&md5=e5314c0fab73a82e8d2900fcea2fd628","Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Department of Computer Science, University of Bristol, Bristol, United Kingdom; School of Computing, University of Leeds, Leeds, United Kingdom; Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden; Department of Electrical Engineering, Linköping University, Linköping, Sweden; SmartFactory KL e.V., Kaiserslautern, Germany; Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal","Bleser, G., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany, Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Damen, D., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Behera, A., School of Computing, University of Leeds, Leeds, United Kingdom, Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Hendeby, G., Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden; Mura, K., SmartFactory KL e.V., Kaiserslautern, Germany; Miezal, M., Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Gee, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Petersen, N., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Maçães, G., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Domingues, H., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Gorecky, D., SmartFactory KL e.V., Kaiserslautern, Germany; Almeida, L., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Mayol-Cuevas, W., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Calway, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Cohn, A.G., School of Computing, University of Leeds, Leeds, United Kingdom; Hogg, D.C., School of Computing, University of Leeds, Leeds, United Kingdom; Stricker, D., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany","Today, the workflows that are involved in industrial assembly and production activities are becoming increasingly complex. To efficiently and safely perform these workflows is demanding on the workers, in particular when it comes to infrequent or repetitive tasks. This burden on the workers can be eased by introducing smart assistance systems. This article presents a scalable concept and an integrated system demonstrator designed for this purpose. The basic idea is to learn workflows from observing multiple expert operators and then transfer the learnt workflow models to novice users. Being entirely learning-based, the proposed system can be applied to various tasks and domains. The above idea has been realized in a prototype, which combines components pushing the state of the art of hardware and software designed with interoperability in mind. The emphasis of this article is on the algorithms developed for the prototype: 1) fusion of inertial and visual sensor information from an on-body sensor network (BSN) to robustly track the user's pose in magnetically polluted environments; 2) learning-based computer vision algorithms to map the workspace, localize the sensor with respect to the workspace and capture objects, even as they are carried; 3) domain-independent and robust workflow recovery and monitoring algorithms based on spatiotemporal pairwise relations deduced from object and user movement with respect to the scene; and 4) context-sensitive augmented reality (AR) user feedback using a head-mounted display (HMD). A distinguishing key feature of the developed algorithms is that they all operate solely on data from the on-body sensor network and that no external instrumentation is needed. The feasibility of the chosen approach for the complete action-perception-feedback loop is demonstrated on three increasingly complex datasets representing manual industrial tasks. These limited size datasets indicate and highlight the potential of the chosen technology as a combined entity as well as point out limitations of the system. © 2015 Bleser et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; cognition; computer interface; computer program; egocentric sensor networks; feedback system; female; head mounted display; human; image display; industrial production; information system; information technology; learning algorithm; male; on body sensor network; online monitoring; operator; sensor; spatiotemporal analysis; virtual reality; visual information; workflow; algorithm; learning; occupational health; occupational medicine; system analysis; three dimensional imaging; workflow; Algorithms; Cognition; Humans; Imaging, Three-Dimensional; Learning; Occupational Health; Occupational Medicine; Systems Integration; User-Computer Interface; Workflow",Article,"Final","",Scopus,2-s2.0-84938704361
"Freina L., Canessa A.","26665429300;57210766595;","Immersive vs desktop virtual reality in game based learning",2015,"Proceedings of the European Conference on Games-based Learning","2015-January",,,"195","202",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955151400&partnerID=40&md5=f62727021ff0a306eef5ab32c8a4bce7","CNR-ITD, Genova, Italy; BioLab - DIBRIS, Università degli Studi di Genova, Italy","Freina, L., CNR-ITD, Genova, Italy; Canessa, A., BioLab - DIBRIS, Università degli Studi di Genova, Italy","Virtual environments are recognized as more effective than other digital approaches for the acquisition of several abilities. This is because the brain recognizes the virtual world as real and this facilitates the transfer of the newly acquired skills to the real world. In this paper, we present a game that has been designed and developed with the aim of teaching spatial orientation abilities to teenagers with mild intellectual impairments. In particular, the game focuses on the training of two basic skills: Perspective taking and mental rotation. Perspective taking refers to the ability of imagining how the world looks like from another person's point of view, while mental rotation is the ability to mentally represent and manipulate physical objects in one's mind. The game, which takes place in a virtual environment, shows the player a scene with some objects on the table. The player has to choose among four provided alternatives, the one that shows how the scene would look like from a different side of the table. The game was first developed to be used with either a desktop pc monitor or an interactive touch table. In this case, a virtual world is represented, but the player is not completely immersed in it, he just looks at the scene from outside. A second version of the same game has then been developed using a Head Mounted Display (HMD), which makes the player feel immersed in the virtual environment, where he can freely move around just as if it was real. In this paper, we discuss both advantages and disadvantages of the immersive Virtual Reality (VR) compared to the desktop VR. In fact, on the one hand, having the possibility to ""dive"" into the virtual world allows the player to: Better build a mental model of the scene and the involved objects by freely moving around the table and examining the objects from all the possible perspectives; Manage by himself the amount of help needed: It is always possible, at any time of the game, to move to the other side of the table and see what the scene looks like. Increase his involvement in the game by exploring the virtual world as he pleases. Have a better learning transfer thanks to the similarities between the virtual and the real worlds. On the other hand, using a HMD can be tiring and cause sickness to some players. Furthermore, the presence of a complete environment in which to move and explore, can draw the attention away from the main task of the game and therefore influence learning negatively. Experiments are planned to verify the foreseen advantages and disadvantages involving young adults with mild intellective disabilities.","Innovative games-based learning; Mental rotation; Perspective taking; Virtual worlds","Helmet mounted displays; Interactive computer graphics; Personnel training; Sensory perception; Desktop virtual reality; Head mounted displays; Immersive virtual reality; Innovative games-based learning; Mental rotation; Perspective taking; Spatial orientations; Virtual worlds; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84955151400
"Abed H., Pernelle P., Carron T., Benamar C., Kechiche M., Baert P.","57069934400;36138922300;14631830600;25959856600;56108613500;55177962500;","Serious game framework focusing on industrial traing: Application to steel industry",2015,"Proceedings of the European Conference on Games-based Learning","2015-January",,,"1","9",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955134969&partnerID=40&md5=34cec6144ed65704c9435f52ce269e5f","LIP6, UPMC, Paris, France; DISP, University of Lyon 1, Villeurbanne, France; REGIM, ENI, Sfax, Tunisia; ENISE, Saint-etienne, France","Abed, H., LIP6, UPMC, Paris, France; Pernelle, P., DISP, University of Lyon 1, Villeurbanne, France; Carron, T., LIP6, UPMC, Paris, France; Benamar, C., REGIM, ENI, Sfax, Tunisia; Kechiche, M., ENISE, Saint-etienne, France; Baert, P., ENISE, Saint-etienne, France","Serious games are growing more and more in the context of lifelong training and initial education. They cover several areas (human science, engineering science, life science, ...) that are used for industrial or academic purposes. However, some fields induce specific issues. Thus, in the industrial area, the constraints inherent to the activity impact the development of a scenario and implementation of a serious gaming environment. Indeed, the objective of the industry training must both lead to the acquisition of knowledge and the transfer of skills. Moreover, the actual validation of these skills is paramount especially if their uses are located on an industrial site, where there are often risks associated with the security of persons and equipment. In many industries, some regulatory constraints impose an obligation of means for the training of staff. In the sector of production, the proliferation of interim requires inevitably targeted training. Finally, it should be noted that even for permanent staff, alternation and fragmentation of training periods seriously complicate the deep learning task. Finally, we can wonder if the serious game can bring relevant answers to specific problems of training in an industrial context? This article offers some answers to this question. Thus, in this work, we propose a serious game scripting framework adapted to the industrial context. This scripting framework is structured around two approaches: The first defines a global framework for scheduling a fun or playful scenario. Moreover, this framework allows to take into account the phases of availability of learners while maintaining motivation. The second approach defines an immersive framework for validating acquired and security compliance that is based on two complementary purposes: Use of alternate observation activities (games / real) and an immersive simulation (Virtual Reality) for security. In partnership with a company in the steel industry, we have developed a prototype of serious game in order to implement this scripting framework. The prototype is based on a generic game platform, on a tablet with use of RFID tag, and with an immersive virtual reality Head-Mounted-Display (HMD type Oculus®). In this article we will present the actions realized in the context of a professional activity related to the manipulation of a bridge crane.","Formatting; Interaction; Serious games; Virtual reality","Helmet mounted displays; Steelmaking; Virtual reality; Engineering science; Formatting; Head mounted displays; Immersive virtual reality; Interaction; Professional activities; Security compliance; Serious games; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-84955134969
"Itoh Y., Klinker G.","56154865900;6603530980;","Performance and sensitivity analysis of INDICA: INteraction-Free DIsplay CAlibration for Optical See-Through Head-Mounted Displays",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948424,"171","176",,20,"10.1109/ISMAR.2014.6948424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945128507&doi=10.1109%2fISMAR.2014.6948424&partnerID=40&md5=a5f7e94ab15349a3b47d9c354073238e","Technische Universität München, Germany","Itoh, Y., Technische Universität München, Germany; Klinker, G., Technische Universität München, Germany","An issue in AR applications with Optical See-Through Head-Mounted Display (OST-HMD) is to correctly project 3D information to the current viewpoint of the user. Manual calibration methods give the projection as a black box which explains observed 2D-3D relationships well (Fig. 1). Recently, we have proposed an INteraction-free DIsplay CAlibration method (INDICA) for OST-HMD, utilizing camera-based eye tracking [7]. It reformulates the projection in two ways: a black box with an actual eye model (Recycle Setup), and a combination of an explicit display model and an eye model (Full Setup). Although we have shown the former performs more stably than a repeated SPAAM calibration, we could not yet prove whether the same holds for the Full Setup. More importantly, it is still unclear how the error in the calibration parameters affects the final results. Thus, the users can not know how accurately they need to estimate each parameter in practice. We provide: (1) the fact that the Full Setup performs as accurately as the Recycle Setup under a marker-based display calibration, (2) an error sensitivity analysis for both SPAAM and INDICA over the on-/offline parameters, and (3) an investigation of the theoretical sensitivity on an OST-HMD justified by the real measurements. © 2014 IEEE.","augmented; H.5.1 [Information Interfaces and Presentation]; Multimedia Information Systems - Artificial; virtual realities","Augmented reality; Calibration; Recycling; Sensitivity analysis; Sensory perception; Street traffic control; Technology transfer; Virtual reality; augmented; Calibration parameters; Display calibrations; Error sensitivity analysis; H.5.1 [Information interfaces and presentation]; Manual calibration; Multimedia information systems-artificial; Optical see-through head-mounted displays; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-84945128507
"Kelly J.W., Hammel W.W., Siegel Z.D., Sjolund L.A.","55346243800;56095556100;56094810300;55547706500;","Recalibration of perceived distance in virtual environments occurs rapidly and transfers asymmetrically across scale",2014,"IEEE Transactions on Visualization and Computer Graphics","20","4", 6777445,"588","595",,35,"10.1109/TVCG.2014.36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897409643&doi=10.1109%2fTVCG.2014.36&partnerID=40&md5=20a9acf9c3a6fc0511fd9c4b60755273","Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Department of Psychology, Iowa State University, Ames, IA 50011, United States","Kelly, J.W., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Hammel, W.W., Department of Psychology, Iowa State University, Ames, IA 50011, United States; Siegel, Z.D., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Sjolund, L.A., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States","Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance. © 2014 IEEE.","Distance perception; recalibration; virtual reality","Depth perception; Experiments; Visual communication; Current projects; Distance perception; Immersive virtual reality; Improved distance; Perceived distances; Recalibrations; Visual feedback; Walking through; Virtual reality",Article,"Final","",Scopus,2-s2.0-84897409643
"Azami H., Hassanpour H., Anisheh S.M.","55337000000;56919429700;35263498300;","An improved automatic EEG signal segmentation method based on generalized likelihood ratio",2014,"International Journal of Engineering, Transactions A: Basics","27","7",,"1015","1022",,7,"10.5829/idosi.ije.2014.27.07a.02","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904656458&doi=10.5829%2fidosi.ije.2014.27.07a.02&partnerID=40&md5=bb8a7ff36a8377d065be0fda9d5a7885","Department of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran; School of Information Technology and Computer Engineering, Shahrood University, Iran; Department of Computer and Electrical Engineering, Khaje Nasir Toosi University of Technology, Tehran, Iran","Azami, H., Department of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran; Hassanpour, H., School of Information Technology and Computer Engineering, Shahrood University, Iran; Anisheh, S.M., Department of Computer and Electrical Engineering, Khaje Nasir Toosi University of Technology, Tehran, Iran","It is often needed to label electroencephalogram(EEG) signals by segments of similar characteristics that are particularly meaningful to clinicians and for assessment by neurophysiologists. Within each segment, the signals are considered statistically stationary, usually with similar characteristics such as amplitude and/or frequency. In order to detect the segment boundaries of a signal, we propose an improved method using time-varying autoregressive (TVAR) model, integral, basic generalized likelihood ratio (GLR) and new particle swarm optimization (NPSO) which is a powerful intelligent optimizer. Since autoregressive (AR) model for the GLR method is valid for only stationary signals, the TVAR as a valuable and powerful tool for non-stationary signals is suggested. Moreover, to improve the performance of the basic GLR and increase the speed of that, we propose to use moving steps formore than one sample for successive windows in the basic GLR method. The purpose of using NPSO is finding two parameters used in this approach. By using synthetic and real EEG data, the proposed method is compared with the conventional ones, i.e. the GLR and wavelet GLR (WGLR). The simulation results indicate the absolute advantages of the proposed method.","Adaptive signal segmentation; Generalized likelihood ratio; Integral; New particle swarm optimization; Time-varying autoregressive model","Computer simulation; Particle swarm optimization (PSO); Adaptive signals; Generalized likelihood ratio; Integral; New particle swarm optimization; Time varying autoregressive model; Electroencephalography",Article,"Final","",Scopus,2-s2.0-84904656458
"Zmuda M.A., Wonser J.L., Bachmann E.R., Hodgson E.","55905032400;35729733600;7005745121;14053915200;","Optimizing constrained-environment redirected walking instructions using search techniques",2013,"IEEE Transactions on Visualization and Computer Graphics","19","11", 6520845,"1872","1884",,37,"10.1109/TVCG.2013.88","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884554083&doi=10.1109%2fTVCG.2013.88&partnerID=40&md5=091fedd90ee3852d2e41955d41a5eb1f","Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Department of Psychology, Miami University, Oxford, OH 45056, United States","Zmuda, M.A., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Wonser, J.L., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Bachmann, E.R., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Hodgson, E., Department of Psychology, Miami University, Oxford, OH 45056, United States","A goal of redirected walking (RDW) is to allow large virtual worlds to be explored within small tracking areas. Generalized steering algorithms, such as steer-to-center, simply move the user toward locations that are considered to be collision free in most cases. The algorithm developed here, FORCE, identifies collision-free paths by using a map of the tracking area's shape and obstacles, in addition to a multistep, probabilistic prediction of the user's virtual path through a known virtual environment. In the present implementation, the path predictions describe a user's possible movements through a virtual store with aisles. Based on both the user's physical and virtual location/orientation, a search-based optimization technique identifies the optimal steering instruction given the possible user paths. Path prediction uses the map of the virtual world; consequently, the search may propose steering instructions that put the user close to walls if the user's future actions eventually lead away from the wall. Results from both simulated and real users are presented. FORCE identifies collision-free paths in 55.0 percent of the starting conditions compared to 46.1 percent for generalized methods. When considering only the conditions that result in different outcomes, redirection based on FORCE produces collision-free path 94.5 percent of the time. © 2013 IEEE.","Backtracking; motion compression; redirected walking; virtual reality","Backtracking; Collision-free paths; Generalized method; Motion compression; Optimization techniques; Probabilistic prediction; Redirected walkings; Steering algorithms; Algorithms; Interactive computer graphics; Virtual reality; Optimization; adult; algorithm; computer graphics; computer interface; computer simulation; environment; human; male; orientation; physiology; procedures; three dimensional imaging; walking; article; methodology; three dimensional imaging; walking; Adult; Algorithms; Computer Graphics; Computer Simulation; Environment; Humans; Imaging, Three-Dimensional; Male; Orientation; User-Computer Interface; Walking; Adult; Algorithms; Computer Graphics; Computer Simulation; Environment; Humans; Imaging, Three-Dimensional; Male; Orientation; User-Computer Interface; Walking",Article,"Final","",Scopus,2-s2.0-84884554083
"Yoshinaga T., Arita D., Masuda K.","35735607100;55943147000;7401446825;","Development of Augmented Reality Body-Mark system to support echography",2012,"2012 IEEE Biomedical Circuits and Systems Conference: Intelligent Biomedical Electronics and Systems for Better Life and Better Environment, BioCAS 2012 - Conference Publications",,, 6418425,"348","351",,,"10.1109/BioCAS.2012.6418425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874132953&doi=10.1109%2fBioCAS.2012.6418425&partnerID=40&md5=6479729009b849104b65c4b05f468b25","Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Graduate School of Bio-Applications and Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan","Yoshinaga, T., Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Arita, D., Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Masuda, K., Graduate School of Bio-Applications and Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan","We propose visualization system of 3D shape of internal organs using Augmented Reality and Virtual Reality technology. Echography has been used in every field of medical diagnosis because of its safety and cost-effectiveness. Therefore, portable echography device so called 'Ubiquitous Echo' was released by many manufacturers. However, the skill to recognize relationship between ultrasound probe and internal organ is required to acquire echogram. Therefore we have developed tracking system of probe orientation to transfer 2D position of echogram into 3D space. Then we have applied Radial Basis Function Interpolation to reconstruct the shape of internal organs. Furthermore, GUI interface to visualize the internal organ was developed by OpenGL. As a result of evaluation experiments, visualization of internal organ was satisfied to show the echogram for diagnosis. © 2012 IEEE.",,"3-D shape; 3-D space; Echography; Evaluation experiments; Internal organs; Radial basis function interpolation; Tracking system; Ultrasound probes; Virtual reality technology; Visualization system; Application programming interfaces (API); Augmented reality; Diagnosis; Probes; Radial basis function networks; Ultrasonic imaging; Virtual reality; Visualization; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84874132953
"Aleotti J., Caselli S.","23026542300;7004694361;","Grasp programming by demonstration in virtual reality with automatic environment reconstruction",2012,"Virtual Reality","16","2",,"87","104",,10,"10.1007/s10055-010-0172-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861707584&doi=10.1007%2fs10055-010-0172-8&partnerID=40&md5=13bb20d3fa723fae654d5a100ec4a9e1","Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy","Aleotti, J., Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy; Caselli, S., Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy","A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer. © 2010 Springer-Verlag London Limited.","Environment modeling; Glove interaction; Grasp programming; Virtual reality","Assembly operations; Environment modeling; Environment modeling systems; Glove interaction; Grasp analysis; High-level programming; Intelligent manipulation; Modeling technique; Monocular vision; Multiple views; Programming by demonstration; Programming robots; Real time; Robot-environment interaction; Robotic devices; Robotic systems; Synthesis phase; User interaction; Virtual reality system; Vision based; Object recognition; Robot learning; Robotics; Three dimensional; Virtual reality; Robot programming",Article,"Final","",Scopus,2-s2.0-84861707584
"Ha T., Woo W.","15020792200;35575439600;","ARWand: Phone-based 3D object manipulation in augmented reality environment",2011,"Proceedings - 2011 International Symposium on Ubiquitous Virtual Reality, ISUVR 2011",,, 6068304,"44","47",,13,"10.1109/ISUVR.2011.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82055162736&doi=10.1109%2fISUVR.2011.14&partnerID=40&md5=2f03f1e5d872add11cc4b5a1988ebc6e","GIST U-VR Lab., 500-712, South Korea","Ha, T., GIST U-VR Lab., 500-712, South Korea; Woo, W., GIST U-VR Lab., 500-712, South Korea","In this paper, we suggest a mobile phone-based indirect 3D object manipulation method that uses sensor information in an augmented reality environment. Specifically, we propose 1) a method that exploits a 2D touch screen, a 3DOF accelerometer, and compass sensors information to manipulate 3D objects in 3D space, 2) design transfer functions to map the control space of mobile phones to an augmented reality (AR) display space, and 3) confirm the feasibility of the transfer functions by implementation. Our work could be applicable to the design and implementation of a future mobile phone-based 3D user interface for AR application in normal indoor and outdoor environments without any special tracking installations. © 2011 IEEE.","3D object manipulation; Augmented reality; HMD-based wearable computing; mobile phone; sensor based interaction","3-D space; 3D object; 3D user interface; AR application; Control spaces; Outdoor environment; Sensor informations; Touch screen; Wearable computing; Accelerometers; Augmented reality; Mobile phones; Sensors; Telephone sets; Transfer functions; User interfaces; Virtual reality; Wearable computers; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-82055162736
"Hodgson E., Bachmann E., Waller D.","14053915200;7005745121;7203026309;","Redirected walking to explore virtual environments: Assessing the potential for spatial interference",2011,"ACM Transactions on Applied Perception","8","4", 22,"","",,46,"10.1145/2043603.2043604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855243871&doi=10.1145%2f2043603.2043604&partnerID=40&md5=849f0a43f0fe124fd20cbdb756064998","Department of Psychology, 90 N. Patterson Ave., Oxford, OH 45056, United States; Department of Computer Science, Benton Hall, Oxford, OH 45056, United States","Hodgson, E., Department of Psychology, 90 N. Patterson Ave., Oxford, OH 45056, United States; Bachmann, E., Department of Computer Science, Benton Hall, Oxford, OH 45056, United States; Waller, D., Department of Psychology, 90 N. Patterson Ave., Oxford, OH 45056, United States","Redirected walking has gained popularity in recent years as a way of enhancing the safety of users immersed in a virtual reality simulation and of extending the amount of space that can be simulated in a virtual environment (VE). Limits imposed by the available physical space and functional tracking area are overcome by inducing immersed users to veer imperceptibly in a way that prevents them from leaving the confines of the tracking space. Redirected walking has been shown to be feasible at levels below noticeable thresholds and to function without increasing the incidence of simulator sickness. The present studies demonstrate that redirected walking can function without negatively impacting memory for spatial locations of landmarks in a VE, despite introducing discrepancies between various spatial senses and distorting the spatial mapping of movement onto the environment. Additionally, the present studies implement what, to our knowledge, is the first generalized redirected walking algorithm that is independent of any task or environment structure, and can adaptively steer users in real time as they engage in spontaneous, unconstrained navigation. The studies also demonstrate that such an algorithm can be implemented successfully in a gymnasium-sized space. © 2011 ACM.","3D interfaces; Human computer interaction; Multimodal sensory integration; Redirected walking; Spatial cognition; Spatial memory; Spatial senses; Virtual reality","3D interface; Multi-modal; Redirected walking; Spatial cognition; Spatial memory; Spatial senses; Algorithms; Human computer interaction; Virtual reality",Article,"Final","",Scopus,2-s2.0-84855243871
"Francis G., Rash C.E.","35748500900;7004825500;","Cognitive considerations for helmet-mounted display design",2010,"Proceedings of SPIE - The International Society for Optical Engineering","7688",, 76880D,"","",,,"10.1117/12.848930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953641789&doi=10.1117%2f12.848930&partnerID=40&md5=5ee099ddc14b578442a26b45fc2d3347","Psychological Sciences, Purdue University, West Lafayette, IN 47907, United States; U.S. Army Aeromedical Research Laboratory, PO Box 620577, Fort Rucker, AL 36330, United States","Francis, G., Psychological Sciences, Purdue University, West Lafayette, IN 47907, United States; Rash, C.E., U.S. Army Aeromedical Research Laboratory, PO Box 620577, Fort Rucker, AL 36330, United States","Helmet-mounted displays (HMDs) are designed as a tool to increase performance. To achieve this, there must be an accurate transfer of information from the HMD to the user. Ideally, an HMD would be designed to accommodate the abilities and limitations of users' cognitive processes. It is not enough for the information (whether visual, auditory, or tactual) to be displayed; the information must be perceived, attended, remembered, and organized in a way that guides appropriate decision-making, judgment, and action. Following a general overview, specific subtopics of cognition, including perception, attention, memory, knowledge, decision-making, and problem solving are explored within the context of HMDs. © 2010 Copyright SPIE - The International Society for Optical Engineering.","attention; cognition; decision making; Helmet-mounted display; HMD; memory; perception; problem solving","Cognitive process; Perception problems; Transfer of information; Aviators; Decision making; Design; Problem solving; Safety devices; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-77953641789
"Tawara T., Ono K.","57196911574;35105793600;","A framework for volume segmentation and visualization using augmented reality",2010,"3DUI 2010 - IEEE Symposium on 3D User Interfaces 2010, Proceedings",,, 5444707,"121","122",,9,"10.1109/3DUI.2010.5444707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953075808&doi=10.1109%2f3DUI.2010.5444707&partnerID=40&md5=22eff03c37d56cbd0d9c527360252df2","RIKEN, Japan","Tawara, T., RIKEN, Japan; Ono, K., RIKEN, Japan","We propose a two-handed direct manipulation system to achieve complex volume segmentation of CT/MRI data in Augmented Reality with a remote controller attached to a motion tracking cube. At the same time segmented data is displayed by direct volume rendering using a programmable GPU. Our system achieves visualization of real time modification of volume data with complex shading including transparency control by changing transfer functions, displaying any cross section, and rendering multi materials using a local illumination model. Our goal is to build a system that facilitates direct manipulation of volumetric CT/MRI data for segmentation in Augmented Reality. Volume segmentation is a challenging problem and segmented data has an important role for visualization and analysis. ©2010 IEEE.","I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques; K.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented, and virtual realities; K.5.2 [Information Interfaces and Presentation]: User Interfaces-Interaction styles","I.3.6 [computer graphics]: methodology and techniques; Information interfaces; Interaction styles; Multimedia information systems-artificial; Augmented reality; Data visualization; Information systems; Three dimensional; Virtual reality; Visualization; Volume rendering; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-77953075808
"Mulder D.W., Boyd E.S., Sarma R., Lange R.K., Endrizzi J.A., Broderick J.B., Peters J.W.","23061326500;7102710627;23062179300;35519644800;6603988413;7102907985;35280742600;","Stepwise FeFe-hydrogenase H-cluster assembly revealed in the structure of HydA ΔeFG",2010,"Nature","465","7295",,"248","251",,209,"10.1038/nature08993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952428962&doi=10.1038%2fnature08993&partnerID=40&md5=7a011cf75c8499957ae3d32db8d2ea0f","Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States; Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States","Mulder, D.W., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Boyd, E.S., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Sarma, R., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Lange, R.K., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Endrizzi, J.A., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Broderick, J.B., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States; Peters, J.W., Astrobiology Biogeocatalysis Research Center, Bozeman, MT 59717, United States, Department of Chemistry and Biochemistry, Montana State University, Bozeman, MT 59717, United States","Complex enzymes containing Fe-S clusters are ubiquitous in nature, where they are involved in a number of fundamental processes including carbon dioxide fixation, nitrogen fixation and hydrogen metabolism. Hydrogen metabolism is facilitated by the activity of three evolutionarily and structurally unrelated enzymes: the NiFe-hydrogenases, FeFe-hydrogenases and Fe-hydrogenases (Hmd). The catalytic core of the FeFe-hydrogenase (HydA), termed the H-cluster, exists as a 4Fe-4S subcluster linked by a cysteine thiolate to a modified 2Fe subcluster with unique non-protein ligands. The 2Fe subcluster and non-protein ligands are synthesized by the hydrogenase maturation enzymes HydE, HydF and HydG; however, the mechanism, synthesis and means of insertion of H-cluster components remain unclear. Here we show the structure of HydA ΔEFG (HydA expressed in a genetic background devoid of the active site H-cluster biosynthetic genes hydE, hydF and hydG) revealing the presence of a 4Fe-4S cluster and an open pocket for the 2Fe subcluster. The structure indicates that H-cluster synthesis occurs in a stepwise manner, first with synthesis and insertion of the 4Fe-4S subcluster by generalized host-cell machinery and then with synthesis and insertion of the 2Fe subcluster by specialized hydE-, hydF-and hydG-encoded maturation machinery. Insertion of the 2Fe subcluster presumably occurs through a cationically charged channel that collapses following incorporation, as a result of conformational changes in two conserved loop regions. The structure, together with phylogenetic analysis, indicates that HydA emerged within bacteria most likely from a Nar1-like ancestor lacking the 2Fe subcluster, and that this was followed by acquisition in several unicellular eukaryotes. © 2010 Macmillan Publishers Limited. All rights reserved.",,"cation; hydrogen; hydrogenase; hydrogenase E; hydrogenase F; hydrogenase G; iron hydrogenase; unclassified drug; carbon dioxide; catalyst; cluster analysis; enzyme activity; genetic analysis; metabolism; nitrogen fixation; protein; article; biosynthesis; Chlamydomonas reinhardtii; conformational transition; controlled study; enzyme active site; enzyme analysis; enzyme synthesis; gene; gene cluster; host cell; nonhuman; phylogeny; priority journal; protein expression; structure analysis; Catalytic Domain; Chlamydomonas reinhardtii; Clostridium; Crystallography, X-Ray; Hydrogen; Hydrogenase; Iron; Models, Molecular; Nitrogenase; Phylogeny; Protein Conformation; Sulfur; Protista",Article,"Final","",Scopus,2-s2.0-77952428962
"Smit F., van Liere R., Beck S., Froehlich B.","18435203500;57195257466;18433681600;18433968300;","A shared-scene-graph image-warping architecture for VR: Low latency versus image quality",2010,"Computers and Graphics (Pergamon)","34","1",,"3","16",,5,"10.1016/j.cag.2009.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149145853&doi=10.1016%2fj.cag.2009.10.006&partnerID=40&md5=5f88c5a0467d50504dd7fd4c9fffe7d5","CWI, Amsterdam, Netherlands; Bauhaus-Universität Weimar, Germany","Smit, F., CWI, Amsterdam, Netherlands; van Liere, R., CWI, Amsterdam, Netherlands; Beck, S., Bauhaus-Universität Weimar, Germany; Froehlich, B., Bauhaus-Universität Weimar, Germany","Designing low end-to-end latency system architectures for virtual reality is still an open and challenging problem. We describe the design, implementation and evaluation of a client-server depth-image warping architecture that updates and displays the scene graph at the refresh rate of the display. Our approach works for scenes consisting of dynamic and interactive objects. The end-to-end latency is minimized as well as smooth object motion generated. However, this comes at the expense of image quality inherent to warping techniques. To improve image quality, we present a novel way of detecting and resolving occlusion errors due to warping. Furthermore, we investigate the use of asynchronous data transfers to increase the architecture's performance in a multi-GPU setting. Besides polygonal rendering, we also apply image-warping techniques to iso-surface rendering. Finally, we evaluate the architecture and its design trade-offs by comparing latency and image quality to a conventional rendering system. Our experience with the system confirms that the approach facilitates common interaction tasks such as navigation and object manipulation. © 2009 Elsevier Ltd. All rights reserved.","Image-based rendering; Latency; Virtual reality","Air navigation; Architecture; Computer architecture; Data transfer; Economic and social effects; Image enhancement; Image quality; Quality control; Virtual reality; Asynchronous data transfers; End to end latencies; Image based rendering; Interactive objects; Latency; Object manipulation; System architectures; Warping techniques; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-75149145853
"Quarles J., Lampotang S., Fischler I., Fishwick P., Lok B.","55868360300;35587861100;35588972600;56252883400;57203616548;","A mixed reality approach for merging abstract and concrete knowledge",2008,"Proceedings - IEEE Virtual Reality",,, 4480746,"27","34",,24,"10.1109/VR.2008.4480746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349097941&doi=10.1109%2fVR.2008.4480746&partnerID=40&md5=e541bedde2a5a643d1d30ad169d0dd8d","Dept. of CISE, University of Florida; Dept. of Anesthesiology, University of Florida; Dept. of Psychology, University of Florida","Quarles, J., Dept. of CISE, University of Florida; Lampotang, S., Dept. of Anesthesiology, University of Florida; Fischler, I., Dept. of Psychology, University of Florida; Fishwick, P., Dept. of CISE, University of Florida; Lok, B., Dept. of CISE, University of Florida","Mixed reality's (MR) ability to merge real and virtual spaces is applied to merging different knowledge types, such as abstract and concrete knowledge. To evaluate whether the merging of knowledge types can benefit learning, MR was applied to an interesting problem in anesthesia machine education. The Virtual Anesthesia Machine (VAM) is an interactive, abstract 2D transparent reality [14] simulation of the internal components and invisible gas flows of an anesthesia machine. It is widely used in anesthesia education. However when presented with an anesthesia machine, some students have difficulty transferring abstract VAM knowledge to the concrete real device. This paper presents the Augmented Anesthesia Machine (AAM). The AAM applies a magic-lens approach to combine the VAM simulation and a real anesthesia machine. The AAM allows students to interact with the real anesthesia machine while visualizing how these interactions affect the internal components and invisible gas flows in the real world context. To evaluate the AAM's learning benefits, a user study was conducted. Twenty participants were divided into either the VAM (abstract only) or AAM (concrete+abstract) conditions. The results of the study show that MR can help users bridge their abstract and concrete knowledge, thereby improving their knowledge transfer into real world domains. © 2008 IEEE.","Anesthesiology; Mixed reality; Modeling and simulation; Psychology; User studies","Gas flowing; Interesting problem; Knowledge transfer; Knowledge types; Mixed reality; Modeling and simulation; Psychology; Real-world; Real-world domains; User studies; Virtual spaces; Abstracting; Aerodynamics; Anesthesiology; Concrete construction; Education; Flow of gases; Gas dynamics; Information management; Knowledge management; Merging; Students; Virtual reality; Machine components",Conference Paper,"Final","",Scopus,2-s2.0-50349097941
"Dias P., Campos G., Santos V., Casaleiro R., Seco R., Santos B.S.","22333370800;8603962700;35616433200;24477040900;24477807400;7006476948;","3D reconstruction and Auralisation of the ""painted dolmen"" of antelas",2008,"Proceedings of SPIE - The International Society for Optical Engineering","6805",, 68050Y,"","",,2,"10.1117/12.766607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949110073&doi=10.1117%2f12.766607&partnerID=40&md5=8cf6b741616e6b2cc561071a4c67c4e0","DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal","Dias, P., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Campos, G., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Santos, V., Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal; Casaleiro, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Seco, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Santos, B.S., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal","This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with ""free-space nodes"", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of ""free-space nodes"" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.","3D acquisition; Augmented reality; Auralisation; Laser range finder; Virtual reality","Absorption; Acoustic wave absorption; Acoustics; Architectural acoustics; Audio acoustics; Computational efficiency; Display devices; Energy absorption; Functions; Headphones; Helmet mounted displays; Image reconstruction; Imaging systems; Impulse response; Lasers; Loudspeakers; Medical imaging; Mergers and acquisitions; Range finders; Range finding; Reflection; Restoration; Reverberation; Surfaces; Three dimensional computer graphics; Virtual reality; 3D acquisition; 3D geometries; 3D reconstructions; 3d visual models; Acoustic absorption coefficients; Archaeological sites; Audio sources; Audio visuals; Augmented reality; Auralisation; Dark rooms; Geometric acoustics; Geometric models; Head Related Transfer Functions; In-situ; Irregular surfaces; Iterative algorithms; Iterative Closest points; Laser range finder; Orientation sensors; Reconstruction softwares; Reverberation times; Software computes; Sound waves; Source localisation; Stereo headphones; Uniform grids; Visual scenes; Visualization toolkits; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-47949110073
"Boring S., Altendorfer M., Broll G., Hilliges O., Butz A.","18233691700;36166401100;55999882700;14041644100;55150450600;","Shoot & copy: Phonecam-based information transfer from public displays onto mobile phones",2007,"Mobility Conference 2007 - The 4th Int. Conf. Mobile Technology, Applications and Systems, Mobility 2007, Incorporating the 1st Int. Symp. Computer Human Interaction in Mobile Technology, IS-CHI 2007",,,,"24","31",,36,"10.1145/1378063.1378068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450242388&doi=10.1145%2f1378063.1378068&partnerID=40&md5=7acdaa32a6daa23971ddae2c28b40dca","University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany","Boring, S., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Altendorfer, M., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Broll, G., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Hilliges, O., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Butz, A., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany","Large public displays have become pervasive in our everyday lives, but up to now, they are mostly information screens without any interaction possibilities. Users tend to forget what they saw relatively fast after leaving such a display. In this paper, we present a new interaction technique for transferring information from a public display onto a personal mobile phone with its built-in camera. Instead of having to rely on their memory, users simply take a picture of the information of interest. Instead of just storing the image, our system then retrieves the actual data represented on the screen, such as a stock quote, news text, or piece of music. The Shoot & Copy technique does not require visual codes that interfere with shown content or reduce screen real estate. Our prototype allows users to capture an arbitrary region of a standard desktop screen, containing icons, which represent pieces of data. The captured image is then analyzed and a reference to the corresponding data is sent back to the mobile phone. Once the user has time to view the information in more detail, our system allows retrieving the actual data from this reference. We present our prototype and the methods it uses for image processing, as well as an evaluation of our interaction technique illustrating its potential use and applications. Copyright 2007 ACM.","image processing; large public displays; mobile camera phones","Information transfers; Interaction techniques; Mobile camera phones; Public display; Real estate; Standard desktop; Stock quotes; Cameras; Image processing; Imaging systems; Mobile phones; Mobile telecommunication systems; Technology; Telecommunication equipment; Telephone; Telephone sets; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-70450242388
"Wang X., Dunston P.S.","8945580300;6602079727;","Design, strategies, and issues towards an Augmented Reality-based construction training platform",2007,"Electronic Journal of Information Technology in Construction","12",,,"363","380",,60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547518881&partnerID=40&md5=be256dd99a26324e23d3ff80f33d8f84","Key Centre of Design Computing and Cognition, Faculty of Architecture, Design and Planning, University of Sydney, Australia; School of Civil Engineering, Purdue University, United States","Wang, X., Key Centre of Design Computing and Cognition, Faculty of Architecture, Design and Planning, University of Sydney, Australia; Dunston, P.S., School of Civil Engineering, Purdue University, United States","This paper provides information on Augmented Reality (AR) and their potential applications in heavy construction equipment operator training. Augmented Reality involves the use of special display and tracking technology that are capable of seamlessly merging digital (virtual) contents into real environments. Augmented Reality technology has been applied in many application domains outside construction (e.g., medical applications and surgeries, military training and warfare, manufacturing assembly and maintenance, design and modeling, precinct specific instant information, and various forms of entertainment) and the ever-increasing power of hardware rendering systems and tracking technology should motivate the creation of AR-based systems to benefit construction industry. This paper discusses the potentials of AR in construction equipment operation and operator training. A construction application for AR technology focused in this paper is an AR-hased real world Training System (ARTS) that trains the novice operators in a real worksite environment populated with virtual materials and instructions. This paper focuses on the conceptual design and development of mechanisms/strategies for the ARTS in the context of certain identified application scenarios. Discussion of limitations of Augmented Reality technology for construction applications include mature of technology, data resource, technology transfer, social attitude, etc., is also presented.","Augmented reality; System design; Training; Virtual reality; Virtual training","Construction industry; Hardware; Systems analysis; Virtual reality; AR-hased real world Training System (ARTS); Augmented Reality technology; Virtual training; Personnel training",Article,"Final","",Scopus,2-s2.0-34547518881
"Mohler B.J., Creem-Regehr S.H., Thompson W.B.","10239373300;6602436425;36985831800;","The influence of feedback on egocentric distance judgments in real and virtual environments",2006,"Proceedings - APGV 2006: Symposium on Applied Perception in Graphics and Visualization",,,,"93","100",,84,"10.1145/1140491.1140493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250700905&doi=10.1145%2f1140491.1140493&partnerID=40&md5=3aeaa29111ed8a2a56507c9e6ce22e2a","University of Utah, United States","Mohler, B.J., University of Utah, United States; Creem-Regehr, S.H., University of Utah, United States; Thompson, W.B., University of Utah, United States","A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications. Copyright © 2006 by the Association for Computing Machinery, Inc.","Adaptation; Feedback; Space perception; Virtual environments","Egocentric distance; Feedback on egocentric distance; Space perception; Adaptive algorithms; Cognitive systems; Feedback; Parameter estimation; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-34250700905
"Grossman T., Wigdor D., Balakrishnan R.","7003520062;6507569914;7006221860;","Exploring interaction with 3D volumetric displays",2005,"Proceedings of SPIE - The International Society for Optical Engineering","5664",, 35,"323","331",,2,"10.1117/12.587714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-21944443996&doi=10.1117%2f12.587714&partnerID=40&md5=673ea419c25b0f8f5ebb26f8176ed2c3","Dept. of Computer Science, Univ. of Toronto, 40 St. George St, Toronto, Ont. M5S 3G4, Canada","Grossman, T., Dept. of Computer Science, Univ. of Toronto, 40 St. George St, Toronto, Ont. M5S 3G4, Canada; Wigdor, D., Dept. of Computer Science, Univ. of Toronto, 40 St. George St, Toronto, Ont. M5S 3G4, Canada; Balakrishnan, R., Dept. of Computer Science, Univ. of Toronto, 40 St. George St, Toronto, Ont. M5S 3G4, Canada","Volumetric displays generate true volumetric 3D images by actually illuminating points in 3D space. As a result, viewing their contents is similar to viewing physical objects in the real world. These displays provide a 360 degree field of view, and do not require the user to wear hardware such as shutter glasses or head-trackers. These properties make them a promising alternative to traditional display systems for viewing imagery in 3D. Because these displays have only recently been made available commercially (e.g., www.actuality-systems.com), their current use tends to be limited to non-interactive output-only display devices. To take full advantage of the unique features of these displays, however, it would be desirable if the 3D data being displayed could be directly interacted with and manipulated. We investigate interaction techniques for volumetric display interfaces, through the development of an interactive 3D geometric model building application. While this application area itself presents many interesting challenges, our focus is on the interaction techniques that are likely generalizable to interactive applications for other domains. We explore a very direct style of interaction where the user interacts with the virtual data using direct finger manipulations on and around the enclosure surrounding the displayed 3D volumetric image. © 2005 SPIE and IS&T.","3D interaction; Multi-finger and two-handed gestural input; Volumetric display","Image quality; Liquid crystal displays; Mathematical models; Stereo vision; Three dimensional; Virtual reality; Microscopic patterns; Multi-finger and two-handed gestural input; Three dimensional (3D) interaction; Volumetric display; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-21944443996
"Mouloua M., Smither J., Kennedy R.C., Kennedy R.S., Compton D.E., Drexler J.M.","6701614142;6603999134;13406461800;7402555615;16314916200;7006657013;","Training effects in a sickness-inducing environment",2005,"Proceedings of the Human Factors and Ergonomics Society",,,,"2206","2210",,,"10.1177/154193120504902519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44349192006&doi=10.1177%2f154193120504902519&partnerID=40&md5=03b280ddde1154685e6bf357ba20b8c0","University of Central Florida, Orlando, FL, United States; RSK Assessments, Inc., Orlando, FL, United States","Mouloua, M., University of Central Florida, Orlando, FL, United States; Smither, J., University of Central Florida, Orlando, FL, United States; Kennedy, R.C., University of Central Florida, Orlando, FL, United States; Kennedy, R.S., RSK Assessments, Inc., Orlando, FL, United States; Compton, D.E., RSK Assessments, Inc., Orlando, FL, United States; Drexler, J.M., RSK Assessments, Inc., Orlando, FL, United States","The present study was conducted to empirically examine the effect of adaptation training and transfer of training on simulation sickness by inducing graded motion sickness through the systematic distortion of the relevant characteristics of three VR devices (VE, OKN, and Real-World Entertainment Ride). It was hypothesized that for people who are highly susceptible to motion-induced sickness, the perceptual adaptation training method would transfer from a controlled laboratory environment to a real-world situation. Ten participants from a previous laboratory study were tested on a real-world virtual reality entertainment device. The results showed that subjects who had experienced adaptation training on the optokinetic OKN device had lower dizziness scores on the real world game. However, subjects who had experienced adaptation training on the VE device (HMD) did not have lower dizziness scores. This pattern of results suggests that crossover training from controlled laboratory environments to real-world environments is likely to be one-directional and platform specific. Our present findings imply a technique for mitigating sickness through pre-adaptation training in sensory re-arrangement that is feasible and has major practical implications in business, industry, the military and the private sector, where motion sickness symptoms limit previous exposure.",,"Behavioral research; Computer simulation; Personnel training; Adaptation training; Graded motion sickness; Human engineering",Conference Paper,"Final","",Scopus,2-s2.0-44349192006
"Zhou Z., Cheok A.D., Yang X., Qiu Y.","23986510500;7003447496;22236065700;36724983800;","An experimental study on the role of 3D sound in augmented reality environment",2004,"Interacting with Computers","16","6",,"1043","1068",,23,"10.1016/j.intcom.2004.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-9944230931&doi=10.1016%2fj.intcom.2004.06.016&partnerID=40&md5=c593a65389cf3e596874fc1cfe187c82","Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Zhou, Z., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Cheok, A.D., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Yang, X., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Qiu, Y., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Investigation of augmented reality (AR) environments has become a popular research topic for engineers, computer and cognitive scientists. Although application oriented studies focused on audio AR environments have been published, little work has been done to vigorously study and evaluate the important research questions of the effectiveness of three-dimensional (3D) sound in the AR context, and to what extent the addition of 3D sound would contribute to the AR experience. Thus, we have developed two AR environments and performed vigorous experiments with human subjects to study the effects of 3D sound in the AR context. The study concerns two scenarios. In the first scenario, one participant must use vision only and vision with 3D sound to judge the relative depth of augmented virtual objects. In the second scenario, two participants must cooperate to perform a joint task in a game-based AR environment. Hence, the goals of this study are (1) to access the impact of 3D sound on depth perception in a single-camera AR environment, (2) to study the impact of 3D sound on task performance and the feeling of 'human presence and collaboration', (3) to better understand the role of 3D sound in human-computer and human-human interactions, (4) to investigate if gender can affect the impact of 3D sound in AR environments. The outcomes of this research can have a useful impact on the development of audio AR systems, which provide more immersive, realistic and entertaining experiences by introducing 3D sound. Our results suggest that 3D sound in AR environment significantly improves the accuracy of depth judgment and improves task performance. Our results also suggest that 3D sound contributes significantly to the feeling of human presence and collaboration and helps the subjects to 'identify spatial objects'. © 2004 Elsevier B.V. All rights reserved.","3D sound; Augmented reality; User study","Acoustic waves; Cameras; Cognitive systems; Feedback; Human computer interaction; Multiprocessing systems; Three dimensional; Three dimensional computer graphics; Transfer functions; Head related transfer functions (HRTF); Human-human interactions; Three-dimensional (3D) sound; User study; Virtual reality",Article,"Final","",Scopus,2-s2.0-9944230931
"Simons R., Melzer J.E.","7202166557;7004881040;","HMD-based Training for the U.S. Army's AVCATT-A Collective Aviation Training Simulator",2003,"Proceedings of SPIE - The International Society for Optical Engineering","5079",,,"1","6",,5,"10.1117/12.487192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242467447&doi=10.1117%2f12.487192&partnerID=40&md5=ed9309ebf802ad27c70e33a106f88c2c","Visual Systems Engineer, AVCATT-A, U.S. Army PEO STRI, Orlando, FL, United States; Head-Mounted and Miniature Displays, Kaiser Electro-Opt. Rockwell C.C., Carlsbad, CA, United States","Simons, R., Visual Systems Engineer, AVCATT-A, U.S. Army PEO STRI, Orlando, FL, United States; Melzer, J.E., Head-Mounted and Miniature Displays, Kaiser Electro-Opt. Rockwell C.C., Carlsbad, CA, United States","The Aviation Combined Arms Tactical Trainer - Aviation (AVCATT-A) reconfigurable manned simulator is the Army's newest helicopter training simulator. AVCATT-A is a dynamic reconfigurable system developed for use for combined arms collective training and mission rehearsal through a suite of networked simulators in a simulated battlefield environment. The main purpose of AVCATT-A is to effectively train aviation aircrews in a collective training environment. The AVCATT-A visual system is required to support realistic collective/combined arms training with the required fidelity to fly nap-of-the- earth (NOE) or conduct multi-ship operations. To meet this high level of expectation requires that the visual system be capable of performing all necessary collective tasks and supporting individual tasks with no negative training transfer or physical impacts on crewmembers. In addition to these stringent training requirements for the visual system, the AVCATT-A simulator must be reconfigurable for five different cockpits and be readily deployable. The solution was found in a combination of state-of-the-art technology where the standard projection dome is replaced by a combination of rear-screen projection displays and a Head-Mounted Display (HMD) which provides all the Out-The-Window (OTW) visual imagery.","AVCATT-A; Helicopter simulation and training; HMD; SIM EYE","Cockpits (aircraft); Flight simulators; Image sensors; Military aviation; Military helicopters; Personnel training; Projection screens; Training aircraft; Multifunction displays (MFD); Pilots; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-0242467447
"Correia N., Romão T., Santos C., Trabuco A., Santos R., Romero L., Danado J., Dias E., Câmara A., Nobre E.","7006236302;6507404367;56983263200;7801337962;7201375307;56870548700;7801472052;57219399648;7005091449;6602500818;","A flexible augmented reality architecture applied to environmental management",2003,"Proceedings of SPIE - The International Society for Optical Engineering","5006",,,"519","528",,,"10.1117/12.477347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042861462&doi=10.1117%2f12.477347&partnerID=40&md5=ccf8b1a84bcdb565f71772da938151bc","Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal; Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal","Correia, N., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Romão, T., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Santos, C., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Trabuco, A., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Santos, R., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Romero, L., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Danado, J., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Dias, E., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Câmara, A., Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Nobre, E., Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal","Environmental management often requires in loco observation of the area under analysis. Augmented Reality (AR) technologies allow real time superimposition of synthetic objects on real images, providing augmented knowledge about the surrounding world. Users of an AR system can visualize the real surrounding world together with additional data generated in real time in a contextual way. The work reported in this paper was done in the scope of ANTS (Augmented Environments) project. ANTS is an AR project that explores the development of an augmented reality technological infrastructure for environmental management. This paper presents the architecture and the most relevant modules of ANTS. The system's architecture follows the client-server model and is based on several independent, but functionally interdependent modules. It has a flexible design, which allows the transfer of some modules to and from the client side, according to the available processing capacities of the client device and the application's requirements. It combines several techniques to identify the user's position and orientation allowing the system to adapt to the particular characteristics of each environment. The determination of the data associated to a certain location involves the use of both a 3D Model of the location and the multimedia geo-referenced database.","Augmented reality; Contextual information; Geo-referenced information; Mobile information services; Positioning; Software Components; System Architecture","Environmental impact; Multimedia systems; Servers; Environmental management; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0042861462
"Arnold P., Farrell M.J., Pettifer S., West A.J.","7202944024;7202679231;6602097064;7202674494;","Performance of a skilled motor task in virtual and real environments",2002,"Ergonomics","45","5",,"348","361",,10,"10.1080/00140130110120510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037091920&doi=10.1080%2f00140130110120510&partnerID=40&md5=134b431ac4de031b9630b406988c9e9b","Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom","Arnold, P., Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Farrell, M.J., Department of Psychology, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; Pettifer, S., Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom; West, A.J., Advanced Interfaces Group, Department of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, United Kingdom","Three experiments compared the performances of adult participants (three groups of 10) on a perceptuo-motor task in both real world (RW) and virtual environments (VEs). The task involved passing a hoop over a bent wire course, and three versions of the task were used: a 3-D wire course with no background, a flattened version of the 3-D course (2 1/2-D course) with no background, and the 2 1/2-D course with added background to provide spatial context. In all three experiments the participants had to prevent the hoop from touching the wire as they moved it. In the first experiment, the VE condition produced about 18 times more errors than the RW task. The VE 2 1/2-D task was found to be as difficult as the 3-D, and the 2 1/2-D with the added background produced more errors than the other two experiments. Taken together, the experiments demonstrate the difficulty of performing fine motor tasks in VEs, a phenomenon that has not been given due attention in many previous studies of motor control in VEs.","Head mounted display (HMD); Motor skill; Real world analogue; Virtual environment","Motors; Virtual reality; Motor task; Ergonomics; adult; article; comparative study; controlled study; human; human experiment; motor activity; motor control; motor performance; movement perception; perception; physical performance; sensorimotor function; skill; spatial summation; task performance; virtual reality; work environment; Adolescent; Adult; Analysis of Variance; Depth Perception; Humans; Psychomotor Performance; Regression Analysis; Transfer (Psychology); User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0037091920
"Mania K., Chalmers A.","6602471750;7102938771;","The effects of levels of immersion on memory and presence in virtual environments: A reality centered approach",2001,"Cyberpsychology and Behavior","4","2",,"247","264",,119,"10.1089/109493101300117938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035025714&doi=10.1089%2f109493101300117938&partnerID=40&md5=26852521731b4b257f035d8219dcfc28","University of Bristol, Bristol, United Kingdom","Mania, K., University of Bristol, Bristol, United Kingdom; Chalmers, A., University of Bristol, Bristol, United Kingdom","Simulation fidelity is characterized as the extent to which a Virtual Environment (VE) and relevant interactions with it are indistinguishable from a user's interaction with a real environment. The growing number of VE training applications which target a high level of simulation fidelity mainly for transfer of training in the real world have made it crucial to examine the manner in which these particular implementations and designs are evaluated. The methodology presented in this study focuses on real versus simulated virtual worlds comparing participants' level of presence task performance and cognition state employed to complete a memory task. A 15-minute seminar was presented in four different conditions including real 3D desktop 3D Head Mounted Display (HMD) and Audio-only (between-subjects design). Four independent groups of 18 participants took part in the experiment which investigated the effects of levels of immersion on participants' memory recall and memory awareness state (relevant to episodic and semantic memory types) as well as on their perception of the experimental space and sense of presence for every condition. The level of reported presence was not positively associated with accurate memory recall in all conditions although the scores for both presence and seminar memory recall in the ""real"" condition were statistically higher. Memory awareness states' analysis gave a invaluable insight into ""how"" participants remembered both communicated information and space as opposed to ""what, "" most interestingly across specific conditions where results for presence and accurate memory recall were not proven to be significant.",,"article; auditory stimulation; awareness; cognition; female; human; human experiment; image display; immersion; male; memory; normal human; recall; simulation; task performance; virtual reality; Adult; Female; Humans; Internet; Male; Memory; Motion Sickness; Questionnaires; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0035025714
"McGee J.S., Van Der Zaag C., Buckwalter J.G., Thiebaux M., Van Rooyen A., Neumann U., Sisemore D., Rizzo A.A.","7102023121;6507558646;7103071789;6701770846;6701803717;7103378063;55297389800;57189943380;","Issues for the assessment of visuospatial skills in older adults using virtual environment technology",2000,"Cyberpsychology and Behavior","3","3",,"469","482",,26,"10.1089/10949310050078931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033922544&doi=10.1089%2f10949310050078931&partnerID=40&md5=1fcd853f3831221f78ca63d182897805","Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States; Information Sciences Institute, University of Southern California, Los Angeles, CA, United States; Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Integrated Media Systems Center, University of Southern California, 3715 McClintock Avenue, Los Angeles, CA 90089-0191, United States","McGee, J.S., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States, Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Van Der Zaag, C., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Buckwalter, J.G., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Thiebaux, M., Information Sciences Institute, University of Southern California, Los Angeles, CA, United States; Van Rooyen, A., Grad. Sch. Psychol. Fuller Theol. S., Pasadena, CA, United States; Neumann, U., Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States; Sisemore, D., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States; Rizzo, A.A., Andrus Gerontology Center, University of Southern California, Los Angeles, CA, United States, Integrated Media Systems Center, University of Southern California, Los Angeles, CA, United States, Integrated Media Systems Center, University of Southern California, 3715 McClintock Avenue, Los Angeles, CA 90089-0191, United States","Virtual Environment (VE) technology offers clinical assessment and rehabilitation options that are currently not available using traditional neuropsychological methods. Advancements in this type of immersive information technology could produce tools that enhance the scientific study of human cognitive/functional processes and improve our capacity to more accurately assess and treat impairments found in persons with central nervous system (CNS) dysfunction. Through the creation of dynamic three-dimensional (3D) stimulus environments, in which all behavioral responding can be recorded, VE technology offers the possibility to more sensitively address a range of age-related CNS disorders including Alzheimer's Disease, Vascular Dementia, Parkinson's Disease, and stroke. Advances in this area could impact quality of life issues for an increasingly aging world population. The VE Laboratory at the University of Southern California has developed a suite of ImmersaDesk-format, 3D projection-based VEs. These scenarios target assessment of visuospatial skills including visual field-specific reaction time, depth perception, 3D field dependency (virtual rod and frame test), static and dynamic manual target tracking in 3D space, and spatial rotation. The current project tested healthy older adults (ages of 65 and 92). Participants were administered a standard neuropsychological battery and a suite of VE-delivered visuospatial tasks. Issues addressed in this project include: the occurrence of VE-related side effects in healthy older adults; the relationship between performance on VE measures and standard neuropsychological tests; the assessment of gender specific performance differences; the relationship between immersive tendencies, presence ratings, and VE performance in older adults; learning and generalization; and VE visuospatial performance differences between younger and older participants. This article will address the motivation, rationale, and relevant issues for use of VEs with older adults. A description of our VE system/methodology in the context of a recent study targeting assessment and possible rehabilitation of visuospatial skills with this population will then be detailed.",,"Alzheimer disease; central nervous system; conference paper; depth perception; information; neuropsychological test; psychologic assessment; rating scale; reaction time; virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0033922544
"Pausch R., Proffitt D., Williams G.","57204319989;7005887383;55469196100;","Quantifying immersion in virtual reality",1997,"Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997",,,,"13","18",,38,"10.1145/258734.258744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009782003&doi=10.1145%2f258734.258744&partnerID=40&md5=8abf9e12747b0103845b53e53bc673a6","Carnegie Mellon University and University of Virginia, United States; University of Virginia, United States; Fakespace, Mountain View, Inc. 241 Polaris AveCA, United States","Pausch, R., Carnegie Mellon University and University of Virginia, United States; Proffitt, D., University of Virginia, United States; Williams, G., Fakespace, Mountain View, Inc. 241 Polaris AveCA, United States","Virtual Reality (VR) has generated much excitement but little formal proof that it is useful. Because VR interfaces are difficult and expensive to build, the computer graphics community needs to be able to predict which applications will benefit from VR. In this paper, we show that users with a VR interface complete a search task faster than users with a stationary monitor and a hand-based input device. We placed users in the center of the virtual room shown in Figure 1 and told them to look for camouflaged targets. VR users did not do significantly better than desktop users. However, when asked to search the room and conclude if a target existed, VR users were substantially better at determining when they had searched the entire room. Desktop users took 41% more time, re-examining areas they had already searched. We also found a positive transfer of training from VR to stationary displays and a negative transfer of training from stationary displays to VR. Copyright © 1997 by the Association for Computing Machinery, Inc.",,"Computer graphics; Virtual reality; Formal proofs; Input devices; Search tasks; Transfer of trainings; Virtual rooms; Interactive computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85009782003
"Pausch Randy, Proffitt Dennis, Williams George","57204319989;7005887383;55469196100;","Quantifying immersion in virtual reality",1997,"Proceedings of the ACM SIGGRAPH Conference on Computer Graphics",,,,"13","18",,155,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030684946&partnerID=40&md5=5a107a9475f718f238fe210450502fab","Univ of Virginia, Charlottesville, United States","Pausch, Randy, Univ of Virginia, Charlottesville, United States; Proffitt, Dennis, Univ of Virginia, Charlottesville, United States; Williams, George, Univ of Virginia, Charlottesville, United States","Virtual Reality (VR) has generated much excitement but little formal proof that it is useful. Because VR interfaces are difficult and expensive to build, the computer graphics community needs to be able to predict which applications will benefit from VR. In this paper, we show that users with a VR interface complete a search task faster than users with a stationary monitor and a hand-based input device. We placed users in the center of the virtual room shown in Figure 1 and told them to look for camouflaged targets. VR users did not do significantly better than desktop users. However, when asked to search the room and conclude if a target existed, VR users were substantially better at determining when they had searched the entire room. Desktop users took 41% more time, re-examining areas they had already searched. We also found a positive transfer of training from VR to stationary displays and a negative transfer of training from stationary displays to VR.",,"Display devices; Graphical user interfaces; Human computer interaction; Subjective testing; Three dimensional computer graphics; Head mounted display (HMD); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0030684946
"Pioch Nicholas J., Roberts Bruce, Zeltzer David","6506235587;7402979005;6701417238;","Virtual environment for learning to pilot remotely operated vehicles",1997,"Proceedings of the Annual International Conference on Virtual Systems and Multimedia, VSMM",,,,"218","226",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030716629&partnerID=40&md5=7f0c52dce04e1b4cb040f24c162b8650","BBN Corp, Cambridge, United States","Pioch, Nicholas J., BBN Corp, Cambridge, United States; Roberts, Bruce, BBN Corp, Cambridge, United States; Zeltzer, David, BBN Corp, Cambridge, United States","Remotely Operated Vehicles (ROVs) are used extensively for underwater search and salvage, inspection, surveying, scientific exploration, and mine countermeasures. ROV pilots must learn to rely on limited data from video and sonar displays and a few other positional indicators to maintain a sense of their vehicle, its tether, and its surroundings. Pilot training typically occurs on-the-job, where equipment is placed at risk, controlled learning situations are hard to create, and time for instruction is minimal. The TRANSoM project is developing a training environment that combines two emerging technologies to overcome these limitations. A Virtual Environment (VE) simulates the ROV and its surroundings as well as instructional enhancements such as external views of the ROV, directional cues, and alternate modes of interaction with the vehicle; e.g. a head-tracked head-mounted display (HMD). An Intelligent Tutoring System (ITS) monitors student pilot behavior and offers verbal and graphical feedback, mission review, and performance assessment. Near-transfer experiments comparing the utility of different artificial viewpoints have been completed, with full transfer experiments involving a real ROV to follow.",,"Artificial intelligence; Computer aided instruction; Computer simulation; Learning systems; Personnel training; Submersibles; Intelligent tutoring systems (ITS); Remotely operated vehicles (ROV); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0030716629
