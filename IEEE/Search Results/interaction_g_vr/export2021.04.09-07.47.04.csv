"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Development of An Autonomous Guide Robot Based on Active Interactions with Users","B. Zhang; T. Nakamura; M. Kaneko; H. Lim","Kanagawa University, Yokohama,Kanagawa,Japan,2218686; The University of Electro-Communications,Tokyo,Japan,1820021; The University of Electro-Communications,Tokyo,Japan,1820021; Kanagawa University, Yokohama,Kanagawa,Japan,2218686","2020 IEEE/SICE International Symposium on System Integration (SII)","9 Mar 2020","2020","","","601","604","An autonomous guide robot based on active interactions with users has been developed. Here, active interactions mean that the users can ask the robot to wait for them, explain contents around, etc. The robot can adjust its speed and moving path automatically to adapt to the motions of users so that it can provide considerate services to the users. The robot is controlled by a general control model based on integrated potential field, by which the robot can adapt to different motions of users without changing the moving rules case by case. The effectiveness of our proposed method is proven by both simulation and experiments where the users are guided by a robot under indoor environments.","2474-2325","978-1-7281-6667-4","10.1109/SII46433.2020.9026301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9026301","","System integration","human-robot interaction;mobile robots;path planning;velocity control","active interactions;robot moving path;autonomous guide robot;robot speed","","1","","10","","9 Mar 2020","","","IEEE","IEEE Conferences"
"Integration of force-position control and haptic interface facilities for a virtual excavator simulator","H. I. Torres-Rodriguez; V. Parra-Vega; F. J. Ruiz-Sanchez","Electr. Eng. Dept., CINVESTAV-IPN, Mexico; Electr. Eng. Dept., CINVESTAV-IPN, Mexico; Electr. Eng. Dept., CINVESTAV-IPN, Mexico","ICAR '05. Proceedings., 12th International Conference on Advanced Robotics, 2005.","19 Sep 2005","2005","","","761","768","In this paper we present the advances in the design and develop of a system used to simplify the excavator digging tasks based in a dynamical model of an excavator an anthropomorphic auto-balanced haptic interface and a force-position control law. The objective of this system is to reduce the training period and simplify the execution of digging tasks. This system makes possible the kinesthetic coupling between the operator and the machine providing information to the operator about the interaction forces between the excavator and the soil. The dynamical models of the excavator and the haptic devise were calculated employing the Euler-Lagrange formalism. With this formalism we obtain a new general dynamical model of an excavator including the dynamical effects of the hydraulic actuators. We present simulation results of the closed loop system considering the interaction between the haptic devise and the excavator","","0-7803-9178-0","10.1109/ICAR.2005.1507494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507494","","Force control;Haptic interfaces;Costs;Soil;Construction industry;Industrial training;Electrical equipment industry;Stress;Automation;Control systems","closed loop systems;digital simulation;excavators;force control;haptic interfaces;hydraulic actuators;position control;virtual reality","force control;position control;virtual excavator simulator;excavator digging;anthropomorphic autobalanced haptic interface;kinesthetic coupling;interaction force;soil;dynamical model;haptic devise;Euler-Lagrange formalism;hydraulic actuators;closed loop system","","2","","10","","19 Sep 2005","","","IEEE","IEEE Conferences"
"Modeling the system-user dialog using interaction traces","M. El-Ramly; P. Iglinski; E. Stroulia; P. Sorenson; B. Matichuk","Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada; NA; NA; NA; NA","Proceedings Eighth Working Conference on Reverse Engineering","7 Aug 2002","2001","","","208","217","It is generally the case that some user interface (UI) reverse engineering is needed for every non-trivial reengineering project. Typically, this is done through code analysis, which can be very difficult and/or expensive. When code analysis is not a must, as for wrapping purposes, system-user interaction can be an alternative input for the reverse engineering process. In the CelLEST project, we have developed a prototype, called LeNDI (Legacy Navigation Domain Identifier), to test this idea. LeNDI records traces of the legacy screen snapshots and user actions, while the user interacts with the legacy system. Then, it extracts a set of features for every snapshot and employs artificial intelligence methods to build a model of the legacy UI, called the state-transition graph. LeNDI uses two clustering methods to group similar snapshots together as one system screen modeled by one node on the graph. LeNDI uses the user actions recorded in traces to model the behavior of the legacy screens as the graph arcs. Evaluation results of this process are encouraging. The state-transition graph is used to classify each individual snapshot forwarded by the legacy system to the user while he interacts with it and is a main input to the forward engineering phase of the project.","1095-1350","0-7695-1303-4","10.1109/WCRE.2001.957825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=957825","","Reverse engineering;Graphical user interfaces;Artificial intelligence;Wrapping;Prototypes;Testing;Forward contracts;User interfaces;Application software;Clustering methods","user interface management systems;reverse engineering;systems re-engineering;artificial intelligence;graphs;interactive systems;system monitoring","system-user dialog modeling;interaction traces;user interface reverse engineering;reengineering;code analysis;wrapping;system-user interaction;CelLEST project;LeNDI;Legacy Navigation Domain Identifier;legacy screen snapshot traces;user action traces;feature extraction;artificial intelligence;state-transition graph;clustering methods;system screen;graph node;legacy screen behavior modeling;forward engineering","","17","2","17","","7 Aug 2002","","","IEEE","IEEE Conferences"
"1-inch UniTouch System using Kinect","Seok-Min Hong; Yung-Fu Tan; Hui-Shyong Yeo; Byung-Gook Lee","International Conference on Signal Processing, Image Processing and Pattern Recognition (ICSIPR'13), Visual Contents, General Graduate, Dongseo University, IAI Lab, 6F, New-Millennium Building, 47 Jurye-ro, Sasang-gu, Busan 617-716, Korea; International Conference on Signal Processing, Image Processing and Pattern Recognition (ICSIPR'13), Visual Contents, General Graduate, Dongseo University, IAI Lab, 6F, New-Millennium Building, 47 Jurye-ro, Sasang-gu, Busan 617-716, Korea; International Conference on Signal Processing, Image Processing and Pattern Recognition (ICSIPR'13), Ubiquitous IT, General Graduate, Dongseo University, IAI Lab, 6F, New-Millennium Building, 47 Jurye-ro, Sasang-gu, Busan 617-716, Korea; International Conference on Signal Processing, Image Processing and Pattern Recognition (ICSIPR'13), Visual Contents, General Graduate, Dongseo University, IAI Lab, 6F, New-Millennium Building, 47 Jurye-ro, Sasang-gu, Busan 617-716, Korea","2013 International Conference on Signal Processing , Image Processing & Pattern Recognition","15 Apr 2013","2013","","","351","355","Multi-touch system is based on touch technology that utilizes touch sensing as an input. It existed ubiquitously in our daily life such as on smartphones or different types of portable devices. However, current touch systems suffer from many limitations. In this paper, we propose a new large surface multi-touch system, named as “1-inch UniTouch System”. Our solution uses only a single Kinect camera, which is easy to setup and has low entry cost. We also utilize several essential image-processing techniques to improve the system robustness and accuracy. Our prototype system shows that the touch sensing is very robust and works flawlessly even on non-flat surface. Our solution can effectively overcome several limitations existed in previous systems. The outcome of our research shows great potential and we hope to further explore new possibilities with this touch system.","","978-1-4673-4862-1","10.1109/ICSIPR.2013.6497994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497994","","Cameras;Noise;Image resolution;Surface treatment;Sensors","human computer interaction;image processing;touch sensitive screens","UniTouch system;multitouch system;touch technology;touch sensing;smart phones;Kinect camera;image processing technique","","","","15","","15 Apr 2013","","","IEEE","IEEE Conferences"
"Key techniques of haptic related computation in virtual liver surgery","Y. Shi; Y. Xiong; X. Hua; K. Tan; X. Pan","School of Computer Science, National University of Defense Technology, Changsha, China; School of Computer Science, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; Educational Technology Center, The PLA General Hospital, Beijing, China; Educational Technology Center, The PLA General Hospital, Beijing, China","2015 8th International Conference on Biomedical Engineering and Informatics (BMEI)","11 Feb 2016","2015","","","355","359","With or without a realistic haptic effect is the key difference between virtual surgery and the traditional training methods. On the real-time operation stage, users contact with the virtual scene and implement the operation by manipulating the haptic interaction device. Obviously, haptic related computation is the most important part for a virtual surgery system. This paper studied the haptic related computation of the virtual liver surgery, including the mechanism of haptic computation in liver virtual surgery, liver cutting area delineation and liver parenchyma splitting simulation. We developed two novel methods to simulate the delineation and splitting on the virtual liver surface. Results show our techniques can not only provide a realistic effect in our virtual surgery system, but also can be used in other simulation and geometry fields.","","978-1-5090-0022-7","10.1109/BMEI.2015.7401529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401529","virtual liver surgery;haptic computation;simulation","Liver;Surgery;Haptic interfaces;Computational modeling;Training;Real-time systems;Solid modeling","haptic interfaces;liver;medical computing;surgery","virtual liver parenchyma splitting simulation;haptic computation mechanism;haptic interaction device;virtual liver surgery","","6","","8","","11 Feb 2016","","","IEEE","IEEE Conferences"
"The versatility of the Wii controller in CS education","V. B. Petrović; D. Ivetić; Z. Konjović","Faculty of Technical Sciences, Novi Sad, Serbia; Faculty of Technical Sciences, Novi Sad, Serbia; Faculty of Technical Sciences, Novi Sad, Serbia","2011 IEEE 9th International Symposium on Intelligent Systems and Informatics","3 Oct 2011","2011","","","59","64","This paper discusses the applicability of the Wii controller to CS (computer science) education. The controller can be used as an inexpensive sensor platform or as a human interface device. Its applicability is studied trough two examples: virtual reality and advanced human computer interaction on one side and artificial intelligence on the other. Both these examples showcase the flexibility of the Wii controller in particular and consumer electronics game accessories in general. It is also observed that despite the seemingly disparate nature of the example subjects chosen, project from both merged seamlessly. This illustrates that the Wii controller is very good at exploiting the deep synergy that exists between varying fields of CS study.","1949-0488","978-1-4577-1974-5","10.1109/SISY.2011.6034383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034383","","Tracking;Virtual reality;Neurons;Artificial intelligence;Artificial neural networks;Education;Hardware","artificial intelligence;computer games;computer science education;consumer electronics;human computer interaction;sensors;telecontrol;user interfaces;virtual reality","Wii controller;CS education;computer science education;human interface device;virtual reality;human computer interaction;artificial intelligence;consumer electronics game accessory","","3","","15","","3 Oct 2011","","","IEEE","IEEE Conferences"
"Distance perception during cooperative virtual locomotion","W. E. Marsh; J. Chardonnet; F. Mérienne","Arts et Métiers ParisTech - CNRS Le2i, Institut Image, France; Arts et Métiers ParisTech - CNRS Le2i, Institut Image, France; Arts et Métiers ParisTech - CNRS Le2i, Institut Image, France","2015 IEEE Symposium on 3D User Interfaces (3DUI)","25 Jun 2015","2015","","","173","174","Virtual distances are often misperceived, though most past research ignores co-located cooperative systems. Because active locomotion plays a role in spatial perception, cooperative viewpoint control may impact perceived distances. Additionally, the center of projection is generally optimized for a single tracked user, meaning that a single action will result in different visual feedback for each user. We describe a study investigating the effect of a co-located cooperative locomotion interface on virtual distance perception. Results indicate that a slight center-of-projection offset did affect distance estimates for the untracked user, but that the cooperation actions themselves did not play a role. This study brings new insights to designing interfaces which facilitate accurate spatial perception in cooperative applications.","","978-1-4673-6886-5","10.1109/3DUI.2015.7131756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131756","","Legged locomotion;Visualization;Head;Target tracking;Atmospheric measurements;Particle measurements","groupware;human computer interaction;user interfaces;virtual reality","cooperative virtual locomotion;co-located cooperative locomotion interface;virtual distance perception;center-of-projection offset;virtual reality;VR","","","","6","","25 Jun 2015","","","IEEE","IEEE Conferences"
"Surgical strike: interface design across task domains","L. J. Hettinger; R. S. Tannen; E. E. Geiselman; B. J. Brickman; B. W. Moroney; M. W. Haas","Logicon Tech. Services Inc., Dayton, OH, USA; NA; NA; NA; NA; NA","Proceedings Fourth Annual Symposium on Human Interaction with Complex Systems","6 Aug 2002","1998","","","131","136","To what extent can similarities in the general demands placed on human performance in apparently disparate task domains be used to enhance human-machine interface design in each? How can the ""lessons learned"" in one area of human endeavour be systematically and successfully applied to another? What is the nature of the information that permits such transfer to occur? These are important questions currently being encountered in the realm of virtual environment (VE) systems design. Applications of VE technology originally developed for use in advanced tactical aviation settings are currently being used as models for the development of neurosurgical interface concepts. While at first glance the connection between air combat and surgery may seem a distant one at best, there are many important similarities between them. For instance, both task domains share at least ten common, important human performance attributes, each of which may provide key insights into how to import knowledge gained from one domain into the other. Each of these areas have important implications for designing successful technical systems to support human performance. This paper explores the notion that general principles of interface design, derived from a concern with optimizing the above aspects of task performance, can be derived from the study of apparently disparate task domains. To illustrate this general idea, we elaborate how cross-fertilization between neurosurgery and air combat can be used to successfully advance the design of interfaces to support both.","","0-8186-8341-4","10.1109/HUICS.1998.659968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=659968","","Surgery;Biomedical imaging;Medical diagnostic imaging;Humans;Educational technology;Medical treatment;Laboratories;Virtual environment;Neurosurgery;Medical simulation","user interfaces;technology transfer;task analysis;surgery;aerospace computing;medical computing;human factors;virtual reality;military computing","human-machine interface design;task domains;human performance;virtual environment systems design;advanced tactical aviation;neurosurgical interface concepts;air combat;surgery;technical systems;task performance optimization","","1","","5","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Tracking target motion under combined visual and kinesthetic disturbances","L. Masia; V. Squeri; M. Casadio; P. Morasso; V. Sanguineti; G. Sandini","IIT, Genoa, Italy; IIT, Genoa, Italy; IIT, Genoa, Italy; DIST, University of Genoa, Italy; DIST, University of Genoa, Italy; IIT, Genoa, Italy","2009 IEEE International Conference on Rehabilitation Robotics","21 Aug 2009","2009","","","688","693","This study addresses a major problem in the design of HCI (human-computer interface) systems: how to avoid or reduce the long learning/adaptation process and the corresponding attentional load of the underlying hand-eye coordination task that frequently affects HCI systems. In particular, we considered a tracking task to a visual target whose frame of reference is rotated with respect to a body-fixed frame in a time-varying manner. We investigated it by means of a wrist robot coupled with a virtual reality system: kinesthetic and visual disturbances were applied during a tracking task, in a unimodal and bimodal manner, and we observed the performance. Tracking involved two degrees of freedom and the kinesthetic disturbance was applied by the robot through the third degree of freedom. Visual disturbance was provided by rotating the visual feedback. The results suggest that the combination of a suitable proprioceptive feedback with the kinematic redundancy of the HCI system might be a rather general principle for improving the efficiency of HCI systems.","1945-7901","978-1-4244-3788-7","10.1109/ICORR.2009.5209541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209541","wrist robot;visual and proprioceptive disturbance;visuo-manual tracking;virtual reality","Target tracking;Human computer interaction;Robot kinematics;Motor drives;Mice;Wrist;Virtual reality;Feedback;Brushes;Minimally invasive surgery","robots;user interfaces;virtual reality","tracking target motion;visual disturbances;kinesthetic disturbances;human-computer interface;hand-eye coordination task;wrist robot;virtual reality system;proprioceptive feedback","","","","14","","21 Aug 2009","","","IEEE","IEEE Conferences"
"Designing telepresence robots for K-12 education","E. Cha; S. Chen; M. J. Mataric","Interaction Lab, Department of Computer Science, University of Southern California, Los Angeles, CA 90089, USA; Interaction Lab, Department of Computer Science, University of Southern California, Los Angeles, CA 90089, USA; Interaction Lab, Department of Computer Science, University of Southern California, Los Angeles, CA 90089, USA","2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","14 Dec 2017","2017","","","683","688","Telepresence robots have the potential to improve access to K-12 education for students who are unable to attend school for a variety of reasons. Since previous telepresence research has largely focused on the needs of adult users in workplace settings, it is unknown what challenges must be addressed for these robots to be effective tools in classrooms. In this paper, we seek to better understand how a telepresence robot should function in the classroom when operated by a remote student. Toward this goal, we conducted field sessions in which four designers operated a telepresence robot in a real K-12 classroom. Using the results, we identify key research challenges and present design insights meant to inform the HRI community in particular and robot designers in general.","1944-9437","978-1-5386-3518-6","10.1109/ROMAN.2017.8172377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8172377","","Robots;Education;Employment;Navigation;Tools;Collaboration","computer aided instruction;educational institutions;human-robot interaction;telerobotics;virtual reality","classroom;HRI community;K-12 classroom;remote students;adult users;K-12 education;telepresence research;robot designers;telepresence robots","","","","40","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Parallel processing for object oriented robotic simulation of tracheal-oesophageal puncture","C. Chui; Chin-Boon Chng; D. P. Lau","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Otolaryngology, Singapore General Hospital, Singapore","2011 IEEE/SICE International Symposium on System Integration (SII)","9 Feb 2012","2011","","","144","149","An approach in advancing the field of medical robotics is via a more efficient preparation of robotic surgical intervention using accurate simulation. Our intra-operative virtual system for robotic tracheal-oesophageal puncture is modeled using an object oriented method. The interaction between rigid tools and flexible organs is simulated with finite element method codes integrated within the framework of multi-body dynamics codes. We investigate LogP and MLogP models for multi-core CPU computing, and propose Topology Independent Model (TIN) model for a heterogeneous computing system, which mimics parallel processing with multi-core CPU and GPU, to provide the required computational power for tool-tissue simulation.","","978-1-4577-1524-2","10.1109/SII.2011.6147435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147435","","Computational modeling;Robots;Program processors;Object oriented modeling;Finite element methods;Computers;Tin","control engineering computing;digital simulation;finite element analysis;graphics processing units;medical robotics;multiprocessing systems;object-oriented methods;parallel processing;surgery;virtual reality","parallel processing;object oriented robotic simulation;medical robotics;robotic surgical intervention;intraoperative virtual system;robotic tracheal-oesophageal puncture;rigid tools;flexible organs;finite element method codes;LogP;MLogP;multicore CPU computing;topology independent model;heterogeneous computing system;GPU;tool tissue simulation","","3","","15","","9 Feb 2012","","","IEEE","IEEE Conferences"
"The GRASP Taxonomy of Human Grasp Types","T. Feix; J. Romero; H. -B. Schmiedmayer; A. M. Dollar; D. Kragic","Department of Mechanical Engineering and Materials Science, Yale University, New Haven, CT, USA; Max Planck Institute for Intelligent Systems, Tübingen, Germany; Institute for Mechanics and Mechatronics, Vienna University of Technology, Vienna, Austria; Department of Mechanical Engineering and Materials Science, Yale University, New Haven, CT, USA; Centre for Autonomous Systems, School of Computer Science and Communication, Computational Vision and Active Perception Laboratory, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Human-Machine Systems","20 May 2017","2016","46","1","66","77","In this paper, we analyze and compare existing human grasp taxonomies and synthesize them into a single new taxonomy (dubbed “The GRASP Taxonomy” after the GRASP project funded by the European Commission). We consider only static and stable grasps performed by one hand. The goal is to extract the largest set of different grasps that were referenced in the literature and arrange them in a systematic way. The taxonomy provides a common terminology to define human hand configurations and is important in many domains such as human-computer interaction and tangible user interfaces where an understanding of the human is basis for a proper interface. Overall, 33 different grasp types are found and arranged into the GRASP taxonomy. Within the taxonomy, grasps are arranged according to 1) opposition type, 2) the virtual finger assignments, 3) type in terms of power, precision, or intermediate grasp, and 4) the position of the thumb. The resulting taxonomy incorporates all grasps found in the reviewed taxonomies that complied with the grasp definition. We also show that due to the nature of the classification, the 33 grasp types might be reduced to a set of 17 more general grasps if only the hand configuration is considered without the object shape/size.","2168-2305","","10.1109/THMS.2015.2470657","GRASP; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243327","Hand/wrist posture;human–robot interaction;taxonomies;robotics;human factors;Hand/wrist posture;human–robot interaction;taxonomies;robotics;human factors","Taxonomy;Thumb;Force;Robots;Grasping;Shape;Man machine systems","control engineering computing;haptic interfaces;human computer interaction;manipulators;virtual reality","GRASP taxonomy;human grasp type;human grasp taxonomy;European Commission;static grasp;stable grasp;human hand configuration;human-computer interaction;tangible user interface;virtual finger assignment;grasp definition;classification","","234","1","66","","4 Sep 2015","","","IEEE","IEEE Journals"
"A scalable approach to human-robot interaction","A. D. Tews; M. J. Mataric; G. S. Sukhatme","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA","2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)","10 Nov 2003","2003","2","","1665","1670 vol.2","Much of the current research in human-robot interaction is concerned with single systems and single or few users. These systems and their interfaces are generally tightly-coupled and well-defined. For large-scale human-robot applications, the systems may be unknown prior to designing the interface for potential human interaction. This presents a difficult goal for allowing multiple users to interact with many possibly unknown systems. In this paper, we present an interaction infrastructure aligned with providing this interface. It operates in two phases that accommodate both many-to-many interaction and generalized, one-to-one interaction between users and robotic systems. Our previous research has demonstrated the infrastructure to scale to a large number of users and several systems in simulation. The experiments in this paper substantiate these results in a smaller-scale real robotic environment.","1050-4729","0-7803-7736-2","10.1109/ROBOT.2003.1241833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241833","","Human robot interaction;Scalability;Large-scale systems;Intelligent robots;Embedded system;Application software;Joining processes;Intelligent systems;Computer science;Robot control","multi-robot systems;user interfaces;man-machine systems;interactive systems;multi-access systems","human-robot interaction;user interface;interaction infrastructure;many-to-many user interaction;one-to-one user interaction;robotic systems;robotic environment","","9","","17","","10 Nov 2003","","","IEEE","IEEE Conferences"
"Just-in-time weather in the synthetic natural environment","P. West; J. Melendez","Dept. of Syst. Eng., US Mil. Acad., West Point, NY, USA; NA","Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0","6 Aug 2002","2000","1","","472","477 vol.1","Military mission rehearsals and virtual prototypes increasingly rely upon combat simulations to predict feasibility and success. The current US force projection doctrine requires these operations to be conducted anywhere, anytime, and under any conditions. Standard scenarios provide accurate location data, but generally lack realistic weather conditions that can easily reverse the fortunes of war. This paper describes an approach to bring realistic weather to combat simulations by merging historical archives, engineering-level mobility and detection models, and simulation databases into a robust synthetic natural environment. The conduit is the Simulation-Weather interface Module (SWIM), a Java-based tool set connecting simulations and their managers via the World Wide Web. This work allows analysts and planners to accurately determine and plan for weather conditions in any part of the world at any time. It also enables reliable what-if scenarios for mission rehearsal by forecasting operational weather, which can then be customized by the planner. The Web-based interface allows scenarios to be revised via the Internet from anywhere in the world.","1062-922X","0-7803-6583-6","10.1109/ICSMC.2000.885037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885037","","Weather forecasting;Virtual prototyping;Predictive models;Merging;Data engineering;Databases;Robustness;Java;Joining processes;Web sites","military computing;weather forecasting;planning;information resources;Internet;discrete event simulation;geophysics computing;virtual reality","synthetic natural environment;military mission rehearsals;virtual prototypes;combat simulations;mission feasibility prediction;mission success prediction;US force projection doctrine;location data;realistic weather conditions;historical archives;engineering-level mobility models;engineering-level detection models;simulation databases;Simulation-Weather interface Module;SWIM;Java-based tool set;World Wide Web;weather conditions;what-if scenarios;operational weather forecasting;Web-based interface;mission planning;scenario revision;Internet","","","","16","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Simplified industrial robot programming: Effects of errors on multimodal interaction in WoZ experiment","Z. Materna; M. Kapinus; M. Španěl; V. Beran; P. Smrž","Brno University of Technology, Faculty of Information Technology, Centre of Excellence IT4Innovations, Bozetechova 1/2, 612 66, Czech Republic; Brno University of Technology, Faculty of Information Technology, Centre of Excellence IT4Innovations, Bozetechova 1/2, 612 66, Czech Republic; Brno University of Technology, Faculty of Information Technology, Centre of Excellence IT4Innovations, Bozetechova 1/2, 612 66, Czech Republic; Brno University of Technology, Faculty of Information Technology, Centre of Excellence IT4Innovations, Bozetechova 1/2, 612 66, Czech Republic; Brno University of Technology, Faculty of Information Technology, Centre of Excellence IT4Innovations, Bozetechova 1/2, 612 66, Czech Republic","2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)","17 Nov 2016","2016","","","200","205","This paper presents results of an exploratory study comparing various modalities employed in an industrial-like robot-human shared workplace. Experiments involved 39 participants who used a touch table, a touch display, hand gestures, a 6D pointing device, and a robot arm to show the robot how to assemble a simple product. To rule out a potential dependence of results on the number of misrecognized actions (resulting, e.g., from unreliable gesture recognition), a controlled amount of interaction errors was introduced. A Wizard-of-Oz setting with three user groups differing in the amount of simulated recognition errors helped us to show that hand gestures and 6D pointing are the fastest modalities that are also generally preferred by users for setting parameters of certain robot operations.","1944-9437","978-1-5090-3929-6","10.1109/ROMAN.2016.7745111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745111","","Service robots;Welding;Manipulators;Robot sensing systems;Augmented reality","human-robot interaction;industrial robots;manipulators;robot programming","industrial robot programming;multimodal interaction errors;WoZ experiment;industrial-like robot-human shared workplace;touch table;touch display;hand gestures;6D pointing device;robot arm;Wizard-of-Oz setting;simulated recognition errors","","6","","22","","17 Nov 2016","","","IEEE","IEEE Conferences"
"Virtual Environment Positioning Utilizing Play-Script Spatiotemporal Reasoning","C. Talbot; G. M. Youngblood","Game Intelligence Group, University of North Carolina at Charlotte, Charlotte, NC, USA; Game Intelligence Group, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Games","15 Sep 2020","2020","12","3","225","235","Automatically staging characters in order to facilitate the performance and blocking of a scene in a virtual environment is a difficult task today. There is only a limited set of techniques used in practice. Some general methods include Behavior Markup Language (BML) and motion capture replay (the most popular); however they require either detailed technical knowledge or are not adaptable to different environmental configurations. In this work, we block and perform scenes with synthetic actors utilizing only a text-based, standard play-script as the primary input to the positioning of these characters in an environment. Using natural language processing techniques, we extract the annotated movements from the script, then add additional movements and adjustments from our rules engine built with theater, stage performance, and human interaction spatiotemporal relationships. In addition, we incorporate force-directed graph algorithms to adjust positions of the artificially intelligent (AI)-driven characters based on human-controlled character movements for interaction. These techniques have been quantitatively and qualitatively evaluated, revealing both similar blocking, and indistinguishably good performances from a human's perspective, when compared to an actual human performance.","2475-1510","","10.1109/TG.2019.2927706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759991","Force-directed graphs;natural language processing;play-scripts;rules;spatiotemporal reasoning;virtual environments","Games;Markup languages;Broadcasting;Natural language processing;Engines;Robots;Spatiotemporal phenomena","artificial intelligence;directed graphs;natural language processing;text analysis;virtual reality","motion capture replay;environmental configurations;synthetic actors;natural language processing techniques;human interaction spatiotemporal relationships;human-controlled character movements;virtual environment positioning;standard play-script spatiotemporal reasoning;force-directed graph algorithms;behavior markup language;text-based script;artificially intelligent-driven characters","","","","43","IEEE","11 Jul 2019","","","IEEE","IEEE Journals"
"Hitting the Road: Exploring Human-Robot Trust for Self-Driving Vehicles","Y. S. Razin; K. M. Feigh","Georgia Institute of Technology,School of Aerospace Engineering,Atlanta,Georgia; Georgia Institute of Technology,School of Aerospace Engineering,Atlanta,Georgia","2020 IEEE International Conference on Human-Machine Systems (ICHMS)","30 Sep 2020","2020","","","1","6","With self-driving cars making their way on to our roads, we ask not what it would take for them to gain acceptance among consumers, but what impact they may have on other drivers. How they will be perceived and whether they will be trusted will likely have a major effect on traffic flow and vehicular safety. This work first undertakes an exploratory factor analysis to validate a trust scale for human-robot interaction and shows how previously validated metrics and general trust theory support a more complete model of trust that has increased applicability in the driving domain. We experimentally test this expanded model in the context of human-automation interaction during simulated driving, revealing how using these dimensions uncovers significant biases within human-robot trust that may have particularly deleterious effects when it comes to sharing our future roads with automated vehicles.","","978-1-7281-5871-6","10.1109/ICHMS49158.2020.9209525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209525","human-robot interaction trust;autonomous vehicles;trust scale validation","Automobiles;Standards;Roads;Autonomous automobiles;Automation;Safety","automobiles;fires;human computer interaction;human-robot interaction;intelligent transportation systems;mobile robots;road safety;road vehicles","exploratory factor analysis;trust scale;human-robot interaction;general trust theory support;driving domain;expanded model;human-automation interaction;simulated driving;automated vehicles;exploring human-robot trust;self-driving vehicles;cars;road;traffic flow;vehicular safety","","","","38","","30 Sep 2020","","","IEEE","IEEE Conferences"
"Interactive information visualization for exploratory intelligence data analysis","J. Risch; R. May; J. Thomas; S. Dowson","Pacific Northwest Nat. Lab., Richland, WA, USA; Pacific Northwest Nat. Lab., Richland, WA, USA; Pacific Northwest Nat. Lab., Richland, WA, USA; Pacific Northwest Nat. Lab., Richland, WA, USA","Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium","6 Aug 2002","1996","","","230","238","This paper describes an experimental exploratory information visualization system under development at PNNL for integrating multimedia data and visualizing the contents of large multimedia databases. Our system prototype, named Starlight, represents multimedia corpora graphically as collections of icons in 3-space. Each icon in the information display represents an individual element of a multimedia database. The proximity of any two icons in the display is an indication of their general similarity, providing fast access to thematically related information. We extend the effectiveness of this approach by coupling it with a linkage display system to enable the simultaneous visualization of database structure and contents at multiple levels of abstraction. The utility of the system is further extended by enabling a variety of graphical and text-based interactions with the information display. The system enables the interactive generation of multiple simultaneous ""views"" into multimedia databases, providing intelligence analysts with the potential to process large multimedia data volumes quickly and effectively.","","0-8186-7296-X","10.1109/VRAIS.1996.490532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=490532","","Data visualization;Data analysis;Information analysis;Multimedia databases;Displays;Deductive databases;Multimedia systems;Laboratories;Prototypes;Couplings","data visualisation;virtual reality;multimedia computing;data analysis;very large databases;graphical user interfaces;interactive systems","information visualization;exploratory intelligence data analysis;multimedia data;large multimedia databases;prototype;Starlight;icons;information display;linkage display system;text-based interaction;graphical interaction;graphical interface;virtual reality","","4","11","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Predicting academic performance with learning analytics in virtual learning environments: A comparative study of three interaction classifications","Á. F. Agudo-Peregrina; Á. Hernández-García; S. Iglesias-Pradas","Departamento de Ingeniería de Organización, Administración de Empresas y Estadística, Universidad Politécnica de Madrid, Spain; Departamento de Ingeniería de Organización, Administración de Empresas y Estadística, Universidad Politécnica de Madrid, Spain; Departamento de Ingeniería de Organización, Administración de Empresas y Estadística, Universidad Politécnica de Madrid, Spain","2012 International Symposium on Computers in Education (SIIE)","7 Jan 2013","2012","","","1","6","Learning analytics is the analysis of static and dynamic data extracted from virtual learning environments, in order to understand and optimize the learning process. Generally, this dynamic data is generated by the interactions which take place in the virtual learning environment. At the present time, many implementations for grouping of data have been proposed, but there is no consensus yet on which interactions and groups must be measured and analyzed. There is also no agreement on what is the influence of these interactions, if any, on learning outcomes, academic performance or student success. This study presents three different extant interaction typologies in e-learning and analyzes the relation of their components with students' academic performance. The three different classifications are based on the agents involved in the learning process, the frequency of use and the participation mode, respectively. The main findings from the research are: a) that agent-based classifications offer a better explanation of student academic performance; b) that at least one component in each typology predicts academic performance; and c) that student-teacher and student-student, evaluating students, and active interactions, respectively, have a significant impact on academic performance, while the other interaction types are not significantly related to academic performance.","","978-84-939814-7-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6403184","interactions;learning analytics;academic performance;typologies","Data mining;Electronic learning;Context;Databases;Real-time systems;Data analysis;Humans","computer aided instruction;student experiments;virtual reality","learning analytics;virtual learning environments;interaction classifications;student success;e-learning;agent-based classifications;student academic performance;student-teacher interactions;student-student interactions","","","","27","","7 Jan 2013","","","IEEE","IEEE Conferences"
"A case for human-driven software development","E. Balland; C. Consel; B. N'Kaoua; H. Sauzéon","Inria/University of Bordeaux, France; Inria/University of Bordeaux, France; Inria/University of Bordeaux, France; Inria/University of Bordeaux, France","2013 35th International Conference on Software Engineering (ICSE)","26 Sep 2013","2013","","","1229","1232","Human-Computer Interaction (HCI) plays a critical role in software systems, especially when targeting vulnerable individuals (e.g., assistive technologies). However, there exists a gap between well-tooled software development methodologies and HCI techniques, which are generally isolated from the development toolchain and require specific expertise. In this paper, we propose a human-driven software development methodology making User Interface (UI) a full-fledged dimension of software design. To make this methodology useful in practice, a UI design language and a user modeling language are integrated into a tool suite that guides the stakeholders during the development process, while ensuring the conformance between the UI design and its implementation.","1558-1225","978-1-4673-3076-3","10.1109/ICSE.2013.6606685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606685","","Abstracts;Electronic mail;Software;Human computer interaction;Programming;Computational modeling;Context","human computer interaction;simulation languages;software engineering;user interfaces","human-computer interaction;HCI techniques;human-driven software development methodology;user interface;software design;UI design language;user modeling language;tool suite","","3","","10","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Development of interactive fashion design support system: Using IDS (interactive display on any surface)-A case study","K. S. Devi; A. S. Murugan; A. Sivaranjani; P. Srinivasan","Dept of CSE, University College of Engineering, Panruti; Dept of CSE, University College of Engineering, Panruti; Dept of CSE, University College of Engineering, Panruti; Dept of Physics, University College of Engineering, Panruti","2017 4th International Conference on Advanced Computing and Communication Systems (ICACCS)","24 Aug 2017","2017","","","1","6","The innovative creation, IDS (Interactive Display on any Surface) is converting any surface as a touch screen is playing its major role in various consumers and business environments. An IDS is a new interactive methodology which is surrounded by different modes of natural interaction. With its projection based technology it makes the business applications to improve towards the next level and making reality more contented from small showroom to big malls and restaurants, healthcare units, gaming, entertainment etc. Moreover the new-fangled proposal may deploy the IDS in promising, talented and fascinating field called Fashion Design. Today fashion design has reached new heights by computer aided methods of design. But generally the Fashion Designer predominately does the practical designing and computer aided design separately. This paper aims to provide the Interactive UI design support system on the fashion technology designer workbench itself for IDS. Hence the main objective of this paper is to recommend the idea using IDS to enhance the User experience and productivity. In addition it provides NUI based UI for IDS which can be deployed in fashion technology.","","978-1-5090-4559-4","10.1109/ICACCS.2017.8014663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014663","IDS;Natural User Interface;Human Computer Interaction","Human computer interaction;Design automation;Business;Cameras;Infrared sensors;Communication systems","CAD/CAM;interactive devices;interactive systems;touch sensitive screens;user interfaces","interactive fashion design support system;IDS;interactive display;innovative creation;Interactive Display on any Surface;touch screen;business environments;interactive methodology;natural interaction;projection based technology;business applications;omputer aided methods;computer aided design;interactive UI design support system;fashion technology designer workbench;user experience;productivity","","1","","28","","24 Aug 2017","","","IEEE","IEEE Conferences"
"General purpose simulation tool for analyzing switching systems","E. Valdimarsson","Comput. & Commun. Res. Center, Washington Univ., St. Louis, MO, USA","Proceedings of GLOBECOM '93. IEEE Global Telecommunications Conference","6 Aug 2002","1993","","","1358","1362 vol.3","Modern high speed communication networks use highly parallel switching systems to achieve high throughput. Various switching systems have been proposed for this purpose. To make evaluation and design of such systems easier and to gain a better understanding the authors have designed and implemented a general purpose tool for evaluating switching systems. The goal of the design is to create an efficient tool which is general enough to allow performance evaluation of a variety of different switching architectures. The novel aspects of the tool presented include easy network construction and interaction, visualization and animation of network state and statistics, and fast simulation without compilation.<>","","0-7803-0917-0","10.1109/GLOCOM.1993.318300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318300","","Analytical models;Switching systems;Object oriented modeling;Communication networks;Asynchronous transfer mode;Switches;Graphics;Throughput;Animation;Statistics","digital simulation;object-oriented methods;network topology;telecommunications computing;switching networks","general purpose simulation tool;modern high speed communication networks;highly parallel switching systems;throughput;performance evaluation;network construction;interaction;visualization;animation;network state;network statistics;C++ language","","","","13","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Toward the role of interaction in Visual Analytics","A. Kerren; F. Schreiber","Linnaeus University, School of Computer Science, Physics and Mathematics, Vejdes Plats 7, SE-351 95 Växjö, Sweden; Martin Luther University Halle-Wittenberg, Von-Seckendorff-Platz 1, D-06120 Halle, GERMANY, and IPK Gatersleben, Corrensstrasse 3, D-06466 Gatersleben, Germany","Proceedings of the 2012 Winter Simulation Conference (WSC)","21 Feb 2013","2012","","","1","13","This paper firstly provides a general introduction in the most important aspects and ideas of Visual Analytics. This multidisciplinary field focuses on the analytical reasoning of typically large and complex (often heterogeneous) data sets and combines techniques from interactive visualizations with computational analysis methods. Hereby, intuitive and efficient user interactions are a fundamental component which has to be efficiently supported by any Visual Analytics system. This integration of interaction techniques into both visual representations and automatic analysis methods supports the human-information discourse and can be realized in various ways which is discussed in the second part of the paper. We give examples of possible applications of Visual Analytics from the domain of biological simulations and highlight the importance and role of the human in the analysis loop.","1558-4305","978-1-4673-4782-2","10.1109/WSC.2012.6465208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465208","","Data visualization;Visual analytics;Data models;Analytical models;Humans;Data collection","biology computing;data structures;data visualisation;inference mechanisms;user interfaces","human-in-the-analysis loop;biological simulations;human-information discourse;automatic analysis methods;visual representations;user interactions;computational analysis methods;interactive visualizations;heterogeneous data sets;complex data sets;analytical reasoning;visual analytics system;interaction role","","19","","41","","21 Feb 2013","","","IEEE","IEEE Conferences"
"A Framework for Scalable Virtual Worlds Using Spatially Organized P2P Networks","R. Cavagna; M. Abdallah; E. Buyukkaya; C. Bouville","IRISA, Rennes, France; IRISA, Rennes, France; IRISA, Rennes, France; IRISA, Rennes, France","2008 14th IEEE International Conference on Parallel and Distributed Systems","22 Dec 2008","2008","","","853","858","The general craze for virtual environments, the potential of augmented reality applications and the announced revolution of the Internet world (Web 2.0, Web 3D.0) are key points for the emergence of an 'ambient' Web which will make it possible for users to communicate, collaborate, entertain, work and exchange content. In this context, content storage, delivery, and reproduction are among the essential points for the deployment of a highly scalable platform of wide reality. In this paper, we propose a self-scalable peer-to-peer architecture for the navigation in network-based virtual worlds. To reach this goal, we propose a fully distributed and adaptive streaming method that quickly adapts the reproduced content according to user interaction. Our content delivery strategy has been implemented and tested on a dedicated simulator with a large 3D city model. The presented results show the efficiency of our strategy in very critical conditions.","1521-9097","978-0-7695-3434-3","10.1109/ICPADS.2008.125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724407","virtual environments;peer-to-peer;self-scalabity;self-repartition;self-adaptation;self-distribution","Virtual environment;Cities and towns;Costs;Hardware;Network servers;Augmented reality;IP networks;Collaborative work;International collaboration;Context","augmented reality;peer-to-peer computing","scalable virtual worlds;spatially organized P2P networks;virtual environments;augmented reality;Internet world;peer-to-peer architecture;network-based virtual worlds;adaptive streaming method","","3","","17","","22 Dec 2008","","","IEEE","IEEE Conferences"
"HumTouch: Finger gesture recognition on hydrogel-painted paper using hum-driven signals","K. Tachi; S. Okamoto; Y. Akiyama; Y. Yamada","Nagoya University,Department of Mechanical Systems Engineering,Nagoya,Japan; Nagoya University,Department of Mechanical Systems Engineering,Nagoya,Japan; Nagoya University,Department of Mechanical Systems Engineering,Nagoya,Japan; Nagoya University,Department of Mechanical Systems Engineering,Nagoya,Japan","2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)","27 Feb 2020","2019","","","155","157","We propose a passive touch sensor that can be implemented on semi-conductive materials, such as paper. This sensor detects electric currents leaked from a human body when touched with bare fingers. Such electric currents are mostly produced by AC power lines in the environment. We tested the potential of the sensor for gesture recognition by using a paper sheet painted with a hydrogel ink. A gesture classification algorithm combining the k-means method and Mahalanobis' generalized distance could successfully classify 90% of the gestures drawn on the paper with a fingertip. Gestures including multi-directional lines and circles were nearly perfectly classified whereas digits, such as 2 and 8 exhibited difficulties with classification. The hydrogel ink can turn other non-conductive materials comprising woods and plastic into touch sensors, and such hum-based touch sensors are expected to find practical applications in gesture recognition.","2378-8143","978-1-7281-3575-5","10.1109/GCCE46687.2019.9015264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9015264","Hum noise;touch sensor;gesture","Tactile sensors;Electrodes;Gesture recognition;Ink;Voltage measurement;Trajectory","gesture recognition;human computer interaction;tactile sensors;touch (physiological);touch sensitive screens;wood","paper sheet;hydrogel ink;gesture classification algorithm;nonconductive materials;hum-based touch sensors;HumTouch;finger gesture recognition;hydrogel-painted paper;hum-driven signals;passive touch sensor;semiconductive materials;electric currents;human body;bare fingers;AC power lines;Mahalanobis generalized distance","","2","","5","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Human Pose Tracking from RGB Inputs","R. R. Barioni; L. Figueiredo; K. Cunha; V. Teichrieb",Universidade Federal de Pernambuco; Universidade Federal de Pernambuco; Universidade Federal de Pernambuco; Universidade Federal de Pernambuco,"2018 20th Symposium on Virtual and Augmented Reality (SVR)","19 Aug 2019","2018","","","176","182","In the context of Virtual and Augmented Reality, in order to allow systems to provide natural interaction through gestures and general understanding of user body behavior it is fundamental to obtain the configuration of human poses. Once achieved, the goal of obtaining such poses from RGB images through cameras brings the possibility of a wide range of applications in the areas of security (i.e.: local activity monitoring), healthcare (i.e.: postural analysis) and entertainment (i.e.: games and animations motion capture). However, the acquisition of human poses solely through RGB images is still considered a challenge, once that pure visual data doesnt explicitly give us information about the human body joints (keypoints in pixels) localization in the image. In this work we propose the a machine learning method, more specifically deep learning based on convolutional neural networks, capable of tackling this problem.","","978-1-7281-0604-5","10.1109/SVR.2018.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802469","Machine Learning;Gesture Recognition","Bones;Heating systems;Elbow;Sensors;Annotations;Hip;Cameras","augmented reality;cameras;convolutional neural nets;gesture recognition;image colour analysis;image motion analysis;learning (artificial intelligence);object tracking;pose estimation","human pose tracking;natural interaction;user body behavior;human poses;RGB images;local activity monitoring;animations motion capture;human body joints;augmented reality;RGB inputs;virtual reality;cameras;machine learning method;deep learning;convolutional neural networks","","","","14","","19 Aug 2019","","","IEEE","IEEE Conferences"
"Building specific contexts for on-line learning of dynamical tasks through non-verbal interaction","A. de Rengervé; S. Hanoune; P. Andry; M. Quoy; P. Gaussier","ETIS, CNRS ENSEA University of Cergy-Pontoise, F-95000 Cergy-Pontoise, France; ETIS, CNRS ENSEA University of Cergy-Pontoise, F-95000 Cergy-Pontoise, France; ETIS, CNRS ENSEA University of Cergy-Pontoise, F-95000 Cergy-Pontoise, France; ETIS, CNRS ENSEA University of Cergy-Pontoise, F-95000 Cergy-Pontoise, France; ETIS, CNRS ENSEA University of Cergy-Pontoise, F-95000 Cergy-Pontoise, France","2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)","4 Nov 2013","2013","","","1","6","Trajectories can be encoded as attraction basin resulting from recruited associations between visually based localization and orientations to follow (low level behaviors). Navigation to different places according to some other multimodal information needs a particular learning. We propose a minimal model explaining such a behavior adaptation from non-verbal interaction with a teacher. Specific contexts can be recruited to prevent the behaviors to activate in cases the interaction showed they were inadequate. Still, the model is compatible with the recruitment of new low level behaviors. The tests done in simulation show the capabilities of the architecture, the limitations regarding the generalization and the learning speed. We also discuss the possible evolutions towards more bio-inspired models.","2161-9476","978-1-4799-1036-6","10.1109/DevLrn.2013.6652564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6652564","","Context;Robot sensing systems;Trajectory;Neurons;Navigation;Adaptation models","generalisation (artificial intelligence);human-robot interaction;learning (artificial intelligence);learning systems;navigation;neurocontrollers;trajectory control","specific context building;online learning;dynamical task;nonverbal interaction;trajectory encoding;visual based localization;visual based orientation;navigation;multimodal information;behavior adaptation;generalization;learning speed;bioinspired model;neural network based architecture","","","","17","","4 Nov 2013","","","IEEE","IEEE Conferences"
"Web-based virtual walkthrough of panoramas","G. Zhang; L. Zhao","Zhejiang University of Media and Communications, Hangzhou, P.R. China; The Department of Computer Science and Technology of Zhejiang University","2009 First International Conference on Networked Digital Technologies","29 Sep 2009","2009","","","233","237","Recent advances in Internet and WWW techniques motivate desire for systems with interactive 3D graphics on the Web, which draws attention and effectively assist human understanding with spatial intuitiveness and interactivity. Web-based application with virtual environment walkthrough capability is expected to have a profound impact to the travel based applications, such as virtual e-commerce, online travelling and shopping, just to name a few. However, it is well known that it is extremely difficult to render complex 3D scenes at interactive frame rates through the Web known for their lack of proper ability needed to process large volume of 3D virtual environment data. In order to provide virtual environment navigation on the Web, we propose a hybrid technique which combines both remote geometry rendering and image compression transportation technique. In our approach, the server renders a partial panoramic view, which is based on the user's viewpoint and last movements or sends image from real life. The server then compress images, and streams the images to the client device, which will progressively build the panoramic representation of the scene. Furthermore, in order to enhance streaming performance and quality of the interaction, we propose to use a prediction of the user's movements as well as a rate control mechanism within the virtual scene. In this paper we discuss our scheme for Web-based remote rendering and streaming of progressive panoramas. Our results indicate clearly that the proposed solution is able to achieve stable frame rates and throughput in general network environment.","2155-8736","978-1-4244-4614-8","10.1109/NDT.2009.5272064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5272064","","Virtual environment;Rendering (computer graphics);Layout;Image coding;Streaming media;Internet;World Wide Web;Graphics;Humans;Navigation","computational geometry;data compression;electronic commerce;home shopping;image coding;Internet;rendering (computer graphics);travel industry;virtual reality","Web-based virtual panoramas walkthrough;WWW;Internet;interactive 3D graphics;virtual environment walkthrough capability;virtual e-commerce;online travelling;online shopping;virtual environment navigation;remote geometry rendering;image compression transportation technique;Web-based remote rendering;progressive panoramas","","","","23","","29 Sep 2009","","","IEEE","IEEE Conferences"
"Usability of a domain-specific language for a gesture-driven IDE","M. Baćíková; M. Marićák; M. Vanćík","Technical university of Koćice, Letná 9, 042-00, Slovakia; Technical university of Koćice, Letná 9, 042-00, Slovakia; Technical university of Koćice, Letná 9, 042-00, Slovakia","2015 Federated Conference on Computer Science and Information Systems (FedCSIS)","9 Nov 2015","2015","","","909","914","User interfaces (UIs) are advancing in every direction. The usage of touch screen devices and adaptation their UIs lives its boom. However integrated development environments (IDEs) that are used to develop the same UIs are oversleeping the time. They are directed to developing usable software, but forgot to be usable by themselves. Our goal is to design a new way of user interaction for common IDEs with the help of touch. The target group are hybrid devices formed by a physical keyboard and either an integrated, or separate, touch screen display. In this paper we describe a set of general purpose and domain-specific gestures which represents a language for working with a touch-driven IDE and provide a method their design. We performed two studies with developers from industry and university and developed a prototype of a gesture-driven IDE to evaluate the usability of the presented approach.","","978-8-3608-1065-1","10.15439/2015F274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321540","","Usability;Keyboards;Shape;Semantics;Pattern recognition;Androids;Humanoid robots","human computer interaction;programming environments;software reusability;touch sensitive screens;user interfaces","domain-specific language;gesture-driven IDE;user interface;touch screen device;integrated development environment;software usability;user interaction;hybrid device;touch screen display;domain-specific gesture","","1","","24","","9 Nov 2015","","","IEEE","IEEE Conferences"
"Skill generalization relevant to robotic neuro-rehabilitation","D. Bansal; R. Kenyon; J. L. Patton","University of Illinois-Chicago, USA; University of Illinois-Chicago, USA; University of Illinois-Chicago, USA","2010 Annual International Conference of the IEEE Engineering in Medicine and Biology","11 Nov 2010","2010","","","2250","2254","Upper limb extremity rehabilitation practices are increasingly involving robotic interaction for repetitive practice, and there is increasing skepticism whether such systems can provide the relevant practice that can be generalized (or transferred) to functional activities in the real world. Most importantly, will patients be able to generalize in three critical ways: (1) to unpracticed directions, (2) to unpracticed movement distances, and (3) to unpracticed weight-eliminated conditions? Rather than presuming that patients could generalize in three conditions, this study tested whether there was any evidence of such generalization ability in healthy individuals. We found that there was some evidence in all conditions except for the ability of healthy subjects to generalize to large movements after practicing small. Such results suggest that larger robotic systems are advantageous for training the functional motions that can include large actions.","1558-4615","978-1-4244-4123-5","10.1109/IEMBS.2010.5627308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627308","","Training;Gravity;Visualization;Virtual reality;Performance evaluation;Robot sensing systems","medical robotics;neurophysiology;patient rehabilitation;robot kinematics;virtual reality","skill generalization;robotic neuro-rehabilitation;upper limb extremity rehabilitation;robotic interaction;repetitive practice;unpracticed directions;unpracticed movement distances;unpracticed weight-eliminated condition","Activities of Daily Living;Adult;Algorithms;Arm Injuries;Equipment Design;Female;Humans;Male;Motor Activity;Movement;Movement Disorders;Nervous System;Robotics;Upper Extremity","","","19","","11 Nov 2010","","","IEEE","IEEE Conferences"
"Attention-Based Autism Spectrum Disorder Screening With Privileged Modality","S. Chen; Q. Zhao",University of Minnesota; University of Minnesota,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1181","1190","This paper presents a novel framework for automatic and quantitative screening of autism spectrum disorder (ASD). It is motivated to address two issues in the current clinical settings: 1) short of clinical resources with the prevalence of ASD (1.7% in the United States), and 2) subjectivity of ASD screening. This work differentiates itself with three unique features: first, it proposes an ASD screening with privileged modality framework that integrates information from two behavioral modalities during training and improves the performance on each single modality at testing. The proposed framework does not require overlap in subjects between the modalities. Second, it develops the first computational model to classify people with ASD using a photo-taking task where subjects freely explore their environment in a more ecological setting. Photo-taking reveals attentional preference of subjects, differentiating people with ASD from healthy people, and is also easy to implement in real-world clinical settings without requiring advanced diagnostic instruments. Third, this study for the first time takes advantage of the temporal information in eye movements while viewing images, encoding more detailed behavioral differences between ASD people and healthy controls. Experiments show that our ASD screening models can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, our framework using diverse modalities demonstrates performance improvement on both the photo-taking and image-viewing tasks, providing a general paradigm that takes in multiple sources of behavioral data for a more accurate ASD screening. The framework is also applicable to various scenarios where one-to-one pairwise relationship is difficult to obtain across different modalities.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010066","","Task analysis;Variable speed drives;Visualization;Computational modeling;Training;Testing;Biological system modeling","cognition;eye;feature extraction;handicapped aids;human computer interaction;learning (artificial intelligence);medical computing;medical disorders","diverse modalities;ASD screening models;ASD people;real-world clinical settings;photo-taking reveals attentional preference;photo-taking task;single modality;behavioral modalities;privileged modality framework;clinical resources;clinical settings;quantitative screening;automatic screening;autism spectrum disorder screening","","1","","37","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Learning to Sequence Multiple Tasks with Competing Constraints","A. Duan; R. Camoriano; D. Ferigo; Y. Huang; D. Calandriello; L. Rosasco; D. Pucci","Istituto Italiano di Tecnologia, Via San Quirico 19D,Dynamic Interaction Control Lab,Genoa,Italy,16163; Istituto Italiano di Tecnologia and Massachusetts Institute of Technology,Laboratory for Computational and Statistical Learning (IIT@MIT),Cambridge,MA,USA; Istituto Italiano di Tecnologia, Via San Quirico 19D,Dynamic Interaction Control Lab,Genoa,Italy,16163; Istituto Italiano di Tecnologia, Via Morego 30,Department of Advanced Robotics,Genova,Italy,16163; Istituto Italiano di Tecnologia and Massachusetts Institute of Technology,Laboratory for Computational and Statistical Learning (IIT@MIT),Cambridge,MA,USA; DIBRIS, Università degli Studi di Genova,Genoa,Italy; Istituto Italiano di Tecnologia, Via San Quirico 19D,Dynamic Interaction Control Lab,Genoa,Italy,16163","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","2672","2678","Imitation learning offers a general framework where robots can efficiently acquire novel motor skills from demonstrations of a human teacher. While many promising achievements have been shown, the majority of them are only focused on single-stroke movements, without taking into account the problem of multi-tasks sequencing. Conceivably, sequencing different atomic tasks can further augment the robot's capabilities as well as avoid repetitive demonstrations. In this paper, we propose to address the issue of multi-tasks sequencing with emphasis on handling the so-called competing constraints, which emerge due to the existence of the concurrent constraints from Cartesian and joint trajectories. Specifically, we explore the null space of the robot from an information-theoretic perspective in order to maintain imitation fidelity during transition between consecutive tasks. The effectiveness of the proposed method is validated through simulated and real experiments on the iCub humanoid robot.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968496","","","humanoid robots;human-robot interaction;learning (artificial intelligence)","imitation learning;motor skills;human teacher;single-stroke movements;atomic tasks;competing constraints;concurrent constraints;consecutive tasks;iCub humanoid robot;multiple task sequencing;multitask sequencing;Cartesian trajectory;joint trajectory","","","","28","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Voice and gesture recognition system facilitating communication between man and virtual agent","T. Yoshikawa; S. Uchino; N. Abe; K. Tanaka; H. Taki; T. Yagi; S. He","Kyushu Inst. of Technol., Japan; Kyushu Inst. of Technol., Japan; Kyushu Inst. of Technol., Japan; Kyushu Inst. of Technol., Japan; NA; NA; NA","20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)","15 May 2006","2006","2","","5 pp.","","In this research, a dialog environment between human and virtual environment has been constructed. At preset, if a person wants to interact with virtual environment, special device such a data glove is required, but it makes difficult for general users to manipulate virtual objects. When we cannot manipulate objects directly, it is natural that we ask someone with a privilege to do the operation in place of us. In the case, it is convenient if methods used in daily life are allowed. That is, the method to be proposed here is based on the integration between verbal information through the utterances and non-verbal information by the gestures such as finger pointing. The experimental results have proved the effectiveness of this approach in terms of facilitating man-machine interaction and communication. The environment constructed in this research allows a user to communicate by talking and showing gestures to a personified agent in virtual environment. A user can use his/her finger to point at a virtual object and ask him to manipulate it.","2332-5658","0-7695-2466-4","10.1109/AINA.2006.347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1620457","","Speech recognition;Virtual environment;Fingers;Assembly systems;Data gloves;Systems engineering and theory;Helium;Yagi-Uda antennas;Humans;Man machine systems","gesture recognition;interactive systems;speech recognition;speech-based user interfaces;voice communication","dialog environment;human-virtual environment;verbal information;man-machine interaction;voice-gesture recognition system;communication facility","","1","","7","","15 May 2006","","","IEEE","IEEE Conferences"
"Artificial curiosity driven autonomous knowledge discovery based on learning by interaction","D. M. Ramik; C. Sabourin; K. Madani","Images, Signals & Intelligent Systems Lab. (LISSI / EA 3956) - University PARIS-EST Créteil (UPEC) - Campus de Sénart IUT, Bat. A - GEII - 36-37 rue Charpak - F-77127 Lieusaint, France; Images, Signals & Intelligent Systems Lab. (LISSI / EA 3956) - University PARIS-EST Créteil (UPEC) - Campus de Sénart IUT, Bat. A - GEII - 36-37 rue Charpak - F-77127 Lieusaint, France; Images, Signals & Intelligent Systems Lab. (LISSI / EA 3956) - University PARIS-EST Créteil (UPEC) - Campus de Sénart IUT, Bat. A - GEII - 36-37 rue Charpak - F-77127 Lieusaint, France","2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)","14 Nov 2013","2013","02","","855","860","In this work we investigate the development of a real-time intelligent system allowing a humanoid robot to discover its surrounding world and to learn autonomously new knowledge about it by semantically interacting with human. The learning is performed by observation and by interaction with a human tutor. We describe the system in a general manner, and then we apply it to autonomous learning of objects and their colors. We provide experimental results as well using simulated environment as implementing the approach on a humanoid robot in a real-world environment including every-day objects. We show, that our approach allows a humanoid robot to learn without negative input and from small number of samples.","","978-1-4799-1429-6","10.1109/IDAACS.2013.6663049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6663049","Visual saliency;Autonomous learning;Intelligent system;Artificial curiosity;Automated interpretation;Semantic robot-human interaction","Image color analysis;Robot sensing systems;Humanoid robots;Feature extraction;Human-robot interaction;Organisms","data mining;humanoid robots;human-robot interaction;learning (artificial intelligence)","artificial curiosity driven autonomous knowledge discovery;learning by interaction;real-time intelligent system;humanoid robot;autonomous learning","","","","19","","14 Nov 2013","","","IEEE","IEEE Conferences"
"Robot Presence and Human Honesty: Experimental Evidence","G. Hoffman; J. Forlizzi; S. Ayal; A. Steinfeld; J. Antanitis; G. Hochman; E. Hochendoner; J. Finkenaur","Media Innovation Lab, IDC Herzliya, Herzliya, Israel; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Psychology, IDC Herzliya, Herzliya, Israel; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Fuqua School of Business, Duke University, Durham, NC, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","4 Nov 2018","2015","","","181","188","Robots are predicted to serve in environments in which human honesty is important, such as the workplace, schools, and public institutions. Can the presence of a robot facilitate honest behavior? In this paper, we describe an experimental study evaluating the effects of robot social presence on people's honesty. Participants completed a perceptual task, which is structured so as to allowthem to earn more money by not complying with the experiment instructions. We compare three conditions between subjects: Completing the task alone in a room; completing it with a non-monitoring human present; and completing it with a non-monitoring robot present. The robot is a new expressive social head capable of 4-DoF head movement and screen-based eye animation, specifically designed and built for this research. It was designed to convey social presence, but not monitoring. We find that people cheat in all three conditions, but cheat equally less when there is a human or a robot in the room, compared to when they are alone. We did not find differences in the perceived authority of the human and the robot, but did find that people felt significantly less guilty after cheating inthe presence of a robot as compared to a human. This has implications for the use of robots in monitoring and supervising tasks in environments in which honesty is key. Categories and Subject Descriptors H.1.2 [Models and Principles]: User/Machine Systems; J.4 [Computer Applications]: Social and Behavioral Sciences-- psychology. General Terms Experimentation, Human Factors.","2167-2121","978-1-4503-2882-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520621","Human-robot interaction;honesty;experimental study;social presence;monitoring","Monitoring;Robot sensing systems;Prototypes;Head;Animation;Shape","computer animation;human factors;human-robot interaction;mobile robots;psychology;robot vision;user interfaces","supervising tasks;human honesty;experimental evidence;public institutions;honest behavior;robot social presence;perceptual task;nonmonitoring robot present;expressive social head;screen-based eye animation;monitoring;human factors;4-DoF head movement;peoples honesty;nonmonitoring human present;human -robot interaction","","","","38","","4 Nov 2018","","","IEEE","IEEE Conferences"
"Assembly motion teaching system using position/force simulator - extracting a sequence of contact state transition","H. Onda; H. Hirukawa; K. Takase","Electrotech. Lab., Ibaraki, Japan; Electrotech. Lab., Ibaraki, Japan; NA","Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots","6 Aug 2002","1995","1","","9","16 vol.1","Since general automatic assembly motion planning using a geometric CAD model is computationally difficult, automation of a general assembly task is not feasible. However, if a human operator roughly specifies an assembly motion, the remainder of the process except for planning can be automated. We construct a teaching system for assembly tasks according to the above concept. This paper describes our teaching system for assembly tasks by using a position/force simulator. We show how to construct the position/force simulator and analyse the hybrid position/force control which is needed when dealing with general rotational motion. We show the example of extracting a sequence of contact state transitions from the motion which the operator performs in the position simulator of our system. The sequence of contact states automatically extracted from the motion shown by the operator makes it feasible to achieve an error-tolerant automated assembly motion. If a sequence of contact states is obtained, studies of the automatic assembly task system can progress based upon this information.","","0-8186-7108-4","10.1109/IROS.1995.525768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=525768","","Assembly systems;Education;Computational modeling;Solid modeling;Automation;Humans;Process planning;Analytical models;Force control;Motion analysis","assembling;industrial robots;learning systems;force control;position control;path planning;simulation;computational geometry;digital simulation;robot programming","automatic assembly;motion planning;position/force simulator;contact state transition;geometric CAD model;rotational motion;assembly task teaching;contact state transition sequences","","23","","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Toward memory-based human motion simulation: development and validation of a motion modification algorithm","Woojin Park; D. B. Chaffin; B. J. Martin","Dept. of Mech., Univ. of Cincinnati, OH, USA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","19 Apr 2004","2004","34","3","376","386","Computer simulation of human motions helps test hypotheses on human motion planning and fosters timely and high-quality human-machine/environment interaction design. The current study introduces a novel simulation approach termed memory-based motion simulation (MBMS), and presents its key element ""motion modification"" (MoM) algorithm. The proposed approach implements a computational model inspired by the generalized motor program (GMP) theory. Operationally, when a novel motion scenario is submitted to the MBMS system, its motion database is searched to find relevant existing motions. The selected motions, referred to as ""root motions"", most likely do not meet exactly the novel motion scenario, and therefore, they need to be modified by the MoM algorithm. This algorithm derives a parametric representation of possible variants of a root motion in a GMP-like manner, and adjusts the parameter values such that the new modified motion satisfies the novel motion scenario, while retaining the root motion's overall angular movement pattern and inter-joint coordination. An evaluation of the prediction capability of the algorithm, using both seated upper body reaching and whole-body load-transfer motions, indicated that the algorithm can accurately predict various human motions with errors comparable to the inherent variability in human motions when repeated under identical task conditions.","1558-2426","","10.1109/TSMCA.2003.822965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288349","","Humans;Biological system modeling;Ergonomics;Computational modeling;Predictive models;Testing;Design automation;Man machine systems;Algorithm design and analysis;Prototypes","human computer interaction;ergonomics;digital simulation;motion estimation;CAD","memory-based human motion simulation;motion modification algorithm;computer simulation;human motion planning;human machine interaction;generalized motor program theory;motion database;parametric representation;root motion;angular movement pattern;interjoint coordination;load transfer motions;human motions","","49","2","51","","19 Apr 2004","","","IEEE","IEEE Journals"
"A Low-Cost Multi-modal Auditory-Visual-Tactile Framework for Remote Touch","F. Sanfilippo; C. Pacchierotti","University of Agder (UiA); CNRS, Univ Rennes, Inria, IRISA","2020 3rd International Conference on Information and Computer Technologies (ICICT)","14 May 2020","2020","","","213","218","Haptic technology for human augmentation provides gains in ability for different applications, whether the aim is to enhance ""disabilities"" to ""abilities"", or ""abilities"" to ""super-abilities"". Commercially-available devices are generally expensive and tailored to specific applications and hardware. To give researchers a haptic feedback system that is economical, customisable, and fast to fabricate, our group developed a low-cost immersive haptic, audio, and visual experience built by using off-the-shelf (COTS) components. It is composed of a vibrotactile glove, a Leap Motion sensor, and an head-mounted display, integrated together to provide compelling immersive sensations. This paper proposes a higher technology readiness level (TRL) for the system to make it modular and reliable. To demonstrate its potential, we present two human subject studies in Virtual Reality. They evaluate the capability of the system in providing (i) guidance during simulated drone operations, and (ii) contact haptic feedback during virtual objects interaction. Results prove that the proposed haptic-enabled framework improves the performance and illusion of presence.","","978-1-7281-7283-5","10.1109/ICICT50521.2020.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092180","Haptics;Human-Computer Interaction;Multimodality","Haptic interfaces;Visualization;Actuators;Hardware;Software;Rendering (computer graphics);Force","data gloves;haptic interfaces;helmet mounted displays;virtual reality","low-cost multimodal auditory-visual-tactile framework;remote touch;haptic technology;human augmentation;super-abilities;commercially-available devices;haptic feedback system;visual experience;off-the-shelf components;vibrotactile glove;leap motion sensor;head-mounted display;compelling immersive sensations;higher technology readiness level;human subject studies;contact haptic feedback;COTS components;TRL;virtual reality","","","","24","","14 May 2020","","","IEEE","IEEE Conferences"
"Ultrasonic glove input device for distance-based interactions","T. N. Hoang; R. T. Smith; B. H. Thomas",Wearable Computer Lab - University of South Australia; Wearable Computer Lab - University of South Australia; Wearable Computer Lab - University of South Australia,"2013 23rd International Conference on Artificial Reality and Telexistence (ICAT)","30 Jan 2014","2013","","","46","53","This paper presents distance-based interactions for wearable augmented reality systems enabled by an Ultrasonic Glove input device. The ultrasonic glove contains a tilt sensor and a pair of ultrasonic transducers in the palms. The transducers are distance-ranging sensors that emit ultrasonic waves from the palm of the hand. We developed distance-based interactions including modeling by measurement, numeric entry, and affine transformation interaction techniques. The interactions are based on natural gestures such as facing the palms towards each other or other surfaces. Virtual models of physical objects are created by the user specifying the dimensions with hand gestures. Distance data from the ultrasonic transducers is combined with orientation data to create dimensional vectors and construct models. We conducted an evaluation for the techniques and input device, including a pilot experiment, a user study, and an expert study session. The results indicated that for the task of modeling physical objects, the ultrasonic glove reduced completion time and, in many cases, task error. Our techniques can be generalized to different sensor technologies.","","978-4-904490-11-2","10.1109/ICAT.2013.6728905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728905","ultrasonic gloves;distance-based techniques;modeling;manipulation","Acoustics;Ultrasonic variables measurement;Thumb;Numerical models;Computational modeling;Solid modeling","augmented reality;data gloves;distance measurement;ultrasonic transducers;ultrasonic waves;wearable computers","ultrasonic glove input device;distance-based interactions;wearable augmented reality systems;tilt sensor;ultrasonic transducers;distance-ranging sensors;ultrasonic waves;affine transformation interaction techniques;measurement technique;numeric entry technique;natural gestures;virtual model;hand gestures;orientation data;dimensional vectors;physical object modeling","","1","","24","","30 Jan 2014","","","IEEE","IEEE Conferences"
"A Markerless Deep Learning-based 6 Degrees of Freedom Pose Estimation for Mobile Robots using RGB Data","L. Kästner; D. Dimitrov; J. Lambrecht","Technical University of Berlin,Chair Industry Grade Networks and Clouds Department,Berlin,Germany; Technical University of Berlin,Chair Industry Grade Networks and Clouds Department,Berlin,Germany; Technical University of Berlin,Chair Industry Grade Networks and Clouds Department,Berlin,Germany","2020 17th International Conference on Ubiquitous Robots (UR)","21 Jul 2020","2020","","","391","396","Augmented Reality has been subject to various integration efforts within industries due to its ability to enhance human machine interaction and understanding. Neural networks have achieved remarkable results in areas of computer vision, which bear great potential to assist and facilitate an enhanced Augmented Reality experience. However, most neural networks are computationally intensive and demand huge processing power, thus are not suitable for deployment on Augmented Reality devices. In this work, we propose a method to deploy state of the art neural networks for real time 3D object localization on augmented reality devices. As a result, we provide a more automated method of calibrating the AR devices with mobile robotic systems. To accelerate the calibration process and enhance user experience, we focus on fast 2D detection approaches which are extracting the 3D pose of the object fast and accurately by using only 2D input. The results are implemented into an Augmented Reality application for intuitive robot control and sensor data visualization. For the 6D annotation of 2D images, we developed an annotation tool, which is, to our knowledge, the first open source tool to be available. We achieve feasible results which are generally applicable to any AR device, thus making this work promising for further research in combining high demanding neural networks with Internet of Things devices.","2325-033X","978-1-7281-5715-3","10.1109/UR49135.2020.9144789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144789","","Robot sensing systems;Neural networks;Calibration;Three-dimensional displays;Robot kinematics;Two dimensional displays","augmented reality;data visualisation;learning (artificial intelligence);mobile robots;neural nets;pose estimation;robot vision","intuitive robot control;sensor data visualization;AR device;high demanding neural networks;mobile robots;RGB data;integration efforts;human machine interaction;computer vision;augmented reality devices;mobile robotic systems;calibration process;fast 2D detection approaches;augmented reality application;markerless deep learning-based 6 degrees of freedom pose estimation;enhanced augmented reality experience;neural networks","","","","28","","21 Jul 2020","","","IEEE","IEEE Conferences"
"Optimization-Based Wearable Tactile Rendering","A. G. Perez; D. Lobo; F. Chinello; G. Cirio; M. Malvezzi; J. S. Martín; D. Prattichizzo; M. A. Otaduy","Department of Computer Science, Universidad Rey Juan Carlos, Madrid, Spain; Department of Computer Science, Universidad Rey Juan Carlos, Madrid, Spain; University of Siena, Italy; Department of Computer Science, Universidad Rey Juan Carlos, Madrid, Spain; University of Siena, Italy; Department of Computer Science, Universidad Rey Juan Carlos, Madrid, Spain; University of Siena, Italy; Department of Computer Science, Universidad Rey Juan Carlos, Madrid, Spain","IEEE Transactions on Haptics","16 Jun 2017","2017","10","2","254","264","Novel wearable tactile interfaces offer the possibility to simulate tactile interactions with virtual environments directly on our skin. But, unlike kinesthetic interfaces, for which haptic rendering is a well explored problem, they pose new questions about the formulation of the rendering problem. In this work, we propose a formulation of tactile rendering as an optimization problem, which is general for a large family of tactile interfaces. Based on an accurate simulation of contact between a finger model and the virtual environment, we pose tactile rendering as the optimization of the device configuration, such that the contact surface between the device and the actual finger matches as close as possible the contact surface in the virtual environment. We describe the optimization formulation in general terms, and we also demonstrate its implementation on a thimble-like wearable device. We validate the tactile rendering formulation by analyzing its force error, and we show that it outperforms other approaches.","2329-4051","","10.1109/TOH.2016.2619708","EU; FP7; WEARHAP; H2020; European Research Council; Spanish Ministry of Economy; Spanish Ministry of Science and Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604131","Tactile rendering;wearable haptics;soft skin;virtual environments","Rendering (computer graphics);Skin;Computational modeling;Optimization;Virtual environments;Haptic interfaces;Force","haptic interfaces;optimisation;rendering (computer graphics);virtual reality","optimization;wearable tactile rendering;wearable tactile interfaces;tactile interactions simulation;haptic rendering;virtual environment;thimble-like wearable device","Equipment Design;Fingers;Humans;Models, Biological;Physical Stimulation;Signal Processing, Computer-Assisted;Touch;Touch Perception;User-Computer Interface;Virtual Reality;Wearable Electronic Devices","11","","46","","20 Oct 2016","","","IEEE","IEEE Journals"
"An object oriented model of behavior encoded virtual tools","T. Kesavadas; H. V. Subramanium","Virtual Reality Lab., State Univ. of New York, Buffalo, NY, USA; NA","SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)","9 Jul 2003","1998","1","","338","343 vol.1","We propose a model of attribute laden virtual tools to aid manipulation, interaction and navigation in a virtual manufacturing environment for controlling robots and machines. Fundamental to the concept of using such virtual tools, and other object-oriented virtual entities of this type generally (e.g. virtual obstacles, virtual workpieces, virtual machines etc.), is that each virtual entity represents both the graphical representation, and other physical plus ""intellectual"" attributes of the real entity. The aim of the virtual tool concept is to create the next generation of interface which will solve problems associated with multiple hierarchies of menu, information and other tasks such as interaction, and manipulation associated with cyberspace.","1062-922X","0-7803-4778-1","10.1109/ICSMC.1998.725432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=725432","","Object oriented modeling;Humans;Virtual reality;Aerospace engineering;Navigation;Virtual manufacturing;Robot control;Virtual machining;Orbital robotics;Virtual environment","object-oriented methods;virtual reality;user interfaces;computer integrated manufacturing;industrial robots;industrial control","object oriented model;behavior encoded virtual tools;user interaction;virtual manufacturing environment;robot control;graphical representation;multiple hierarchies;menu;cyberspace","","","1","15","","9 Jul 2003","","","IEEE","IEEE Conferences"
"Whole-hand kinesthetic feedback and haptic perception in dextrous virtual manipulation","C. S. Tzafestas","Lab. de Robotique de Paris, France","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","20 Jun 2003","2003","33","1","100","113","One of the key requirements for a Virtual Reality system is the multimodal, real-time interaction between the human operator and a computer simulated and animated environment. This paper investigates problems related particularly to the haptic interaction between the human operator and a virtual environment. The work presented here focuses on two issues: 1) the synthesis of whole-hand kinesthetic feedback, based on the application of forces (torques) on individual phalanges (joints) of the human hand, and 2) the experimental evaluation of this haptic feedback system, in terms of human haptic perception of virtual physical properties (such as the weight of a virtual manipulated object), using psychophysical methods. The proposed kinesthetic feedback methodology is based on the solution of a generalized force distribution problem for the human hand during virtual manipulation tasks. The solution is computationally efficient and has been experimentally implemented using an exoskeleton force-feedback glove. A series of experiments is reported concerning the perception of weight of manipulated virtual objects and the obtained results demonstrate the feasibility of the concept. Issues related to the use of sensory substitution techniques for the application of haptic feedback on the human hand are also discussed.","1558-2426","","10.1109/TSMCA.2003.812600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206459","","Haptic interfaces;Humans;Force feedback;Virtual reality;Real time systems;Computational modeling;Computer simulation;Animation;Virtual environment;Application software","virtual reality;haptic interfaces;dexterous manipulators","dextrous virtual manipulation;human operator;kinesthetic feedback;haptic feedback;human haptic perception;virtual physical properties;exoskeleton glove;grasping force distribution;psychophysics;virtual reality","","33","3","53","","20 Jun 2003","","","IEEE","IEEE Journals"
"Hardware-in-the-loop simulation for autonomous driving","W. Deng; Y. H. Lee; A. Zhao","General Motors Corporation, Japan; General Motors Corporation, Japan; Quantech Global Services, Japan","2008 34th Annual Conference of IEEE Industrial Electronics","23 Jan 2009","2008","","","1742","1747","This paper describes the design and execution of a hardware-in-the-loop (HIL) simulation system as an integral part of various autonomous driving programs. The intent of this work is to design and develop a laboratory environment to support the development, test and verification of many functions and algorithms related to sensor-guided autonomous driving. The focus is on extending conventional HIL simulation to vehicle interactions with other vehicles on traffic and with simulated surrounding environment sensed by simulated sensors. Included in the HIL simulation are active steering system with combined electric power steer and active front steer actuators, four corner by-wire electromechanical brakes and their corresponding electric control units, in addition to the prototype processors, which are all connected under the same communication architecture as implemented in a prototype vehicle. As a result, this HIL simulation seamlessly bridges the gap between pure offline simulation and in-vehicle development, and greatly shortens the research and development process.","1553-572X","978-1-4244-1767-4","10.1109/IECON.2008.4758217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4758217","","Remotely operated vehicles;Virtual prototyping;Algorithm design and analysis;Testing;Traffic control;Electromechanical sensors;Steering systems;Actuators;Communication system control;Bridges","automotive engineering;brakes;computer aided engineering;digital simulation;road safety;road vehicles;steering systems;vehicle dynamics","hardware-in-the-loop simulation;sensor-guided autonomous driving;vehicle interaction;active steering system;electric power steer actuator;active front steer actuator;corner by-wire electromechanical brake;electric control unit;in-vehicle development;automotive engineering","","4","","8","","23 Jan 2009","","","IEEE","IEEE Conferences"
"CHRISTINE 3D: a 3D multi-frequency large signal simulation code for helix traveling wave tubes","D. Chernin; T. M. Antonsen; B. Levush; D. Whaley","Sci. Applications Int. Corp., McLean, VA, USA; NA; NA; NA","Abstracts. International Vacuum Electronics Conference 2000 (Cat. No.00EX392)","6 Aug 2002","2000","","","2 pp.","","A generalization of the theory and implementation of the original CHRISTINE code to three dimensions will be described. The beam is treated as a collection of rays, rather than as disks as in 1D codes like CHRISTINE, and the complete 3D trajectories are followed by the code. This treatment permits the computation of helix intercept current and transverse beam distributions at the entrance to the collector-important design data that are unavailable from a 1D code. The beam may be initialized either by an internal parametric model, including transverse beam current distribution and beam temperature, or by importation of initial data from a gun code. The magnetic focusing field may also be represented by simple parametric models or by actual magnetic field data that is imported from a magnetics code. The RF fields supported by the helix are represented by modal expansion; the full (Bessel function) radial dependence is included. As in the original CHRISTINE, the new code employs models for sheath and tape helices that may be used in parametric studies. Alternatively, the code accepts a table of user-provided data for the circuit dispersion and interaction impedance. Also as in the original 1D code, the new code allows for tapered circuit properties, and for user specified attenuation profiles.","","0-7803-5987-9","10.1109/OVE:EC.2000.847429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=847429","","Parametric statistics;Circuits;Distributed computing;Current distribution;Temperature distribution;Magnetic fields;Radio frequency;Parametric study;Impedance;Attenuation","digital simulation;helical waveguides;travelling wave tubes;electron beams;microwave tubes;space charge","CHRISTINE 3D;multi-frequency large signal simulation code;helix traveling wave tubes;3D trajectories;intercept current;transverse beam distributions;internal parametric model;beam temperature;parametric models;modal expansion;sheath helices;tape helices;circuit dispersion;interaction impedance;tapered circuit properties;user specified attenuation profiles","","6","","2","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A comprehensive interactive approach for training simulation of equipment decontamination","Sijiang Liu; Yongshi Jiang; Yiping Yang; Liang Yu; Xiaobing Geng","Integrate Information System Research Center, Institute of Automation, Chinese Academy of Science, Beijing, China; Integrate Information System Research Center, Institute of Automation, Chinese Academy of Science, Beijing, China; Integrate Information System Research Center, Institute of Automation, Chinese Academy of Science, Beijing, China; Department of Nuclear Defense, Institute of Chemical Defense, Beijing, China; Department of Nuclear Defense, Institute of Chemical Defense, Beijing, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)","5 Sep 2011","2011","","","1810","1816","To improve effectiveness of simulation training of equipment decontamination, it's essential to emulate and process interactions in virtual scenes accurately, wishing to produce the same results as in reality. So we firstly build mathematical and attribute models for simulation targets, and then propose a processing algorithm based on those models to dispose various interactions and mark down critical changes. Finally an evaluation method is carefully chosen to score virtual trainings. The whole three steps constitute an original approach for training simulation of equipment decontamination, which features comprehensive interaction handling and incorporate solution from training to assessment. Tests have shown that our approach not only conducts virtual decontaminating as true as in real world, but also evaluate it automatically with both quantitative result and detailed text records. Besides the example equipment used in this article, the approach is also applicable to others, which reveals excellent generality. All these advantages make the simulation substitute for real training more reasonably and convincingly.","","978-1-4577-0536-6","10.1109/AIMSEC.2011.6011018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011018","training simulation;equipment decontamination;modeling;processing algorithm;evaluation","Mathematical model;Training;Solid modeling;Decontamination;Algorithm design and analysis;Equations;Three dimensional displays","computer based training;digital simulation;military computing;military equipment","interactive approach;equipment decontamination;mathematical model;attribute model;virtual training;training simulation;interaction handling;virtual decontaminating;virtual reality;military equipment training","","","","4","","5 Sep 2011","","","IEEE","IEEE Conferences"
"ARmy: A study of multi-user interaction in spatially augmented games","A. Dolce; J. Nasman; B. Cutler",Department of Computer Science Rensselaer Polytechnic Institute; Department of Computer Science Rensselaer Polytechnic Institute; Department of Computer Science Rensselaer Polytechnic Institute,"2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","16 Jul 2012","2012","","","43","50","We present ARmy, a two-player military strategy game that uses spatially augmented reality to combine physical tabletop games with the virtual elements and computation characteristic of modern video games. As players move plastic miniatures within a small scale physical environment, the application moderates and augments play by maintaining a 3D representation of the scene, which it uses to validate movement paths and perform automatic line-of-sight calculations for combat. We describe the design and implementation of the ARmy gaming system. Furthermore, we conducted a user study to gauge the effectiveness, in-tuitiveness, and robustness of the application. We describe the process of this user study, present quantitative data of the study results, and discuss general design principles for the design and implementation of other engaging spatially-augmented games.","2160-7516","978-1-4673-1612-5","10.1109/CVPRW.2012.6239198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239198","","Games;Visualization;Education;Augmented reality;Cameras;Image color analysis;Robustness","augmented reality;computer games;solid modelling","ARmy;multiuser interaction;two-player military strategy game;spatially augmented reality game;3D representation;automatic line-of-sight calculations","","3","1","33","","16 Jul 2012","","","IEEE","IEEE Conferences"
"Maneuver-based Control Interventions During Automated Driving: Comparing Touch, Voice, and Mid-Air Gestures as Input Modalities","H. Detjen; S. Geisler; S. Schneegass","University of Applied Sciences Ruhr West,Germany; University of Applied Sciences Ruhr West,Germany; University of Duisburg-Essen,Germany","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","3268","3274","Self-driving cars will relief the human from the driving task. Nevertheless, the human might want to intervene in the driving process and thus needs the possibility to control the car. Switching back to fully manual controls is uncomfortable once being passive and engaging in non-driving-related activities. A more comfortable way is controlling the car with elemental maneuvers (e.g., ""turn left"" or ""stop""). Whereas touch interaction concepts exist, contactless interaction through voice and mid-air gestures has not yet been explored for maneuver-based car control. In this paper, we, therefore, compare the general eligibility of voice and mid-air gesture with touch interaction as the primary maneuver selection mechanism in a driving simulator study. Our results show high usability for all modalities. Contactless interaction leads to a more positive emotional perception of the interaction, yet mid-air gestures lead to higher task load. Overall, voice and touch control are preferred over mid-air gestures by most users.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283431","","TV;Shape;Switches;Autonomous automobiles;Automobiles;Task analysis;Usability","automobiles;gesture recognition;human computer interaction;road safety;touch sensitive screens;traffic engineering computing","primary maneuver selection mechanism;driving simulator study;contactless interaction;mid-air gesture;touch control;maneuver-based control interventions;automated driving;driving task;driving process;nondriving-related activities;elemental maneuvers;touch interaction concepts;maneuver-based car control;voice control","","","","23","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Abstract robots with an attitude: Applying interpersonal relation models to human-robot interaction","L. Hiah; L. Beursgens; R. Haex; L. P. Romero; Y. Teh; M. ten Bhömer; R. van Berkel; E. I. Barakova","Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands; Eindhoven University of Technology, P.O. Box 513, The Netherlands","2013 IEEE RO-MAN","15 Oct 2013","2013","","","37","44","This paper explores new possibilities for social interaction between a human user and a robot with an abstract shape. The social interaction takes place by simulating behaviors such as submissiveness and dominance and analyzing the corresponding human reactions. We used an object that has no resemblance with human features in its shape or expression mode, in order to exclude the effect of these features on the human behavior. An intelligent walk-in closet was made to behave either dominantly or submissively using lighting effects. The behaviors of the closet were rated by participants using the Bem Sex Role Inventory in a pilot study, resulting in the selection of one submissive and one dominant lighting behavior for the closet. Participants' personality was measured using the Social Dominance Orientation questionnaire. These data were then compared to measurements of user satisfaction and feelings of dominance, arousal, and valence after scenario completion. A surprising effect was revealed as participants with a dominant personality reported feeling submissive to a dominant system, while in comparison, persons with a submissive personality felt more dominant in the same condition. Furthermore, it was found that a submissive system was generally more preferred by users. We draw a careful conclusion that people interact differently with systems that show human-like attitudes, than they would in response to similar attitude expressed by other person. These findings need to be investigated further with dominant/submissive nonverbal behaviors that are then simulated on a humanoid robot.","1944-9437","978-1-4799-0509-6","10.1109/ROMAN.2013.6628528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628528","","Lighting;Robots;Atmospheric measurements;Particle measurements;Abstracts;Sensors;Clothing","artificial intelligence;humanoid robots;human-robot interaction;intelligent robots","interpersonal relation models;human-robot interaction;abstract robots;social interaction;human user;abstract shape;behavior simulation;human behavior;intelligent walk-in closet;Bem Sex Role Inventory;dominant lighting behavior;submissive lighting behavior;social dominance orientation questionnaire;user satisfaction;dominance feelings;arousal feelings;submissive personality;human-like attitudes;nonverbal behaviors;humanoid robot","","3","","31","","15 Oct 2013","","","IEEE","IEEE Conferences"
"User Involvement in Design and Application of Virtual Reality Gamification to Facilitate the Use of Hearing Aids","H. Patel; S. Cobb; M. Hallewell; M. D'Cruz; R. Eastgate; L. Picinali; S. Tamascelli","Human Factors Res. Group, Univ. of Nottingham, Nottingham, UK; Human Factors Res. Group, Univ. of Nottingham, Nottingham, UK; Human Factors Res. Group, Univ. of Nottingham, Nottingham, UK; Human Factors Res. Group, Univ. of Nottingham, Nottingham, UK; Human Factors Res. Group, Univ. of Nottingham, Nottingham, UK; Dyson Sch. of Design Eng., Imperial Coll. London, London, UK; XTeam Software Solutions SRLS, Rovigo, Italy","2016 International Conference on Interactive Technologies and Games (ITAG)","15 Dec 2016","2016","","","77","81","The 3D Tune-In project aims to create an innovative toolkit based on 3D sound, visuals and gamification techniques to facilitate different target audiences in understanding and using the varied settings of their hearing aid to attain optimum performance in different social contexts. In the early stages of project development, hearing aid (HA) users participated in activities to identify user requirements regarding the difficulties and issues they face in everyday situations due to their hearing loss. The findings from questionnaire and interview studies and identification of current personas and scenarios of use indicate that the project can clearly and distinctly support the requirements of people with hearing loss as well as improve the general public's understanding of hearing loss. Five Future Scenarios of use have been derived to describe how the technologies and games to be developed by the 3D Tune-In project will address these requirements.","","978-1-5090-3738-4","10.1109/iTAG.2016.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782518","hearing aid users;gamification;scenarios of use","Auditory system;Three-dimensional displays;Context;Interviews;Hearing aids;Noise measurement;Stakeholders","handicapped aids;human computer interaction;user interfaces;virtual reality","user involvement;virtual reality gamification;hearing aids;3D Tune-In project;3D sound;social contexts;user requirements identification;people-with-hearing loss","","3","","8","","15 Dec 2016","","","IEEE","IEEE Conferences"
"Improving Generalisation in Learning Assistance by Demonstration for Smart Wheelchairs","V. Schettino; Y. Demiris","Imperial College London,Personal Robotics Laboratory,United Kingdom; Imperial College London,Personal Robotics Laboratory,United Kingdom","2020 IEEE International Conference on Robotics and Automation (ICRA)","15 Sep 2020","2020","","","5474","5480","Learning Assistance by Demonstration (LAD) is concerned with using demonstrations of a human agent to teach a robot how to assist another human. The concept has previously been used with smart wheelchairs to provide customised assistance to individuals with driving difficulties. A basic premise of this technique is that the learned assistive policy should be able to generalise to environments different than the ones used for training; but this has not been tested before. In this work we evaluate the assistive power and the generalisation capability of LAD using our custom teleoperation and learning system for smart wheelchairs, while seeking to improve it by experimenting with different combinations of dimensionality reduction techniques and machine learning models. Using Autoencoders to reduce the dimension of laserscan data and a Gaussian Process as the learning model, we achieved a 23% improvement in prediction performance against the combination used by the latest work on the field. Using this model to assist a driver exposed to a simulated disability, we observed a 9.8% reduction in track completion times when compared to driving without assistance.","2577-087X","978-1-7281-7395-5","10.1109/ICRA40945.2020.9197490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197490","","Wheelchairs;Training;Robots;Vehicles;Haptic interfaces;Sensors;Navigation","Gaussian processes;generalisation (artificial intelligence);handicapped aids;human-robot interaction;learning (artificial intelligence);neural nets;wheelchairs","Gaussian process;learning assistance by demonstration;human agent;machine learning models;dimensionality reduction techniques;learning system;custom teleoperation;LAD;generalisation capability;assistive power;learned assistive policy;customised assistance;smart wheelchairs","","","","30","","15 Sep 2020","","","IEEE","IEEE Conferences"
"Unified Neural Adaptive Control for Multiple Human–Robot–Environment Interactions","L. Han; W. Xu; P. Kang; H. Yuan","School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China","IEEE Transactions on Industrial Informatics","19 Nov 2020","2021","17","2","1166","1175","To go from human demonstration to robot independent operation, there are generally three phases of interaction to undergo, including human-robot interaction (HRI), human-robot-environment interaction (HREI), and robot-environment interaction (REI). Most existing methods address problems of a single stage. In this article, a unified neural adaptive control method that organically fuses multiple interactions is proposed. HRI, REI, and their coupling effects in HREI are comprehensively considered. First, the iterative least squares method is used for robot dynamics identification based on the linearized momentum observer. The accuracy of external force observation is improved to deal with dynamic uncertainties. The human force and the environmental force are achieved and decoupled by using only a force sensor, a momentum observer, and a selection matrix S. Next, the neural adaptive control method compensating position errors caused by the model uncertainty is addressed. The control system is proved to be stable based on the Lyapunov theorem. The trajectory tracking error under the model uncertainties is reduced. Then, the adaptive admittance control method is introduced. The interaction force of HRI is minimized and the interaction force control of REI is realized. Finally, the proposed method is verified by simulations and experiments.","1941-0050","","10.1109/TII.2020.2977051","National Key Research and Development Program of China; National Natural Science Foundation of China; Key Research and Development Program of Guangdong Province; Basic Research Program of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018090","Admittance control;human–robot–environment interaction (HREI);iterative least squares (ILS) method;momentum observer;neural adaptive control","Force;Dynamics;Adaptive control;Robot sensing systems;Observers;Adaptation models","adaptive control;force control;force sensors;human-robot interaction;Lyapunov methods;mobile robots;neurocontrollers;position control;robot dynamics","human-robot-environment interaction;robot independent operation;human demonstration;multiple human-robot-environment interactions;interaction force control;adaptive admittance control method;external force observation;robot dynamics identification;iterative least squares method;REI;HRI;unified neural adaptive control method","","","","29","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"The Geant4-Based Simulation Software of the ATLAS Detector","D. Costanzo; A. Dell'Acqua; M. Gallas; A. Rimoldi; J. Boudreau; V. Tsulaia; A. Di Simone","University of Sheffield, UK; CERN (Switzerland); CERN (Switzerland); INFN and Univ. of Pavia (Italy); Univ. of Pittsburgh (USA); Univ. of Pittsburgh (USA); INFN-CNAF and CERN. andrea.di.simone@cern.ch","2006 IEEE Nuclear Science Symposium Conference Record","7 May 2007","2006","1","","5","11","The ATLAS detector is a general purpose experiment which will study frontier physics by observing the collisions of two proton beams. It is presently under construction at CERN's Large Hadron Collider. Its instrumentation will cover a volume of about 22800 m around the interaction point. The simulation software is foreseen to be in use for more than 15 years and it must be, during this long period of time, easily maintainable and extensible. The intrinsic complexity of the geometry calls for a high degree of configurability so to allow to perform the simulation of only one part of the detector, or with different configurations of the magnetic field. Moreover, being the simulation of full physical events a time consuming process, simulation jobs will be run using extensively Grid technologies. We will give a complete review of the status of the Geant4-based ATLAS simulation software, nowadays fully functional, with particular stress on configurability issues and performance measurements.","1082-3654","1-4244-0560-2","10.1109/NSSMIC.2006.356099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178938","High Energy Physics;Computing;Simulation;Geant4","Detectors;Discrete event simulation;Physics;Particle beams;Large Hadron Collider;Instruments;Software maintenance;Geometry;Solid modeling;Magnetic field measurement","computational complexity;digital simulation;grid computing;high energy physics instrumentation computing;particle detectors;proton beams;software maintenance","Geant4-based simulation software;ATLAS Detector;frontier physics;proton beam collisions;CERN Large Hadron Collider;software maintenance;geometric complexity;configurability;magnetic field configuration;Grid technologies;ATLAS simulation software;high energy physics","","4","","6","","7 May 2007","","","IEEE","IEEE Conferences"
"Toward the New Version of D-MASON: Efficiency, Effectiveness and Correctness in Parallel and Distributed Agent-Based Simulations","G. Cordasco; C. Spagnuolo; V. Scarano","Dipt. di Psicologia, Seconda Univ. degli Studi di Napoli, Naples, Italy; Dipt. di Inf., Univ. degli Studi di Salerno, Salerno, Italy; Dipt. di Inf., Univ. degli Studi di Salerno, Salerno, Italy","2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","4 Aug 2016","2016","","","1803","1812","Agent-Based Models (ABMs) denote a class of models which, by simulating the behavior of multiple agents (i.e., independent actions, interactions and adaptation), aim to emulate and/or predict complex phenomena. One of the general features of ABM simulations is their experimental capacity, that requires a viable and reliable infrastructure to interact with a running simulation, monitoring its behaviour, as it proceeds, and applying changes to the configurations at run time, in order to study ""what if"" scenarios. A common approach for improving the efficiency and the effectiveness of ABMs as a research tool is to distribute the overall computation on a number of machines, which makes the design of the simulation model particularly challenging. D-MASON framework is a distributed version of the MASON library for writing and running Agent-based simulations. We briefly present D-MASON architecture and functionalities. Then we presents its novel features: a distributed network field and a novel communication layer dedicated to massive parallel machines. The main contribution of the paper is in providing a memory consistency modeling, where the previous state of theagent is made available (consistently) for all other agents (even the one on other processors) and this is obtained by exploiting the Java Method Handler mechanism. Full documentation, additional tutorials and other material can be found at www.dmason.org where the framework can be downloaded.","","978-1-5090-3682-0","10.1109/IPDPSW.2016.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530088","D-MASON;MASON;Agent-based simulations;Parallel Computing;Distributed Systems;High Performance Computing","Computational modeling;Biological system modeling;Load modeling;Synchronization;Predictive models;Visualization;Adaptation models","digital simulation;Java;multi-agent systems;parallel machines;software libraries","parallel agent-based simulations;distributed agent-based simulations;agent-based models;ABM simulations;multiple agents behavior simulation;MASON library;D-MASON architecture;massive parallel machines;memory consistency modeling;Java Method Handler mechanism","","5","","31","","4 Aug 2016","","","IEEE","IEEE Conferences"
"A unified representation to interact with simulated deformable objects in virtual environments","D. Zerbato; P. Fiorini","BBZ srl, Verona, 37134 Italy; Department of Computer Science, University of Verona, 37134, Italy","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","2710","2717","Deformations are an essential aspect of our interaction with real bodies, prompting the development of many modelling and simulation methods for virtual environments. Some of the methods address specific classes of interaction, e.g. pushing, grabbing, cutting or needle insertion, whereas hybrid approaches have been proposed to deal with more complex scenarios. However, a general strategy to combine different simulation methods is still not available. This paper presents a unified approach to combine different methods, each optimised for a specific interaction and object type. Our approach is tailored to the needs of simulating haptic Human-Robot Interactions and allows abstracting from the implementation details of different methods for modelling deformable objects. It has been integrated with collision detection and friction models, ported to a graphic processing unit (GPU), and demonstrated with realistic simulations and experiments.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487432","","Computational modeling;Deformable models;Adaptation models;Finite element analysis;Skin;Mathematical model;Spatial resolution","collision avoidance;graphics processing units;human-robot interaction","unified representation;simulated deformable objects;virtual environments;unified approach;specific interaction;object type;human-robot interactions;graphic processing unit;GPU;collision detection;friction models","","3","","27","","9 Jun 2016","","","IEEE","IEEE Conferences"
"A Study of User Interaction with Context Aware Electronic Updates from a Moodle Learning Environment","L. Crane; P. Benachour; P. Coulton","Sch. of Comput. & Commun. Syst., Lancaster Univ., Lancaster, UK; Sch. of Comput. & Commun. Syst., Lancaster Univ., Lancaster, UK; Sch. of Comput. & Commun. Syst., Lancaster Univ., Lancaster, UK","2013 IEEE 13th International Conference on Advanced Learning Technologies","19 Sep 2013","2013","","","50","52","This paper reports on a user study to gauge user interaction with RSS based mobile electronic updates from a Moodle based virtual learning environment. The electronic updates can be received in three dimensions of context: time, location and activity. The study aims to compare and evaluate the effectiveness of these context dimensions by comparing the level of user engagement. The mobile updates relate to teaching material, course work feedback, and general announcements from academic staff across a number of academic departments. As well as user profiling when interacting with the updates, early investigations show that there exists peak times when users interact with these applications. Results also show that time based electronic updates are the most popular, engagement wise, when compared to location and activity.","2161-377X","978-0-7695-5009-1","10.1109/ICALT.2013.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601863","mobile learning;context aware;RSS;virtual learning environments;ambient information","Context;Educational institutions;Context-aware services;Mobile communication;Materials;Conferences","computer aided instruction;mobile computing;user interfaces;virtual reality","user interaction;context aware electronic updates;Moodle learning environment;RSS based mobile electronic updates;virtual learning environment","","","","6","","19 Sep 2013","","","IEEE","IEEE Conferences"
"Simulating the experience of home environments","K. Ponto; R. Tredinnick; G. Casper","University of Wisconsin - Madison, Wisconsin Institute for Discovery, 53715, USA; University of Wisconsin - Madison, Wisconsin Institute for Discovery, 53715, USA; University of Wisconsin - Madison, Wisconsin Institute for Discovery, 53715, USA","2017 International Conference on Virtual Rehabilitation (ICVR)","14 Aug 2017","2017","","","1","9","Growing evidence indicates that transitioning patients are often unprepared for the self-management role they must assume when they return home. Over the past twenty five years, LiDAR scanning has emerged as a fascinating technology that allows for the rapid acquisition of three dimensional data of real world environments while new virtual reality (VR) technology allows users to experience simulated environments. However, combining these two technologies can be difficult as previous approaches to interactively rendering large point clouds have generally created a trade-off between interactivity and quality. For instance, many techniques used in commercially available software have utilized methods to sub-sample data during interaction, only showing a high-quality render when the viewpoint is kept static. Unfortunately, for displays in which viewpoints are rarely static, such as virtual reality systems, these methods are not useful. This paper presents a novel approach to the problem of quality-interactivity trade-off through a progressive feedback-driven rendering algorithm. This technique uses reprojections of past views to accelerate the reconstruction of the current view and can be used to extend existing point cloud viewing algorithms. The presented method is tested against previous methods, demonstrating marked improvements in both rendering quality and interactivity. This algorithm and rendering application could serve as a tool to enable virtual rehabilitation within 3D models of one's own home from a remote location.","2331-9569","978-1-5090-3053-8","10.1109/ICVR.2017.8007521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007521","","Rendering (computer graphics);Three-dimensional displays;Laser radar;Graphics processing units;Virtual reality;Hardware;Solid modeling","data acquisition;medical computing;virtual reality","home environments;self-management role;LiDAR scanning;three dimensional data acquisition;virtual reality;high-quality render;progressive feedback-driven rendering algorithm;virtual rehabilitation;3D models;cloud viewing algorithms","","","","34","","14 Aug 2017","","","IEEE","IEEE Conferences"
"Use of multimedia to augment simulation","P. Aiken; P. Brouse; M. Hassanpour; F. Armour; A. Fields; J. Liang; J. D. Palmer","Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA; Sch. of Inf. Technol. & Eng., George Mason Univ., Fairfax, VA, USA","1990 Winter Simulation Conference Proceedings","6 Aug 2002","1990","","","775","783","The authors describes efforts to integrate multimedia and simulation, the general domain of interest being regional mobility. Multimedia have been used to augment simulation of highway maintenance, interactive sessions of a transportation management association (TMA), traffic mobility in a highly congested area, and traffic signage. Results thus far show that one of the more important aspects for successful application of multimedia techniques is a graphical user interface. It is a difficult task to prevent information overload; however, the graphical user interface permits management of information transfer rates and presentation dynamics while reducing the potential impact of information overload. Other results indicate the value added from proper useage of audio, video, text, and graphics, and various combinations of these to augment ease of understanding of simulation model presentations.<>","","0-911801-72-3","10.1109/WSC.1990.129613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=129613","","Graphical user interfaces;Computational modeling;Human computer interaction;Decision making;Information technology;Workstations;Couplings;Road transportation;Information management;Graphics","civil engineering computing;digital simulation;graphical user interfaces;multimedia systems;road traffic;town and country planning","civil engineering;urban planning;multimedia;simulation;regional mobility;highway maintenance;transportation management association;traffic mobility;congested area;traffic signage;graphical user interface;information overload;information transfer rates;presentation dynamics;simulation model presentations","","1","","34","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Research on the Realization of Substation Virtual Equipment Based on 3D Engine","Z. Qu; T. Jiang","Sch. of Inf. Eng., Northeast Dianli Univ., Jilin, China; Sch. of Inf. Eng., Northeast Dianli Univ., Jilin, China","2009 International Forum on Computer Science-Technology and Applications","19 Jan 2010","2009","2","","299","302","According to training requirements that a substation virtual training scene needs substation virtual equipment with high actual sense and high interaction. This paper proposes a thought which is used to achieve substation virtual equipment by 3D engine, and then generalizes a method which contains geometrical characteristic, interactive characteristic and behavioral characteristic towards Virtual equipment achieved by 3D engine. This method uses modeling tool to construct the geometrical appearance of substation virtual equipment, takes the 3D engine VR technique as core to realize interactive characteristic and behavioral characteristic of the virtual equipment. Combining with substation the application characteristics of substation virtual equipment, the paper proposes an interactive algorithm suiting to virtual equipment to improve the interaction of virtual equipment.","","978-1-4244-5423-5","10.1109/IFCSTA.2009.195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384578","virtual reality;the 3D engine;substation;virtual equipment;object-oriented","Substations;Engines;Virtual reality;Object oriented modeling;Layout;Cities and towns;Solid modeling;Educational technology;Isolation technology;Switches","computational geometry;computer based training;engineering graphics;power engineering computing;power engineering education;solid modelling;substations;virtual reality","substation virtual equipment realization;3D engine;geometrical characteristic;interactive characteristic;behavioral characteristic;virtual training scene","","","","8","","19 Jan 2010","","","IEEE","IEEE Conferences"
"New Magnetic Microactuator Design Based on PDMS Elastomer and MEMS Technologies for Tactile Display","J. Streque; A. Talbi; P. Pernod; V. Preobrazhensky","Joint International Laboratory LEMAC: Institute of Electronics, Microelectronics and Nanotechnology (IEMN-UMR CNRS), Villeneuve d'Ascq; Joint International Laboratory LEMAC: Institute of Electronics, Microelectronics and Nanotechnology (IEMN-UMR CNRS), Villeneuve d'Ascq; Joint International Laboratory LEMAC: Institute of Electronics, Microelectronics and Nanotechnology (IEMN-UMR CNRS), Villeneuve d'Ascq; Joint International Laboratory LEMAC: Institute of Electronics, Microelectronics and Nanotechnology, Villeneuve d'Ascq and Wave Research Center of Prokhorov General Physics Institute, RAS","IEEE Transactions on Haptics","28 Jun 2010","2010","3","2","88","97","Highly efficient tactile display devices must fulfill technical requirements for tactile stimulation, all the while preserving the lightness and compactness needed for handheld operation. This paper focuses on the elaboration of highly integrated magnetic microactuators for tactile display devices. FEM simulation, conception, fabrication, and characterization of these microactuators are presented in this paper. The current demonstrator offers a 4 × 4 flexible microactuator array with a resolution of 2 mm. Each actuator is composed of a Poly (Dimethyl-Siloxane) (PDMS) elastomeric membrane, magnetically actuated by coil-magnet interaction. It represents a proof of concept for fully integrated MEMS tactile devices, with fair actuation forces provided for a power consumption up to 100 mW per microactuator. The prototypes are destined to provide both static and dynamic tactile sensations, with an optimized membrane geometry for actuation frequencies between DC and 350 Hz. On the basis of preliminary experiments, this display device can offer skin stimulations for various tactile stimuli for applications in the fields of Virtual Reality or Human-Computer Interaction (HCI). Moreover, the elastomeric material used in this device and its global compactness offer great advantages in matter of comfort of use and capabilities of integration in haptic devices.","2329-4051","","10.1109/TOH.2009.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374397","Actuators;MEMS;micromagnetomechanical systems (MMMS);tactile display;magnetic actuation.","Micromagnetics;Micromechanical devices;Displays;Microactuators;Biomembranes;Fabrication;Actuators;Energy consumption;Virtual prototyping;Geometry","display devices;elastomers;electromagnetic actuators;haptic interfaces;human computer interaction;microactuators","magnetic microactuator design;PDMS elastomer;MEMS technologies;tactile display;tactile stimulation;handheld operation;FEM simulation;elastomeric membrane;coil magnet interaction;virtual reality;human computer interaction;HCI;elastomeric material","","44","2","27","","8 Jan 2010","","","IEEE","IEEE Journals"
"Finger tracking for the Digital Desk","T. Brown; R. C. Thomas","INTERLINK, Perth, WA, Australia; NA","Proceedings First Australasian User Interface Conference. AUIC 2000 (Cat. No.PR00515)","6 Aug 2002","2000","","","11","16","One trend in computing environments today is to move towards more 'natural' interaction. Another is to make hardware invisible to the user. Both these ideas converge into ubiquitous computing-the Digital Desk is an example of this idea. In this paper, we concentrate on an input device for the Digital Desk, namely the user's fingertip, which is made to act like a mouse. Tracking such an input device is common to a number of augmented reality environments and involves vision and motion analysis. However, previous attempts have focused more on the vision aspect of tracking general objects than on using the information already known about the user's hand, which is the approach taken in this paper. We adopted the goal of tracking the user's fingertips as fast as possible in real time, so that the system could be compared with other input devices by using models such as Fitts' law. Our system is shown to comply with the law adequately.","","0-7695-0515-5","10.1109/AUIC.2000.822058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=822058","","Fingers;Electrical capacitance tomography;Computer displays;Mice;Pervasive computing;Cameras;Ubiquitous computing;Virtual reality;Augmented reality;Computer science","interactive devices;augmented reality;tracking;real-time systems;active vision;motion estimation","Digital Desk;real-time finger tracking;natural interaction;user-transparent hardware;ubiquitous computing;input device tracking;user's fingertip;mouse;augmented reality environments;vision;motion analysis;user's hand;Fitts' law","","5","1","14","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multitasking in emotion modelling: Attention Control","M. R. Sidoumou; S. Turner; P. Picton; K. Bechkoum; K. Benatchba","School of Science and Technology, The University of Northampton, St George's Avenue, Northampton, NN2 6JD, UK; School of Science and Technology, The University of Northampton, St George's Avenue, Northampton, NN2 6JD, UK; School of Science and Technology, The University of Northampton, St George's Avenue, Northampton, NN2 6JD, UK; School of Computing and Technology, University of Gloucestershire, Park Campus, The Park, Cheltenham, GL50 2RH, UK; Ecole Nationale Supérieure d'Informatique, B.P. 68 M, 16270 Oued-Smar, Algiers, Algeria","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","7 Dec 2015","2015","","","308","314","The work described in this paper is about building a general model capable of simulating human behaviour and emotions using virtual characters. To make the simulation realistic enough, virtual characters need to express emotions according to the environment and deal with those emotions in a parallel way where an emotional experience can be triggered at the same time as another emotional response. The virtual character will have perceptions, feel and express emotions and respond to different situations. To make the simulation realistic, we used a method allowing the virtual characters to execute tasks, perceive events and display emotions in a parallel way. To do that, we used the multiple resources model [1] to control the attention and to predict when two or more actions can be executed at the same time. The used emotional model is based on Scherer's theory [2]. However, in this paper we focus on the control of the attention as a part of the emotional process.","2156-8111","978-1-4799-9953-8","10.1109/ACII.2015.7344588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344588","Affective Computing;Emotion;Simulation;Multitasking;Workload","Cognition;Computational modeling;Biological system modeling;Appraisal;Visualization;Encoding;Organisms","behavioural sciences computing;emotion recognition;virtual reality","multitasking process;emotion modelling;general model;human behaviour simulation;human emotion simulation;virtual characters;realistic simulation;emotion expression;emotional experience;emotional response;user perceptions;emotion feeling;multiple resource model;attention control;action prediction;Scherer theory","","","","14","","7 Dec 2015","","","IEEE","IEEE Conferences"
"Agent-based model of maritime search operations: A validation using test-driven simulation modelling","B. S. Onggo; M. Karatas","Department of Management Science, Lancaster University Management School, Lancaster University, LA1 4YX, UNITED KINGDOM; Department of Industrial Engineering, Turkish Naval Academy, Tuzla, Istanbul, 34942, TURKEY","2015 Winter Simulation Conference (WSC)","18 Feb 2016","2015","","","254","265","Maritime search operations (and search operations in general) are one of the classic applications of Operational Research (OR). This paper presents a generic agent-based model for maritime search operations which can be used to analyse operations such as search and rescue and patrol. Agent-based simulation (ABS) is a relatively new addition to existing OR techniques. The key elements of an ABS model are agents, their behaviours and their interactions with other agents and the environment. A search operation involves at least two types of agent: a searcher and a target. The unique characteristic of ABS is that we model agents' behaviours and their interactions at the individual level. Hence, ABS offers an alternative modelling approach to analyse search operations. The second objective of our work is to show how test-driven simulation modelling (TDSM) can be used to validate the agent-based maritime search-operation model.","1558-4305","978-1-4673-9743-8","10.1109/WSC.2015.7408169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408169","","Libraries","digital simulation;marine engineering;military computing;operations research;software agents","agent-based model;maritime search operations;test-driven simulation modelling;operational research;agent-based simulation;OR techniques;ABS model;agent behaviour modeling;TDSM","","3","","34","","18 Feb 2016","","","IEEE","IEEE Conferences"
"A real-time motion capture system with multiple camera fusion","S. Yonemoto; A. Matsumoto; D. Arita; R. -. Taniguchi","Dept. of Intelligent Syst., Kyushu Univ., Fukuoka, Japan; NA; NA; NA","Proceedings 10th International Conference on Image Analysis and Processing","6 Aug 2002","1999","","","600","605","This paper presents a real-time motion capture system of 3D multi-part objects, whose purpose is to do seamless mapping of objects in the real world into virtual environments easily. In general, virtual environment applications such as man-machine seamless interaction require the system to estimate accurate motion parameters at real-time for natural objects such as human bodies. To achieve this requirement, we have been developing a vision-based motion capture system which reconstructs time-varying motion parameters of 3D multi-part objects. The advantage of such a vision-based system is that it is possible to acquire the other scene parameters such as shape and surface properties at the same time, using the same equipment in measuring motion. In this paper, as our first system, we have implemented a color-marker-based motion capture system which realizes multi-view fusion and have demonstrated our motion capture and reconstruction system works at real-time on PC-clusters.","","0-7695-0040-4","10.1109/ICIAP.1999.797662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=797662","","Real time systems;Cameras;Virtual environment;Shape measurement;Man machine systems;Motion estimation;Parameter estimation;Humans;Time varying systems;Layout","computer vision;real-time systems;motion estimation;video signal processing;virtual reality;user interfaces;image reconstruction;sensor fusion;parameter estimation;time-varying systems","real-time motion capture system;multiple camera fusion;3D multi-part objects;seamless mapping;virtual environments;man-machine seamless interaction;motion parameters;human bodies;vision;reconstruction;time-varying motion parameters;color-marker-based motion capture system;multi-view fusion","","28","1","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Dynamic Associative E-Learning Model based on a Spreading Activation Network","P. Nilas; N. Nilas; S. Mitatha","Faculty of Engineering, King Mongkut's Inst. of Tech., Ladkrabang, Thailand. email: knphongc@kmitl.ac.th; Faculty of Engineering, Rajamangala University of Tech., Phra Nakhon, Bangkok, Thailand. email: s5061212@kmitl.ac.th; Faculty of Engineering, King Mongkut's Inst. of Tech., Ladkrabang, Thailand. email: kmsomsak@kmitl.ac.th","2006 Canadian Conference on Electrical and Computer Engineering","15 Jan 2007","2006","","","2472","2475","This paper proposes a dynamic semantic model for e-learning system based on the psycholinguistic theories of human memory; spreading activation network (SAN). This work employs a SAN as a technique to provide the interface's action selection mechanism in an uncertain environment. The paper combines the SAN with the temporal logic to provide an e-learning system that a learning activity level evolves according to their expected contextual relevance. The system differs from the other e-learning by representing dynamic associations between learning activities and the relevance subjects. This system equipped with an event-triggered learning interface (context) adaptation component. This component provides multiple parallel processes for perception. These processes provide context screen selection and learning task operation based upon the user current situation. The SAN attempts to achieve a number of goals in an unpredictable complex dynamic environment. Spreading activation explains the predictive top-down effect of knowledge. It supports general heuristics which may be used as the first step of more elaborated methods. This model is suited to deal with the interaction between semantic and episodic memories, as well as many other practical issues regarding e-learning, including the retroactive effect of semantics over perception. The system uses the SAN to activate the most suitable interface screen (context) in response to the current conditions (learning activities) while the system continues working towards the learning objective goal. The paper presents our efforts to realize such e-learning system. The proposed paradigm has been implemented to develop a prototypical system, and the experiments also illustrate the robustness of such an e-learning framework","0840-7789","1-4244-0038-4","10.1109/CCECE.2006.277285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4055069","E-Learning;Human-Machine Interaction","Electronic learning;Storage area networks;Multimedia systems;Psychology;Humans;Learning systems;Computer architecture;IP networks;Discussion forums;Electronic mail","computer aided instruction;human computer interaction;human factors;psychology;temporal logic","dynamic associative e-learning model;spreading activation network;dynamic semantic model;psycholinguistic theory;human memory;action selection mechanism;temporal logic;event-triggered learning interface adaptation component;parallel process;context screen selection","","1","","7","","15 Jan 2007","","","IEEE","IEEE Conferences"
"Robust speaker's location detection in a vehicle environment using GMM models","Jwu-Sheng Hu; Chieh-Cheng Cheng; Wei-Han Liu","Dept. of Electr. & Control Eng., Nat. Chiao-Tung Univ., Hsinchu, Taiwan; Dept. of Electr. & Control Eng., Nat. Chiao-Tung Univ., Hsinchu, Taiwan; Dept. of Electr. & Control Eng., Nat. Chiao-Tung Univ., Hsinchu, Taiwan","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","13 Mar 2006","2006","36","2","403","412","Human-computer interaction (HCI) using speech communication is becoming increasingly important, especially in driving where safety is the primary concern. Knowing the speaker's location (i.e., speaker localization) not only improves the enhancement results of a corrupted signal, but also provides assistance to speaker identification. Since conventional speech localization algorithms suffer from the uncertainties of environmental complexity and noise, as well as from the microphone mismatch problem, they are frequently not robust in practice. Without a high reliability, the acceptance of speech-based HCI would never be realized. This work presents a novel speaker's location detection method and demonstrates high accuracy within a vehicle cabinet using a single linear microphone array. The proposed approach utilize Gaussian mixture models (GMM) to model the distributions of the phase differences among the microphones caused by the complex characteristic of room acoustic and microphone mismatch. The model can be applied both in near-field and far-field situations in a noisy environment. The individual Gaussian component of a GMM represents some general location-dependent but content and speaker-independent phase difference distributions. Moreover, the scheme performs well not only in nonline-of-sight cases, but also when the speakers are aligned toward the microphone array but at difference distances from it. This strong performance can be achieved by exploiting the fact that the phase difference distributions at different locations are distinguishable in the environment of a car. The experimental results also show that the proposed method outperforms the conventional multiple signal classification method (MUSIC) technique at various SNRs.","1941-0492","","10.1109/TSMCB.2005.859084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1605386","Gaussian mixture models (GMM);human–computer interaction (HCI);microphone array;sound localization","Vehicle detection;Robustness;Microphone arrays;Human computer interaction;Working environment noise;Multiple signal classification;Oral communication;Safety;Signal processing;Speech enhancement","speaker recognition;human computer interaction;Gaussian processes;microphone arrays;signal classification","robust speaker location detection method;vehicle cabinet;Gaussian mixture model;GMM model;speech-based human-computer interaction;HCI;speech communication;driving safety;speaker identification;speech localization algorithm;environmental complexity;room acoustics;microphone mismatch problem;linear microphone array;multiple signal classification method","Acoustics;Algorithms;Artificial Intelligence;Computer Simulation;Data Interpretation, Statistical;Ecosystem;Humans;Models, Statistical;Normal Distribution;Sound Localization;Sound Spectrography;Transportation","13","2","41","","13 Mar 2006","","","IEEE","IEEE Journals"
"A framework for interaction of distributed autonomous systems and human supervisors","H. J. W. Spoelder; D. M. Germans; L. Renambot; H. E. Bal; P. J. de Waal; F. C. A. Groen","Dept. of Phys. & Astron., Vrije Univ., Amsterdam, Netherlands; NA; NA; NA; NA; NA","IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188)","7 Aug 2002","2001","3","","1937","1941 vol.3","Autonomous systems are rapidly gaining importance in a large number of situations relevant to the general public. By their nature their need for external control is low but still necessary. In this paper we present a framework for interaction of distributed autonomous systems and human supervisors. This framework exploits progress made in a number of related areas and shows that they can be effectively combined into one single framework. To this end it combines an environment for computational steering with virtual reality techniques for visualization and WAP-based communication for ubiquitous intervention. Given the described setup for the technology the current version must be seen as a prototype that shows the feasibility of this approach.","1091-5281","0-7803-6646-8","10.1109/IMTC.2001.929538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=929538","","Humans;Virtual reality;Internet;Extraterrestrial measurements;Anthropometry;Mobile handsets;Data visualization;IP networks;Physics;Astronomy","virtual reality;software agents;cooperative systems;virtual instrumentation;Internet telephony","distributed autonomous systems;human supervisors;interaction framework;external control;computational steering;virtual reality techniques;visualization;WAP-based communication;ubiquitous intervention;human in the loop;Internet-based systems;second-generation mobile phones;CAVE environment;Internet robots;virtual instrument;shared world","","","1","8","","7 Aug 2002","","","IEEE","IEEE Conferences"
"The design and analysis of the Space Exploration 3D simulation game","R. Kurniawan; A. S. Rohman; E. M. Husni",Electrical Engineering Magister Program - ITB Jln. Ganesha 10 Bandung 40132 Indonesia; Electrical Engineering Magister Program - ITB Jln. Ganesha 10 Bandung 40132 Indonesia; Electrical Engineering Magister Program - ITB Jln. Ganesha 10 Bandung 40132 Indonesia,"2012 International Conference on System Engineering and Technology (ICSET)","25 Oct 2012","2012","","","1","6","Instructional media can help students understand the material described in the teacher as well as reducing the verbal explanation by the teacher. Media use 3D simulation game based learning can make students more interested and not bored in following the learning process. Domain there are two issues that need to be studied are: (1) analyze the effectiveness of learning from media interaction design of simulation-based learning game 3D Space Exploration, (2) establish a prototype design and implementation of learning materials for junior high school students in Indonesia for three classes in the introduction of solar system by using a 3D simulation game based on Celestia platform. The results of the study obtained an average score of 3.20 of scale 4 shows that the respondents agree on its use 3D simulation game Space Exploration. Product development still needs to be done primarily on the addition of the completeness of the material so it can be used also by the general user.","","978-1-4673-2376-5","10.1109/ICSEngT.2012.6339307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6339307","The Space Exploration 3D simulation game;Celestia platform","Games;Solid modeling;Space exploration;Media;Education;Planets","astronomy;astronomy computing;computer aided instruction;computer games;computer graphics;digital simulation;further education","analysis;space exploration 3D simulation game;instructional media;media use 3D simulation game based learning;media interaction design;simulation-based learning game 3D space exploration;prototype design;learning materials;junior high school students;Indonesia;solar system;Celestia platform","","","","10","","25 Oct 2012","","","IEEE","IEEE Conferences"
"An Information System prototype for analysis of astronaut/computer interaction during simulated EVA","M. A. Mackin; P. T. Gonia; J. A. Lombay-González","NASA Glenn Research Center, 21000 Brookpark Rd, Cleveland, OH 44135; NASA Glenn Research Center, 21000 Brookpark Rd, Cleveland, OH 44135; NASA Glenn Research Center, 21000 Brookpark Rd, Cleveland, OH 44135","2012 IEEE Aerospace Conference","19 Apr 2012","2012","","","1","8","During human exploration of space, a suited crewmember needs effective and accurate information about their spacesuit's operation. Ideally, the information should be presented in a manner that provides real-time situational awareness and increases task efficiency and operational autonomy. Typically, however, the effective display of information has been limited by the relatively low resolution of radiation-tolerant sunlight readable displays and the low processing power available on currently deployed spacesuits. As part of NASA's Enabling Technology Development and Demonstrations Program, a prototype EVA Information System has been constructed to test and study human-computer interaction and system operations. In one tested configuration, the Information System provides acquisition and display of science data via a cuff-mounted graphical display and keypad and features a camera capable of capturing both still images and high-definition video. The Information System provides a capability for astronaut-computer interaction beyond currently deployed state-of-the art EVA systems. Using the Information System, crewmembers may receive timeline-based procedures, general procedures, and text messages from either ground or space-based EVA mission controllers. Additionally, using the EVA Information System prototype, crewmembers can generate and analyze scientific artifacts (pictures, videos, or voice notes) and deliver them to the ground-based science team for detailed analysis. This paper discusses the software architecture of the prototype Information System software and the user feedback obtained from field testing the system at NASA's Desert Research and Technologies Studies (Desert RATS) activity.","1095-323X","978-1-4577-0557-1","10.1109/AERO.2012.6187352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187352","","Global Positioning System;Cameras;Rats;Information systems;Prototypes;NASA;Testing","aerospace computing;cameras;computer graphics;data acquisition;high definition video;human computer interaction;information systems;software architecture;video signal processing","astronaut-computer interaction analysis;human space exploration;spacesuit operation;real-time situational awareness;task efficiency;operational autonomy;radiation-tolerant sunlight readable display;NASA;EVA information system;human-computer interaction;system operation;data acquisition;data display;science data;cuff-mounted graphical display;keypad;camera;still image;high-definition video;timeline-based procedure;text message;space-based EVA mission controller;ground-based science team;software architecture;user feedback;Desert Research and Technologies Studies;Desert RATS activity","","1","","13","","19 Apr 2012","","","IEEE","IEEE Conferences"
"Projection-based registration using a multi-view camera for indoor scene reconstruction","Sehwan Kim; Woontack Woo","GIST U-VR Lab., Gwangju, South Korea; GIST U-VR Lab., Gwangju, South Korea","Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05)","27 Jun 2005","2005","","","484","491","A registration method is proposed for 3D reconstruction of an indoor environment using a multi-view camera. In general, previous methods have a high computational complexity and are not robust for 3D point cloud with low precision. Thus, a projection-based registration is presented. First, depth are refined based on temporal property by excluding 3D points with a large variation, and spatial property by filling holes referring neighboring 3D points. Second, 3D point clouds acquired at two views are projected onto the same image plane, and two-step integer mapping enables the modified KLT to find correspondences. Then, fine registration is carried out by minimizing distance errors. Finally, a final color is evaluated using colors of corresponding points and an indoor environment is reconstructed by applying the above procedure to consecutive scenes. The proposed method reduces computational complexity by searching for correspondences within an image plane. It not only enables an effective registration even for 3D point cloud with low precision, but also need only a few views. The generated model can be adopted for interaction with as well as navigation in a virtual environment.","1550-6185","0-7695-2327-7","10.1109/3DIM.2005.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1443282","","Cameras;Layout;Image reconstruction;Clouds;Indoor environments;Computational complexity;Robustness;Filling;Karhunen-Loeve transforms;Navigation","virtual reality;image reconstruction;image registration;cameras;computational complexity","projection-based image registration;multiview camera;indoor scene reconstruction;computational complexity;3D point cloud;distance error minimization;virtual environment","","1","","14","","27 Jun 2005","","","IEEE","IEEE Conferences"
"To help or not to help a service robot","H. Huttenrauch; K. S. Eklundh","Interaction & Presentation Lab., R. Inst. of Technol., Stockholm, Sweden; Interaction & Presentation Lab., R. Inst. of Technol., Stockholm, Sweden","The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.","19 Dec 2003","2003","","","379","384","This paper reports an experimental study in which people who had never encountered our service robot before were requested to assist it with a task. We call these visiting users ""bystanders"" to differentiate them from people who belong to the social setting and group in which the robot is operated in and thus are familiar with the robot. In our study 32 subjects were exposed to our robot and requested by it to provide a cup of coffee as part of a delivery mission. We anticipated that people in general would help the robot, dependent upon whether they were busy or had received a demonstration of the robot as introduction. Our results indicate that the willingness of bystanders to help a robot not only is a consequence of the robot initiated interaction, but equally depends on the situation and state of occupation people are in when requested to interact with and assist the robot.","","0-7803-8136-X","10.1109/ROMAN.2003.1251875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251875","","Service robots;Human robot interaction;Graphical user interfaces;Laboratories;Collaboration;Robot control;Mobile robots;Transportation;Speech;Mobile handsets","mobile robots;human computer interaction","service robot;robot demonstration;robot initiated interaction;socially interactive robots;collaborative control","","2","","20","","19 Dec 2003","","","IEEE","IEEE Conferences"
"Simulation study of oscillatory vehicle roll behavior during fishhook Maneuvers","N. Moshchuk; C. Mousseau; K. Norman","General Motors, USA; General Motors, USA; General Motors, USA","2008 American Control Conference","5 Aug 2008","2008","","","3933","3940","Sustained roll oscillations were observed while performing the National Highway Traffic Safety Administration (NHTSA) fishhook (FH) test on several vehicles. This phenomenon has also been observed on several manufacturers' high center of gravity (CG) vehicles with both solid and independent rear suspensions. Roll oscillation can be accompanied by non-convergent yaw, heave, and pitch. A study was initiated to quantify the influence of vehicle inertia, suspension, powertrain, and tire characteristics on the FH vehicle response. Design of experiments (DOE) methods were used to quantify the main effects and the significant interactions between vehicle design variables. The CarSim program was used to simulate the vehicle dynamics and the iSIGHT program was used to automate the DOE analysis. Summary findings show that tire lateral force high slip behavior influences yaw instability. This alone is not sufficient for developing diverging roll oscillations. Additionally, results show that suspension jounce travel and bumper rate, in conjunction with tire overturning moment and non-positive tire cornering stiffness, influence yaw, roll, and pitch stability. Simulation results suggest that the primary cause of roll oscillations is the transfer of some energy from the longitudinal mode into the roll and heave modes. This effect can also be influenced by other factors like the distance between the CG and the roll axis, yaw-roll cross product of inertia, for example. Also described is optimization of FH performance (minimization of wheel lift and roll oscillations) with respect to some suspension characteristics.","2378-5861","978-1-4244-2078-0","10.1109/ACC.2008.4587107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4587107","","Suspensions;Tires;US Department of Transportation;Character generation;US Department of Energy;Performance evaluation;Testing;Road vehicles;Vehicle safety;Manufacturing","automobile industry;design of experiments;digital simulation;production engineering computing;testing;vehicle dynamics","oscillatory vehicle roll behavior;fishhook maneuvers;roll oscillations;National Highway Traffic Safety Administration;fishhook testing;vehicle testing;vehicle inertia;design of experiments methods;vehicle design;CarSim program;vehicle dynamics simulation;iSIGHT program;suspension characteristics","","2","2","8","","5 Aug 2008","","","IEEE","IEEE Conferences"
"Virtual Launch & Validation of Manufacturing Automation Controls","L. G. Barajas; S. R. Biller; F. Gu; C. Yuan","Manufacturing Systems Research Laboratory, General Motors R&D Center, Warren, MI 48090, USA; Manufacturing Systems Research Laboratory, General Motors R&D Center, Warren, MI 48090, USA; Manufacturing Systems Research Laboratory, General Motors R&D Center, Warren, MI 48090, USA; Manufacturing Systems Research Laboratory, General Motors R&D Center, Warren, MI 48090, USA","2010 IEEE International Conference on Automation Science and Engineering","21 Oct 2010","2010","","","412","419","In the automotive industry an integrated set of manufacturing engineering activities lie on the critical path of the Vehicle Development Process (VDP). After the product integration design is finalized, the Production Tool Design and Build (PTDB) processes becomes the VDP critical path. It is in the PTDB where virtual launch and validation activities are performed. To increase throughput of the engineering factory and reduce product development cost, we propose a research initiative in the area of Virtual Launch & Validation (VLV) of Manufacturing Automation Controls (MAC). VLV of MAC is composed of a series of PTDB launch and pre-launch activities in a virtual environment using physical, emulated, and simulated hardware and software. It encompasses the testing of MAC, its interactions with MAC analysis tools, and its interfaces with IT systems and related business processes. VLV of MAC also supports the development and testing of new IT systems and business processes as well as its hardware and software plant floor systems constituents. The mission of VLV of MAC is to minimize the duration of the launch and ramp up activities by reducing the implementation and execution times for plant floor systems and equipments via a priori system emulation, validation and testing. Based on our findings, we conclude that the most pressing needs for VLV of MAC lie in the thrust areas of object-oriented standard logic, auto-generation of virtual models, logic validation and system wide emulation. In summary, we conservatively estimate that a successful VLV of MAC implementation will reduce the VDP duration by several weeks hence providing automakers with savings in the order of hundreds of millions of dollars per year.","2161-8089","978-1-4244-5449-5","10.1109/COASE.2010.5583977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583977","","Object oriented modeling;Vehicles;Production;Solid modeling;Computational modeling;Data models;Software","automobile industry;factory automation;machine tools;product design;product development;virtual reality","automotive industry;manufacturing engineering activity;vehicle development process;product integration design;production tool design and build;PTDB process;VDP critical path;virtual launch and validation;engineering factory;product development cost;manufacturing automation control;virtual environment;MAC analysis tool;IT system;business process;plant floor system;object-oriented standard logic;virtual model;logic validation","","","","45","","21 Oct 2010","","","IEEE","IEEE Conferences"
"Market power analysis in the EEX electricity market: An agent-based simulation approach","Jianhui Wang; A. Botterud; G. Conzelmann; V. S. Koritarov","Decision and Information Sciences Division, Argonne National Laboratory, 9700 S. Cass Avenue, IL 60439, USA; Decision and Information Sciences Division, Argonne National Laboratory, 9700 S. Cass Avenue, IL 60439, USA; Decision and Information Sciences Division, Argonne National Laboratory, 9700 S. Cass Avenue, IL 60439, USA; Decision and Information Sciences Division, Argonne National Laboratory, 9700 S. Cass Avenue, IL 60439, USA","2008 IEEE Power and Energy Society General Meeting - Conversion and Delivery of Electrical Energy in the 21st Century","12 Aug 2008","2008","","","1","8","In this paper, an agent-based modeling and simulation (ABMS) approach is used to model the German wholesale electricity market. The spot market prices in the European Energy Exchange (EEX) are studied as the wholesale market prices. Each participant in the market is modeled as an individual rationality-bounded agent whose objective is to maximize its own profit. By simulating the market clearing process, the interaction among agents is captured. The market clearing price formed by agentspsila production cost bidding is regarded as the reference marginal cost. The gap between the marginal cost and the real market price is measured as an indicator of possible market power exertion. Various bidding strategies such as physical withholding and economic withholding can be simulated to represent strategic bidding behaviors of the market participants. The preliminary simulation results show that some generation companies (GenCos) are in the position of exerting market power by strategic bidding.","1932-5517","978-1-4244-1905-0","10.1109/PES.2008.4596563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596563","Agent-based modeling and simulation;EEX market;electricity markets;electricity prices;market power","Biological system modeling;Power markets;Load modeling;Adaptation model;Production;Companies;Electricity","digital simulation;multi-agent systems;power engineering computing;power generation economics;power markets;pricing","power market analysis;agent-based simulation approach;EEX electricity market prices;European Energy Exchange;German market clearing process;cost bidding;generation companies;GenCos","","3","","25","","12 Aug 2008","","","IEEE","IEEE Conferences"
"Parallel processing simulations of the propagation of ultrasonic waves through material interfaces","P. P. Delsanto; N. K. Batra; R. B. Mignogna; M. Scalerandi","Dipartimento di Fisica, Politecnico di Torino, Italy; NA; NA; NA","1998 IEEE Ultrasonics Symposium. Proceedings (Cat. No. 98CH36102)","6 Aug 2002","1998","2","","1129","1138 vol.2","Computer simulations of the propagation of ultrasonic pulses in multilayers require a specific physical model both for the material layers and for the interfaces. In the Local Interaction Simulation Approach (LISA) a ""spring model"" can be conveniently adopted for this purpose. In the spring model, the propagation medium is replaced by an analog set of tensorial springs. The springs within the layers are assumed to simulate the propagation inside the laminates, while the springs representing the interface (""internal springs"") are assumed to predict the interface effect on the wave propagation due to its physical condition. The latter depends on a six component ""contact quality tensor"", Q/sub ij/, for each discretization node along the interfaces. When all Q-components are equal to one, the bond at the corresponding node is considered ""perfect"". A smaller or zero value for any component of Q/sub ij/ indicates and characterizes possible interface flaws, which is useful for NDE applications. Due to the flexible nature of the model, many other physical features affecting the wave propagation, such as attenuation, nonlinearity, hysteretic behavior and plasticity can be easily incorporated and the treatment extended to general 3-D heterogeneous and anisotropic media.","1051-0117","0-7803-4095-7","10.1109/ULTSYM.1998.765038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765038","","Parallel processing;Springs;Computational modeling;Nonhomogeneous media;Computer simulation;Predictive models;Laminates;Bonding;Attenuation;Hysteresis","multilayers;laminates;ultrasonic propagation;digital simulation;parallel processing;tensors;ultrasonic materials testing;ultrasonic absorption","parallel processing simulations;propagation;ultrasonic waves;material interfaces;computer simulations;ultrasonic pulses;multilayers;local interaction simulation approach;spring model;tensorial springs;laminates;interface effect;wave propagation;contact quality tensor;discretization node;interface flaws;NDE applications;attenuation;nonlinearity;hysteretic behavior;plasticity;3-D heterogeneous media;anisotropic media","","3","","25","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Overview of tools used and work done on data acquisition system simulations","E. C. Milner","Superconducting Super Collider Lab., Dallas, TX, USA","IEEE Conference on Nuclear Science Symposium and Medical Imaging","6 Aug 2002","1992","","","484 vol.1","","Summary form only. Recent work simulating data acquisition (DAQ) systems is considered, with emphasis given to simulation software tools. DAQ systems have been studied in two ways: behaviorally and operationally. Behavioral studies address the interaction of system components without requiring detailed programming of functionality. Operational simulations use code closely resembling hardware needed for implementing system functions. In general, two techniques are independent, although an example has been considered in which simulation was done at both levels.<>","","0-7803-0884-0","10.1109/NSSMIC.1992.301302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=301302","","Data acquisition;Laboratories;Software tools;Functional programming;Hardware","data acquisition;digital simulation;physics computing;software tools","data acquisition system;simulations;simulation software tools","","","","","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Building Innovative Speech Interfaces Using Patterns and Antipatterns of Commands for Controlling Loader Cranes","M. Majewski; W. Kacalak","Fac. of Mech. Eng., Koszalin Univ. of Technol., Koszalin, Poland; Fac. of Mech. Eng., Koszalin Univ. of Technol., Koszalin, Poland","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20 Mar 2017","2016","","","525","530","The paper describes research and development of innovative intelligent speech interaction systems between mobile lifting devices and their human operators, which use patterns and antipatterns of commands. A general processing scheme, using several functional modules, for the human-machine interactive communication has been presented, covering also the integration with vision and sensorial systems. The aim of the experimental research is to design a prototype of an innovative interaction system, equipped with a speech interface in a natural language, augmented reality and interactive manipulators with force feedback. The system is equipped with several adaptive intelligent layers for human biometric identification, speech recognition, word recognition, analysis and recognition of commands and messages, sentence meaning analysis, command effect analysis and safety assessment, process supervision and human reaction assessment. The paper also proposes a concept of synergistic control systems using patterns and antipatterns of commands. The concept consists of a methodology based on hybrid binary neural networks and deep learning convolutional networks.","","978-1-5090-5510-4","10.1109/CSCI.2016.0105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881398","speech interface;intelligent communication;interactive system;pattern recognition;antipatterns;natural language processing;neural networks;intelligent control","Cranes;Speech recognition;Speech;Force feedback;Control systems;Text recognition;Machine vision","augmented reality;control engineering computing;cranes;learning (artificial intelligence);manipulators;natural language processing;neural nets;speech recognition","innovative intelligent speech interaction systems;mobile lifting devices;human operators;loader cranes;human-machine interactive communication;speech interface;natural language;augmented reality;interactive manipulators;adaptive intelligent layers;human biometric identification;speech recognition;word recognition;sentence meaning analysis;command effect analysis;safety assessment;process supervision;human reaction assessment;synergistic control systems;hybrid binary neural networks;deep learning convolutional networks","","4","","16","","20 Mar 2017","","","IEEE","IEEE Conferences"
"Body-based interfaces","Changseok Cho; Huichul Yang; G. J. Kim; S. H. Han","Imaging Media Res. Center, Korea Inst. of Sci. & Technol., South Korea; NA; NA; NA","Proceedings. Fourth IEEE International Conference on Multimodal Interfaces","22 Jan 2003","2002","","","466","472","This research explores different ways to use features of one's own body for interacting with computers. In the future, such ""body-based"" interfaces may be put into good use for wearable computing or virtual reality systems as part of a 3D multi-modal interface, freeing the user from holding interaction devices. We have identified four types of body-based interfaces: the Body-inspired-metaphor uses various parts of the body metaphorically for interaction; the Body-as-interaction-surface simply uses parts of the body as points of interaction; Mixed-mode mixes the former two; Object-mapping spatially maps the interaction object to the human body. These four body-based interfaces were applied to three different applications (and associated tasks) and were tested for their performance and utility. It was generally found that, while the body-inspired-metaphor produced the lowest error rate, it required a longer task completion time and caused more fatigue due to the longer hand moving distance. On the other hand, the body-as-interaction-surface was the fastest, but produced many more errors.","","0-7695-1834-6","10.1109/ICMI.2002.1167040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167040","","Wearable computers;Virtual reality;Humans;Guidelines;Computer science;Industrial engineering;Testing;Error analysis;Fatigue;Eyes","user interfaces;human factors;wearable computers;virtual reality","body-based interfaces;wearable computing;virtual reality;3D multi-modal interface;interaction devices;object-mapping;performance;error;interaction styles;task completion time","","1","1","9","","22 Jan 2003","","","IEEE","IEEE Conferences"
"Helix traveling wave tube simulation","T. A. Hargreaves; R. B. True; M. L. Barsanti; C. M. Armstrong",NA; NA; NA; NA,"IEEE Conference Record - Abstracts. PPPS-2001 Pulsed Power Plasma Science 2001. 28th IEEE International Conference on Plasma Science and 13th IEEE International Pulsed Power Conference (Cat. No.01CH37","7 Aug 2002","2001","","","280","","Summary form only given. One of Litton's goals is to collect a set of design codes that are sufficiently accurate to reduce the need for costly hardware iterations in the development of helix traveling wave tubes (TWTs). As a first step toward this goal, we are modeling existing TWTs to compare the various available design tools and to verify their accuracy. Where possible, multiple codes with similar capabilities are being examined and compared to measured data. HFSS and CTLSS are three-dimensional, finite-element based codes that solve for RF fields in general structures. They can each be used to calculate the phase velocity and interaction impedance of helix TWT circuits by modeling one turn of the helix and applying the appropriate symmetry conditions at the boundaries. Once the calculations and measurements of the cold circuit parameters are in agreement, the small signal characteristics of the TWT can be calculated. Stand alone small-signal codes have been developed in the past, however, with the advances in computing power, we have focussed on CHRISTINE1D, a one-dimensional, large-signal code developed by Science Applications, Inc. (SAIC) and the Naval Research Laboratory (NRL). This code typically calculates the output power for a single set of input parameters in a few seconds or less, making it an easy code to use for initial design optimization. After agreement is obtained for the small signal gain of the TWT, the large signal response can be investigated. Due to the growth of the beam size in the output circuit, good agreement between measurements and calculations is not expected, in general, with a one-dimensional code. Therefore, we have chosen to investigate CHRISTINE3D, currently a 2 1/2 dimensional, large-signal code developed by SAIC and NRL. We will present calculations and measured data comparing cold circuit parameters as well as small- and large-signal performance.","","0-7803-7141-0","10.1109/PPPS.2001.960928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=960928","","Circuits;Hardware;Finite element methods;Radio frequency;Impedance;Laboratories;Power generation;Design optimization;Size measurement;Electromagnetic measurements","travelling wave tubes;digital simulation;finite element analysis","helix traveling wave tubes;helix TWT simulation;design codes;design tools;three-dimensional finite-element based codes;RF fields;HFSS;CTLSS;phase velocity;interaction impedance;symmetry conditions;cold circuit parameters;small signal characteristics;one-dimensional large-signal code;output power;input parameters;initial design optimization;signal gain;output circuit;2 1/2 dimensional large-signal code","","","","3","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Mid-Air Display for Physical Exercise and Gaming","I. Rakkolainen; T. Erdem; B. Utku; C. E. Erdem; M. Ozkan","Tampere University of Technology, PL 553, 33710 Tampere, Finland; FogScreen Inc., Helsinki, Finland; Momentum AS, TTÜBİTAK-MAM-TEKSEB A-206, Gebze 41470 Kocaeli, Turkey; Momentum AS, TTÜBİTAK-MAM-TEKSEB A-206, Gebze 41470 Kocaeli, Turkey; Momentum AS, TTÜBİTAK-MAM-TEKSEB A-206, Gebze 41470 Kocaeli, Turkey; Momentum AS, TTÜBİTAK-MAM-TEKSEB A-206, Gebze 41470 Kocaeli, Turkey","2007 3DTV Conference","12 Nov 2007","2007","","","1","4","We present some possibilities and our experiments with the ""immaterial"" walk-through FogScreen for gaming and physical exercise. We use real-time 3D graphics and interactivity for creating visually and physically compelling games with the immaterial screens. An immaterial projection screen has many advantages for physical exercise, games and other activities. It is visually intriguing and can also be made two-sided so that the opposing gamers on each side see both their side of the screen and each other through it, and can even walk through it. The immaterial nature of the screen helps also on maintenance, as the screen is unbreakable and stays always clean. The initial results show that the audience stayed with the game over extended periods of time. The overall comments of the gaming and physical exercise environment were positive and it was generally assessed to be captivating and inspiring.","2161-203X","978-1-4244-0721-7","10.1109/3DTV.2007.4379415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379415","display;FogScreen;mid-air;gaming;interaction;3D graphics","Large screen displays;Graphics;Space technology;Three dimensional displays;TV;Solids;Holographic optical components;Motion control;Holography;Optical distortion","computer games;interactive systems;three-dimensional displays","midair display;physical exercise;gaming;FogScreen;three-dimensional graphics;real-time graphics;interactivity;immaterial screens;projection screen","","4","","11","","12 Nov 2007","","","IEEE","IEEE Conferences"
"Realtime Coarse Pose Recognition Using a Multi-scaled Local Integral Histograms","D. Jang; Y. Chai; X. Jin; T. Kim","Chung-Ang Univ., Seoul; Chung-Ang Univ., Seoul; Chung-Ang Univ., Seoul; Chung-Ang Univ., Seoul","2007 International Conference on Convergence Information Technology (ICCIT 2007)","7 Jan 2008","2007","","","1982","1987","We present a fast and robust algorithm for segmenting foreground object from background image by comparing local histograms. Background subtraction is a important preprocessing step for extracting the features that can be used for object tracking in surveillance system or HCI system in virtual environment. In this paper, the local histograms of the same area are used to compute a foreground probability. The histogram-based method is partially robust against illumination change and small moving objects in background. However without data quantization to reduce bin size, histograms are generally not suitable for realtime applications. Moreover quantization errors are a major drawback of using histograms. We propose a new method to keep the advantages of histograms without suffering computational load and quantization error using local kernel histogram with the multi-scaled integral histograms. We implement the video game interface with a trained neural network to prove the proposed method is highly applicable to coarse pose recognition.","","0-7695-3038-9","10.1109/ICCIT.2007.273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420542","","Histograms;Quantization;Robustness;Image segmentation;Feature extraction;Surveillance;Human computer interaction;Virtual environment;Lighting;Kernel","computer games;feature extraction;human computer interaction;image motion analysis;image segmentation;learning (artificial intelligence);object detection;pose estimation;tracking;video surveillance","realtime coarse pose recognition;multiscaled local integral histograms;foreground object segmentation;background subtraction;features extraction;object tracking;surveillance system;HCI system;foreground probability;local kernel histogram;video game interface;trained neural network","","","","16","","7 Jan 2008","","","IEEE","IEEE Conferences"
"Repeatable Folding Task by Humanoid Robot Worker Using Deep Learning","P. Yang; K. Sasaki; K. Suzuki; K. Kase; S. Sugano; T. Ogata","Department of Modern Mechanical Engineering, Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Modern Mechanical Engineering, Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan","IEEE Robotics and Automation Letters","20 May 2017","2017","2","2","397","403","We propose a practical state-of-the-art method to develop a machine-learning-based humanoid robot that can work as a production line worker. The proposed approach provides an intuitive way to collect data and exhibits the following characteristics: task performing capability, task reiteration ability, generalizability, and easy applicability. The proposed approach utilizes a real-time user interface with a monitor and provides a first-person perspective using a head-mounted display. Through this interface, teleoperation is used for collecting task operating data, especially for tasks that are difficult to be applied with a conventional method. A two-phase deep learning model is also utilized in the proposed approach. A deep convolutional autoencoder extracts images features and reconstructs images, and a fully connected deep time delay neural network learns the dynamics of a robot task process from the extracted image features and motion angle signals. The “Nextage Open” humanoid robot is used as an experimental platform to evaluate the proposed model. The object folding task utilizing with 35 trained and 5 untrained sensory motor sequences for test. Testing the trained model with online generation demonstrates a 77.8% success rate for the object folding task.","2377-3766","","10.1109/LRA.2016.2633383","AIST; Research Institute for Science and Engineering, Waseda University; MEXT Grant-in-Aid for Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762066","Humanoid robots;learning and adaptive systems;motion control of manipulators;neurorobotics","Robot sensing systems;Machine learning;Feature extraction;Humanoid robots;Training;Data models","data handling;delays;feature extraction;generalisation (artificial intelligence);humanoid robots;human-robot interaction;image reconstruction;industrial robots;learning (artificial intelligence);neurocontrollers;robot dynamics;robot vision;telerobotics;user interfaces","repeatable folding task;humanoid robot worker;machine-learning-based humanoid robot;production line worker;task performing capability;task reiteration ability;generalizability;user interface;first-person perspective;head-mounted display;teleoperation;task operating data collection;two-phase deep learning;deep convolutional autoencoder;image feature extraction;image reconstruction;deep time delay neural network learning;robot task process dynamics;Nextage Open humanoid robot;sensory motor sequences","","76","","21","","29 Nov 2016","","","IEEE","IEEE Journals"
"A novel multimedia human-computer interaction (HCI) system based on Kinect and depth image understanding","Z. Xu; X. Qiu; J. He","Sichuan University of Media and Communications, Chengdu, China, 611745; Sichuan University of Media and Communications, Chengdu, China, 611745; Dalian Nationalities University, Dalian, China, 116600","2016 International Conference on Inventive Computation Technologies (ICICT)","26 Jan 2017","2016","3","","1","6","This paper presents the novel multimedia human-computer interaction system based on Kinect and depth image understanding. For human-machine interface of the user intent is one of the research directions. Industrialized society goal is the satisfaction of the material quantity and quality, which is the measure of people's living standards. It is the purpose of the man-machine interface design and let the computer more intelligent, more intelligent that can do a wider range of work. While gradually reduce, who use it can be aimed at the lack of any computer knowledge and experience of users. The user is the user of computer resources. In feature extraction, good character is capable of different categories of samples has a high degree of differentiation, and as far as possible to reduce general feature dimension and the amount of calculation. Gradient and point is visible human body recognition feature extraction in the two categories of common characteristics. With basis, we integrate the Kinect and depth image understanding paradigm for the core implementation of new multimedia human-computer interaction system. The experimental simulation proves the effectiveness and overall feasibility of the method that is meaningful.","","978-1-5090-1285-5","10.1109/INVENTIVE.2016.7830149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830149","Multimedia;Depth Image;Kinect;Human-Computer Interaction;Systematic Design;Implementation","Computers;Human computer interaction;Three-dimensional displays;Object segmentation;Multimedia communication;Feature extraction","feature extraction;human computer interaction;image sensors","multimedia human-computer interaction system;HCI system;Kinect;depth image understanding;human-machine interface;user intent;material quantity;material quality;differentiation degree;feature dimension;human body recognition feature extraction","","4","","34","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Sofia, design and implementation of a virtual assistance agent for attention a financial institution","J. A. E. Arias; G. Urrea; H. G. Martinez","Universidad de Medellín, Programa Ingenieria de Sistemas Medellín, Colombia; Banco Falabella, Bogotá, Colombia; Banco Falabella, Bogotá, Colombia","2018 13th Iberian Conference on Information Systems and Technologies (CISTI)","28 Jun 2018","2018","","","1","5","Different systems of human-machine interaction with capacities more and more similar to humans are diffused and generate different perspectives of incorporation in organizations. A growing trend is the construction of artificial intelligence software to execute tasks without human intervention, such as collecting and presenting information to users, making a reservation at a restaurant on a special date, and generally carrying out tasks related to customer service by automated bots for solving to solve everyday situations, normally employees in the messaging through the development of advanced conversational interfaces. The bots with the ability to simulate conversations through conversational interfaces are already a reality and their boom is increasing. A field where they have a prominent presence has to do with customer service. This paper presents the components designed and developed of a bot used by a company in the financial sector consolidated for more than a decade in Latin America to manage customer service in order to solve everyday situations.","","978-989-98434-8-6","10.23919/CISTI.2018.8399336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399336","chatbot;Artificial Intelligence;Intelligent Agents;natural lenguage","Software;Irrigation;Bot (Internet);Customer services;Facebook;Artificial intelligence;Task analysis","customer services;financial data processing;human computer interaction;multi-agent systems;virtual reality","virtual assistance agent;human-machine interaction;organizations;artificial intelligence software;human intervention;restaurant;customer service;automated bots;advanced conversational interfaces;financial sector;financial institution;Sofia","","","","","","28 Jun 2018","","","IEEE","IEEE Conferences"
"Tool Support for Parametric Analysis of Large Software Simulation Systems","J. Schumann; K. Gundy-Burlet; C. Pasareanu; T. Menzies; T. Barrett","RIACS/USRA, NASA Ames, Ames, IA; NA; NA; NA; NA","2008 23rd IEEE/ACM International Conference on Automated Software Engineering","7 Oct 2008","2008","","","497","498","The analysis of large and complex parameterized software systems, e.g., systems simulation in aerospace, is very complicated and time-consuming due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such an analysis with a tool to support envelope assessment: we utilize a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations to limit the number of cases, but still explore important interactions in the parameter space in a systematic fashion. Additional test-cases, automatically generated from models (e.g., UML, Simulink, Stateflow) improve the coverage. The distributed test runs of the software system produce vast amounts of data, making manual analysis impossible. Our tool automatically analyzes the generated data through a combination of unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. The tool has been developed around the Trick simulation environment, which is widely used within NASA. We will present this tool with a GN&C (Guidance, Navigation and Control) simulation of a small satellite system.","1938-4300","978-1-4244-2187-9","10.1109/ASE.2008.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4639382","","Unified modeling language;Object oriented modeling;Analytical models;Data models;Atmospheric modeling;NASA;Software","Bayes methods;digital simulation;Monte Carlo methods;program diagnostics;software tools","tool support;parametric analysis;large software simulation systems;complex parameterized software systems;systems simulation;advanced Monte Carlo generation;n-factor combinatorial parameter variations;UML;Simulink;Stateflow;unsupervised Bayesian clustering techniques;AutoBayes;satellite system","","5","","7","","7 Oct 2008","","","IEEE","IEEE Conferences"
"Visualizing Human Behavioral Features based on Signature Haptic Data","R. Iglesias; M. Orozco; J. J. Valdes; A. E. Saddik","Ikerlan Technological Research Centre, Po. J. Ma. Arizmendiarrieta, 2 20500 Arrasate-Mondragón, Gipuzkoa, Spain riglesias@ikerlan.es; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, morozco@mcrlab.uottawa.ca; National Research Council of Canada, Institute for Information Technology, 1200 Montreal Road, Ottawa ON K1A 0R6, Canada. julio. valdes@nrc-cnrc.gc.ca; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, abed@mcrlab.uottawa.ca","2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","31 Mar 2008","2008","","","451","456","Haptic technology refers to the technology that connects the user to a computerized system via the sense of touch by applying forces, vibrations and/or motions to the user. With this technology it is possible to measure and record haptic data generated directly as users interact with the system. Haptics can be seen as a mechanism to extract behavioral features that characterize a biometric profile for an identity authentication process. Generally, the haptic data captured during an individual interaction are very large (measured every few milliseconds) and with a high number of attributes (position, velocity, force, angular orientation of the end-effector and torque data, among others). Therefore, the behavioral haptic data that describe users are defined in terms of a large number of features, which adds complexity to the analysis. It is desirable to find a 3D virtual representation that shows the similarity of a user's haptic data during different trials, as well as the existing relationship among other users' features. In this paper, through this approach and with data collected from a multimodal system for signing a virtual cheque, interesting patterns and relationships, which are somewhat hidden in the original data, are described for individual authentication.","2324-7355","978-1-4244-2005-6","10.1109/HAPTICS.2008.4479992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479992","Biometrics;Data Visualization;Haptics;Visual Data Mining;Pattern Recognition;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality;I.5 [Computing Methodologies]: Pattern Recognition;E.0 [Data]: General;I.1 [Computing Methodologies]: Symbolic and Algebraic Manipulation","Data visualization;Humans;Haptic interfaces;Authentication;Force measurement;Data mining;Feature extraction;Biometrics;Position measurement;Torque measurement","digital signatures;end effectors;haptic interfaces;virtual reality","human behavioral features;signature haptic data;haptic technology;biometric profile;identity authentication process;end-effector;torque data;3D virtual representation;multimodal system;virtual cheque","","5","","21","","31 Mar 2008","","","IEEE","IEEE Conferences"
"Learning 6-DOF Grasping Interaction via Deep Geometry-Aware 3D Representations","X. Yan; J. Hsu; M. Khansari; Y. Bai; A. Pathak; A. Gupta; J. Davidson; H. Lee",Universitv of Michigan during internship with Goozle Brain; Google; X Inc.; X Inc.; Google; Google; Google; Google,"2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","3766","3773","This paper focuses on the problem of learning 6- DOF grasping with a parallel jaw gripper in simulation. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. We introduce a deep geometry-aware grasping network (DGGN) that decomposes the learning into two steps. First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10% relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460609","","Grasping;Three-dimensional displays;Shape;Geometry;Solid modeling;Two dimensional displays;Robots","convolution;dexterous manipulators;geometry;grippers;image reconstruction;image representation;intelligent robots;learning (artificial intelligence);optimisation;recurrent neural nets;virtual reality","outcome prediction model;3D shape modeling;DGGN;analysis-by-synthesis optimization;6-DOF grasping net;virtual reality;sensory annotations;data augmentation strategy;CNN;3D occupancy grid;mental geometry-aware representation;deep geometry-aware grasping network;3D geometry prediction;grasping interaction learning;parallel jaw gripper;RGBD input;internal geometry-aware representation","","15","","43","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Realistic haptic rendering of interacting deformable objects in virtual environments","C. Duriez; F. Dubois; A. Kheddar; C. Andriot","CIMIT Simulation Group, Cambridge, MA, USA; NA; NA; NA","IEEE Transactions on Visualization and Computer Graphics","21 Nov 2005","2006","12","1","36","47","A new computer haptics algorithm to be used in general interactive manipulations of deformable virtual objects is presented. In multimodal interactive simulations, haptic feedback computation often comes from contact forces. Subsequently, the fidelity of haptic rendering depends significantly on contact space modeling. Contact and friction laws between deformable models are often simplified in up to date methods. They do not allow a ""realistic"" rendering of the subtleties of contact space physical phenomena (such as slip and stick effects due to friction or mechanical coupling between contacts). In this paper, we use Signorini's contact law and Coulomb's friction law as a computer haptics basis. Real-time performance is made possible thanks to a linearization of the behavior in the contact space, formulated as the so-called Delassus operator, and iteratively solved by a Gauss-Seidel type algorithm. Dynamic deformation uses corotational global formulation to obtain the Delassus operator in which the mass and stiffness ratio are dissociated from the simulation time step. This last point is crucial to keep stable haptic feedback. This global approach has been packaged, implemented, and tested. Stable and realistic 6D haptic feedback is demonstrated through a clipping task experiment.","1941-0506","","10.1109/TVCG.2006.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541998","Computer hatics;Signorini's law;Coulomb's friction law;corotational deformable objects;Delassus operator;Gauss-Seidel type resolution;real-time simulation.","Haptic interfaces;Virtual environment;Friction;Deformable models;Computational modeling;Force feedback;Extraterrestrial phenomena;Gaussian processes;Iterative algorithms;Packaging","virtual reality;haptic interfaces;graphical user interfaces;rendering (computer graphics);realistic images;force feedback;iterative methods","realistic haptic rendering;deformable object interaction;virtual environments;haptic feedback computation;contact space modeling;Signorini contact law;Coulomb friction law;Delassus operator;Gauss-Seidel type algorithm","Algorithms;Computer Graphics;Computer Peripherals;Computer Simulation;Computer Systems;Elasticity;Environment, Controlled;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models, Biological;Signal Processing, Computer-Assisted;Stress, Mechanical;Touch;User-Computer Interface","141","1","35","","21 Nov 2005","","","IEEE","IEEE Journals"
"Multi Degree-of-Freedom Input-to-State Stable approach for stable haptic interaction","A. Jafari; M. Nabeel; J. Ryu","School of Mechanical Engineering, Korea University of Technology and Education, Cheonan-city, South Korea; School of Mechanical Engineering, Korea University of Technology and Education, Cheonan-city, South Korea; School of Mechanical Engineering, Korea University of Technology and Education, Cheonan-city, South Korea","2015 IEEE World Haptics Conference (WHC)","6 Aug 2015","2015","","","293","298","Passivity has been the most often used constraint for the controller design of haptic interfaces. However, the designed controller based on passivity constraint has been suffering from its conservatism, especially when the user wants to increase the maximum achievable impedance. To overcome this problem, our group have proposed Input-to-State Stable (ISS) approach [1], which reduce the design conservatism of the passivity-based controller by allowing bigger output energy from the haptic interface compared with the passivity-based controller while guaranteeing the stability. However, the previous paper was limited to single Degree-of-Freedom (DoF) systems. This paper extends the ISS approach for multi-DoF haptic interaction. For multi-DoF haptic interaction, penetration depth-based rendering method using Virtual Proxy (VP) is adopted, and VP allows us to decouple the interaction into each axis. Although the interaction can be decoupled, previous ISS analysis “cannot” be directly implemented because the decoupled system, unlike to the previous case, has unconstrained end point, that is a moving Virtual Environment (VE). To include the moving VE into the ISS approach, we extend the previous one-port ISS approach to two-port ISS approach, and generalize this into multi-DoF ISS approach by augmenting each two-port analysis. Proposed approach is experimentally verified with Phantom Pre. 1.5, and showed the effectiveness of the proposed multi-DoF ISS approach.","","978-1-4799-6624-0","10.1109/WHC.2015.7177728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177728","","Haptic interfaces;Virtual environments;Force;Hysteresis;Probes;Stability analysis;Rendering (computer graphics)","haptic interfaces;rendering (computer graphics);virtual reality","multidegree-of-freedom input-to-state stable approach;stable haptic interaction;controller design;haptic interfaces;passivity constraint;input-to-state stable approach;design conservatism;passivity-based controller;output energy;single degree-of-freedom systems;multiDoF haptic interaction;penetration depth-based rendering method;virtual proxy;VP;ISS analysis;virtual environment;VE;two-port ISS approach;two-port analysis;Phantom Pre 1.5","","3","","9","","6 Aug 2015","","","IEEE","IEEE Conferences"
"A two-port framework for the design of unconditionally stable haptic interfaces","R. J. Adams; B. Hannaford","Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA","Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)","6 Aug 2002","1998","2","","1254","1259 vol.2","A haptic interface is a kinesthetic link between a human operator and a virtual environment. This paper addresses stability and performance issues associated with haptic interaction. It generalizes and extends the concept of a virtual coupling network, an artificial connection between a haptic display and a virtual world, to include both the impedance and admittance models of haptic interaction. A benchmark example exposes an important duality between these two cases. Linear circuit theory is used to develop necessary and sufficient conditions for the stability of a haptic simulation, assuming the human operator and virtual environment are passive. These equations lead to an explicit design procedure for virtual coupling networks which give maximum performance while guaranteeing stability. By decoupling the haptic display control problem from the design of virtual environments, the use of a virtual coupling network frees the developer of haptic-enabled virtual reality models from issues of mechanical stability.","","0-7803-4465-0","10.1109/IROS.1998.727471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=727471","","Haptic interfaces;Virtual environment;Humans;Coupling circuits;Displays;Circuit stability;Impedance;Admittance;Linear circuits;Sufficient conditions","virtual reality;haptic interfaces;two-port networks;stability","two-port framework;unconditionally stable haptic interface design;kinesthetic link;virtual environment;haptic interaction;virtual coupling network;haptic display;impedance model;admittance model;duality;linear circuit theory;necessary and sufficient conditions;human operator;virtual coupling networks;mechanical stability","","55","2","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"AirTouch: Interacting with computer systems at a distance","D. R. Schlegel; A. Y. C. Chen; C. Xiong; J. A. Delmerico; J. J. Corso","Dept. of Computer Science and Engineering, SUNY at Buffalo; Dept. of Computer Science and Engineering, SUNY at Buffalo; Dept. of Computer Science and Engineering, SUNY at Buffalo; Dept. of Computer Science and Engineering, SUNY at Buffalo; Dept. of Computer Science and Engineering, SUNY at Buffalo","2011 IEEE Workshop on Applications of Computer Vision (WACV)","10 Feb 2011","2011","","","1","8","We present AirTouch, a new vision-based interaction system. AirTouch uses computer vision techniques to extend commonly used interaction metaphors, such as multitouch screens, yet removes any need to physically touch the display. The user interacts with a virtual plane that rests in between the user and the display. On this plane, hands and fingers are tracked and gestures are recognized in a manner similar to a multitouch surface. Many of the other vision and gesture-based human-computer interaction systems presented in the literature have been limited by requirements that users do not leave the frame or do not perform gestures accidentally, as well as by cost or specialized equipment. AirTouch does not suffer from these drawbacks. Instead, it is robust, easy to use, builds on a familiar interaction paradigm, and can be implemented using a single camera with off-the-shelf equipment such as a webcam-enabled laptop. In order to maintain usability and accessibility while minimizing cost, we present a set of basic AirTouch guidelines. We have developed two interfaces using these guidelines-one for general computer interaction, and one for searching an image database. We present the workings of these systems along with observational results regarding their usability.","1550-5790","978-1-4244-9497-2","10.1109/WACV.2011.5711476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711476","","Cameras;Computers;Histograms;Mice;Calibration;Human computer interaction;Tracking","computer vision;gesture recognition;human computer interaction","AirTouch;computer systems;vision-based interaction system;computer vision;interaction metaphors;multitouch screens;gesture recognition;human-computer interaction systems","","4","2","25","","10 Feb 2011","","","IEEE","IEEE Conferences"
"The Use of Agent-Based Models As Non-Player Characters in Serious Games","D. Babichenko; P. Healy; M. Gomez; S. Kane-Gill; E. B. Littleton; P. Brusilovsky; P. Cohen; R. Patel","School of Computing and Information, University of Pittsburgh,Pittsburgh,PA; School of Computing and Information, University of Pittsburgh,Pittsburgh,PA; School of Computing and Information, University of Pittsburgh,Pittsburgh,PA; School of Pharmacy, University of Pittsburgh,Pittsburgh,PA; School of Medicine, University of Pittsburgh,Department of Surgery,Pittsburgh,PA; School of Computing and Information, University of Pittsburgh,Pittsburgh,PA; School of Computing and Information, University of Pittsburgh,Pittsburgh,PA; School of Pharmacy, University of Pittsburgh,Pittsburgh,PA","2020 IEEE 8th International Conference on Serious Games and Applications for Health (SeGAH)","22 Sep 2020","2020","","","1","8","One of the shortcomings of many modern serious games and medical simulations lies in their inability to model even some modicum of unpredictability of real life situations. Interactions with a standardized patient may teach healthcare professional students how to diagnose a clinical condition, better manage a patient, or help them improve their bedside manners, but such simulated interactions will not prepare the learners to deal with unpredictability of clinical situations, interruption, and task switching. Distractions occur from colleagues, clinical decision support alerts, pagers, smartphones, or audible alarms. All these interruptions can potentially alter the course of patient care and the outcome of a patient's treatment. A simulated virtual patient (VP) may teach critical thinking skills, but once a student has successfully diagnosed a VP, the simulation stops providing educational value. In this paper we propose a generalizable method for integrating agent-based models into serious games and simulations. In the proposed paradigm, a human player (learner) takes on the role of a single agent in the model (e.g, a healthcare professional), while the output of the model controls the environment, the rules of agent interactions, and all the other agents that the human player interacts with (non-player characters). Moreover, we will present two use cases demonstrating that the use of agent-based models as behavior controllers for non-player characters introduces a degree of unpredictability in a virtual patient simulation and in a serious game designed to teach middle and high-school students about the spread of infectious diseases.","2573-3060","978-1-7281-9042-6","10.1109/SeGAH49190.2020.9201889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201889","virtual patients;simulations;serious games;non-player characters;agent-based models","Games;Medical services;Artificial intelligence;Task analysis;Computational modeling;Switches;Biological neural networks","computer aided instruction;decision support systems;diseases;health care;medical computing;medical information systems;multi-agent systems;patient care;serious games (computing);teaching;virtual reality","agent-based models;nonplayer characters;serious game;medical simulations;standardized patient;healthcare professional students;clinical condition;simulated interactions;clinical decision support;patient care;simulated virtual patient;virtual patient simulation","","","","64","","22 Sep 2020","","","IEEE","IEEE Conferences"
"Influence of visual information on bimanual haptic manipulation","S. Contu; C. Hughes; L. Masia","School of Mechanical & Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical & Aerospace Engineering, Nanyang Technological University, Singapore; School of Mechanical & Aerospace Engineering, Nanyang Technological University, Singapore","2015 IEEE International Conference on Rehabilitation Robotics (ICORR)","1 Oct 2015","2015","","","961","966","The coordination of the upper limbs has been shown to be beneficial for post-stroke treatment. In virtual reality based rehabilitation, bimanual exercises can be performed by exploiting haptic rendering techniques that allow object manipulation with two haptic devices. Haptic interaction generally involves spherical end-effectors with invariant shapes. Furthermore, the position of the end-effectors can only be sensed haptically after object contact, which impacts the ability to determine the real position of the end effector and dynamically manipulate the object. The present study sought to examine whether additional visual information regarding the penetration of the wrists into the virtual object (i.e., the color and shape of the spheres changed according to the level of force exerted by the subject) leads to improved bimanual task performance in a virtual environment. To this end, six neurologically healthy participants performed an object manipulation task with haptic feedback (haptic condition) and with haptic feedback as well as additional visual cues (haptic+visual condition). Results demonstrated that interlimb coordination was enhanced during the haptic+visual condition. It is speculated that the presence of visual information provides a more natural way for individuals to exploit inter-limb coordination synergies, and may have useful implications for VR game development and post stroke rehabilitation protocols.","1945-7901","978-1-4799-1808-9","10.1109/ICORR.2015.7281328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281328","","Haptic interfaces;Visualization;Wrist;Force;Virtual environments;Hip;Robots","artificial limbs;end effectors;medical computing;medical robotics;patient rehabilitation;rendering (computer graphics);virtual reality","visual information;bimanual haptic manipulation;upper limb coordination;post-stroke treatment;virtual reality based rehabilitation;bimanual exercises;haptic rendering;haptic interaction;spherical end-effectors;virtual environment;post stroke rehabilitation protocols;VR game development;interlimb coordination synergies;haptic feedback;object manipulation task","","3","","26","","1 Oct 2015","","","IEEE","IEEE Conferences"
"A performance evaluation of Internet access via the General Packet Radio Service of GSM","S. Hoff; M. Meyer; A. Schieder","Ericsson Eurolab Deutschland, Herzogenrath, Germany; NA; NA","VTC '98. 48th IEEE Vehicular Technology Conference. Pathway to Global Wireless Revolution (Cat. No.98CH36151)","6 Aug 2002","1998","3","","1760","1764 vol.3","The General Packet Radio Service (GPRS) is currently being standardized by the European Telecommunications Standard Institute (ETSI) to extend the services provided by the Global System for Mobile Communications (GSM). The GPRS is dedicated to support packet-oriented traffic, e.g. Internet traffic. Packet oriented transmission is by nature better suited to convey bursty traffic, as it is generated by Internet applications. Applications contributing most to the data volume in the Internet are WWW, FTP and e-mail. This paper presents a simulator, which was developed to assess the performance of the GPRS when used as a means of access to the Internet. The focus is on the evaluation of the interaction between the protocols applied in the Internet and for the GPRS. Only by considering both parts is the assessment of the overall performance, which will be recognized by the end-user, possible. This paper presents the simulation environment and discusses the simulation results regarding the end-to-end performance of TCP/IP based applications. It is shown that the GPRS is a suitable wireless access to Internet applications.","1090-3038","0-7803-4320-4","10.1109/VETEC.1998.686058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=686058","","Internet;Ground penetrating radar;Telecommunication traffic;Traffic control;Telecommunication standards;GSM;Packet radio networks;World Wide Web;Electronic mail;Access protocols","cellular radio;packet radio networks;Internet;telecommunication traffic;transport protocols;data communication;digital simulation","performance evaluation;Internet access;General Packet Radio Service;GSM;European Telecommunications Standard Institute;ETSI;Global System for Mobile Communications;packet-oriented traffic;Internet traffic;packet oriented transmission;bursty traffic;WWW;FTP;e-mail;TCP/IP;wireless access","","12","1","10","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Interconection of virtual environments with mechatronic systems","L. -C. Bazavan; H. Roibu; F. Besnea Petcu; S. Irinel Cismaru; B. N. George","University of Craiova,Faculty of Control, Computers and Electronics,Mechatronics and Robotics Department,Craiova,Romania; University of Craiova,Faculty of Control, Computers and Electronics,Mechatronics and Robotics Department,Craiova,Romania; University of Craiova,Faculty of Control, Computers and Electronics,Mechatronics and Robotics Department,Craiova,Romania; University of Craiova,Faculty of Control, Computers and Electronics,Mechatronics and Robotics Department,Craiova,Romania; University of Craiova,Faculty of Control, Computers and Electronics,Mechatronics and Robotics Department,Craiova,Romania","2020 24th International Conference on System Theory, Control and Computing (ICSTCC)","23 Nov 2020","2020","","","654","659","The development of both hardware and software technology in recent years has led to the creation of complex virtual environments and boosted the desire of both users and developers to make them as immersive as possible. To achieve this immersion, virtual reality applications must interact with physical systems, that generally consist of command and control electronics, actuators and a sensory system. This paper presents a series of experiments performed by the authors in order to determine three major steps in the process of interaction between hardware systems and software emulating virtual environments: transpose the events from Unity virtual environment into signals necessary to control a physical system, transmission of the signals generated by the sensory system and, interpretation and conversion of those signals into events for the virtual reality application. Because interconnection involves both receiving and transmitting signals, the authors propose a two-way control interface implemented on different development boards. At the same time, the experiments aimed to observe the response of the physical systems and to determine an algorithm for correlating the action of the physical system, with a proportionality factor of one to one, related to the events that occurred in the virtual reality.","2372-1618","978-1-7281-9809-5","10.1109/ICSTCC50638.2020.9259668","European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259668","Virtual environment;Unity;Control;Communication","Hardware;Games;Virtual environments;Software;Microcontrollers;Three-dimensional displays;Mechatronics","control engineering computing;mechatronics;signal processing;user interfaces;virtual reality","complex virtual environments;virtual reality application;command and control electronics;sensory system;hardware systems;Unity virtual environment;mechatronic systems;actuators;physical system control;signal transmission;two-way control interface","","","","9","","23 Nov 2020","","","IEEE","IEEE Conferences"
"A framework for interaction of distributed autonomous systems and human supervisors","H. J. Spoelder; D. M. Germans; L. Renambot; H. E. Bal; P. J. de Waal; F. C. A. Groen","Fac. of Sci., Vrije Univ., Amsterdam, Netherlands; Fac. of Sci., Vrije Univ., Amsterdam, Netherlands; Fac. of Sci., Vrije Univ., Amsterdam, Netherlands; Fac. of Sci., Vrije Univ., Amsterdam, Netherlands; NA; NA","IEEE Transactions on Instrumentation and Measurement","18 Apr 2005","2002","51","4","798","803","Semi-autonomous systems are rapidly gaining importance in a large number of situations relevant to the general public. By their nature, their need for external control is low but still necessary. In this paper, we present a framework for interaction of distributed autonomous systems and human supervisors. This framework exploits progress made in several related areas and shows that they can be effectively combined into one single framework. To this end, it combines an environment for computational steering with virtual reality techniques for visualization and WAP-based communication for ubiquitous intervention. Given the current state for the technology, the current version must be regarded as a proof of principle.","1557-9662","","10.1109/TIM.2002.803400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044749","","Humans;Wireless application protocol;Extraterrestrial measurements;Internet;Mobile handsets;Virtual reality;Anthropometry;Pervasive computing;Visualization;Safety","virtual reality;interactive systems;distributed processing;measurement systems;Internet;protocols","distributed autonomous systems;human supervisors;virtual reality;visualization;WAP-based communication;human in the loop;wireless application protocol;Internet-based systems;CAVE system;distributed measurements","","1","","8","","18 Apr 2005","","","IEEE","IEEE Journals"
"Design and evaluation of a rapid programming system for service robots","J. Huang; T. Lau; M. Cakmak","Computer Science & Engineering, University of Washington, Seattle, United States; Savioke, Inc., Santa Clara, CA, United States; Computer Science & Engineering, University of Washington, Seattle, United States","2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","14 Apr 2016","2016","","","295","302","This paper introduces CustomPrograms, a rapid programming system for mobile service robots. With CustomPrograms, roboticists can quickly create new behaviors and try unexplored use cases for commercialization. In our system, the robot has a set of primitive capabilities, such as navigating to a location or interacting with users on a touch screen. Users can then compose these primitives with general-purpose programming language constructs like variables, loops, conditionals, and functions. The programming language is wrapped in a graphical interface. This allows inexperienced or novice programmers to benefit from the system as well. We describe the design and implementation of CustomPrograms on a Savioke Relay robot in detail. Based on interviews conducted with Savioke roboticists, designers, and business people, we learned of several potential new use cases for the robot. We characterize our system's ability to fulfill these use cases. Additionally, we conducted a user study of the interface with Savioke employees and outside programmers. We found that experienced programmers could learn to use the interface and create 3 real-world programs during the 90 minute study. Inexperienced programmers were less likely to create complex programs correctly. We provide an analysis of the errors made during the study, and highlight the most common pieces of feedback we received. Two case studies show how the system was used internally at Savioke and at a major trade show.","2167-2148","978-1-4673-8370-7","10.1109/HRI.2016.7451765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451765","","Programming profession;Relays;Computer languages;Visualization;Service robots","graphical user interfaces;human-robot interaction;mobile robots;programming languages;robot programming;service robots;touch sensitive screens","rapid programming system;CustomPrograms;mobile service robots;touch screen;general-purpose programming language;graphical interface;Savioke Relay robot","","22","","28","","14 Apr 2016","","","IEEE","IEEE Conferences"
"Analysis of perceived workload when using a PDA for mobile robot teleoperation","J. A. Adams; H. Kaymaz-Keskinpala","EECS Dept., Vanderbilt Univ., Nashville, TN, USA; EECS Dept., Vanderbilt Univ., Nashville, TN, USA","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","4","","4128","4133 Vol.4","A Personal Digital Assistant (PDA) based interface has been developed to provide teleoperation of a mobile robot. The interface provides three different screen designs, all of which employ touch (finger) based interaction rather than stylus based interaction. The interface provides general interaction capabilities for driving the robot based upon the information display. The Vision-only screen provides the forward facing camera image, the Sensory-only screen provides the on-board ultrasonic sensors and laser range finder information, while the Vision with sensory overlay screen integrates all three data sets. A user evaluation was conducted to evaluate the usability and perceived workload required for each screen. Thirty participants completed the quantitative evaluation. The focus of this paper is the obtained perceived workload results.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1308919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1308919","","Mobile robots;Personal digital assistants;Fingers;Robot sensing systems;Displays;Robot vision systems;Cameras;Image sensors;Usability;Focusing","telerobotics;mobile robots;notebook computers;man-machine systems;user interfaces;robot vision;touch sensitive screens","perceived workload analysis;PDA;personal digital assistant;mobile robot teleoperation;teleoperation interface;finger touch based interaction;stylus based interaction;robot driving;information display;vision only screen;forward facing camera image;sensory only screen;onboard ultrasonic sensors;laser range finder;sensory overlay screen;quantitative evaluation","","11","1","18","","6 Jul 2004","","","IEEE","IEEE Conferences"
"Open Affordable Mixed Reality: A Manifesto","E. Bran; E. Bautu; D. M. Popovici","Ovidius Universiy,Department of Mathematics and Computer Science,Constanta,Romania; Ovidius Universiy,Department of Mathematics and Computer Science,Constanta,Romania; Ovidius Universiy,Department of Mathematics and Computer Science,Constanta,Romania","2020 International Conference on Development and Application Systems (DAS)","5 Jun 2020","2020","","","177","184","This article describes the main traits of a proposed Mixed Reality (MR) platform designed to be used by the general public in order to create and share MR experiences. The purpose of this MR Platform is to provide interaction with the Internet of Everything (IoE). We embrace the Ubiquitous computing paradigm, where services follow the user seamlessly across physical locations. An emphasis is made on context awareness, which is central in designing a MR application, by anchoring the right multimodal content to the right circumstance. We aim at providing an architecture which splits awareness into four levels with respect to knowledge representation, in order to provide a tool for non-programmers to design applications.","","978-1-7281-6870-8","10.1109/DAS49615.2020.9108963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108963","Ubiquitous computing;Internet of Things;Augmented reality;Knowledge representation;Information services;Social Networks","","Internet of Things;mobile computing;virtual reality","IoE;ubiquitous computing paradigm;context awareness;multimodal content;mixed reality platform;MR platform;Internet of Everything","","","","30","","5 Jun 2020","","","IEEE","IEEE Conferences"
"Real-time visually guided human figure control using IK-based motion synthesis","S. Yonemoto; D. Arita; R. -. Taniguchi","Dept. of Intelligent Syst., Kyushu Univ., Fukuoka, Japan; NA; NA","Proceedings Fifth IEEE Workshop on Applications of Computer Vision","6 Aug 2002","2000","","","194","200","This paper presents a real-time human figure motion control method using color blob tracking, and human motion synthesis based on real-time inverse kinematics. Our purpose is to do seamless mapping of human motion in the real world into virtual environments. In general, virtual environment applications such as man-machine 'smart' interaction require real-time human full-body motion capturing systems without special devices or markers. However, since such vision-based human motion capturing systems are essentially unstable and can only acquire partial information because of self-occlusion, we have to introduce a robust pose estimation strategy, or an appropriate human motion synthesis based on motion filtering. In this paper, we have demonstrated a real-time and online real-virtual interaction system which realizes human full-body motion capturing and synthesis.","","0-7695-0813-8","10.1109/WACV.2000.895422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895422","","Humans;Virtual environment;Real time systems;Motion control;Tracking;Control system synthesis;Kinematics;Man machine systems;Robustness;Motion estimation","virtual reality;motion estimation","human figure control;IK-based motion synthesis;color blob tracking;human motion synthesis;inverse kinematics;seamless mapping;virtual environment;pose estimation;motion filtering","","4","","10","","6 Aug 2002","","","IEEE","IEEE Conferences"
