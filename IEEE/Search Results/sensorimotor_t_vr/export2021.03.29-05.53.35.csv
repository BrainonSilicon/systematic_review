"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Model for Sensorimotor Affordances in Virtual Reality Environments","U. Meyer; S. Draheim; K. von Luck","CSTI, Hamburg University of Applied Sciences, Hamburg, Germany; CSTI, Hamburg University of Applied Sciences, Hamburg, Germany; CSTI, Hamburg University of Applied Sciences, Hamburg, Germany","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","14 Oct 2019","2019","","","1","4","Study results on virtual reality (VR) environment properties and their impact on presence have been contradictory. And due to the media specificity of VR, which includes place illusion, rules for environment design cannot directly be transferred from 3D computer games or stereoscopic film to VR. This study develops a model for VR environments based on Mel Slater's observation that place illusion in VR is caused by the use of sensorimotor contingencies (SMC). It defines properties of the environment which provide action possibilities for SMC as sensorimotor affordances (SMA). SMC and SMA form a bidirectional feedback loop of perception. This model helps to clarify former contradictory study results on VR environments, and also provides the basis for a framework of preceptual design rules for VR environments.","2474-0489","978-1-7281-4540-2","10.1109/VS-Games.2019.8864514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864514","Virtual reality;virtual environment;illusion;sensorimotor contingencies;affordances;perceptual design","Robot sensing systems;Optical sensors;Optical feedback;Real-time systems;Media;Rendering (computer graphics);Optical imaging","social aspects of automation;virtual reality","sensorimotor affordances;virtual reality environments;place illusion;sensorimotor contingencies;VR media specificity;sensory distortion","","","","20","","14 Oct 2019","","","IEEE","IEEE Conferences"
"Extracting space dimension information from the auditory modality sensori-motor flow using a bio-inspired model of the cochlea","C. Couverture; B. Gas","ISIR laboratory, UPMC Univ Paris 06, 4, place Jussieu, 75005, France; ISIR laboratory, UPMC Univ Paris 06, 4, place Jussieu, 75005, France","2009 IEEE/RSJ International Conference on Intelligent Robots and Systems","15 Dec 2009","2009","","","2742","2747","First task robots have to realise is sensing and acting in the environment. Can a robot learn the way it is able to sense and act in the world without any hardwired notions? Is it able to learn it from the only data he has access to, that is high-dimension sensory inputs and motor outputs? This paper presents experimental results obtained on a simulated human listener using a bio-inspired model of the cochlea and real records from human related transfer functions (HRTF). These results show that a naive system that interacts with its environment without knowing the laws governing these interactions can discover information about dimensionality of space. Moreover, the laws determining the sensations of the system as a function of the state of the system and the environment, called the ¿sensorimotor law¿, are not simplified as usually in simulations. They are bio-realistic as they are determined by the HRTF recorded on human beings.","2153-0866","978-1-4244-3803-7","10.1109/IROS.2009.5354100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5354100","sensorimotor contingencies;auditive sensorimotor flow;cochlea;space dimension","Data mining;Robot sensing systems;Orbital robotics;Humans;Mobile robots;Organisms;Intelligent robots;Navigation;Laboratories;USA Councils","auditory evoked potentials;biomechanics;ear;medical robotics;neurophysiology;physiological models","space dimension;auditory modality;sensori-motor flow;bio-inspired model;cochlea;simulated human listener;human related transfer functions","","3","","12","","15 Dec 2009","","","IEEE","IEEE Conferences"
"Towards the grounding of abstract words: A Neural Network model for cognitive robots","F. Stramandinoli; A. Cangelosi; D. Marocco","School of Computing and Mathematics, University of Plymouth, United Kingdom; School of Computing and Mathematics, University of Plymouth, United Kingdom; School of Computing and Mathematics, University of Plymouth, United Kingdom","The 2011 International Joint Conference on Neural Networks","3 Oct 2011","2011","","","467","474","In this paper, a model based on Artificial Neural Networks (ANNs) extends the symbol grounding mechanism to abstract words for cognitive robots. The aim of this work is to obtain a semantic representation of abstract concepts through the grounding in sensorimotor experiences for a humanoid robotic platform. Simulation experiments have been developed on a software environment for the iCub robot. Words that express general actions with a sensorimotor component are first taught to the simulated robot. During the training stage the robot first learns to perform a set of basic action primitives through the mechanism of direct grounding. Subsequently, the grounding of action primitives, acquired via direct sensorimotor experience, is transferred to higher-order words via linguistic descriptions. The idea is that by combining words grounded in sensorimotor experience the simulated robot can acquire more abstract concepts. The experiments aim to teach the robot the meaning of abstract words by making it experience sensorimotor actions. The iCub humanoid robot will be used for testing experiments on a real robotic architecture.","2161-4407","978-1-4244-9637-2","10.1109/IJCNN.2011.6033258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033258","","Robot sensing systems;Grounding;Training;Pragmatics;Semantics;Biological neural networks","cognitive systems;humanoid robots;intelligent robots;mobile robots;neural nets;sensors;software engineering;word processing","abstract word;cognitive robot;artificial neural network;semantic representation;sensorimotor experience;software environment;simulated robot;higher-order word;linguistic description;iCub humanoid robot","","6","","35","","3 Oct 2011","","","IEEE","IEEE Conferences"
"Gibson Env: Real-World Perception for Embodied Agents","F. Xia; A. R. Zamir; Z. He; A. Sax; J. Malik; S. Savarese","Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Stanford Univ., Stanford, CA, USA","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","9068","9079","Developing visual perception models for active agents and sensorimotor control in the physical world are cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we investigate developing real-world perception for active agents, propose Gibson Environment for this purpose, and showcase a set of perceptual tasks learned therein. Gibson is based upon virtualizing real spaces, rather than artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism ""Goggles"" enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579043","","Rendering (computer graphics);Cameras;Neural networks;Three-dimensional displays;Visualization;Physics;Robot sensing systems","learning (artificial intelligence);multi-agent systems;rendering (computer graphics);robot vision;virtual reality","physical world;robots;learning-in-simulation;real-world perception;active agents;Gibson Environment;visual perception models;sensorimotor control;floor spaces;perceptual task learning;space virtualization;semantic complexity;Goggles internal synthesis mechanism","","48","","100","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Human sit-to-stand transfer modeling for optimal control of assistive robots","M. Geravand; P. Z. Korondi; A. Peer","Institute of Automatic Control Engineering, Technische Universita¨t Mu¨nchen, Munich, Germany; Institute of Automatic Control Engineering, Technische Universita¨t Mu¨nchen, Munich, Germany; Institute of Automatic Control Engineering, Technische Universita¨t Mu¨nchen, Munich, Germany","5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics","2 Oct 2014","2014","","","670","676","Sit-to-stand (STS) transfers are a common human task which involves very complex sensorimotor processes to control the highly nonlinear musculoskeletal system. In this paper, typical unassisted and assisted human STS transfers are formulated as optimal feedback control problem that finds a compromise between task end-point accuracy, human balance, jerk, effort, and torque change and takes further human biomechanical control constraints into account. Differential dynamic programming is employed, which allows taking the full, nonlinear human dynamics into consideration. The biomechanical dynamics of the human is modeled by a six link rigid body including leg, trunk and arm segments. Accuracy of the proposed modelling approach is evaluated for different human healthy subjects by comparing simulations and experimentally collected data. Acceptable model accuracy is achieved with a generic set of constant weights that prioritize the different criteria. The proposed STS model is finally used to determine optimal assistive strategies to be performed by a robotic mobility assistant suitable for either a person with specific body segment weakness or a more general weakness.","2155-1782","978-1-4799-3128-6","10.1109/BIOROB.2014.6913855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913855","","Biological system modeling;Joints;Trajectory;Data models;Torque;Cost function;Biomechanics","assisted living;feedback;gait analysis;handicapped aids;human-robot interaction;medical robotics;mobile robots;nonlinear dynamical systems;optimal control;robot dynamics;service robots;torque control","human sit-to-stand transfer modeling;assistive robots;very complex sensorimotor processes;nonlinear musculoskeletal system;unassisted human STS transfer;assisted human STS transfer;optimal feedback control problem;task end-point accuracy;human balance;jerk;effort;torque change;human biomechanical control constraints;differential dynamic programming;nonlinear human dynamics;human biomechanical dynamics;rigid body;robotic mobility assistant","","2","","27","","2 Oct 2014","","","IEEE","IEEE Conferences"
"Towards improving incremental learning of manipulator kinematics with inter-robot knowledge transfer","N. Makondo; B. Rosman","Mobile Intelligent Autonomous Systems, Council for Scientific and Industrial Research (CSIR), Pretoria, South Africa; Mobile Intelligent Autonomous Systems, Council for Scientific and Industrial Research (CSIR), Pretoria, South Africa","2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)","2 May 2019","2019","","","68","73","This paper investigates the improvement of learning sensorimotor models for developmental robots, in particular robot arm kinematics models, with inter-robot knowledge transfer. Developmental robots progressively learn through embodied interaction with the physical environment. In the single-robot case, exploration in the world is performed in isolation and the robot explores its own capabilities. In a multi-robot case, with one or more experienced robots, we argue that it may be beneficial for the robots to be able to share the knowledge they have acquired through their individual exploration. We explore knowledge transfer in the context of learning arm kinematics models, where an experienced robot shares its kinematic data with a new robot that is autonomously exploring its environment. We show that the sensorimotor models of the new robot can be bootstrapped by the shared knowledge, converge faster and also achieve a better asymptotic performance compared to individual exploration from scratch. We perform an analysis of knowledge transfer in simulation, ranging from simple two-link planar robots to redundant systems.","","978-1-7281-0369-3","10.1109/RoboMech.2019.8704838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704838","Model learning;Transfer learning;Robot kinematics;Robot learning;Developmental robots","Robot sensing systems;Kinematics;Data models;Space exploration;Knowledge transfer;Aerospace electronics","learning (artificial intelligence);manipulator kinematics;mobile robots;multi-robot systems;path planning","particular robot arm kinematics models;inter-robot knowledge transfer;single-robot case;multirobot case;experienced robots;shared knowledge;two-link planar robots;incremental learning;manipulator kinematics;learning sensorimotor models;redundant systems","","","","20","","2 May 2019","","","IEEE","IEEE Conferences"
"Spatially Separated Cutaneous Haptic Guidance for Training of a Virtual Sensorimotor Task","C. Smith; E. Pezent; M. K. O’Malley","Rice University,Mechatronics and Haptic Interfaces Laboratory,Houston,77005; Rice University,Mechatronics and Haptic Interfaces Laboratory,Houston,77005; Rice University,Mechatronics and Haptic Interfaces Laboratory,Houston,77005","2020 IEEE Haptics Symposium (HAPTICS)","7 May 2020","2020","","","974","979","Haptic devices enable multi-modal feedback to a user when training to perform novel motor skills in controlled, virtual environments. Haptic feedback has been proposed as a means to provide additional guidance cues that might improve training efficacy; however, recent studies have identified drawbacks to haptic guidance, including reliance on guidance forces and an inability to distinguish between forces that are part of the virtual environment and those that communicate task completion strategies. Recently, we proposed a novel approach to providing haptic guidance that separates task and guidance forces. We used a kinesthetic haptic interface to communicate task forces and a spatially separated tactile skin-stretch device to transmit guidance forces. Our experiments showed that feed-forward control using this paradigm was effective for improving performance in a trajectory following task. In this paper, we explore the potential for spatially separated cutaneous haptic guidance to train a user to optimally control an inverted pendulum system. We present and execute a task and training protocol designed to determine whether error-based haptic feedback provided cutaneously can accelerate learning of a task, and whether participants can retain or transfer task skills even after guidance is no longer present. We found that subject performance improved while spatially separated cutaneous haptic guidance was active. Despite this finding, performance in the pendulum balancing task was not affected once the haptic assistance was removed.","2324-7355","978-1-7281-0234-4","10.1109/HAPTICS45997.2020.ras.HAP20.11.2032900c","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086314","","","feedforward;haptic interfaces;pendulums;virtual reality","spatially separated cutaneous haptic guidance;virtual sensorimotor task;haptic devices;virtual environments;guidance cues;guidance forces;kinesthetic haptic interface;task forces;spatially separated tactile skin-stretch device;error-based haptic feedback;haptic assistance;multimodal feedback;feed-forward control;inverted pendulum system","","","","27","","7 May 2020","","","IEEE","IEEE Conferences"
"Empirical Movement Models for Brain Computer Interfaces","C. B. Matlack; H. J. Chizeck; C. T. Moritz","Electrical Engineering Department, University of Washington, Seattle, WA, USA; Electrical Engineering Department, University of Washington, Seattle, WA, USA; Departments of Rehabilitation Medicine and Physiology and Biophysics, University of Washington, Seattle, WA, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","20 Jun 2017","2017","25","6","694","703","For brain-computer interfaces (BCIs) which provide the user continuous position control, there is little standardization of performance metrics or evaluative tasks. One candidate metric is Fitts's law, which has been used to describe aimed movements across a range of computer interfaces, and has recently been applied to BCI tasks. Reviewing selected studies, we identify two basic problems with Fitts's law: its predictive performance is fragile, and the estimation of `information transfer rate' from the model is unsupported. Our main contribution is the adaptation and validation of an alternative model to Fitts's law in the BCI context. We show that the Shannon-Welford model outperforms Fitts's law, showing robust predictive power when target distance and width have disproportionate effects on difficulty. Building on a prior study of the Shannon-Welford model, we show that identified model parameters offer a novel approach to quantitatively assess the role of control-display gain in speed/accuracy performance tradeoffs during brain control.","1558-0210","","10.1109/TNSRE.2016.2584101","American Heart & Stroke Association Scientist Development; DARPA Young Faculty; Center for Sensorimotor Neural Engineering (CNSE), a National Science Foundation Engineering Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502071","Brain–computer interface (BCI);brain–machine interface (BMI);Fitts’s law;neural engineering;neural prosthesis;performance metric;Shannon–Welford","Measurement;Brain models;Trajectory;Predictive models;Computational modeling;Throughput","biomechanics;brain-computer interfaces;information theory;medical computing;neurophysiology;physiological models","empirical movement models;brain computer interfaces;Fitts law;information transfer rate;Shannon-Welford model;speed-accuracy performance;brain control","Animals;Brain Mapping;Brain-Computer Interfaces;Computer Simulation;Electroencephalography;Humans;Macaca nemestrina;Models, Neurological;Models, Statistical;Movement;Reproducibility of Results;Sensitivity and Specificity;Task Performance and Analysis;Visual Cortex;Visual Perception","","","29","","30 Jun 2016","","","IEEE","IEEE Journals"
"Feedback, Affordances, and Accelerators for Training Sports in Virtual Environments","E. Ruffaldi; B. Bardy; D. Gopher; M. Bergamasco","PERCRO Lab Scuola Superiore S. Anna 56100 Pisa, Italy; M2H EuroMov Montpellier University France; Technion Haifa, Israel; PERCRO Lab Scuola Superiore S. Anna 56100 Pisa, Italy","Presence","19 May 2014","2011","20","1","33","46","The use of virtual environments (VE) for training sports is quite natural when considering strategic or cognitive aspects. Using VE for sensorimotor training is more challenging, in particular with the difficulty of transferring the task learned in the virtual world to the real. Of special concern for the successful transfer is the adequate combination of training experience protocols and the delivery modes of multimodal feedback. Analyzing feedback in terms of information exchange, this work discusses different feedback combinations and their application to virtual reality training of rowing skills.","1054-7460","","10.1162/pres_a_00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797590","","","","","","5","","","","19 May 2014","","","MIT Press","MIT Press Journals"
"Unsupervised learning of predictive parts for cross-object grasp transfer","R. Detry; J. Piater","University of Liège, Belgium; University of Innsbruck, Austria","2013 IEEE/RSJ International Conference on Intelligent Robots and Systems","2 Jan 2014","2013","","","1720","1727","We present a principled solution to the problem of transferring grasps across objects. Our approach identifies, through autonomous exploration, the size and shape of object parts that consistently predict the applicability of a grasp across multiple objects. The robot can then use these parts to plan grasps onto novel objects. By contrast to most recent methods, we aim to solve the part-learning problem without the help of a human teacher. The robot collects training data autonomously by exploring different grasps on its own. The core principle of our approach is an intensive encoding of low-level sensorimotor uncertainty with probabilistic models, which allows the robot to generalize the noisy autonomously-generated grasps. Object shape, which is our main cue for predicting grasps, is encoded with surface densities, that model the spatial distribution of points that belong to an object's surface. Grasp parameters are modeled with grasp densities, that correspond to the spatial distribution of object-relative gripper poses that lead to a grasp. The size and shape of grasp-predicting parts are identified by sampling the cross-object correlation of local shape and grasp parameters. We approximate sampling and integrals via Monte Carlo methods to make our computer implementation tractable. We demonstrate the applicability of our method in simulation. A proof of concept on a real robot is also provided.","2153-0866","978-1-4673-6358-7","10.1109/IROS.2013.6696581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696581","","Shape;Computational modeling;Grasping;Training;Robot sensing systems;Visualization","control engineering computing;grippers;Monte Carlo methods;robot vision;shape recognition;unsupervised learning","unsupervised learning;predictive parts;cross-object grasp transfer;autonomous exploration;part-learning problem;human teacher;robot;training data;low-level sensorimotor uncertainty;probabilistic models;noisy autonomously-generated grasps;object shape;surface densities;spatial points distribution;grasp parameters;grasp densities;object-relative gripper;grasp-predicting parts;cross-object correlation;local shape parameters;grasp parameters;Monte Carlo methods;computer implementation","","8","","37","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Asynchronous BCI Based on Motor Imagery With Automated Calibration and Neurofeedback Training","R. Kus; D. Valbuena; J. Zygierewicz; T. Malechka; A. Graeser; P. Durka","Faculty of Physics, University of Warsaw, Warsaw, Poland; Institute of Automation, University of Bremen, Bremen, Germany; Faculty of Physics, University of Warsaw, Warsaw, Poland; Institute of Automation, University of Bremen and the Friedrich Wilhelm Bessel Institute m.b.H., Bremen, Germany; Institute of Automation, University of Bremen, Bremen, Germany; Faculty of Physics, University of Warsaw, Warsaw, Poland","IEEE Transactions on Neural Systems and Rehabilitation Engineering","15 Nov 2012","2012","20","6","823","835","A new multiclass brain-computer interface (BCI) based on the modulation of sensorimotor oscillations by imagining movements is described. By the application of advanced signal processing tools, statistics and machine learning, this BCI system offers: 1) asynchronous mode of operation, 2) automatic selection of user-dependent parameters based on an initial calibration, 3) incremental update of the classifier parameters from feedback data. The signal classification uses spatially filtered signals and is based on spectral power estimation computed in individualized frequency bands, which are automatically identified by a specially tailored AR-based model. Relevant features are chosen by a criterion based on Mutual Information. Final recognition of motor imagery is effectuated by a multinomial logistic regression classifier. This BCI system was evaluated in two studies. In the first study, five participants trained the ability to imagine of the right hand, left hand and feet in response to visual cues. The accuracy of the classifier was evaluated across four training sessions with feedback. The second study assessed the information transfer rate (ITR) of the BCI in an asynchronous application. The subjects' task was to navigate a cursor along a computer rendered 2-D maze. A peak information transfer rate of 8.0 bit/min was achieved. Five subjects performed with a mean ITR of 4.5 bit/min and an accuracy of 74.84%. These results demonstrate that the use of automated interfaces to reduce complexity for the intended operator (outside the laboratory) is indeed possible. The signal processing and classifier source code embedded in BCI2000 is available from https://www.brain-project.org/downloads.html.","1558-0210","","10.1109/TNSRE.2012.2214789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311479","Brain–computer interface (BCI);electro encephalography (EEG);event-related synchronization and desynchronization (ERD/ERS);motor imagery;neurofeedback","Brain computer interfaces;Training;Calibration;Electroencephalography;Neurofeedback","augmented reality;brain-computer interfaces;calibration;electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology;oscillations;regression analysis;signal classification","multiclass asynchronous brain-computer interface;automated calibration;motor imagery;neurofeedback training;sensorimotor oscillation modulation;advanced signal processing tools;statistics;machine learning;user-dependent parameter automatic selection;classifier parameters;feedback data;signal classification;filtered signals;spectral power estimation;individualized frequency bands;tailored AR-based model;mutual information;multinomial logistic regression classifier;right hand movements;left hand movement;feet movement;transfer rate;computer rendered 2D maze;classifier source code;EEG data;movement imagination;BCI2000","Adult;Algorithms;Brain-Computer Interfaces;Calibration;Computer Graphics;Cortical Synchronization;Cues;Electroencephalography;Female;Humans;Imagination;Joints;Joints;Logistic Models;Male;Movement;Neurofeedback;Neurofeedback;Photic Stimulation;Psychomotor Performance;User-Computer Interface;Young Adult","38","","45","","24 Sep 2012","","","IEEE","IEEE Journals"
"Effects of 2D/3D visual feedback and visuomotor collocation on motor performance in a Virtual Peg Insertion Test","M. Fluet; O. Lambercy; R. Gassert","Rehabilitation Engineering Lab, ETH Zurich, Switzerland; Rehabilitation Engineering Lab, ETH Zurich, Switzerland; Rehabilitation Engineering Lab, ETH Zurich, Switzerland","2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","10 Nov 2012","2012","","","4776","4779","This paper evaluates the influence of three different types of visual feedback on the motor performance of healthy subjects during the repeated execution of a Virtual Peg Insertion Test developed for the assessment of sensorimotor function of arm and hand in neurologically impaired subjects. One test trial consists of the grasping and insertion of 9 pegs into 9 holes using a haptic display with instrumented grasping handle. Three groups performed 10 trials initially on three different setups (group 1 with standard 2D visual feedback, group 2 with 3D, and group 3 with collocated 3D visual feedback) followed by 10 more trials with the setup with 2D visual feedback. The total execution time and the mean collision force as well as the time and the collision force for 6 different movement phases were compared between groups and analyzed in function of the number of repetitions. Results showed significantly lower time to approach and align the visual cursor with the peg with the 2D setup over the first 10 trials compared to the two other groups, suggesting limitations of the 3D setup. Furthermore, a significant decrease of the total execution time was found in the first 10 trials for all groups. For the 10 following trials, only group 3 showed a significant decrease in the total execution time, suggesting that the learning did not transfer to the 2D setup for this group.","1558-4615","978-1-4577-1787-1","10.1109/EMBC.2012.6347035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6347035","","Force;Visualization;Two dimensional displays;Grasping;Virtual reality;Three dimensional displays","haptic interfaces;mechanoception;neurophysiology;patient rehabilitation","2D visual feedback;3D visual feedback;visuomotor collocation;motor performance;virtual peg insertion test;sensorimotor function;arm;hand;neurologically impaired subject;grasping;haptic display;total execution time;mean collision force;visual cursor","Adult;Biofeedback, Psychology;Biofeedback, Psychology;Feedback, Sensory;Female;Humans;Imaging, Three-Dimensional;Movement;Psychomotor Performance;Reaction Time;User-Computer Interface;Young Adult","6","","10","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Causality detected by transfer entropy leads acquisition of joint attention","H. Sumioka; Y. Yoshikawa; M. Asada","JST ERATO, Graduate School of Eng., Osaka University, 2-1 Yamadaoka, Suita, 565-0871, Japan; JST ERATO, 2-1 Yamadaoka, Suita, Osaka, 565-0871 Japan; JST ERATO, Graduate School of Eng., Osaka University 2-1 Yamadaoka, Suita, 565-0871, Japan","2007 IEEE 6th International Conference on Development and Learning","22 Oct 2007","2007","","","264","269","Joint attention, i.e., the behavior of looking at the same object another person is looking at, plays an important role in both human communication and human-robot communication. Previous synthetic studies have focused on modeling the developmental process of joint attention and have proposed learning methods without any explicit instructions for joint attention. The causal structure between a perception variable (the caregiver's face directions or individual objects) and an action variable (gaze shift to the caregiver's face or object locations) is given in advance to learn joint attention. However, such a structure is expected to be found by the robot through the interaction experiences. This paper investigates how the transfer entropy, that is an information theoretic measure, can be used to quantify the causality inherent in the face-to-face interaction. In the computer simulation of human-robot interaction, we examined which pair of perceptions and actions are selected as the causal pair and showed that the selected pairs can be used to learn a sensorimotor map for achieving joint attention.","2161-9476","978-1-4244-1115-3","10.1109/DEVLRN.2007.4354069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4354069","joint attention;transfer entropy;contingency learning","Entropy;Robot kinematics;Humans;Pediatrics;Robot sensing systems;Robotics and automation;Object detection;Learning systems;Computer simulation;Buildings","entropy;robot vision","causality detection;transfer entropy leads acquisition;joint attention;human-robot communication;information theoretic measure;face-to-face interaction","","11","","9","","22 Oct 2007","","","IEEE","IEEE Conferences"
"Dissociable Processes for Learning the Surface Structure and Abstract Structure of Sensorimotor Sequences","P. F. Dominey; T. Lelekov; J. Ventre-Dominey; M. Jeannerod","NA; CNRS Institut des Sciences Cognitives, Lyon, France; INSERM Vision et Motricité, Lyon, France; CNRS Institut des Sciences Cognitives, Lyon, France","Journal of Cognitive Neuroscience","19 May 2014","1998","10","6","734","751","A sensorimotor sequence may contain information structure at several different levels. In this study, we investigated the hypothesis that two dissociable processes are required for the learning of surface structure and abstract structure, respectively, of sensorimotor sequences. Surface structure is the simple serial order of the sequence elements, whereas abstract structure is defined by relationships between repeating sequence elements. Thus, sequences ABCBAC and DEFEDF have different surface structures but share a common abstract structure, 123213, and are therefore isomorphic. Our simulations of sequence learning performance in serial reaction time (SRT) tasks demonstrated that (1) an existing model of the primate fronto-striatal system is capable of learning surface structure but fails to learn abstract structure, which requires an additional capability, (2) surface and abstract structure can be learned independently by these independent processes, and (3) only abstract structure transfers to isomorphic sequences. We tested these predictions in human subjects. For a sequence with predictable surface and abstract structure, subjects in either explicit or implicit conditions learn the surface structure, but only explicit subjects learn and transfer the abstract structure. For sequences with only abstract structure, learning and transfer of this structure occurs only in the explicit group. These results are parallel to those from the simulations and support our dissociable process hypothesis. Based on the synthesis of the current simulation and empirical results with our previous neuropsychological findings, we propose a neuro-physiological basis for these dissociable processes: Surface structure can be learned by processes that operate under implicit conditions and rely on the fronto-striatal system, whereas learning abstract structure requires a more explicit activation of dissociable processes that rely on a distributed network that includes the left anterior cortex.","0898-929X","","10.1162/089892998563130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787909","","","","","","","","","","19 May 2014","","","MIT Press","MIT Press Journals"
"Time-Frequency Maximal Information Coefficient Method and its Application to Functional Corticomuscular Coupling","T. Liang; Q. Zhang; X. Liu; C. Lou; X. Liu; H. Wang","Key Laboratory of Intelligent Rehabilitation and Neromodulation of Hebei Province, Yanshan University, Qinhuangdao, China; Key Laboratory of Digital Medical Engineering of Hebei Province, Hebei University, Baoding, China; Key Laboratory of Digital Medical Engineering of Hebei Province, Hebei University, Baoding, China; Key Laboratory of Digital Medical Engineering of Hebei Province, Hebei University, Baoding, China; Key Laboratory of Digital Medical Engineering of Hebei Province, Hebei University, Baoding, China; Key Laboratory of Intelligent Rehabilitation and Neromodulation of Hebei Province, Yanshan University, Qinhuangdao, China","IEEE Transactions on Neural Systems and Rehabilitation Engineering","6 Nov 2020","2020","28","11","2515","2524","An important challenge in the study of functional corticomuscular coupling (FCMC) is an accurate capture of the coupling relationship between the cerebral cortex and the effector muscle. The coherence method is a linear analysis method, which has certain limitations in further revealing the nonlinear coupling between neural signals. Although mutual information (MI) and transfer entropy (TE) based on information theory can capture both linear and nonlinear correlations, the equitability of these algorithms is ignored and the nonlinear components of the correlation cannot be separated. The maximal information coefficient (MIC) is a suitable method to measure the coupling between neurophysiological signals. This study extends the MIC to the time-frequency domain, named time-frequency maximal information coefficient (TFMIC), to explore the FCMC in a specific frequency band. The effectiveness, equitability, and robustness of the algorithm on the simulation data was verified and compared with coherence, TE- and MI- based methods. Simulation results showed that the TFMIC could accurately detect the coupling for different functional relationships at low noise levels. The dorsiflexion experimental results revealed that the beta-band (14-30 Hz) significant coupling was observed at channels Cz, C4, FC4, and FCz. Additionally, the results showed that the coupling was higher in the alpha-band (8-13 Hz) and beta-band (14-30 Hz) than in the gamma-band (31-45 Hz). This might be related to a transition between sensorimotor states. Specifically, the nonlinear component of FCMC was also observed at channels Cz, C4, FC4, and FCz. This study expanded the research on nonlinear coupling components in FCMC.","1558-0210","","10.1109/TNSRE.2020.3028199","National Key Research and Development Program of China; Hebei Province Postdoctoral Scientific Research Project; Natural Science Foundation of Hebei Province; Key Project of Hebei Province Department of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210548","Ankle dorsiflexion;functional corticomuscular coupling;specific frequency band;time–frequency maximal information coefficient","Couplings;Microwave integrated circuits;Correlation;Coherence;Time-frequency analysis;Electroencephalography;Muscles","electroencephalography;electromyography;entropy;medical signal processing;neurophysiology","time-frequency maximal information coefficient method;functional corticomuscular coupling;FCMC;coupling relationship;coherence method;linear analysis method;neural signals;transfer entropy;information theory;nonlinear correlations;nonlinear component;neurophysiological signals;time-frequency domain;named time-frequency maximal information coefficient;specific frequency band;beta-band;nonlinear coupling components;frequency 14.0 Hz to 30.0 Hz;frequency 31.0 Hz to 45.0 Hz;frequency 8.0 Hz to 13.0 Hz","","","","59","IEEE","1 Oct 2020","","","IEEE","IEEE Journals"
"Robust Sensorimotor Representation to Physical Interaction Changes in Humanoid Motion Learning","T. Shimizu; R. Saegusa; S. Ikemoto; H. Ishiguro; G. Metta","Department of Mechanical Engineering, Kobe City College of Technology, Kobe, Japan; Center for Human-Robot Symbiosis Research, Toyohashi University of Technology, Toyohashi, Japan; Institute for Academic Initiatives, Osaka University, Toyonaka, Japan; Department of Systems InnovationGraduate School of Engineering Science, Osaka University, Toyonaka, Japan; Department of Robotics, Brain and Cognitive Sciences, Istituto Italiano di Tecnologia, Genoa, Italy","IEEE Transactions on Neural Networks and Learning Systems","20 May 2017","2015","26","5","1035","1047","This paper proposes a learning from demonstration system based on a motion feature, called phase transfer sequence. The system aims to synthesize the knowledge on humanoid whole body motions learned during teacher-supported interactions, and apply this knowledge during different physical interactions between a robot and its surroundings. The phase transfer sequence represents the temporal order of the changing points in multiple time sequences. It encodes the dynamical aspects of the sequences so as to absorb the gaps in timing and amplitude derived from interaction changes. The phase transfer sequence was evaluated in reinforcement learning of sitting-up and walking motions conducted by a real humanoid robot and compatible simulator. In both tasks, the robotic motions were less dependent on physical interactions when learned by the proposed feature than by conventional similarity measurements. Phase transfer sequence also enhanced the convergence speed of motion learning. Our proposed feature is original primarily because it absorbs the gaps caused by changes of the originally acquired physical interactions, thereby enhancing the learning speed in subsequent interactions.","2162-2388","","10.1109/TNNLS.2014.2333092","Department of Robotics, Brain and Cognitive Sciences, Istituto Italiano di Tecnologia, Genoa, Italy; Grants-in-Aid for Scientific Research, Japan Society for the Promotion of Science (JSPS); JSPS Core-to-Core Program A; Advanced Research Network; European Union (EU) FP7 Project; Cooperative Human Robot Interaction Systems; EU FP7 Project Xperience entitled the Robots Bootstrapped through Learning and Experience; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851925","Change detection;dimensionality reduction;learning from demonstration (LfD);physical human-robot interaction.;Change detection;dimensionality reduction;learning from demonstration (LfD);physical human-robot interaction","Robot sensing systems;Legged locomotion;Joints;Timing;Indexes;Robustness","humanoid robots;human-robot interaction;intelligent robots;learning (artificial intelligence);legged locomotion;motion control","robust sensorimotor representation;physical interaction changes;humanoid robot motion learning;learning-from-demonstration system;motion feature;phase transfer sequence;knowledge synthesis;humanoid whole-body motions;teacher-supported interactions;temporal order changing points;dynamical sequence aspect encoding;reinforcement learning;sitting-up motions;walking motions;convergence speed;learning speed enhancement","Computer Simulation;Functional Laterality;Humans;Knowledge;Learning;Motion;Motor Activity;Robotics;Symbolism;Time Factors;Transfer (Psychology);Walking","6","","24","","10 Jul 2014","","","IEEE","IEEE Journals"
"Local and Remote Cooperation With Virtual and Robotic Agents: A P300 BCI Study in Healthy and People Living With Spinal Cord Injury","E. Tidoni; M. Abu-Alqumsan; D. Leonardis; C. Kapeller; G. Fusco; C. Guger; C. Hintermüller; A. Peer; A. Frisoli; F. Tecchia; M. Bergamasco; S. M. Aglioti","Department of Psychology, University of Rome “La Sapienza”, Rome, Italy; Chair of Automatic Control Engineering, Technical University of Munich, TUM, Munich, Germany; Percro Laboratory, Scuola Superiore Sant’Anna, Pisa, Italy; Guger Technologies OG, Graz, Austria; Department of Psychology, University of Rome “La Sapienza”, Rome, Italy; Guger Technologies OG, Graz, Austria; Guger Technologies OG, Graz, Austria; Bristol Robotics Laboratory, University of the West of England, Bristol, Bristol, U.K.; Percro Laboratory, Scuola Superiore Sant’Anna, Pisa, Italy; Percro Laboratory, Scuola Superiore Sant’Anna, Pisa, Italy; Percro Laboratory, Scuola Superiore Sant’Anna, Pisa, Italy; Department of Psychology, University of Rome “La Sapienza”, Rome, Italy","IEEE Transactions on Neural Systems and Rehabilitation Engineering","6 Sep 2017","2017","25","9","1622","1632","The development of technological applications that allow people to control and embody external devices within social interaction settings represents a major goal for current and future brain-computer interface (BCI) systems. Prior research has suggested that embodied systems may ameliorate BCI end-user's experience and accuracy in controlling external devices. Along these lines, we developed an immersive P300-based BCI application with a head-mounted display for virtual-local and robotic-remote social interactions and explored in a group of healthy participants the role of proprioceptive feedback in the control of a virtual surrogate (Study 1). Moreover, we compared the performance of a small group of people with spinal cord injury (SCI) to a control group of healthy subjects during virtual and robotic social interactions (Study 2), where both groups received a proprioceptive stimulation. Our attempt to combine immersive environments, BCI technologies and neuroscience of body ownership suggests that providing realistic multisensory feedback still represents a challenge. Results have shown that healthy and people living with SCI used the BCI within the immersive scenarios with good levels of performance (as indexed by task accuracy, optimizations calls and Information Transfer Rate) and perceived control of the surrogates. Proprioceptive feedback did not contribute to alter performance measures and body ownership sensations. Further studies are necessary to test whether sensorimotor experience represents an opportunity to improve the use of future embodied BCI applications.","1558-0210","","10.1109/TNSRE.2016.2626391","EU Information and Communication Technologies Grant (VERE project); Italian Ministry of Health to SMA; BIAL Foundation (2014/150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797151","Body illusions - tendon vibration;brain–computer interface (BCI) P300;spinal cord injury;teleoperation;virtual reality","Visualization;Electroencephalography;Games;Robot sensing systems;Tendons;IEEE members","brain-computer interfaces;injuries;medical robotics;neurophysiology","body ownership sensations;information transfer rate;BCI technologies;virtual surrogate control;proprioceptive feedback;robotic-remote social interactions;virtual-local social interactions;immersive P300-based BCI application;BCI end-user experience;brain-computer interface;spinal cord injury","Adult;Brain-Computer Interfaces;Event-Related Potentials, P300;Female;Humans;Imagination;Male;Man-Machine Systems;Movement;Reproducibility of Results;Robotics;Sensitivity and Specificity;Spinal Cord Injuries;Task Performance and Analysis;User-Computer Interface;Young Adult","6","","59","","23 Dec 2016","","","IEEE","IEEE Journals"
