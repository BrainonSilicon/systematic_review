"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Improving human interactions in complex product development","O. Waller; F. Isnard; G. Dodds","Dept. of Electr. & Electron. Eng., Queen's Univ., Belfast, UK; NA; NA","IEE Colloquium on Virtual Reality Personal Mobile and Practical Applications","6 Aug 2002","1998","","","6/1","6/7","It is possible, using virtual reality (VR), to create virtual prototypes which can be fully tested with respect to electrical, mechanical, aesthetic and ergonomic properties while the product is still in the design stage. Current VR software packages do not yet offer an idea solution to the problem of creating realistic virtual prototypes. This problem is especially true for large industrial products such as in the aerospace industry. These large products would especially benefit from virtual prototyping in areas such as maintenance, assembly and training. It is important to minimise the preparation and evaluation time in these short production runs and individually specified products. Three areas are examined: data reduction, economic conversion costs and real time implementation methods are discussed. These include task based reduction and more general compression methods. The use of human factors with intelligent responsive manikins is outlined along with a range of desirable features. Real environments require complex interactions and open systems with user specified code and debugging. The state of some VR systems are described and it is shown how research in these areas can overcome the current limitations of industrial virtual reality packages. The work is based on experience with large models, multiple VR packages and manikin interactions.","","","10.1049/ic:19980753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744430","","","virtual reality","human interactions;complex product development;virtual reality;virtual prototypes;electrical properties;mechanical properties;aesthetic properties;ergonomic properties;design stage;software packages;virtual prototyping;maintenance;assembly;training;data reduction;economic conversion costs;real time implementation methods;task based reduction;compression methods;human factors;intelligent responsive manikins;open systems;complex interactions;user specified code;debugging;industrial virtual reality packages;large models","","2","","","","6 Aug 2002","","","IET","IET Conferences"
"Virtual-reality based visualization of cardiac arrhythmias on mobile devices","J. Greiner; T. Oesterlein; G. Lenis; O. Dössel","Institute of Biomedical Engineering, Karlsruhe Institute of Technology, Germany; Institute of Biomedical Engineering, Karlsruhe Institute of Technology, Germany; Institute of Biomedical Engineering, Karlsruhe Institute of Technology, Germany; Institute of Biomedical Engineering, Karlsruhe Institute of Technology, Germany","2016 Computing in Cardiology Conference (CinC)","2 Mar 2017","2016","","","1081","1084","Computer simulations and imaging of human physiology and anatomy are effectively used for diagnostics and medical treatments and are thus a focus of scientific research. Suitable representation of data is a critical aspect to achieve best results. Therefore, we developed an interactive visualization scheme especially for the representation of cardiac arrhythmias based on a conventional mobile device and virtual reality (VR) goggles (Google Cardboard and Samsung Gear VR) in combination with a game engine. The aim of this paper is to raise awareness for this new technique, evaluate its potential and propose a general workflow for such a visualization environment. The use of a conventional mobile device in combination with VR goggles creates a portable and low-cost system, equipped with enough processing power and pixel density for many types of applications. The user can interact with the data through head movement or a secondary controller As current game engines support a wide range of additional input methods and controllers, the interaction method can be customized to fit the target audience. To evaluate this method, we conducted a survey with eight typical phenomena from the field of cardiac arrhythmias. The participants were asked to rate different performance aspects on a scale from one (very bad) to five (very good). All participants (N=27) rated the performance as fluent (median=5). Furthermore, most participants (70%) ranked the overall impression as very good (median=5). On the long run, the system can be used for education and presentations as well as improved planning and guidance of medical procedures.","2325-887X","978-1-5090-0895-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868934","","Data visualization;Engines;Mobile handsets;Games;Computational modeling;Eye protection;Solid modeling","biomechanics;cardiology;computer games;medical disorders;medical image processing;mobile handsets;virtual reality","virtual reality-based visualization;cardiac arrhythmias;mobile devices;human physiology;anatomy;diagnostics;medical treatments;data representation;interactive visualization scheme;mobile device;VR goggles;Google Cardboard VR;Samsung Gear VR;game engine;pixel density;processing power;head movement;secondary controller","","","","9","","2 Mar 2017","","","IEEE","IEEE Conferences"
"Using Augmented Reality Technology to Learn Cube Expansion Diagram in Spatial Geometry of Elementary Mathematics","S. Wu; C. Liu; H. Shi; S. Cai","Beijing Normal University,VR/AR+Education Lab, School of Educational Technology, Faculty of Education,Beijing,China; Beijing Normal University,VR/AR+Education Lab, School of Educational Technology, Faculty of Education,Beijing,China; Beijing Normal University,VR/AR+Education Lab, School of Educational Technology, Faculty of Education,Beijing,China; Beijing Normal University,VR/AR+Education Lab, School of Educational Technology, Faculty of Education, Beijing Advanced Innovation Center for Future Education,Beijing,China","2019 IEEE International Conference on Engineering, Technology and Education (TALE)","15 Oct 2020","2019","","","1","6","Spatial geometry has always been the key and difficult part in the curriculum of elementary mathematics. Compared with traditional teaching methods using, Augmented Reality (AR) technology shows its potential to teach spatial geometry through visualization, interaction and situation. This study focused on cube expansion diagram in spatial geometry and developed an AR learning tool based on inquiry for the course. The research aimed to examine the effectiveness of AR-assisted math lesson by designing and implementing two lessons in an elementary school. Two classes including 92 students in grade 5 participated in this study, and they were assigned to an experimental and a control group. This study adopted mixed methods methodology and utilized pre/post-tests, questionnaires and interviews. Results showed that students can general accept of using AR to learn spatial geometry, and AR-assisted teaching methods significantly improved students' learning outcome. It is also noticed that students' learning level affected their performance in AR-assisted math lessons.","2470-6698","978-1-7281-2665-4","10.1109/TALE48000.2019.9225978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225978","augmented reality;elementary mathematics;spatial geometry;education","Education;Geometry;Tools;Interviews;Three-dimensional displays;Animation","augmented reality;computer aided instruction;educational courses;educational institutions;mathematics computing;teaching","augmented reality technology;cube expansion diagram;spatial geometry;elementary mathematics;AR-assisted math lesson;AR learning tool;elementary school;experimental group;control group;AR-assisted teaching methods","","","","22","","15 Oct 2020","","","IEEE","IEEE Conferences"
"Localisation and interaction for augmented maps","G. Reitmayr; E. Eade; T. Drummond","Dept. of Eng., Cambridge Univ., UK; Dept. of Eng., Cambridge Univ., UK; Dept. of Eng., Cambridge Univ., UK","Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)","5 Dec 2005","2005","","","120","129","Paper-based cartographic maps provide highly detailed information visualisation with unrivalled fidelity and information density. Moreover, the physical properties of paper afford simple interactions for browsing a map or focusing on individual details, managing concurrent access for multiple users and general malleability. However, printed maps are static displays and while computer-based map displays can support dynamic information, they lack the nice properties of real maps identified above. We address these shortcomings by presenting a system to augment printed maps with digital graphical information and user interface components. These augmentations complement the properties of the printed information in that they are dynamic, permit layer selection and provide complex computer mediated interactions with geographically embedded information and user interface controls. Two methods are presented which exploit the benefits of using tangible artifacts for such interactions.","","0-7695-2459-1","10.1109/ISMAR.2005.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544673","Spatially Augmented Reality;Projection displays;Tangible User Interfaces;Optical Tracking","User interfaces;Computer displays;Personal digital assistants;Computer interfaces;Augmented reality;Inspection;Embedded computing;Data visualization;Spatial resolution;Surfaces","cartography;augmented reality;graphical user interfaces;data visualisation","augmented map interaction;paper-based cartographic map;information visualisation;map browsing;printed map augmentation system;computer-based map display;digital graphical information;user interface component;computer mediated interaction;geographically embedded information;user interface control;spatially augmented reality;projection display;optical tracking","","37","1","24","","5 Dec 2005","","","IEEE","IEEE Conferences"
"Skynetz: A Playful Experiential Robotics Simulator","R. Han; D. Auer; S. Edenhofer; S. von Mammen","Org. Comput. Group, Univ. of Augsburg, Augsburg, Germany; Org. Comput. Group, Univ. of Augsburg, Augsburg, Germany; Org. Comput. Group, Univ. of Augsburg, Augsburg, Germany; Org. Comput. Group, Univ. of Augsburg, Augsburg, Germany","2016 8th International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES)","18 Oct 2016","2016","","","1","4","We introduce SkyNetz, a playful interactive robotics simulator for computer science students. Its focus lies on the visualization of a probabilistic robot localization algorithm considering noise in the sensors and actuators of the robot on the one hand, as well as an environment filled with obstacles which damage the robot on contact on the other hand. The goal of the simulation is training students on the intricacies of the algorithm and to develop a notion on the impact of the considered factors such as the degree of sustained sensory noise. In order to facilitate learning and promote exploration, we embed a game mode that conveys the basic interactions with the simulator and the factors shaping the robot's behavior. In the game, the player helps the simulated robot to reach its destination with as little damage as possible. This is done by setting waypoints for the robot by adjusting the parameters of the deployed localization algorithm as well as the quality of sensors and the accuracy of the robots movements. By playing with these parameters, the user playfully learns their effects, which are visualized in 3D - contrary to the hard mathematical approach presented in books. SkyNetz also has the capacity to communicate with a real robot, to show its current position and position estimates. In the long run, this will provide the foundation for novel augmented reality games. The paper includes a general introduction to the topic of interactive robot simulation, background on the specific problem of localization estimation, the presentation of our approach and the results from a small user study.","","978-1-5090-2722-4","10.1109/VS-GAMES.2016.7590365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590365","","Robot sensing systems;Collision avoidance;Kalman filters;Games;Probabilistic logic","augmented reality;computer aided instruction;computer games;data visualisation;digital simulation;human-robot interaction;mobile robots;probability","Skynetz;playful experiential robotics simulator;playful interactive robotics simulator;computer science students;probabilistic robot localization algorithm;actuators;training students;sensory noise;game mode;robot behavior;3D visualization;augmented reality games;localization estimation problem","","","","9","","18 Oct 2016","","","IEEE","IEEE Conferences"
"Research on Federate Compliance Test method","Ronghua Zhong; Jian Huang; Bin Chen; Yong Peng","Military Simulation Laboratory, School of Mechatronics and Automation school, National University of Defense Technology, Changsha, Hunan Province, China; Military Simulation Laboratory, School of Mechatronics and Automation school, National University of Defense Technology, Changsha, Hunan Province, China; Military Simulation Laboratory, School of Mechatronics and Automation school, National University of Defense Technology, Changsha, Hunan Province, China; Military Simulation Laboratory, School of Mechatronics and Automation school, National University of Defense Technology, Changsha, Hunan Province, China","2008 Asia Simulation Conference - 7th International Conference on System Simulation and Scientific Computing","17 Nov 2008","2008","","","851","856","Federations based on HLA should conform to a series of rules and be able to exchange data effectively through Run Time Infrastructure (RTI). Therefore, it is necessary to design effective Federate Compliance Test Software (FCT) to test federate, which assure the correctness of federates. Existing federate compliance test approaches that adopted dynamic compiling methods are difficult and complicated in using and realization. Based on the former test experience, a new method is brought forward in this paper. It uses the General Simulation Data Collect (GSDC) Technique which exchanges the data with Federate Under Test (FUT) by dynamic publication and subscription of objects and interactions. Therefore, compiling can be avoided and the testing work becomes simpler. The GSDC technique and the improved method are discussed in detail, and an example shows that the method is feasible in the end.","","978-1-4244-1786-5","10.1109/ASC-ICSC.2008.4675481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675481","","Software testing;Laboratories;Mechatronics;Automation;Subscriptions;Large-scale systems;Dynamic compiler;Publishing","conformance testing;digital simulation;software architecture","federate compliance test software;high-level architecture;run time infrastructure;dynamic compiling methods;general simulation data collect technique;federate under test","","","","5","","17 Nov 2008","","","IEEE","IEEE Conferences"
"Scenario management in Web-based simulation","A. F. Seila; J. A. Miller","Dept. of MIS, Georgia Univ., Athens, GA, USA; NA","WSC'99. 1999 Winter Simulation Conference Proceedings. 'Simulation - A Bridge to the Future' (Cat. No.99CH37038)","6 Aug 2002","1999","2","","1430","1437 vol.2","Internet communications in general and the World-Wide Web specifically are revolutionizing the computer industry. Today, the Web is full of important documents and clever applets. Java applets and servlets are beginning to appear that provide useful and even mission critical applications. From the perspective of simulation, a future Web will be full of simulation models and large amounts of simulation-generated data. Many of the models will include two or three dimensional animation as well as virtual reality. Others will allow human interaction with simulation models to control or influence their execution no matter where the user is located in the world. Analysis of data from Web-based simulations involves greater degrees of freedom than traditional simulations. The number of simulation models available and the amount of simulation data are likely to be much greater. In order to assure the quality of data, the execution of models under a variety of scenarios should be well managed. Since the user community will also be larger, quality assurance should be delegated to agents responsible for defining scenarios and executing models. A major element of simulation analysis is the analysis of output data, which manages the execution of simulation models, in order to obtain statistical data of acceptable quality. Such data may be used to predict the performance of a single system, or to compare the performance of two or more alternative system designs using a single or multiple performance measures.","","0-7803-5780-9","10.1109/WSC.1999.816876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816876","","Computational modeling;Analytical models;Data analysis;Quality management;Internet;Computer industry;Java;Mission critical systems;Application software;Animation","Internet;Java;object-oriented programming;distributed programming;virtual reality;information resources;digital simulation","scenario management;Web-based simulation;Internet communications;World-Wide Web;Java applets;servlets;mission critical applications;animation;virtual reality;human interaction;simulation models;Web-based simulations;statistical data;multiple performance measures","","1","","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Using Semantics to Automatically Generate Speech Interfaces for Wearable Virtual and Augmented Reality Applications","F. Lamberti; F. Manuri; G. Paravati; G. Piumatti; A. Sanna","Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy","IEEE Transactions on Human-Machine Systems","20 May 2017","2017","47","1","152","164","This paper presents a framework for automatically generating speech-based interfaces for controlling virtual and augmented reality (AR) applications on wearable devices. Starting from a set of natural language descriptions of application functionalities and a catalog of general-purpose icons, annotated with possible implied meanings, the framework creates both vocabulary and grammar for the speech recognizer, as well as a graphic interface for the target application, where icons are expected to be capable of evoking available commands. To minimize user's cognitive load during interaction, a semantics-based optimization mechanism was used to find the best mapping between icons and functionalities and to expand the set of valid commands. The framework was evaluated by using it with see-through glasses for AR-based maintenance and repair operations. A set of experimental tests were designed to objectively and subjectively assess first-time user experience of the automatically generated interface in relation to that of a fully personalized interface. Moreover, intuitiveness of the automatically generated interface was studied by analyzing the results obtained through trained users on the same interface. Objective measurements (in terms of false positives, false negatives, task completion rate, and average number of attempts for activating functionalities) and subjective measurements (about system response accuracy, likeability, cognitive demand, annoyance, habitability, and speed) reveal that the results obtained by the first-time users and experienced users with the proposed framework's interface are very similar, and their performances are comparable with those of both the considered references.","2168-2305","","10.1109/THMS.2016.2573830","EASE-R³; FoF.NMP.2013-8; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494916","Augmented reality (AR);automatic user interface generation;semantics;speech;virtual reality (VR);wearable devices","Speech;Maintenance engineering;Visualization;Speech recognition;Semantics;Face","augmented reality;graphical user interfaces;natural language interfaces;optimisation;speech recognition;speech synthesis","automatic speech interface generation;wearable augmented reality applications;graphic interface;personalized interface;first-time user experience;AR-based repair;AR-based maintenance;see-through glasses;semantics-based optimization;user cognitive load minimization;speech recognizer;grammar;vocabulary;application functionalities;general-purpose icons;natural language descriptions;wearable virtual reality applications","","9","","63","","20 Jun 2016","","","IEEE","IEEE Journals"
"A generalized God-object method for plausible finger-based interactions in virtual environments","J. Jacobs; M. Stengel; B. Froehlich","Group Research Virtual Technologies, Volkswagen AG; Computer Graphics Lab, TU Braunschweig; Virtual Reality Systems Group, Bauhaus-Universitat, Weimar","2012 IEEE Symposium on 3D User Interfaces (3DUI)","19 Apr 2012","2012","","","43","51","We generalize the six degree-of-freedom God-object approach to enable its use for multi-finger interactions in virtual environments. The connected finger phalanxes are modeled as multiple constrained God objects. The mutual interdependencies between multiple God objects are resolved using Gauss' principle of least constraint. This generalization of the God-object method allows us to avoid the penetration of multiple fingers and their phalanxes with objects within a physically simulated virtual world. Our observations indicate that the generalized God-object approach leads to plausible collision-free positions and motions of the phalanxes of the user's fingers during complex six degree-of-freedom manipulations, while artifacts such as artificial friction or a stuck hand are avoided.","","978-1-4673-1205-9","10.1109/3DUI.2012.6184183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6184183","I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual reality;G.1.6 [Mathematics of Computing]: Numerical Analysis — Optimization;H.5.2 [Information Interfaces and Presentation]: User Interfaces — direct manipulation","Mathematical model;Acceleration;Equations;Jacobian matrices;Vectors;Haptic interfaces;Computational modeling","dexterous manipulators;virtual reality","six degree-of-freedom God-object approach;multifinger interaction;virtual environment;finger phalanxes;multiple constrained God object;mutual interdependencies;Gauss least constraint principle;God-object method generalization;collision-free position;phalanx motion;user finger;six degree-of-freedom manipulation;artificial friction;stuck hand;plausible finger-based interaction","","19","","35","","19 Apr 2012","","","IEEE","IEEE Conferences"
"Dynamic interactions in physically realistic collaborative virtual environments","P. Jorissen; M. Wijnants; M. Lamotte","Expertise Centre for Digital Media, Hasselt Univ., Diepenbeek, Belgium; Expertise Centre for Digital Media, Hasselt Univ., Diepenbeek, Belgium; Expertise Centre for Digital Media, Hasselt Univ., Diepenbeek, Belgium","IEEE Transactions on Visualization and Computer Graphics","26 Sep 2005","2005","11","6","649","660","This work describes our efforts in creating a general object interaction framework for dynamic collaborative virtual environments. Furthermore, we increase the realism of the interactive world by using a rigid body simulator to calculate all actor and object movements. The main idea behind our interactive platform is to construct a virtual world using only objects that contain their own interaction information. As a result, the object interactions are application independent and only a single scheme is required to handle all interactions in the virtual world. In order to have more dynamic interactions, we also created a new and efficient way for human users to dynamically interact within virtual worlds through their avatar. In particular, we show how inverse kinematics can be used to increase the interaction possibilities and realism in collaborative virtual environments. This results in a higher feeling of presence for connected users and allows for easy, on-the-fly creation of new interactions. For the distribution of both the interactive objects and the dynamic avatar interactions, we keep the network load as low as possible. To demonstrate the effectiveness of our techniques, we incorporate them into an existing CVE framework.","1941-0506","","10.1109/TVCG.2005.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512016","Index Terms- Artificial;augmented;and virtual realities;computer-supported cooperative work;synchronous interaction;animation;simulation.","Collaboration;Animation;Virtual environment;Collaborative work;Avatars;Computational modeling;Computer simulation;Application software;Humans;Kinematics","augmented reality;groupware;avatars;motion estimation;computer animation;graphical user interfaces","physically realistic collaborative virtual environment;rigid body simulator;object movement;inverse kinematics;realism;dynamic avatar interaction;augmented virtual reality;computer-supported cooperative work;synchronous interaction;computer animation","Computer Simulation;Computer Systems;Cybernetics;Data Display;Environment;Humans;Imaging, Three-Dimensional;Man-Machine Systems;Models, Biological;Online Systems;Touch;User-Computer Interface","13","8","39","","26 Sep 2005","","","IEEE","IEEE Journals"
"Ubii: Physical World Interaction Through Augmented Reality","S. Lin; H. F. Cheng; W. Li; Z. Huang; P. Hui; C. Peylo","HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, China; Deutsche Telekom AG Laboratories, Ernst-Reuter-Platz 7, Berlin, Germany","IEEE Transactions on Mobile Computing","6 Feb 2017","2017","16","3","872","885","We describe a new set of interaction techniques that allow users to interact with physical objects through augmented reality (AR). Previously, to operate a smart device, physical touch is generally needed and a graphical interface is normally involved. These become limitations and prevent the user from operating a device out of reach or operating multiple devices at once. Ubii (Ubiquitous interface and interaction) is an integrated interface system that connects a network of smart devices together, and allows users to interact with the physical objects using hand gestures. The user wears a smart glass which displays the user interface in an augmented reality view. Hand gestures are captured by the smart glass, and upon recognizing the right gesture input, Ubii will communicate with the connected smart devices to complete the designated operations. Ubii supports common inter-device operations such as file transfer, printing, projecting, as well as device pairing. To improve the overall performance of the system, we implement computation offloading to perform the image processing computation. Our user test shows that Ubii is easy to use and more intuitive than traditional user interfaces. Ubii shortens the operation time on various tasks involving operating physical devices. The novel interaction paradigm attains a seamless interaction between the physical and digital worlds.","1558-0660","","10.1109/TMC.2016.2567378","General Research Fund; Research Grants Council of Hong Kong; Innovation and Technology Fund; Hong Kong Innovation and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469860","Augmented reality;wearable computers;user interfaces;human computer interaction;mobile computing","Computers;Mobile handsets;Glass;Graphical user interfaces;Augmented reality","augmented reality;gesture recognition;interactive devices","augmUbii;augmented reality;smart device;graphical interface;ubiquitous interface and interaction;hand gestures;smart glass;file transfer;printing;projecting;device pairing;image processing","","23","","55","","13 May 2016","","","IEEE","IEEE Journals"
"Virtual reality-based assessment and treatment interventions for the combat-injured service member","C. A. Rábago; A. L. Pruziner; E. R. Esposito","DoD-VA Extremity Trauma and Amputation Center of Excellence, and Center for the Intrepid, Brooke Army Medical Center, Fort Sam Houston, TX USA; DoD-VA Extremity Trauma and Amputation Center of Excellence, and Department of Rehabilitation, Walter Reed National Military Medical Center, Bethesda, MD USA; DoD-VA Extremity Trauma and Amputation Center of Excellence, and Center for the Intrepid, Brooke Army Medical Center, Fort Sam Houston, TX USA","2015 International Conference on Virtual Rehabilitation (ICVR)","17 Dec 2015","2015","","","3","3","Summary form only given. This presentation will highlight clinical cases and empirical results from virtual reality (VR)-based rehabilitation programs at two military medical facilities. These programs utilize VR environments to detect and treat functional deficits often difficult to address with standard clinical methods. Injured service members seen at these facilities are often young and highly fit at the time of their injuries. Their injuries include single and multiple limb traumas such as amputation, burns, and limb salvage. Despite the severity of these injuries and associated co-morbidities, these individuals commonly set rehabilitation goals that include a return to competitive sports and/or military duty. Deficits described by these individuals, that can limit the achievement of these goals, can be difficult to detect and quantify with conventional clinical measures. Novel VR-based assessments, developed by our clinical research team, have helped identify functional deficits across multiple domains using ecologically-valid tasks. Further, VR-based treatment applications have been designed to address these deficits and progress patients toward their goals. In general, we have found that service members following traumatic brain injury, amputation, and severe limb trauma demonstrate significant increases in function with VR therapies. These VR interventions are based on well-established therapeutic techniques and can be used to promote functional interactions with challenging environments while maintaining full safeties and controls.","2331-9569","978-1-4799-8984-3","10.1109/ICVR.2015.7358631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358631","","Extremities;Virtual reality;Standards;Brain injuries;Medical treatment;Safety","injuries;patient rehabilitation;virtual reality","virtual reality-based assessment;treatment intervention;combat-injured service member;virtual reality-based rehabilitation program;military medical facility;virtual reality environment;multiple limb trauma;amputation;limb salvage;competitive sport;military duty;traumatic brain injury;virtual reality therapY","","","","","","17 Dec 2015","","","IEEE","IEEE Conferences"
"Virtual reality interfaces and population-specific models to mitigate public speaking anxiety","M. Yadav; M. N. Sakib; K. Feng; T. Chaspari; A. Behzadan","HUBBS Lab, Texas A&M university College,Station,TX,USA; CIBER Lab, Texas A&M university College,Station,TX,USA; HUBBS Lab, Texas A&M university College,Station,TX,USA; HUBBS Lab, Texas A&M university College,Station,TX,USA; CIBER Lab, Texas A&M university College,Station,TX,USA","2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)","9 Dec 2019","2019","","","1","7","Public speaking is key to effectively exchanging ideas, persuading others, and making a tangible impact. Yet, public speaking anxiety (PSA) ranks as a top social phobia among many people. This paper leverages bio-behavioural indices captured from wearable devices and virtual reality (VR) interfaces to quantify PSA. The significance of individual-specific factors, such as general trait anxiety and personality, as well as contextual factors, such as age, gender, highest education, and native language, in moderating the association between bio-behavioral indices and PSA is further examined through group-based machine learning models. Results highlight the importance of including such factors for detecting PSA with the proposed group-based PSA models yielding Spearman's correlation of 0.55(p <; 0.05) between the actual and predicted state-based anxiety scores. This work further analyzes whether systematic exposure to public speaking tasks in the VR environment can help alleviate PSA. Results indicate that systematic exposure to public speaking in VR can alleviate PSA in terms of both self-reported (p <; 0.05) and physiological (p <; 0.05) indices. Findings of this study will enable researchers to better understand antedecedents and causes of PSA contributing to behavioral interventions using VR.","2156-8111","978-1-7281-3888-6","10.1109/ACII.2019.8925509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925509","public speaking anxiety;virtual reality;physiological signals;speech;wearable devices;group-based clustering","Public speaking;Physiology;Temperature measurement;Biological system modeling;Context modeling;Electrocardiography;Virtual reality","behavioural sciences computing;learning (artificial intelligence);medical computing;patient treatment;psychology;public speaking;virtual reality","public speaking anxiety;VR environment;trait anxiety;contextual factors;bio-behavioral indices;group-based machine learning models;group-based PSA models;public speaking tasks;virtual reality interfaces;population-specific models;bio-behavioural indices;wearable devices;state-based anxiety scores","","1","","46","","9 Dec 2019","","","IEEE","IEEE Conferences"
"IDCam: Precise Item Identification for AR Enhanced Object Interactions","H. Li; E. Whitmire; A. Mariakakis; V. Chan; A. P. Sample; S. N. Patel","CSE, University of Washington, Seattle, Washington; CSE, University of Washington, Seattle, Washington; CSE, University of Washington, Seattle, Washington; Qualcomm Research, San Diego, USA; CSE, University of Michigan, Ann Arbor, USA; CSE, University of Washington, Seattle, Washington","2019 IEEE International Conference on RFID (RFID)","23 May 2019","2019","","","1","7","Augmented reality (AR) promises to revolutionize the way people interact with their surroundings by seamlessly overlaying virtual information onto the physical world. To improve the quality of such information, AR systems need to identify the object with which the user is interacting. AR systems today heavily rely on computer vision for object identification; however, state-of-the-art computer vision systems can only identify the general object categories, rather than their precise identity. In this work, we propose IDCam, a system that fuses RFID and computer vision for precise item identification in AR object-oriented interactions. IDCam simultaneously tracks users' hands using a depth camera and generates motion traces for RFID-tagged objects. The system then correlates traces from vision and RFID to match item identities with user interactions. We tested our system through a simulated retail scenario where 5 participants interacted with a clothing rack simultaneously. In our evaluation study deployed in a lab environment, IDCam identified item interactions with an accuracy of 82.0% within 2 seconds.","2573-7635","978-1-7281-1210-7","10.1109/RFID.2019.8719279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719279","Augmented Reality;RFID;Object Recognition;Sensor Fusion;Object Interaction","Computer vision;Object recognition;RFID tags;Tracking;Headphones","augmented reality;cameras;computer vision;human computer interaction;image matching;image motion analysis;radiofrequency identification;retail data processing","IDCam;precise item identification;AR enhanced object interactions;augmented reality;virtual information;physical world;AR systems;object identification;general object categories;AR object-oriented interactions;RFID-tagged objects;user interactions;item interactions;computer vision systems;item identity matching;clothing rack;motion trace generation;depth camera;time 2.0 s","","1","","30","","23 May 2019","","","IEEE","IEEE Conferences"
"Eldergames project: An innovative mixed reality table-top solution to preserve cognitive functions in elderly people","L. Gamberini; F. Martino; B. Seraglia; A. Spagnolli; M. Fabregat; F. Ibanez; M. Alcaniz; J. M. Andres","HTLab, Department of General Psychology, University of Padova, Italy; HTLab, Department of General Psychology, University of Padova, Italy; HTLab, Department of General Psychology, University of Padova, Italy; HTLab, Department of General Psychology, University of Padova, Italy; AIJU, Toy Research Institute, Spain; Brainstorm Multimedia, Spain; Instituto en Bioingeniería y Tecnología Orientada al Ser Humano, Universidad Politécnica de Valencia, Spain; Instituto en Bioingeniería y Tecnología Orientada al Ser Humano, Universidad Politécnica de Valencia, Spain","2009 2nd Conference on Human System Interactions","23 Jun 2009","2009","","","164","169","Advances in new technologies can provide solutions to prevent impairments associated with normal aging, track performance at specific tasks, and provide an entertaining experience to elderly people. Based on these premises we present Eldergames, an EU funded project that aims at creating an interactive tool for preserving cognitive functions impaired by aging and affording sociability. A first prototype has recently been created and its acceptance has been tested on groups of elderly users. The prototype proved able to provide a pleasant social cognitive training because of its simplicity, usability of the interface, and multiplayer architecture.","2158-2254","978-1-4244-3959-1","10.1109/HSI.2009.5090973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090973","cognitive aging;ergonomics;mixed-reality;table-top;gerontechnology","Virtual reality;Senior citizens;Aging;Psychology;Prototypes;Games;Testing;Usability;Ergonomics;Gerontechnology","cognitive systems;computer games;ergonomics;interactive systems;medical computing","Eldergames project;mixed reality table-top solution;preserve cognitive functions;elderly people;normal aging;social cognitive training;ergonomics","","31","","31","","23 Jun 2009","","","IEEE","IEEE Conferences"
"Immersive authoring of tangible augmented reality applications","G. A. Lee; C. Nelles; M. Billinghurst; G. J. Kim","Virtual Reality Lab., Pohang Univ. of Sci. & Technol., South Korea; NA; NA; NA","Third IEEE and ACM International Symposium on Mixed and Augmented Reality","24 Jan 2005","2004","","","172","181","In this paper, we suggest a new approach for authoring tangible augmented reality applications, called 'immersive authoring.' The approach allows the user to carry out the authoring tasks within the AR application being built, so that the development and testing of the application can be done concurrently throughout the development process. We describe the functionalities and the interaction design for the proposed authoring system that are specifically targeted for intuitive specification of scenes and various object behaviors. Several cases of applications developed using the authoring system are presented. A small pilot user study was conducted to compare the proposed method to a non-immersive approach, and the results have shown that the users generally found it easier and faster to carry out authoring tasks in the immersive environment.","","0-7695-2191-6","10.1109/ISMAR.2004.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383054","","Augmented reality;Authoring systems;Programming profession;Virtual reality;Testing;Layout;Application software;Software libraries;Humans;Educational programs","augmented reality;authoring systems","immersive authoring;tangible augmented reality;authoring tasks;AR application;interaction design;authoring system;immersive environment","","64","6","21","","24 Jan 2005","","","IEEE","IEEE Conferences"
"Wearable tactile device using mechanical and electrical stimulation for fingertip interaction with virtual world","V. Yem; H. Kajimoto","The University of Electro-Communications, Tokyo, Japan; The University of Electro-Communications, Tokyo, Japan","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","99","104","We developed “Finger Glove for Augmented Reality” (FinGAR), which combines electrical and mechanical stimulation to selectively stimulate skin sensory mechanoreceptors and provide tactile feedback of virtual objects. A DC motor provides high-frequency vibration and shear deformation to the whole finger, and an array of electrodes provide pressure and low-frequency vibration with high spatial resolution. FinGAR devices are attached to the thumb, index finger and middle finger. It is lightweight, simple in mechanism, easy to wear, and does not disturb the natural movements of the hand. All of these attributes are necessary for a general-purpose virtual reality system. User study was conducted to evaluate its ability to reproduce sensations of four tactile dimensions: macro roughness, friction, fine roughness and hardness. Result indicated that skin deformation and cathodic stimulation affect macro roughness and hardness, whereas high-frequency vibration and anodic stimulation affect friction and fine roughness.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892236","FinGAR;mechanical stimulation;electrical stimulation;virtual touch","Vibrations;Thumb;Skin;Electrodes;DC motors;Electrical stimulation","augmented reality;data gloves;DC motors;haptic interfaces;shear deformation;vibrations","wearable tactile device;electrical stimulation;mechanical stimulation;virtual world;fingertip interaction;finger glove for augmented reality;skin sensory mechanoreceptors;tactile feedback;virtual objects;DC motor;high-frequency vibration;shear deformation;electrode array;low-frequency vibration;high spatial resolution;FinGAR devices;general-purpose virtual reality system;tactile dimensions;macro roughness;friction;fine roughness;hardness;anodic stimulation","","33","","34","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Multi-agent Modeling of Biological Data Based on Virtual Reality","I. Hamdi; G. Querrec; I. R. Farah; M. B. Ahmed","Laboratoire RIADI, l'Ecole Nationale des Sciences de l'Informatique, Campus Universitaire de la Manouba, 2010 Manouba, Tunisie. Fax: (+216)71600449, Tel: (+216)71600444, Ines.hamdi@riadi.rnu.tn; Equipe d'EBV, CER V (Centre Europeen de Realite Virtuelle)25 rue Claude Chappe BP 38 F-29280 PLOUZANE, France.; Laboratoire RIADI, l'Ecole Nationale des Sciences de l'Informatique,Campus Universitaire de la Manouba, 2010 Manouba, Tunisie.; Laboratoire RIADI, l'Ecole Nationale des Sciences de l'Informatique,Campus Universitaire de la Manouba, 2010 Manouba, Tunisie.","2006 2nd International Conference on Information & Communication Technologies","16 Oct 2006","2006","2","","3046","3051","Many applications related to various fields indeed require the use of temporal data referred in time. Unfortunately, the practical application of such systems is not achieved without difficulties. Complications of a technical but so conceptual nature generally appear. The spatial and temporal data present fundamental problems with the DBMS, not only because of the considerable size and the constant growth of these data, but also because of the complexity of the geometrical and topological relations between these data. From this, the spatio-temporal modeling represents many difficulties. The main goal of our work is to solve this problem in the biological domain and particularly the spatio-temporal modeling and visualisation of molecular and functional network. In order to do that, we propose a system based on multiagent system with the virtual reality allowing the user to interact with the three-dimensional image objects. As a computer-generated representation of a three-dimensional environment, virtual reality enables the user to view and manipulate the contents of an environment. Virtual reality is often referred to as the technology that gives a user the experience of being immersed in a computer-generated virtual world. Although virtual reality is a very useful application for examining and understanding complex data sets. In this paper, we started by the state of the art of the existing systems for modeling the biological data and their interactions (molecular and functional), then we described the multiagent architecture of the spatio-temporal modeling of molecular and functional interactions based on virtual reality techniques","","0-7803-9521-2","10.1109/ICTTA.2006.1684902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684902","","Biological system modeling;Virtual reality;Biology computing;Genetics;Computer networks;Computational modeling;Cells (biology);Computer simulation;Spatiotemporal phenomena;Bioinformatics","biology computing;computer graphics;multi-agent systems;virtual reality","multiagent system modeling;biological data modeling;virtual reality;spatio-temporal modeling;molecular network visualisation;functional network visualisation;three-dimensional image object;computer-generated virtual world;multiagent architecture;molecular interaction;functional interaction","","","","6","","16 Oct 2006","","","IEEE","IEEE Conferences"
"A generalized framework for interactive dynamic simulation for multirigid bodies","Wookho Son; Kyunghwan Kim; N. M. Amato; J. C. Trinkle","Virtual Reality Dept., Electron. & Telecommun. Res. Inst., Taejon, South Korea; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","22 Mar 2004","2004","34","2","912","924","This paper presents a generalized framework for dynamic simulation realized in a prototype simulator called the Interactive Generalized Motion Simulator (I-GMS), which can simulate motions of multirigid-body systems with contact interaction in virtual environments. I-GMS is designed to meet two important goals: generality and interactivity. By generality, we mean a dynamic simulator which can easily support various systems of rigid bodies, ranging from a single free-flying rigid object to complex linkages such as those needed for robotic systems or human body simulation. To provide this generality, we have developed I-GMS in an object-oriented framework. The user interactivity is supported through a haptic interface for articulated bodies, introducing interactive dynamic simulation schemes. This user-interaction is achieved by performing push and pull operations via the PHANToM haptic device, which runs as an integrated part of I-GMS. Also, a hybrid scheme was used for simulating internal contacts (between bodies in the multirigid-body system) in the presence of friction, which could avoid the nonexistent solution problem often faced when solving contact problems with Coulomb friction. In our hybrid scheme, two impulse-based methods are exploited so that different methods are applied adaptively, depending on whether the current contact situation is characterized as ""bouncing"" or ""steady."" We demonstrate the user-interaction capability of I-GMS through online editing of trajectories of a 6-degree of freedom (dof) articulated structure.","1941-0492","","10.1109/TSMCB.2003.818434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275525","","Object oriented modeling;Biological system modeling;Haptic interfaces;Friction;Virtual prototyping;Virtual environment;Couplings;Robots;Humans;Imaging phantoms","multi-robot systems;mobile robots;digital simulation;object-oriented methods;haptic interfaces;virtual reality","interactive dynamic simulation;multirigid bodies;prototype simulator;Interactive Generalized Motion Simulator;virtual environments;free-flying rigid object;robotic systems;human body simulation;object-oriented framework;haptic interface;articulated bodies;PHANTOM haptic device;impulse-based methods","Algorithms;Biomechanics;Computer Simulation;Humans;Joints;Models, Biological;Movement;Nonlinear Dynamics;Robotics;User-Computer Interface","14","","62","","22 Mar 2004","","","IEEE","IEEE Journals"
"Virtual Hands for Risk Prevention Integration in Human-Computer Interactions","M. Pouliquen; A. Bernard; J. Marsot","IRCCyN - Ecole Centrale de Nantes, 1 rue de la Noë, BP 92101, 44321 Nantes Cedex 3, France; IRCCyN - Ecole Centrale de Nantes, 1 rue de la Noë, BP 92101, 44321 Nantes Cedex 3, France; INRS - Institut National de Recherche et de Sécurité, avenue de Bourgogne, BP 27, 54501 Vandoeuvre Cedex, France","2006 IEEE Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems","30 Nov 2006","2006","","","142","147","The development of virtual reality offers new possibilities to better simulate and understand the human/system interactions. The current challenge is to take into account the human being in order to generalize the use of ergonomics in the design stage. We propose to improve the simulation of the interactions between man and machine by estimating the risk level of the working situation. After reviewing the previous work, we present our virtual physically-based hands which are coupled with a virtual press-brake by using a motion capture system. Thus, the operator can interact in real-time with the virtual environment. By integrating a dynamic risk index, we can also estimate the degree of hazard of the current working situation","1944-9410","1-4244-0242-5","10.1109/VECIMS.2006.250809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4016682","Human-computer interactions;Virtual reality;Motion Capture;Risk prevention;Physically-based simulation","Virtual reality;Humans;Grasping;Virtual environment;Finite element methods;Anthropometry;Ergonomics;Hazards;Skeleton;Tendons","human computer interaction;virtual reality","virtual hands;risk prevention integration;human-computer interactions;virtual reality;ergonomics;virtual physically-based hands;virtual press-brake;motion capture system;dynamic risk index","","1","","22","","30 Nov 2006","","","IEEE","IEEE Conferences"
"VR and Empathy: The Bad, the Good, and the Paradoxical","M. Moroz; K. Krol","University of Nevada, Reno; University of Cambridge","2018 IEEE Workshop on Augmented and Virtual Realities for Good (VAR4Good)","16 Dec 2018","2018","","","1","4","Virtual reality (VR) is cited as offering the ultimate empathy machine [31]. This theory makes sense intuitively since VR enables a user to step in to another's shoes and experience the world as they do. We define this specific class of mental state as `emotional empathy' [49].The ability of VR to evoke emotional empathy is widely lauded as a good thing [18], [35], [43]. In this paper we invite labels such as `Luddites' and `technophobes' as we question the soundness of such claims. We instead offer warnings regarding employing VR is this manner and urge caution. Rather than dismiss the usefulness of VR in this realm we offer alternative implementation techniques in order to evoke more positive results in users.VR offers much utility for psychologists, psychiatrists, and neu-roscientists due to the ability it affords to alter cognition. While promoting the medium in general, we offer warnings regarding potential short and long term neurological impacts. We encourage increased research focus on the underlying neural mechanisms that underpin VR's successful multisensory hijack.","","978-1-5386-5977-9","10.1109/VAR4GOOD.2018.8576883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576883","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Applied computing;Law, social and behavioral sciences;Psychology","Virtual reality;Psychology;Pain;Brain modeling;Solid modeling;Functional magnetic resonance imaging;Dairy products","cognition;neurophysiology;psychology;virtual reality","ultimate empathy machine;emotional empathy;VR;neural mechanisms;cognition;multisensory hijack;psychiatrists;psychologists;virtual reality","","1","","53","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Interactive Augmented Reality For The Depth Of An Object Using The Model-Based Occlusion Method","T. Hidayat; I. A. Astuti","Amikom University Yogyakarta,Faculty of Computer Sciences,Yogyakarta,Indonesia; Amikom University Yogyakarta,Faculty of Computer Sciences,Yogyakarta,Indonesia","2020 3rd International Conference on Computer and Informatics Engineering (IC2IE)","4 Dec 2020","2020","","","382","387","The general concept in marker-based Augmented Reality is to add virtual objects in the real world using markers as object tracking. In its development AR devices can detect 3D real objects as object tracking (3D Object Tracking) so as to allow interaction between virtual objects and real objects. However, the application of AR for general devices such as Android smartphones that do not have depth sensors, virtual objects are added without having depth information from the real world so that virtual content is always displayed in front of or on top of real objects and causes Occlusion problems. Occlusion refers to the problem when real objects that are closer to the user are covered by more distant virtual objects. This research formulates the handling of the Occlusion problem using the Model-Based Occlusion method in which the geometry information of the model from real objects must be known and registered in advance to the system. To maintain the suitability of the model's geometry information with its real object, Tracking is needed. In this case using 3D Object Tracking by utilizing real objects that have been registered in the ModelBased Occlusion. The results of this study are the virtual objects that appear can occupy the correct position in the Augmented Reality application. In this case using 3D Object Tracking by utilizing real objects that have been registered in the ModelBased Occlusion. The results of this study are the virtual objects that appear can occupy the correct position in the Augmented Reality application. In this case using 3D Object Tracking by utilizing real objects that have been registered in the ModelBased Occlusion. The results of this study are the virtual objects that appear can occupy the correct position in the Augmented Reality application.","","978-1-7281-8247-6","10.1109/IC2IE50715.2020.9274565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274565","augmented reality;model-based occlusion;3d object tracking","Target tracking;Three-dimensional displays;Solid modeling;Object tracking;Testing;Augmented reality;Lighting","augmented reality;computer vision;object detection;object tracking;smart phones;stereo image processing","geometry information;virtual content;depth information;Android smartphones;3D real object detection;interactive augmented reality;marker-based augmented reality;model-based occlusion method;distant virtual objects;occlusion problem;3D object tracking","","","","19","","4 Dec 2020","","","IEEE","IEEE Conferences"
"Key requirements for CAVE simulations","S. M. Preddy; R. E. Nance","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA","Proceedings of the Winter Simulation Conference","29 Jan 2003","2002","1","","127","135 vol.1","Virtual reality offers a new frontier for human interaction with simulation models. A virtual environment, such as that created with a CAVE, imposes either real-time or quasi-real-time performance on the simulation model. Beyond that general requirement, what others can be identified for simulation programs that drive a virtual reality or virtual environment interface? Based on experience with the Virginia Tech CAVE augmented by a literature search, we propose three key requirements for successful CAVE-based simulations: (1) portability among CAVE-specific input/output devices, (2) effective and efficient interprocess communication, and (3) overcoming the limitations associated with input/output device interaction. Each requirement is described in some detail to both explain and justify its inclusion. Limitations and near- and intermediate-term research needs are identified.","","0-7803-7614-5","10.1109/WSC.2002.1172876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1172876","","Virtual environment;Humans;Computational modeling;Virtual reality;Buildings;Computer science;Professional communication;Education;Writing;Input variables","digital simulation;virtual reality;application program interfaces","CAVE simulations;virtual reality;human interaction;simulation models;real-time performance;quasi-real-time performance;virtual environment interface;Virginia Tech CAVE;CAVE-specific input/output devices;interprocess communication","","4","","18","","29 Jan 2003","","","IEEE","IEEE Conferences"
"New calibration-free approach for augmented reality based on parameterized cuboid structure","Chu-Song Chen; Chi-Kuo Yu; Yi-Ping Hung","Inst. of Inf. Sci., Acad. Sinica, Taipei, Taiwan; NA; NA","Proceedings of the Seventh IEEE International Conference on Computer Vision","6 Aug 2002","1999","1","","30","37 vol.1","A new method called PCS (parameterized cuboid structure) is presented for augmented reality. In particular, our method can insert animated virtual objects into a static scene, with geometric consistency, and also allow the user to interactively position and rotate the virtual objects with respect to a world coordinate system in a physically (or intuitively) meaningful way. Such capability cannot be achieved by using the existing calibration-free methods. To achieve this goal, we develop a new method for estimating camera parameters, which uses a cuboid structure (or more generally, a parallelepiped structure) as the reference object. The reference cuboid structure can be either explicit or implicit-implicit in the sense that the cuboid structure can be inferred by human perception even though it does not appear explicitly in the image. This method can determine the sizes of the cuboid (or parallelepiped) as well as the intrinsic and extrinsic parameters of the camera. To insert a virtual object into a single uncalibrated image, some human interaction is unavoidable. We have implemented an AR authoring system based on the proposed PCS method which provides an auxiliary line and a refinement criterion to assist human interaction. Experimental results have demonstrated that our method can successfully insert virtual objects into both static and dynamic scenes with highly convincing geometric consistency.","","0-7695-0164-8","10.1109/ICCV.1999.791194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791194","","Augmented reality","augmented reality;computer animation;computational geometry;parameter estimation;cameras;authoring systems","calibration-free approach;augmented reality;parameterized cuboid structure;PCS;animated virtual objects;static scene;geometric consistency;world coordinate system;camera parameter estimation;cuboid structure;parallelepiped structure;reference object;reference cuboid structure;human perception;extrinsic parameters;virtual object;single uncalibrated image;human interaction;AR authoring system;auxiliary line;refinement criterion;dynamic scenes","","11","1","18","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Evaluating Embodied Navigation in Virtual Reality Environments","J. DeYoung; J. Berry; S. Riggs; J. Wesson; L. C. Wertz","Yale University, Center for Collaborative Arts and Media, New Haven CT, USA; Yale University, Center for Collaborative Arts and Media, New Haven CT, USA; Sunchaser Entertainment, New York, NY; USA; Yale University, Center for Collaborative Arts and Media, New Haven CT, USA; Yale University, Center for Collaborative Arts and Media, New Haven CT, USA","2018 IEEE Games, Entertainment, Media Conference (GEM)","1 Nov 2018","2018","","","1","9","Virtual reality has become more accessible and affordable to the general public in recent years, introducing the exciting potential of this technology to new audiences. However, the mechanisms of navigating within a virtual environment have primarily been constrained to handheld input devices akin to gaming controllers. For people unfamiliar with traditional gaming input devices, VR navigation devices are not intuitively mapped to real-world modes of locomotion and can be frustrating and disorienting. Designers have largely focused on utility (the ability to efficiently accomplish a task) to the detriment of usability (ease of use). The industry lacks an intuitive, universal method of navigation that can be easily learned by novice participants. Dr. Jakob Nielsen identified five factors that impact usability in human-computer interactions (HCI): learnability, efficiency, memorability, errors, and satisfaction. Previous research in virtual environment locomotion incorporated teaching time periods where the researchers explained the control devices to participants. We believe that this neglected one of the key usability factors in human-computer interactions: learnability, or the ability and ease to accomplish a task the first time a user encounters it. Our research focuses on comparing existing modes of navigation (game controller based) with a mode of controller-less fully embodied navigation between two demographics based on Nielsen's usability factors. Existing research has demonstrated little noticeable learnability difference between modes of rotation and lean-based navigation, and joystick navigation in VR [1]; however, similar study demonstrates that partially embodied leaning mechanics can positively affect sensory perception in VR [2]. While previous Studies in controller-less VR navigation methods have demonstrated an inclination toward subject motion sickness when controllers are removed [3], other research has yielded positive qualitative results (sans motion sickness) when partially embodied alternative controller systems are used [4]. Additional research into partially embodied alternative controller systems has, in fact, indicated a preference toward existing modes of controller-based joystick navigation in VR subjects [5]; however, when partially embodied leaning mechanics are combined with another mode of sensory perception, like foot haptics, self-motion perception (vection) is enhanced [6]. In this study, we test a fully embodied mode of navigation to evaluate whether a fully engaged body experiences more positive usability results according to HCI measures. We test our results within communities of self-identified gamers and non-gamers, evaluating navigation modes designed for joystick control pads, trigger-based teleportation, and controller-less embodied navigation. Our research inquires whether embodied navigation enhances usability in accordance with Nielsen's usability factors, specifically enabling easier access and engagement for inexperienced subj e ct s, compared with controller-based modes of navigation.","","978-1-5386-6304-2","10.1109/GEM.2018.8516499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516499","","Navigation;Hardware;Games;Media;Usability;Virtual environments","computer games;human computer interaction;interactive devices;navigation;teaching;user interfaces;virtual reality","virtual reality environments;gaming controllers;VR navigation devices;real-world modes;human-computer interactions;virtual environment locomotion incorporated teaching time periods;control devices;key usability factors;game controller;Nielsen's usability factors;partially embodied leaning mechanics;controller-less VR navigation methods;controller-based joystick navigation;fully embodied mode;joystick control pads;controller-based modes;navigation modes;alternative controller systems;gaming input devices","","1","","6","","1 Nov 2018","","","IEEE","IEEE Conferences"
"A virtual environment for modeling 3D objects through spatial interaction","H. Nishino; M. Fushimi; K. Utsumiya; K. Korida","Dept. of Comput. Sci. & Intelligent Syst., Oita Univ., Japan; NA; NA; NA","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","6 Aug 2002","1999","6","","81","86 vol.6","While the demand for 3D technologies is growing rapidly, there are some fundamental problems remaining unresolved, such as a way to intuitively create 3D object data in a virtual environment. This paper proposes a new method for creating 3D objects using bi-manual gestures. To realize an easy-to-use yet powerful modeling capability, we designed a framework defining a dozen primitive hand actions to be used for modeling tasks. Then a 3D modeler was developed to substantiate the framework in a virtual environment, allowing the users to create complex shapes by combining the defined hand actions. We prove that the hand actions described in the framework are essential, and are generally applicable for various modeling tasks, by exemplifying some interesting and practical shapes.","1062-922X","0-7803-5731-0","10.1109/ICSMC.1999.816460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816460","","Virtual environment;Shape;Computer graphics;Production;Space technology;Computer science;Intelligent systems;Educational institutions;Art;Virtual reality","virtual reality;solid modelling;gesture recognition;graphical user interfaces","virtual environment;3D object modeling;spatial interaction;3D object data creation;bi-manual gestures;two-handed gestures;primitive hand actions;modeling tasks;complex shape creation;gesture interface;human-computer interaction;computer graphics","","3","7","13","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Tactile Sensation Assisted VR Catheterization Training System for Operator’s Cognitive Skills Enhancement","Y. Wang; F. Yang; Y. Li; T. Yang; C. Ren; Z. Shi","School of Electrical Engineering and Information, Southwest Petroleum University, Chengdu, China; School of Electrical Engineering and Information, Southwest Petroleum University, Chengdu, China; School of Electrical Engineering and Information, Southwest Petroleum University, Chengdu, China; General Hospital of Western Theater Command, Chengdu, China; School of Electrical Engineering and Information, Southwest Petroleum University, Chengdu, China; School of Electrical Engineering and Information, Southwest Petroleum University, Chengdu, China","IEEE Access","31 Mar 2020","2020","8","","57180","57191","In the clinical vascular interventional surgery, experienced surgeons usually rely on visual feedback to reason tool-tissue interaction, because haptic forces are easily contaminated by frictions between the catheter and the introducer sheath, which brings difficulty in collision force perceptions. Thus, cognitive skills are highly demanded for novice surgeons to thoroughly interpret the relative positions between tool and tissue in images and make appropriate decisions about further catheter motions in case of collisions. In this paper, an operator's cognitive skills training system has been introduced, which exploits the tactile sensation to reinforce the visualized spatial positions between catheter tip and vascular wall in the VR simulator. In cooperation with the collision alert module (CAM) in the VR simulator, the newly developed catheter manipulator can provide tactile sensations for novices when catheter tip is threaded beyond safety boundary, so that the VR exhibited tool-tissue interaction can be deliberately intensified. For demonstrating such tactile sensations adequate to strengthen visions, the system model has been established to facilitate the analysis in the perspective of operator's kinesthetic perception. A series of experiments have been conducted at last and the results reveal that the catheter manipulator can not only realize the accurate catheter motions but also provide the enough tactile sensations for novices. Moreover, statistical data prove that subjects under cognitive trainings have developed the ability to interpret the relative spatial positions between catheter tip and vascular wall. Such findings support that the operator's cognitive skills can be enhanced by the tactile reinforcement of VR visualized tool-tissue interaction.","2169-3536","","10.1109/ACCESS.2020.2982219","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043517","Catheterization training system;cognitive skills;tactile sensation;VR simulator;collision detection","Catheters;Training;Surgery;Haptic interfaces;Force;Manipulators","catheters;cognition;haptic interfaces;manipulators;medical computing;medical robotics;surgery;touch (physiological);virtual reality","statistical data;operator kinesthetic perception;operator cognitive skill training system;operator cognitive skill enhancement;VR visualized tool-tissue interaction;tactile reinforcement;relative spatial positions;cognitive trainings;catheter motions;system model;catheter manipulator;collision alert module;VR simulator;vascular wall;catheter tip;visualized spatial positions;novice surgeons;collision force perceptions;introducer sheath;haptic forces;visual feedback;experienced surgeons;clinical vascular interventional surgery;tactile sensation assisted VR catheterization training system","","","","32","CCBY","20 Mar 2020","","","IEEE","IEEE Journals"
"Tangible simulations Generalized haptic devices for human-guided computer simulations","T. Taylor; D. E. Johnson","Department of Mechanical Engineering, University of Utah, Salt Lake City, USA; School of Computing, University of Utah, Salt Lake City, US","2013 International Conference on Collaboration Technologies and Systems (CTS)","25 Jul 2013","2013","","","232","235","This paper presents the idea of a tangible simulation: a computer simulation along with a coupled custom robotic interface that provides a physical representation of the simulation state and a tangible means of controlling key simulation parameters. The tangible simulation concept is prototyped in a three-body gravitational simulation using two haptic arm devices. Interface concepts for controlling planetary position and velocity along with changing simulation parameters such as simulation time step are demonstrated. The final system is able to explore and control the coupled computer simulation through the tangible simulation interface.","","978-1-4673-6404-1","10.1109/CTS.2013.6567234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6567234","HRI Multi-modal Interfaces;User Interface Designs;Interaction Control in Robotic Systems","Computational modeling;Haptic interfaces;Solid modeling;Planets;Computer simulation;Orbits;Couplings","control engineering computing;digital simulation;haptic interfaces;human computer interaction;human-robot interaction","tangible simulations;generalized haptic devices;human-guided computer simulations;coupled custom robotic interface;key simulation parameter control;three-body gravitational simulation;haptic arm devices;planetary position control;velocity control;simulation time step;tangible simulation interface;coupled computer simulation control","","","","12","","25 Jul 2013","","","IEEE","IEEE Conferences"
"Sound and tangible interface for shape evaluation and modification","M. Bordegoni; F. Ferrise; S. Shelley; M. A. Alonso; D. Hermes","Dipartimento di Meccanica, Politecnico di Milano, Via La Masa, 1 - 20156 Milano, Italy; Dipartimento di Meccanica, Politecnico di Milano, Via La Masa, 1 - 20156 Milano, Italy; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands; Human Technology Interaction, Eindhoven University of Technology, P.O. Box 513, NL 5600 MB, Eindhoven, The Netherlands","2008 IEEE International Workshop on Haptic Audio visual Environments and Games","21 Nov 2008","2008","","","148","153","One of the recent research topics in the area of design and virtual prototyping is offering designers tools for creating and modifying shapes in a natural and interactive way. Multimodal interaction is part of this research. It allows conveying to the users information through different sensory channels. The use of more modalities than touch and vision augments the sense of presence in the virtual environment and can be useful to present the same information in various ways. In addition, multimodal interaction can sometimes be used to augment the perception of the user by transferring information that is not generally perceived in the real world, but which can be emulated by the virtual environment. The paper presents a prototype of a system that allows designers to evaluate the quality of a shape with the aid of touch, vision and sound. Sound is used to communicate geometrical data, relating to the virtual object, which are practically undetectable through touch and vision. In addition, the paper presents the preliminary work carried out on this prototype and the results of the first tests made in order to demonstrate the feasibility. The problems related to the development of this kind of application and the realization of the prototype itself are highlighted. This paper also focuses on the potentialities and the problems relating to the use of multimodal interaction, in particular the auditory channel.","","978-1-4244-2668-3","10.1109/HAVE.2008.4685315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4685315","haptic;audio & visual sensors and displays;human-computer interaction","Haptic interfaces;Prototypes;Virtual environment;Product design;Shape control;Virtual reality;Application software;Conferences;Humans;Virtual prototyping","computational geometry;data visualisation;haptic interfaces;human computer interaction;virtual reality","sound interface;tangible interface;shape evaluation;shape modification;virtual prototyping;interactive system;multimodal human computer interaction;sensory channel;virtual object geometrical data;auditory channel;haptic system;visualization system","","8","1","18","","21 Nov 2008","","","IEEE","IEEE Conferences"
"Synchronizing simulations in distributed interactive simulation","S. Cheung; M. Loper","Inst. for Simulation & Training, Orlando, FL, USA; Inst. for Simulation & Training, Orlando, FL, USA","Proceedings of Winter Simulation Conference","6 Aug 2002","1994","","","1316","1323","Distributed interactive simulation (DIS) provides the infrastructure to build large-scale simulations by interconnecting independent simulators via a network. These simulators execute primarily in real time because of the human-in-the-loop requirements. The future of DIS in the general area of distributed simulation will include non real time simulations, such as constructive wargames. Active research in the area of parallel discrete event simulation (PDES) has devised means by which event driven simulations can be parallelized, by utilizing state of the art parallel architectures and scheduling events which can be executed concurrently and in a conflict-free fashion. Simulation has experienced growth in both these communities. Each community has specific needs and requirements which have to be met. This paper examines the areas of DIS in which PDES techniques may be employed to achieve more parallelism. In particular, this paper addresses the need for, and implementation of synchronization in current DIS networks. The interaction between constructive and other categories of simulation will require synchronized DIS applications, and employ mechanisms utilized in PDES.","","0-7803-2109-X","10.1109/WSC.1994.717525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=717525","","Discrete event simulation;Protocols;Computational modeling;Communication standards;Computer networks;Military computing;Time factors;Intelligent networks;Drives;Large-scale systems","discrete event simulation;digital simulation;distributed processing;interactive systems;real-time systems;synchronisation;concurrency control","simulation synchronisation;distributed interactive simulation;large-scale simulations;real time;human-in-the-loop requirements;constructive wargames;parallel discrete event simulation;event driven simulations;parallel architectures;scheduling events;DIS networks","","4","2","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"GestAR: Real Time Gesture Interaction for AR with Egocentric View","S. Hegde; R. Perla; R. Hebbalaguppe; E. Hassan","Smart Machines R&D Group, TCS Res., New Delhi, India; Smart Machines R&D Group, TCS Res., New Delhi, India; Smart Machines R&D Group, TCS Res., New Delhi, India; Smart Machines R&D Group, TCS Res., New Delhi, India","2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)","2 Feb 2017","2016","","","262","267","The existing, sophisticated AR gadgets1 in the market today are mostly exorbitantly priced. This limits their usage for the upcoming academic research institutes and also their reach to the mass market in general. Among the most popular and frugal head mounts, Google Cardboard (GC) and Wearality2 are video-see-through devices that can provide immersible AR and VR experiences with a smartphone. Stereo-rendering of camera feed and overlaid information on smartphone helps us experience AR with GC. These frugal devices have limited user-input capability, allowing user interactions with GC such as head tilting, magnetic trigger and conductive lever. Our paper proposes a reliable and intuitive gesture based interaction technique for these frugal devices. The hand gesture recognition employs the Gaussian Mixture Models (GMM) based on human skin pixels and tracks segmented foreground using optical flow to detect hand swipe direction for triggering a relevant event. Realtime performance is achieved by implementing the hand gesture recognition module on a smartphone and thus reducing the latency. We augment real-time hand gestures as new GC's interface with its evaluation done in terms of subjective metrics and with the available user interactions in GC.","","978-1-5090-3740-7","10.1109/ISMAR-Adjunct.2016.0090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836511","H.5.1 [Information Interfaces and Presentation]: Artificial;Augmented;and Virtual Realities—; H.5.2 [Information Interfaces and Presentation]: User Interfaces—Interaction Styles I.4.8 [Computing Methodologies]: Image Processing and Computer V","Gesture recognition;Feature extraction;Cameras;Magnetic resonance imaging;Inspection;Google;Adaptive optics","augmented reality;Gaussian processes;gesture recognition;image segmentation;image sequences;mixture models;rendering (computer graphics);stereo image processing;tracking","subjective metrics;GC interface;hand gesture recognition module;hand swipe direction detection;optical flow;segmented foreground tracking;human skin pixels;GMM;Gaussian mixture models;intuitive gesture based interaction;user interactions;smartphone helps;stereo-rendering;VR experiences;video-see-through devices;Wearality2;Google Cardboard;head mounts;AR gadgets;egocentric view;augmented reality;real time gesture interaction;GestAR","","7","","36","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Enabling smooth and scalable dynamic 3D visualization of discrete-event construction simulations in outdoor augmented reality","A. H. Behzadan; V. R. Kamat","Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, 48109, U.S.A.; Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, 48109, U.S.A.","2007 Winter Simulation Conference","4 Jan 2008","2007","","","2168","2176","Visualization is a powerful method for verifying, validating, and communicating the results of a simulated model. Lack of visual understanding about a simulated model is one of the major reasons inhibiting contractors and engineers from using results obtained from discrete-event simulation to plan and design their construction processes and commit real resources on the job site. The fast emerging information technology makes the use of modern visualization applications more appealing to engineers and scientists in different domains. This paper presents the design and implementation of an augmented reality (AR) visualization application together with an authoring language that allows the creation of outdoor AR animated scenes of simulated operations while featuring complete user involvement and interaction. The application is based on the concept of scene graphs. It also uses a unique general purpose data transmission method to communicate with hardware components of the system.","1558-4305","978-1-4244-1305-8","10.1109/WSC.2007.4419851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4419851","","Augmented reality;Layout;Design engineering;Power engineering and energy;Discrete event simulation;Job design;Process design;Information technology;Data visualization;Animation","augmented reality;authoring languages;computer animation;data visualisation;discrete event simulation","dynamic 3D visualization;discrete-event construction simulation;outdoor augmented reality;information technology;authoring language;animated scene graph;data transmission method","","6","","13","","4 Jan 2008","","","IEEE","IEEE Conferences"
"An intelligent and modular sensing system for Augmented Reality application","M. F. Alam; S. Katsikas; S. Hadjiefthymiades","Pervasive Computing Research, Dept. of Informatics and Telecommunications, National Kapodistrain University of Athens, Greece; Research and Development Electronics, Informatics and Telecommunications, Prisma Electronics SA, Athens, Greece; Pervasive Computing Research, Dept. of Informatics and Telecommunications, National Kapodistrian University of Athens, Greece","2015 9th International Conference on Sensing Technology (ICST)","24 Mar 2016","2015","","","850","855","The scientific objective of this paper is to describe an innovative architecture of modular form in sensing and supervision system. In our study, a maintenance work at ATLAS detector in Large Hadron Collider at European Organization for Nuclear Research (CERN), Geneva, Switzerland has been considered. The research challenges lie in the development of real-time data-transmission, instantaneous analysis of data coming from different inputs, local intelligences in low power embedded system, interaction with augmented reality in multiple on-site users, complex interfaces, and portability. The proposed architecture is allocated with modular form. The prototype of this modular device is named a PSS (Personnel Supervision System) module. The hardware of the modular system includes with many sensor modules, cameras, IMU (Inertial Measurement Unit) sensors, processors, Wi-Fi module, laser, LED light plus its associated software. The mobile PSS module is responsible for local data processing for various sensors, image processing, 3D pose estimation, audio data acquisition, visualization and wireless interfaced devices. The advantage of modular concept is that it can work independently or together. The Head Mounted Display (HMD) includes HW and SW to communicate the augmented reality content to the user and to display visual information on a worker's field of view (FOV). The module serves as a supervision post, providing sensor data, video and audio stream to the supervisor. It stores data and provide the means for the supervisor to easily communicate and instruct the worker. It decides, selects and serves the AR (Augmented) content on multiple PTUs, automatically or with minor supervisor intervention. The development of this system to be compatible with a wearable use in a highly challenging environment presents an excellent opportunity to integrate today's leading technical knowledge in a product which can become accessible to industry and general public. This study is a part of the EDUSAFE project, a Marie Curie ITN project focusing on research into the use of Virtual and Augmented Reality (VR/AR) during planned and emergency maintenance in extreme environments.","2156-8073","978-1-4799-6314-0","10.1109/ICSensT.2015.7438515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438515","Modular system;Sensors;Augmented Reality;Supervision","Temperature sensors;Program processors;Pins;Augmented reality;Cameras","augmented reality;helmet mounted displays;intelligent sensors","intelligent sensing system;modular sensing system;augmented reality application;head mounted display;personnel supervision system module","","","","26","","24 Mar 2016","","","IEEE","IEEE Conferences"
"VREdu: A Framework for Interactive Immersive Lectures using Virtual Reality","M. Misbhauddin","Information Systems Department, College of Computer Sciences and Information Technology, King Faisal University, Al-Ahsa, Saudi Arabia","2018 21st Saudi Computer Society National Computer Conference (NCC)","30 Dec 2018","2018","","","1","6","In the current education system, we expect the comprehension level of all the students in a classroom to be same. However, this is not the case. There are many factors that may affect the comprehension of lectures in the classroom including class size, visibility of the whiteboard, student's level of concentration, student's level of comprehension to name a few. Our main aim is to find a better way to offer the best education experience for students using the latest innovative solutions in technology. During our survey of Virtual Reality (VR) applications, we identified several readily available applications most of which were related to either medical education, tourism or other domains of sciences. None of the applications were available for general education where a complete classroom is transported into virtual reality. In this paper, we developed a VR framework that enhances the learning experience of students who face difficulties in the classroom. Moreover, we developed a complete prototype system to validate the framework. The proposed framework makes learning immersive, interactive and narrative offering enhanced motivation to students. The prototype system setup involves setting up a camera in the classroom to capture the whiteboard. Visuals from the whiteboard augmented with the course material (lecture slides) are compiled together to create a virtual space for the students. Students can interact with the virtual classroom by a provided set of tools in the virtual space. The VR application is developed using Unity 3D Engine and interaction is implemented using a handheld Bluetooth device.","","978-1-5386-4110-1","10.1109/NCG.2018.8593095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593095","Virtual reality in education;Classroom VR;3D Engine","Education;Virtual reality;Aerospace electronics;Prototypes;Engines;Streaming media;Visualization","computer aided instruction;educational courses;interactive systems;virtual reality","interactive immersive lectures;education experience;general education;VR framework;learning experience;course material;virtual space;virtual classroom;VR application;education system;virtual reality applications","","1","","20","","30 Dec 2018","","","IEEE","IEEE Conferences"
"A control-system architecture for robots used to simulate dynamic force and moment interaction between humans and virtual objects","C. L. Clover","MechDyne Corp., Marshalltown, IA, USA","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","6 Aug 2002","1999","29","4","481","493","Haptic or kinesthetic feedback is essential in many important virtual reality and telepresence applications. Previous research focuses on simulating static forces such as those encountered when interacting with a stiff object such as a wall. Past studies usually employ custom-made devices that are not readily available to other researchers. Consequently, many of the results found in the haptic feedback literature cannot be replicated independently. With experimental results, the paper demonstrates that ""off the shelf,"" general purpose robotics equipment can be incorporated into an effective haptic/kinesthetic feedback system. Such a system can accommodate a wide variety of virtual reality applications including training and telerobotics. An admittance control scheme is utilized, which enables the simulation of dynamic force and moment interaction as well as contact with stiff objects. The paper shows that the mechanical deficiencies (e.g., friction, inertia, and backlash) often associated with general purpose manipulators can be overcome with a suitable control system architecture.","1558-2442","","10.1109/5326.798763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798763","","Robot control;Haptic interfaces;Feedback;Virtual reality;Telerobotics;Admittance;Force control;Friction;Manipulator dynamics;Control systems","haptic interfaces;virtual reality;telerobotics;force feedback;digital simulation;human factors;robot programming;bibliographies","robot control system architecture;dynamic force simulation;moment interaction;human/virtual object interaction;kinesthetic feedback;virtual reality;telepresence applications;off the shelf general purpose robotics equipment;haptic/kinesthetic feedback system;telerobotics;admittance control scheme;stiff objects;mechanical deficiencies;general purpose manipulators","","9","","51","","6 Aug 2002","","","IEEE","IEEE Journals"
"Connecting User Experience to Learning in an Evaluation of an Immersive, Interactive, Multimodal Augmented Reality Virtual Diorama in a Natural History Museum & the Importance of Story","M. C. R. Harrington","University of Central Florida,Games and Interactive Media,Orlando,Florida,USA","2020 6th International Conference of the Immersive Learning Research Network (iLRN)","4 Aug 2020","2020","","","70","78","Reported are the findings of user experience and learning outcomes from a July 2019 study of an immersive, interactive, multimodal augmented reality (AR) application, used in the context of a museum. The AR Perpetual Garden App is unique in creating an immersive multisensory experience of data. It allowed scientifically naïve visitors to walk into a virtual diorama constructed as a data visualization of a springtime woodland understory and interact with multimodal information directly through their senses. The user interface comprised of two different AR data visualization scenarios reinforced with data based ambient bioacoustics, an audio story of the curator's narrative, and interactive access to plant facts. While actual learning and dwell times were the same between the AR app and the control condition, the AR experience received higher ratings on perceived learning. The AR interface design features of ""Story"" and ""Plant Info"" showed significant correlations with actual learning outcomes, while ""Ease of Use"" and ""3D Plants"" showed significant correlations with perceived learning. As such, designers and developers of AR apps can generalize these findings to inform future designs.","","978-1-7348995-0-4","10.23919/iLRN47897.2020.9155202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155202","augmented reality;bioacoustics;data visualization;immersive;information fidelity;informal learning;interactive;multimodal;museums;narrative;photorealistic;place illusion;presence;virtual dioramas;virtual reality","Data visualization;Virtual reality;Forestry;History;Biomedical acoustics;Three-dimensional displays;Games","augmented reality;bioacoustics;computer aided instruction;data visualisation;history;human computer interaction;learning (artificial intelligence);museums;user interfaces;virtual reality","user experience;natural history museum;July 2019 study;immersive, interactive, multimodal augmented reality application;AR Perpetual Garden App;immersive multisensory experience;scientifically naïve visitors;virtual diorama;data visualization;springtime woodland understory;multimodal information;user interface;audio story;interactive access;AR app;AR experience;perceived learning;actual learning outcomes","","","","29","","4 Aug 2020","","","IEEE","IEEE Conferences"
"Multi- Touch Touch Screens on the Flight Deck: The Impact of Display Location, Display Inclination Angle and Gesture Type on Pilot Performance","S. Dodd; J. Lancaster; B. DeMers; S. Boswell","Honeywell Aerospace,Phoenix,AZ,USA; Honeywell Aerospace,Plymouth,MN,USA; Honeywell Aerospace,Plymouth,MN,USA; Honeywell Aerospace,Phoenix,AZ,USA","2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)","30 Apr 2020","2019","","","1","10","Under NextGen operations, multifunction flight deck controls such as touch screens are being used in new ways. Since many of the multifunction touch screen controls (TSCs) being considered for the flight deck are multi-touch, it is important that their usability and human factors considerations be studied in realistic aviation environments. A study was conducted to help understand the impact of multi-touch display location, display inclination angle, and gesture type on task performance, as well as workload and fatigue, for single pilots flying a flight simulator in moderate turbulence conditions. Two preliminary studies were conducted in order to inform a larger single pilot multi-touch study. A preliminary gesture study explored seventeen touch gestures and identified a subset of nine gestures to use in the larger single pilot study. A preliminary inclination study assessed the impact of inclination angle on pilot performance while using touch gestures. The main single-pilot study sought to further explore pilot performance using multi-touch gestures by incorporating the results from both preliminary studies in the context of flight simulation with turbulence. Electromyography (EMG) was used to capture muscle activations at the forearm and shoulder as an indicator of potential fatigue. Tasks included gestures performed on instrument approach procedures (IAP), maps, on a menu and application driven interface, and on a fuel balance interface. Dependent measures included time on task; number of touches to completion, perceptions of workload and fatigue, readability rating, objective fatigue via EMG, maintenance of flight indices, and ratings questionnaires. The results of this study revealed various human factors implications associated with the application of multi-touch screen controls in the flight deck. Generally, both time on task and the number of gesture attempts before successful task completion were elevated for gestures requiring more than two digits. In addition, gestures requiring more than two digits have greater potential to obscure touch targets and flight deck display information. These gestures were often awkward to complete even in a static environment; this may worsen in flight, especially in turbulent conditions. For completing touch screen tasks with gestures, pilots preferred four of the presented display inclination angles. Touch gestures performed at the upper multifunction display (MFD) location often required pilots to “lean forward” or stretch by bending at the waist and lifting their back off the seat to achieve the reach needed. This implies greater propensity for issues with fatigue. The results are intended to help inform the development of guidelines and recommendations for the integration of multi-touch screen controls into flight decks.","2155-7209","978-1-7281-0649-6","10.1109/DASC43569.2019.9081723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9081723","multifunction display;multi-touch screen controls","","electromyography;fatigue;gesture recognition;human computer interaction;occupational health;touch sensitive screens","gesture type;multifunction flight deck controls;multifunction touch screen;multitouch display location;fatigue;flight simulator;multitouch gestures;flight simulation;multitouch screen;touch screen tasks;electromyography;muscle activations;EMG","","","","11","","30 Apr 2020","","","IEEE","IEEE Conferences"
"Exploratory and participatory simulation","G. Wagner","Brandenburg University of Technology, Institute of Informatics, P. O. Box 101344 03013 Cottbus, Germany","2013 Winter Simulations Conference (WSC)","27 Jan 2014","2013","","","1327","1334","We discuss two forms of user-interactive simulation: in exploratory simulation users may explore a system by means of interventions, and in participatory simulation they may participate in a multi-agent simulation scenario by controlling (or `playing') one of the agents. Exploratory simulation can be used by researchers for validating a simulation model and it can be used by students and trainees for learning the dynamics of a system by interacting with a simulation model of it. Participatory simulation allows dealing with simulation problems where one (or more) of the involved human roles cannot be modeled sufficiently faithfully and therefore have to be played by human actors that participate in simulation runs. We elaborate the concepts of exploratory and participatory simulation on a general, implementation-independent level. We also show how they can be implemented with the AOR Simulation (AORS 2012) platform based on the human-computer interaction paradigm of agent control.","1558-4305","978-1-4799-3950-3","10.1109/WSC.2013.6721519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721519","","Mathematical model;Computational modeling;Context modeling;Supply chains;Equations;Graphical user interfaces","digital simulation;human computer interaction;multi-agent systems","participatory simulation;user-interactive simulation;exploratory simulation;multiagent simulation;AOR Simulation platform;AORS 2012 platform;human-computer interaction paradigm;agent control","","3","","8","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)","L. Merino; M. Schwarzl; M. Kraus; M. Sedlmair; D. Schmalstieg; D. Weiskopf",University of Stuttgart; University of Stuttgart; University of Konstanz; University of Stuttgart; Graz University of Technology; University of Stuttgart,"2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","14 Dec 2020","2020","","","438","451","We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.","1554-7868","978-1-7281-8508-8","10.1109/ISMAR50242.2020.00069","Deutsche Forschungsgemeinschaft; Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284762","Mixed and Augmented Reality;Evaluation;Systematic Literature Review;I.3.7 [Computing Methodologies];Three-Dimensional Graphics and Realism;A.1 [General Literature];Computer Graphics;Introductory and Survey","Systematics;Benchmark testing;User interfaces;Rendering (computer graphics);Calibration;Usability;Augmented reality","augmented reality;cognition;decision making;human computer interaction;human factors;Internet;user interfaces","augmented reality;systematic literature review;IEEE VR;future evaluations;evaluation scenario;user performance;technology-centric performance evaluations;human-centric studies;user studies;data collection methods;cognitive aspects;in-the-wild studies","","","","123","","14 Dec 2020","","","IEEE","IEEE Conferences"
"SARIM: A gesture-based sound augmented reality interface for visiting museums","F. Z. Kaghat; A. Azough; M. Fakhour","Cedric, National Conservatory of Arts and Crafts, Paris, France; LIM Laboratory, Faculty of Sciences, Sidi Mohamed Ben Abdellah University, Fez, Morocco; LIM Laboratory, Faculty of Sciences, Sidi Mohamed Ben Abdellah University, Fez, Morocco","2018 International Conference on Intelligent Systems and Computer Vision (ISCV)","7 May 2018","2018","","","1","9","The objective of this work is to explore the use of sound augmented reality in order to enhance the museum visit. We aim to provide an audio guide to immerse the visitor in an audio scene consisting of ambient sounds and comments associated with the exhibits, while minimizing its effort to discover these objects and interact with the sound environment. The first contribution of this work is the implementation the concept proof of SARIM (Sound Augmented Reality Interface for visiting Museum). The second contribution is the modeling of the museum visit augmented by the sound dimension. Inspired from some existing models, the objective is to design a complete and integrated model that includes a representation of the visitor, the soundscape and the navigation parts. The purpose of this model is to facilitate the design of different scenarios based on the concept of audibility zones. The third contribution is the evaluation conducted in a real environment which is the Musée des arts et Métiers (MAM) in Paris. This evaluation has confirmed the usability, as well as the educational and playful positive impact of the audio augmented reality in general, and the SARIM in particular comparing to traditional audio-guides.","","978-1-5386-4396-9","10.1109/ISACV.2018.8354050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354050","Audio Augmented Reality;Sound Spatialisation;Human Behaviour tracking;indoor localisation","Navigation;Augmented reality;Headphones;Hip;Art;Games;Corona","art;audio signal processing;audio user interfaces;augmented reality;gesture recognition;human computer interaction;museums","SARIM;sound augmented reality interface;museums;museum visit;audio guide;audio scene;ambient sounds;sound environment;sound dimension;audio augmented reality;traditional audio-guides;Sound Augmented Reality Interface for visiting Museum","","","","31","","7 May 2018","","","IEEE","IEEE Conferences"
"Natural interaction in VR environments for Cultural Heritage and its impact inside museums: The Etruscanning project","E. Pietroni; C. Ray; C. Rufa; D. Pletinckx; I. Van Kampen","CNR Institute of Technologies Applied to Cultural Heritage, Rome, Italy; Allard Pierson Museum - University of Amsterdam, Amsterdam, Holland; E.V.O. CA s.r.l., Rome, Italy; Visual Dimension, Ename, Belgium; Soprintendenza Etruria Meridionale, Formello, Italy","2012 18th International Conference on Virtual Systems and Multimedia","3 Dec 2012","2012","","","339","346","A basic limit of most Virtual Reality (VR) applications reproducing cultural sites developed by the scientific community is that they often fail to fire up the attention and the involvement of the public. Starting from our experience in this domain, we would like to discuss some of the fundamental concepts about the potentiality of virtual ecosystems and to propose new natural interaction interfaces in VR environments based on body movements. The system we will describe has been derived from the new generation of games, but for the first time, it has been applied to VR environments dedicated to Cultural Heritage (CH) and experimented with inside museums. Interesting research focused on the definition of a new grammar of gestures is in progress, allowing more and more complexity, but with natural interchanges and connections between real and virtual worlds. An important development in this field has been realized in the framework of the Etruscanning project, a European project (Culture 2007) whose aim is to explore the possibilities of new visualization techniques, in order to re-create and restore the original context of the Etruscan graves. In this paper, we will discuss the methodological approach and the VR application dedicated to Regolini Galassi tomb, in the Sorbo necropolis in Cerveteri. Finally, we will present the results of the evaluation of the VR installation presented inside the museums, derived from public feedback. This observation lasted several months and gave us the opportunity to adjust the grammar of gestures and the general infrastructure of the application in order to define and implement the most efficient solution for people.","","978-1-4673-2563-9","10.1109/VSMM.2012.6365943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365943","virtual reality;communication;natural interaction;embodiment;learning;museum;public evaluation","Cultural differences;Global communication;Solid modeling;Visualization;Engines;Real-time systems;Floors","data visualisation;history;museums;virtual reality","VR environments;virtual reality;cultural heritage;museums;Etruscanning project;scientific community;virtual ecosystems;natural interaction interfaces;body movements;European project;Regolini Galassi tomb;Sorbo necropolis;Cerveteri;visualization techniques","","14","","18","","3 Dec 2012","","","IEEE","IEEE Conferences"
"Comparing Four Approaches to Generalized Redirected Walking: Simulation and Live User Data","E. Hodgson; E. Bachmann","Smale Interactive Visualization Center at Miami University, Ohio; Computer Science and Software Engineering and Director of the HIVE at Miami University, Ohio","IEEE Transactions on Visualization and Computer Graphics","13 Mar 2013","2013","19","4","634","643","Redirected walking algorithms imperceptibly rotate a virtual scene and scale movements to guide users of immersive virtual environment systems away from tracking area boundaries. These distortions ideally permit users to explore large and potentially unbounded virtual worlds while walking naturally through a physically limited space. Estimates of the physical space required to perform effective redirected walking have been based largely on the ability of humans to perceive the distortions introduced by redirected walking and have not examined the impact the overall steering strategy used. This work compares four generalized redirected walking algorithms, including Steer-to-Center, Steer-to-Orbit, Steer-to-Multiple-Targets and Steer-to-Multiple+Center. Two experiments are presented based on simulated navigation as well as live-user navigation carried out in a large immersive virtual environment facility. Simulations were conducted with both synthetic paths and previously-logged user data. Primary comparison metrics include mean and maximum distances from the tracking area center for each algorithm, number of wall contacts, and mean rates of redirection. Results indicated that Steer-to-Center out-performed all other algorithms relative to these metrics. Steer-to-Orbit also performed well in some circumstances.","1941-0506","","10.1109/TVCG.2013.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479192","Redirected walking;virtual environments;navigation;human computer interaction;live users;simulation.","Legged locomotion;Orbits;Navigation;Algorithm design and analysis;Space vehicles;Visualization;Tracking","human computer interaction;navigation;virtual reality","live user data;virtual scene;scale movement;immersive virtual environment system;tracking area boundary;virtual world;physical space;steering strategy;generalized redirected walking algorithm;Steer-to-Center;Steer-to-Orbit;Steer-to-Multiple-Targets;Steer-to-Multiple+Center;live-user navigation;large immersive virtual environment facility;synthetic path;tracking area center;wall contact","Algorithms;Biofeedback, Psychology;Computer Graphics;Cues;Humans;Imaging, Three-Dimensional;User-Computer Interface;Visual Perception;Walking","34","","20","","13 Mar 2013","","","IEEE","IEEE Journals"
"Poster: A pilot study on stepwise 6-DoF manipulation of virtual 3D objects using smartphone in wearable augmented reality environment","Taejin Ha; Woontack Woo","KAIST UVR Lab., 305-701, S. Korea; KAIST UVR Lab., 305-701, S. Korea","2013 IEEE Symposium on 3D User Interfaces (3DUI)","5 Sep 2013","2013","","","137","138","In general, 6 degrees of freedom (DoF) manipulation of 3D virtual objects requires cumbersome external tracking installations to localize and track the input device in a normal wearable augmented reality (WAR) environment. In this paper, a smartphone-based 6-DoF manipulation method is proposed to avoid the use of complicated installations. First, the proposed 6-DoF manipulation method (3-DoF translation and 3-DoF rotation) exploits various embedded sensor information of the smartphone. Transfer functions are then designed to transfer the phone's 6-DoF control gain to the 3D display space's object manipulation gain. According to the experimental results, the quadratic transfer function was superior to the linear and acceleration functions with less completion time and errors.","","978-1-4673-6098-2","10.1109/3DUI.2013.6550216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550216","Wearable Augmented Reality Environment;Non-External Tracking Environment;3D User Interaction","Abstracts;Indexes;Wireless sensor networks;Wireless communication;Magnetic resonance imaging;Cameras;Image resolution","augmented reality;smart phones;user interfaces","stepwise 6-DoF manipulation;6 degrees-of-freedom manipulation;smart phone;wearable augmented reality environment;3D virtual object;WAR environment;3-DoF translation;3-DoF rotation;embedded sensor information;quadratic transfer function;linear function;acceleration function","","","","3","","5 Sep 2013","","","IEEE","IEEE Conferences"
"Study on general model of special effect simulation","S. Sun; K. Wang; R. Zhang","Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China; Mil. Simulation Center, Bengbu Tank Inst., Bengbu, China","2010 3rd International Conference on Computer Science and Information Technology","7 Sep 2010","2010","9","","351","355","Warfare simulation needs to render a realistic virtual 3D battlefield environment efficiently to ensure the authenticity and real-time interaction. However, modeling special phenomena which are common in battlefield environment has proved difficult with the existing techniques of computer graphics. A method for special effect modeling and simulation by using particle system is proposed. Based on detailed discussion on the theory of particle system, the general particle system model for special effect simulation is designed, and its concrete implementation is presented. By capturing common features of various effects and carefully structuring into the model composition, the model can be reuse in different effects simulation with little modification, making it an efficient way to create various convincing special effects in virtual battlefield environment. With its flexibility, the model is easy to manipulate and implement. Its Level of Detail can be easily adjusted. Finally, concrete special effects simulation examples by using the model are also given to test its efficiency.","","978-1-4244-5540-9","10.1109/ICCSIT.2010.5564501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564501","Special Effects;Particle System;General Model;Level of Detai;Smoke Simulation;Explosion Simulation","Sun;Computational modeling;Visualization","digital simulation;military computing;virtual reality","special effect simulation;warfare simulation;realistic virtual 3D battlefield environment;computer graphics;special effect modeling;particle system model","","","","5","","7 Sep 2010","","","IEEE","IEEE Conferences"
"Virtual cooperating manipulator control for haptic interaction with NURBS surfaces","G. R. Luecke; J. C. Edwards; B. E. Miller","Iowa State Univ., Ames, IA, USA; NA; NA","Proceedings of International Conference on Robotics and Automation","6 Aug 2002","1997","1","","112","117 vol.1","Virtual manipulators are a new concept in the area of force feedback for virtual reality. This control approach does not make use of any specialized haptic display hardware but instead is formulated for implementation with any general industrial robot that allows six degree of freedom motion. Using this approach many commonly available manipulators can be used as an interface device to a virtual environment. This work extends the virtual manipulator concept, to allow haptic interaction with more complex virtual objects. The time varying virtual manipulator developed here constrains the end effector of a robot to trace along a NURBS surface. This virtual mechanism provides interaction forces consistent with the sensation of contacting the surface. These interaction forces can be coupled with a graphical display to provide a more complete feeling of immersion.","","0-7803-3612-7","10.1109/ROBOT.1997.620024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620024","","Haptic interfaces;Manipulators;Displays;Force feedback;Virtual reality;Motion control;Hardware;Industrial control;Service robots;Virtual environment","virtual reality;manipulators;feedback;Jacobian matrices;graphical user interfaces","virtual cooperating manipulator control;haptic interaction;NURBS surfaces;force feedback;virtual reality;general industrial robot;six degree of freedom motion;virtual environment;time varying virtual manipulator;end effector;virtual mechanism;interaction forces;graphical display","","3","2","15","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Waving Real Hand Gestures Recorded by Wearable Motion Sensors to a Virtual Car and Driver in a Mixed-Reality Parking Game","D. Bannach; O. Amft; K. S. Kunze; E. A. Heinz; G. Troster; P. Lukowicz","Embedded Systems Lab, University of Passau, Passau. david.bannach@uni-passau.de; Wearable Computing Lab, ETH Zürich, Zürich. amft@ife.ee.ethz.ch; Embedded Systems Lab, University of Passau, Passau. kai.kunze@uni-passau.de; Computer Systems and Networks (CSN), UMIT, Hall in Tyrol. ernst.heinz@umit.at; Wearable Computing Lab, ETH Zürich, Zürich. troester@ife.ee.ethz.ch; Embedded Systems Lab, University of Passau, Passau; Computer Systems and Networks (CSN), UMIT, Hall in Tyrol. paul.lukowicz@uni-passau.de, paul.lukowicz@umit.at","2007 IEEE Symposium on Computational Intelligence and Games","4 Jun 2007","2007","","","32","39","We envision to add context awareness and ambient intelligence to edutainment and computer gaming applications in general. This requires mixed-reality setups and ever-higher levels of immersive human-computer interaction. Here, we focus on the automatic recognition of natural human hand gestures recorded by inexpensive, wearable motion sensors. To study the feasibility of our approach, we chose an educational parking game with 3D graphics that employs motion sensors and hand gestures as its sole game controls. Our implementation prototype is based on Java-3D for the graphics display and on our own CRN Toolbox for sensor integration. It shows very promising results in practice regarding game appeal, player satisfaction, extensibility, ease of interfacing to the sensors, and - last but not least - sufficient accuracy of the real-time gesture recognition to allow for smooth game control. An initial quantitative performance evaluation confirms these notions and provides further support for our setup","2325-4289","1-4244-0709-5","10.1109/CIG.2007.368076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4219021","Game Control;Gesture Recognition;Immersive Human-Computer Interaction;Java-3D;Mixed Reality;Motion Sensors;Wearable Computing","Wearable sensors;Virtual reality;Intelligent sensors;Context awareness;Ambient intelligence;Computer applications;Pervasive computing;Application software;Humans;Computer graphics","computer games;gesture recognition;human computer interaction;image motion analysis;image sensors;Java;virtual reality;wearable computers","real hand gestures;wearable motion sensors;virtual car;virtual driver;mixed-reality parking game;context awareness;ambient intelligence;edutainment;computer gaming;human-computer interaction;natural human hand gesture recognition;educational parking game;3D graphics;Java-3D;graphics display;CRN Toolbox;sensor integration;game control;wearable computing","","29","","21","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Online Identification of Interaction Behaviors From Haptic Data During Collaborative Object Transfer","A. Kucukyilmaz; I. Issak","School of Computer Science, University of Lincoln, Lincoln, U.K.; School of Computer Science, University of Lincoln, Lincoln, U.K.","IEEE Robotics and Automation Letters","21 Nov 2019","2020","5","1","96","102","Joint object transfer is a complex task, which is less structured and less specific than what is existing in several industrial settings. When two humans are involved in such a task, they cooperate through different modalities to understand the interaction states during operation and mutually adapt to one another's actions. Mutual adaptation implies that both partners can identify how well they collaborate (i.e. infer about the interaction state) and act accordingly. These interaction states can define whether the partners work in harmony, face conflicts, or remain passive during interaction. Understanding how two humans work together during physical interactions is important when exploring the ways a robotic assistant should operate under similar settings. This study acts as a first step to implement an automatic classification mechanism during ongoing collaboration to identify the interaction state during object co-manipulation. The classification is done on a dataset consisting of data from 40 subjects, who are partnered to form 20 dyads. The dyads experiment in a physical human-human interaction (pHHI) scenario to move an object in an haptics-enabled virtual environment to reach predefined goal configurations. In this study, we propose a sliding-window approach for feature extraction and demonstrate the online classification methodology to identify interaction patterns. We evaluate our approach using 1) a support vector machine classifier (SVMc) and 2) a Gaussian Process classifier (GPc) for multi-class classification, and achieve over 80% accuracy with both classifiers when identifying general interaction types.","2377-3766","","10.1109/LRA.2019.2945261","Engineering and Physical Sciences Research Council; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854968","Classification;Feature Extraction;Force and Tactile Sensing;Haptics and Haptic Interfaces;Human Factors and Human-in-the-Loop;Learning and Adaptive Systems;Physical Human-Human Interaction;Physical Human-Robot Interaction;Recognition","Task analysis;Collaboration;Haptic interfaces;Feature extraction;Robots;Microsoft Windows;Taxonomy","feature extraction;Gaussian processes;haptic interfaces;human-robot interaction;image classification;robot vision;support vector machines;virtual reality","interaction state;physical interactions;interaction patterns;physical human-human interaction;general interaction types;interaction behaviors;collaborative object transfer;joint object transfer;online identification;haptic data;industrial settings;object co-manipulation;haptics-enabled virtual environment;sliding-window approach;feature extraction;online classification methodology;support vector machine classifier;SVMc;Gaussian process classifier;multiclass classification;general interaction type identification;automatic classification mechanism","","","","27","IEEE","2 Oct 2019","","","IEEE","IEEE Journals"
"The wobbly table: Increased social presence via subtle incidental movement of a real-virtual table","M. Lee; K. Kim; S. Daher; A. Raij; R. Schubert; J. Bailenson; G. Welch",University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida and UNC-Chapel Hill; Stanford University; University of Central Florida,"2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","11","17","While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced - however subtly - by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of “Twenty Questions,” where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the “wobbly” group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504683","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, Augmented and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences — Psychology","Electronic mail;Measurement by laser beam;Virtual environments;Games;Training;Haptic interfaces;Force","human computer interaction;virtual reality","wobbly table;social presence;real-virtual table;subtle incidental movement;object movement;human-virtual human interaction;human-VH interaction;shared real-virtual object;real-virtual room;projection screen;virtual world;Twenty Questions game","","17","","42","","7 Jul 2016","","","IEEE","IEEE Conferences"
"Metazoa Ludens: Mixed-Reality Interaction and Play for Small Pets and Humans","A. D. Cheok; R. T. K. C. Tan; R. L. Peiris; O. N. N. Fernando; J. T. K. Soon; I. J. P. Wijesena; J. Y. P. Sen",MXR; MXR; MXR; MXR; MXR; MXR; MXR,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","18 Aug 2011","2011","41","5","876","891","Although animals and pets are so important for families and society, in modern urban lifestyles, we can only spend little time with our animal friends. Interactive media should be aimed to enhance not only human-to-human communication but also human-to-animal communication. Thus, we promote a new type of interspecies media interaction which allows human users to interact and play with their small pet friends (in this case, hamsters) remotely via the Internet through a mixed-reality-based game system “Metazoa Ludens.” We used a two-pronged approach to scientifically examine the system. First, and most importantly, the body condition score study was conducted to evaluate the positive effects to the hamsters. Second, the method of Duncan was used to assess the strength of preference of the hamsters toward Metazoa Ludens. Lastly, the effectiveness of this remote interaction with respect to the human users as an interactive gaming system with their pets/friends (hamster) was examined based on Csikszentmihalyi's Flow theory. Results of both studies inform of positive remote interaction between human users and their pet friends using our research system. This research is not only just aimed at providing specific experimental results on the implemented research system but is also aimed as a wider lesson for human-to-animal interactive media. Therefore, as an addition, we present a detailed framework suited in general for human-to-animal interaction systems inferred from the lessons learned.","1558-2426","","10.1109/TSMCA.2011.2108998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740616","Communication;computer gaming;human–animal interaction;mixed reality;multimodal interaction","Positron emission tomography;Games;Animals;Humans;Virtual reality;Internet;Robots","computer games;interactive systems;Internet","human-to-human communication;human-to-animal communication;interspecies media interaction;Internet;mixed reality based game system;Duncan method;Metazoa Ludens;remote interaction;interactive gaming system;Csikszentmihalyi flow theory;human-to-animal interactive media","","18","","44","","5 Apr 2011","","","IEEE","IEEE Journals"
"A Controlled Experiment on Spatial Orientation in VR-Based Software Cities","M. Rüdel; J. Ganser; R. Koschke","Univ. of Bremen, Bremen, Germany; Univ. of Bremen, Bremen, Germany; Univ. of Bremen, Bremen, Germany","2018 IEEE Working Conference on Software Visualization (VISSOFT)","11 Nov 2018","2018","","","21","31","Multiple authors have proposed a city metaphor for visualizing software. While early approaches have used three-dimensional rendering on standard two-dimensional displays, recently researchers have started to use head-mounted displays to visualize software cities in immersive virtual reality systems (IVRS). For IVRS of a higher order it is claimed that they offer a higher degree of engagement and immersion as well as more intuitive interaction. On the other hand, spatial orientation may be a challenge in IVRS as already reported by studies on the use of IVRS in domains outside of software engineering such as gaming, education, training, and mechanical engineering or maintenance tasks. This might be even more true for the city metaphor for visualizing software. Software is immaterial and, hence, has no natural appearance. Only a limited number of abstract aspects of software are mapped onto visual representations so that software cities generally lack the details of the real world, such as the rich variety of objects or fine textures, which are often used as clues for orientation in the real world. In this paper, we report on an experiment in which we compare navigation in a particular kind of software city (EvoStreets) in two variants of IVRS. One with head-mounted display and hand controllers versus a 3D desktop visualization on a standard display with keyboard and mouse interaction involving 20 participants.","","978-1-5386-8292-0","10.1109/VISSOFT.2018.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530128","Visualization;Software City;Orientation;Experiment;Software Engineering;Virtual Reality","Software;Visualization;Urban areas;Navigation;Measurement;Task analysis;Three-dimensional displays","helmet mounted displays;program visualisation;virtual reality","software city;IVRS;head-mounted display;3D desktop visualization;standard display;spatial orientation;VR-based software cities;city metaphor;two-dimensional displays;immersive virtual reality systems;software engineering;mechanical engineering;visual representations;software visualization;EvoStreets;hand controllers","","3","","45","","11 Nov 2018","","","IEEE","IEEE Conferences"
"On error bound estimation for motion prediction","R. W. H. Lau; K. Lee","Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong","2010 IEEE Virtual Reality Conference (VR)","8 Apr 2010","2010","","","171","178","A collaborative virtual environment (CVE) allows remote users to access and modify shared data through networks, such as the Internet. However, when the users are connected via the Internet, the network latency problem may become significant and affect the performance of user interactions. Existing works to address the network latency problem mainly focus on developing motion prediction methods that appear statistically accurate for certain applications. However, it is often not known how reliable they are in a CVE. In this work, we study the sources of error introduced by a motion predictor and propose to address the errors by estimating the error bounds of each prediction made by the motion predictor. Without loss of generality, we discuss how we may estimate the upper and lower error bounds based on a particular motion predictor. Finally, we evaluate the effectiveness of our method extensively through a number of experiments and show the effectiveness of using the estimated error bound in an area-based visibility culling algorithm for DVE navigation.","2375-5334","978-1-4244-6238-4","10.1109/VR.2010.5444795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444795","collaborative virtual environments;network latency;motion prediction;prediction error","Estimation error;Motion estimation;Delay;IP networks;Virtual environment;Error correction;Geometry;Prefetching;Collaboration;Computer errors","estimation theory;groupware;Internet;virtual reality","error bound estimation;motion prediction;collaborative virtual environment;Internet;network latency problem;user interactions;motion predictor;estimated error bound;visibility culling algorithm;DVE navigation","","1","","18","","8 Apr 2010","","","IEEE","IEEE Conferences"
"Inverse Kinematics and Temporal Convolutional Networks for Sequential Pose Analysis in VR","D. C. Jeong; J. J. Xu; L. C. Miller","Santa Clara University,Department of Communication,Santa Clara,USA; University of Southern California,Annenberg School for Communication,Los Angeles,USA; University of Southern California,Annenberg School for Communication,Los Angeles,USA","2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","15 Jan 2021","2020","","","274","281","Drawing from a recent call to advance generalizability and causal inference in psychological science using contextually representative research designs [1], we introduce a conceptual framework that integrates techniques in machine perception of poses with VR-driven inverse kinematic character animation, leveraging the Unity game engine to mediate between the human user and the machine learner. This Computational Virtual Reality (C-VR) system contains the following components: a) Human motion capture (VR), b) Human to avatar character animation (inverse kinematics), c) character animation recordings (virtual cameras), d) avatar pose detection (OpenPose), d) avatar pose classification (SVM), and e) sequential avatar moving pose analyses (TCN). By leveraging the precision in representation afforded in virtual environments and agents and the precision in perception afforded in computer vision and machine learning in a unified system, we may take steps towards understanding a wider range of human complexity.","","978-1-7281-7463-1","10.1109/AIVR50618.2020.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319069","avatars;neural networks;computer vision","Kinematics;Psychology;Avatars;Pose estimation;Computer vision;Animation;Skeleton","avatars;computer animation;computer games;computer vision;convolutional neural nets;human computer interaction;learning (artificial intelligence);pose estimation;psychology;solid modelling;virtual reality","human complexity;sequential pose analysis;causal inference;psychological science;contextually representative research designs;machine perception;VR-driven inverse kinematic character animation;Unity game engine;human user;machine learner;C-VR;human motion capture;avatar character animation;inverse kinematics;virtual cameras;virtual environments;computer vision;machine learning;computational virtual reality system","","","","81","","15 Jan 2021","","","IEEE","IEEE Conferences"
"Transform children's library into a mixed-reality learning environment: Using smartwatch navigation and information visualization interfaces","K. Wu; C. Chen; T. Chiu; I. Chiang","Department of Interaction Design, National Taipei University of Technology, Taipei, Taiwan; Department of Interaction Design, National Taipei University of Technology, Taipei, Taiwan; Center for General Education, Taipei Medical University, Taipei, Taiwan; Graduate Institute of Data Science, Taipei Medical University, Taipei, Taiwan","2017 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC)","14 Dec 2017","2017","","","1","8","Digital natives are born into an information-dense resource-plentiful world, which greatly stimuluses their numerous needs with regard to libraries. Children have insufficient conceptual framework of knowledge and hard to understand the correlation between call numbers and themes. Young readers need an interactive, learning environment to enable them to find the books which are interested in by thematic order. Researchers apply the Cognitive-developmental theory and children's information seeking behaviors into the invention of a mixed-reality futuristic library for children using cutting-edge technologies. We first created a visualized children's book classified system using facet structured thesaurus, and employed beacon indoor positioning technologies to locate categorization of themes in non-linear arrangement of bookshelves. We then designed navigation interfaces to help children search for books using smartwatches. An integration of RFID smart bookshelf for popular books (placed in thematic-random-order), and visualized browsing interfaces were designed for children. Digital wall screens in clustering (inspired) and categorization (discovering) representations developed to recommend books for children in different cognitive-development groups. For children learning library-use instruction in a virtual-world environment, we created a mixed-reality corridor, in which children were immersed in interactive games and learn what the classification numbers mean. The world children live in is a three-dimensional space. Children at the concrete operational stage of development are apt to use their experiences in this physical world to navigate virtual worlds. Researchers proposed and examined various spatial-metadata schemes for a mixed-reality environment designed for children to find books. Smart library with innovative technologies has the power to transform informationseeking behavior and brings libraries to life, allowing children readers to benefit from digital learning in a state-of-the-art library environment.","","978-9-8695-3170-2","10.23919/PNC.2017.8203526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8203526","Digital library for children;Library navigation;Visualized interface;Customized classification scheme for children books;Serious game","Libraries;Navigation;Virtual reality;Visualization;Knowledge engineering;Pediatrics;Games","cognition;computer aided instruction;data visualisation;digital libraries;human computer interaction;information retrieval;meta data;mobile computing;radiofrequency identification;technology management;user interfaces;virtual reality","mixed-reality futuristic library;children readers;mixed-reality learning environment;smartwatch navigation;information visualization interfaces;visualized children book classified system;children library;cognitive-developmental theory;children information seeking behaviors;RFID smart bookshelf;cutting-edge technologies;facet structured thesaurus;spatial-metadata schemes","","","","8","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Analysis on the current condition of virtual reality and computer graphics and the applications on the digital media interaction","P. Wu","Shenyang Aerospace University, Liaoning province, Shenyang city, 110136, China","2016 International Conference on Inventive Computation Technologies (ICICT)","19 Jan 2017","2016","1","","1","6","In this paper, we conduct analysis on the current condition of virtual reality and the computer graphics and the applications on the digital media interaction. We proposed the systematic review of the digital media interaction modes with the theoretical analysis of the virtual reality and computer graphics. Multimedia gateway is a separate control of the entire media interaction center core components, has the media docking and adaptation, multimedia intelligent routing, configuration and management, and other functions and at the center of the centralized control of all media interaction, the configuration and management of multimedia intelligent routing. With this basis, we integrate the 3D reconstruction methodology to propose the new perspective of the digital media interaction paradigm. The man-machine interface design principles should include the basic principles of general interface design, analysis and man-machine interface specification and the type of interface requirements. Our review summarizes the characteristics of the method well and in the future, the VR integration with the multimedia system and optimized re-construction method and image representation algorithms will be considered.","","978-1-5090-1285-5","10.1109/INVENTIVE.2016.7823249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823249","Virtual Reality;Computer Graphics;Digital Media Interaction;Current Condition;Image Processing","Multimedia communication;Media;Three-dimensional displays;Solid modeling;Streaming media;Computers;Computer graphics","image reconstruction;image representation;man-machine systems;multimedia systems;user interfaces;virtual reality","virtual reality;computer graphics;digital media interaction center core components;media docking;media adaptation;multimedia intelligent routing;multimedia gateway;multimedia configuration;multimedia management;centralized control;3D reconstruction;man-machine interface specification;interface requirements;man-machine interface design principles;VR integration;optimized reconstruction;image representation","","","","47","","19 Jan 2017","","","IEEE","IEEE Conferences"
"The Alps at your fingertips: virtual reality and geoinformation systems","R. Pajarola; T. Ohler; P. Stucki; K. Szabo; P. Widmayer","Dept. of Comput. Sci., Eidgenossische Tech. Hochschule, Zurich, Switzerland; NA; NA; NA; NA","Proceedings 14th International Conference on Data Engineering","6 Aug 2002","1998","","","550","557","Advocates a desktop virtual reality (VR) interface to a geographic information system (GIS). The navigational capability to explore large topographic scenes is a powerful metaphor and a natural way of interacting with a GIS. VR systems succeed in providing visual realism and real-time navigation and interaction, but fail to cope with very large amounts of data and to provide the general functionality of information systems. We suggest a way to overcome these problems. We describe a prototype system, called ViRGIS (Virtual Reality GIS), that integrates two system platforms: a client that runs the VR component interacts via a (local or wide area) network with a server that runs an object-oriented database containing geographic data. For the purpose of accessing data efficiently, we describe how to integrate a geometric index into the database, and how to perform the operations that are requested in a real-time trip through the virtual world.","1063-6382","0-8186-8289-2","10.1109/ICDE.1998.655818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655818","","Virtual reality;Geographic Information Systems;Navigation;Object oriented databases;Spatial databases;Layout;Real time systems;Information systems;Virtual prototyping;Network servers","virtual reality","desktop virtual reality interface;geographic information system;navigational capability;large topographic scenes;visual realism;real-time navigation;real-time interaction;functionality;prototype system;ViRGIS;client-server system;local area network;wide area network;object-oriented database;data access;geometric index","","4","4","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"High-Performance Interaction-Based Simulation of Gut Immunopathologies with ENteric Immunity Simulator (ENISI)","K. Bisset; M. M. Alam; J. Bassaganya-Riera; A. Carbo; S. Eubank; R. Hontecillas; S. Hoops; Y. Mei; K. Wendelsdorf; D. Xie; J. -S. Yeom; M. V. Marathe","Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA; Virginia Bioinf. Inst., Virginia Tech, Blacksburg, VA, USA","2012 IEEE 26th International Parallel and Distributed Processing Symposium","16 Aug 2012","2012","","","48","59","Here we present the ENteric Immunity Simulator (ENISI), a modeling system for the inflammatory and regulatory immune pathways triggered by microbe-immune cell interactions in the gut. With ENISI, immunologists and infectious disease experts can test and generate hypotheses for enteric disease pathology and propose interventions through experimental infection of an in silico gut. ENISI is an agent based simulator, in which individual cells move through the simulated tissues, and engage in context-dependent interactions with the other cells with which they are in contact. The scale of ENISI is unprecedented in this domain, with the ability to simulate 107 cells for 250 simulated days on 576 cores in one and a half hours, with the potential to scale to even larger hardware and problem sizes. In this paper we describe the ENISI simulator for modeling mucosal immune responses to gastrointestinal pathogens. We then demonstrate the utility of ENISI by recreating an experimental infection of a mouse with Helicobacter pylori 26695. The results identify specific processes by which bacterial virulence factors do and do not contribute to pathogenesis associated with H. pylori strain 26695. These modeling results inform general intervention strategies by indicating immunomodulatory mechanisms such as those used in inflammatory bowel disease may be more appropriate therapeutically than directly targeting specific microbial populations through vaccination or by using antimicrobials.","1530-2075","978-1-4673-0975-2","10.1109/IPDPS.2012.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6267823","Computational Immunology;Parallel Efficiency and Scalability;Agent Based Simulation;BioComputing","Immune system;Microorganisms;Automata;Mathematical model;Biological system modeling;Diseases;Scalability","biological tissues;cellular biophysics;digital simulation;diseases;medical computing;software agents;statistical testing","high-performance interaction-based simulation;gut immunopathologies;enteric immunity simulator;modeling system;inflammatory immune pathways;regulatory immune pathways;microbe-immune cell interactions;ENISI;immunologists;infectious disease;hypothesis testing;disease pathology;in silico gut infection;agent based simulator;tissues;context-dependent interactions;mucosal immune response modeling;gastrointestinal pathogens;Helicobacter pylori 26695;bacterial virulence factors;immunomodulatory mechanisms;inflammatory bowel disease;microbial populations","","11","","30","","16 Aug 2012","","","IEEE","IEEE Conferences"
"Estimating Gaze Depth Using Multi-Layer Perceptron","Y. Lee; C. Shin; A. Plopski; Y. Itoh; T. Piumsomboon; A. Dey; G. Lee; S. Kim; M. Billinghurst","Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia; VR/AR Res. Center, KETI, South Korea; Interactive Media Design Lab., NAIST, Japan; Interactive Media Lab., Keio Univ., Yokohama, Japan; Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia; Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia; Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia; Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia; Empathic Comput. Lab., Univ. of South Australia, Adelaide, SA, Australia","2017 International Symposium on Ubiquitous Virtual Reality (ISUVR)","24 Jul 2017","2017","","","26","29","In this paper we describe a new method for determining gaze depth in a head mounted eye-tracker. Eye-trackers are being incorporated into head mounted displays (HMDs), and eye-gaze is being used for interaction in Virtual and Augmented Reality. For some interaction methods, it is important to accurately measure the x-and y-direction of the eye-gaze and especially the focal depth information. Generally, eye tracking technology has a high accuracy in x-and y-directions, but not in depth. We used a binocular gaze tracker with two eye cameras, and the gaze vector was input to an MLP neural network for training and estimation. For the performance evaluation, data was obtained from 13 people gazing at fixed points at distances from 1m to 5m. The gaze classification into fixed distances produced an average classification error of nearly 10%, and an average error distance of 0.42m. This is sufficient for some Augmented Reality applications, but more research is needed to provide an estimate of a user's gaze moving in continuous space.","","978-1-5386-3091-4","10.1109/ISUVR.2017.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7988648","Eye-gaze;3D gaze;Machine Learning;Augmented Reality;Head-mounted display","Three-dimensional displays;Meters;Cameras;Training;Error analysis;Resists;Calibration","augmented reality;cameras;gaze tracking;helmet mounted displays;image classification;learning (artificial intelligence);multilayer perceptrons","gaze depth;head mounted eye-tracker;head mounted displays;virtual reality;augmented reality;focal depth information;eye tracking technology;binocular gaze tracker;eye cameras;gaze vector;MLP neural network;training;performance evaluation;gaze classification;fixed distances;fixed points;average classification error;gaze depth estimation;multilayer perceptron","","4","","23","","24 Jul 2017","","","IEEE","IEEE Conferences"
"Adaptive information density for augmented reality displays","M. Tatzgern; V. Orso; D. Kalkofen; G. Jacucci; L. Gamberini; D. Schmalstieg",Salzburg University of Applied Sciences; University of Padova; Graz University of Technology; University of Helsinki; University of Padova; Graz University of Technology,"2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","83","92","Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504691","","Clutter;Data visualization;Visualization;Browsers;Semantics;Electronic mail;Clustering algorithms","augmented reality;computer displays;online front-ends;pattern clustering;user interfaces","adaptive information density display;augmented reality displays;AR browsers;geo-referenced data;items clustering;spatial attributes;semantic attributes;hierarchical clustering;level-of-detail structure;leaf nodes;user-defined preferences;visual clutter;AR browser interface;search-recall tasks","","8","","24","","7 Jul 2016","","","IEEE","IEEE Conferences"
"Application and Realization of VR Technology in Interior Design","R. wang","Zhejiang Tongji Vocational College of Science and Technology, Hangzhou, China","2019 12th International Conference on Intelligent Computation Technology and Automation (ICICTA)","2 Mar 2020","2019","","","182","186","This paper introduces the development of virtual reality technology at home and abroad, and introduces the basic knowledge of virtual reality. The model of indoor environment is built by VRML, and the roaming technology of virtual scene is also introduced. The reason why VRML is used to establish indoor mode is that Web technology is used in the implementation part of this paper. Finally, based on virtual scene modeling, the establishment of virtual indoor environment is completed, which combines the virtual reality interior design technology with the popular Internet technology. The experiments show that the improved technology-based virtual scene space multi-dimensional display effect is good and expansibility is strong; the online virtual indoor scene designed can be realized with users. The interaction enables users to roam the virtual indoor system according to their own wishes, generally meeting the expected requirements.","","978-1-7281-4284-5","10.1109/ICICTA49267.2019.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9017034","VR;3ds MAX;interior design;VRML","Erbium;Automation","indoor environment;Internet;virtual reality","Web technology;virtual scene modeling;virtual indoor environment;virtual reality interior design technology;popular Internet technology;improved technology-based virtual scene space multidimensional display effect;online virtual indoor scene;virtual indoor system;VR technology;virtual reality technology;VRML;roaming technology;indoor mode","","","","9","","2 Mar 2020","","","IEEE","IEEE Conferences"
"A Participatory Design in Developing Prototype an Augmented Reality Book for Deaf Students","N. M. M. Zainuddin; H. B. Zaman; A. Ahmad","Dept. of Inf. Sci., Univ. Kebangsaan Malaysia, Bangi, Malaysia; Dept. of Inf. Sci., Univ. Kebangsaan Malaysia, Bangi, Malaysia; Dept. of Inf. Sci., Univ. Kebangsaan Malaysia, Bangi, Malaysia","2010 Second International Conference on Computer Research and Development","21 Jun 2010","2010","","","400","404","One of Augmented Reality applications is known as Augmented Reality Book. Due to this, particular AR-Book is based on visually oriented technique and deaf students are generally classified as visual learners. The aim of this paper is to identify the criteria in developing an AR-Book for the deaf students. The AR-Book for deaf students is not the same as normal students because it has to include the sign language. In this ongoing study, qualitative approaches such as participatory design philosophy were used in designing and developing an AR-Book for deaf students. Three deaf students and three teachers who taught the deaf students were involved in this study. Two main findings showed that the sign language marker should be avoided and meanwhile, for the 3D model marker, it can work with the 2D picture or text. Therefore, it is more suitable to use sign language symbols in the AR-Book.","","978-0-7695-4043-6","10.1109/ICCRD.2010.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489589","Augemted reality;deaf;special education","Prototypes;Augmented reality;Books;Deafness;Virtual reality;Information science;Handicapped aids;Augmented virtuality;Job design;Research and development","augmented reality;computer aided instruction;handicapped aids;human computer interaction","augmented reality book;deaf students;visually oriented technique;visual learners;3D model marker;sign language symbols","","13","","33","","21 Jun 2010","","","IEEE","IEEE Conferences"
"Virtual reality for electrons in vacuum devices","A. J. Sangster","Dept. of Electr. & Electron. Eng., Heriot-Watt Univ., Edinburgh, UK","IEE Colloquium on Real World Visualisation - Virtual World - Virtual Reality","6 Aug 2002","1991","","","7/1","7/3","Vacuum tube oscillators and amplifiers at microwave frequencies have presented a considerable challenge to CAD design engineers, since successful computer modelling of such devices can lead to considerable savings in their development costs. In principle, the vacuum tube is potentially capable of very precise mathematical simulation (virtual reality) if three basic conditions prevail. These are: (1) the devices are perfectly evacuated; (2) electrodes and waveguiding structures are perfectly conducting; and (3) electrons can be viewed as 'particles'. The compilation of a computer simulation package for electron beam tubes requires the solution of two distinct field problems. Firstly, a finite difference, or finite element, representation of the Laplace of Poisson equation in the collector or gun regions of the device is required together with the observance of Lorentz forces on the electrons if magnetic focussing fields are present. Secondly, gain or oscillation in a microwave tube occurs in the beam/circuit interaction region. The 'circuit' is generally a slow-wave structure which supports a guided electromagnetic wave. The field solution for such circuits involves the numerical evaluation of the Helmholtz equation together with appropriate boundary conditions. One of the main tasks in the computer simulation of such structures entails the development of algorithms which can flexibly accommodate a wide variety of geometrical shapes.<>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=263724","","","digital simulation;electronic engineering computing;engineering graphics;finite element analysis;vacuum tubes","vacuum tube oscillators;vacuum tube amplifiers;finite difference representation;finite element representation;Laplace equation;electron gas;vacuum devices;microwave frequencies;CAD;computer modelling;virtual reality;electrodes;waveguiding structures;computer simulation package;electron beam tubes;Poisson equation;collector;Lorentz forces;magnetic focussing fields;gain;beam/circuit interaction region;slow-wave structure;Helmholtz equation;boundary conditions;computer simulation;geometrical shapes","","","","","","6 Aug 2002","","","IET","IET Conferences"
"Multi-agent based architecture for virtual reality intelligent simulation system of vehicles","Y. Yu; A. El Kamel; G. Gong","LAGIS, CNRS UMR 8129, Ecole Centrale de Lille, Villeneuve d'Ascq Cedex, France; LAGIS, CNRS UMR 8129, Ecole Centrale de Lille, Villeneuve d'Ascq Cedex, France; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)","27 Jun 2013","2013","","","597","602","Most existed researches on traffic simulation with virtual reality are mainly concentrated on the visualization or the real-time simulation to create immersion experiences in virtual world, but little attention has been paid on system modeling and formal specification of traffic simulation. Based on multi-agent technology, Virtual Reality Intelligent Simulation System of Vehicles (VR-ISSV) is proposed to investigate the modeling method of vehicles simulation system, with the advantages of reusability, scalability and flexibility. Firstly, the framework modeling is introduced to depict the general intelligent simulation system. Then, intelligent vehicle agent and environment agent are presented for the simulation of interactions among the vehicles, the traffic situation and the environment. Finally, the application results of the proposed system are realized.","","978-1-4673-5200-0","10.1109/ICNSC.2013.6548806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548806","","Solid modeling;Vehicles;Intelligent vehicles;Artificial intelligence;Three-dimensional displays;Virtual reality;Visualization","automated highways;digital simulation;multi-agent systems;road traffic;road vehicles;software reusability;traffic engineering computing;virtual reality","multiagent based architecture;Virtual Reality Intelligent Simulation System of Vehicles;vehicles simulation system modeling method;software reusability;VR-ISSV scalability;VR-ISSV flexibility;intelligent simulation system;intelligent vehicle agent;environment agent","","2","","17","","27 Jun 2013","","","IEEE","IEEE Conferences"
"Augmented reality-based assistive technology for handicapped children","L. Chien-Yu; J. Chao; H. Wei","Graduate Institute of Assistive Technology, National University of Tainan 33, Sec. 2, Shu-Lin St. Tainan, Taiwan 700; Kuo General Hospital, 22, Sec.2, Minsheng Rd. Tainan Cityy, Taiwan 700; Tainan Municipal SinSing elementary School, 22, Sinsing Rd. Tainan City, Taiwan 702","2010 International Symposium on Computer, Communication, Control and Automation (3CA)","29 Jul 2010","2010","1","","61","64","This study attempts to integrate virtual objects into real scenery based on augmented reality(AR) technology. When detected by a web-cam device, corresponding information appears on a screen to increase the interaction of assistive technology aimed at handicapped children by adopting an enhanced intuitive learning method. This study is divided training and testing steps. The training step allows researchers involved in special education to acquire AR skills and develop a unit course for handicapped individuals. In the testing step, handicapped children adopt AR as an assistive technology. Feedback of those children from the system is observed as well. Study participants are physically challenged children from kindergarten to 1st grade in elementary school. Results of this study demonstrate that AR is a highly effective assistive technology for handicapped children, offering them an innovative and interesting learning medium.","2324-8017","978-1-4244-5568-3","10.1109/3CA.2010.5533735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5533735","augmented realityt;interaction;assistive technology;childrent (key words)","Educational technology;Education;Application software;Image processing;Cameras;Virtual reality;Radiofrequency identification;Image databases;Image converters;Testing","augmented reality;computer based training;handicapped aids;image sensors","augmented reality-based assistive technology;handicapped children;virtual objects;Web-cam device;intuitive learning method;training step;testing step","","5","","14","","29 Jul 2010","","","IEEE","IEEE Conferences"
"CyPhone-experimenting mobile real-time telepresence","P. Pulli; T. Pyssysalo; J. -. Metsavainio; O. Komulainen","Oulu Univ., Finland; NA; NA; NA","Proceeding. 10th EUROMICRO Workshop on Real-Time Systems (Cat. No.98EX168)","6 Aug 2002","1998","","","10","17","Advances in multimedia, virtual reality, and immersive environments have expanded human computer interaction beyond text and vision to include touch, gestures, voice and 3D sound. Although there exist well developed single modalities for communication, we do not really understand the general problem of designing integrated multimodal systems. Recent advances in mobile communication based on picocellular technologies allow the transmission of high bandwidth data over personal surrounding networks. We analyse the sources of real time constraints in telepresence and augmented reality applications. We offer an approach to adding aspects of mobility and augmented reality to real time mobile telepresence, discuss the technology and potential future product concept vision, the CyPhone, and depict the general architecture and integration framework briefly. Finally, a survey of relevant telecooperation services are introduced.","1068-3070","0-8186-8503-4","10.1109/EMWRTS.1998.684927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=684927","","Virtual reality;Telecommunication computing;Mobile communication;Embedded computing;Augmented reality;Computer displays;Electronic mail;Telecommunication services;Pervasive computing;Ubiquitous computing","virtual reality;multimedia systems;mobile communication;user interfaces;interactive systems;real-time systems;telephony","CyPhone;mobile real time telepresence;multimedia;virtual reality;immersive environments;human computer interaction;integrated multimodal systems;mobile communication;picocellular technologies;high bandwidth data transmission;personal surrounding networks;real time constraints;augmented reality applications;real time mobile telepresence;future product concept vision;integration framework;telecooperation services","","1","1","21","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Encountered haptic Augmented Reality interface for remote examination","E. Ruffaldi; A. Filippeschi; F. Brizzi; J. M. Jacinto; C. A. Avizzano","Scuola Superiore Sant'Anna, Pisa, Italy; Scuola Superiore Sant'Anna, Pisa, Italy; Scuola Superiore Sant'Anna, Pisa, Italy; Scuola Superiore Sant'Anna, Pisa, Italy; Scuola Superiore Sant'Anna, Pisa, Italy","2015 IEEE Symposium on 3D User Interfaces (3DUI)","25 Jun 2015","2015","","","179","180","This paper presents an interaction system for haptic based remote palpation and in general remote examination. In particular the proposed approach combines 3D representation of the remote environment with encountered haptic feedback aiming at high transparency and natureleness of interaction. The paradigm is described as interaction design and system implementation.","","978-1-4673-6886-5","10.1109/3DUI.2015.7131759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131759","","Haptic interfaces;Medical services;Three-dimensional displays;Robots;Solid modeling;Augmented reality;Visualization","augmented reality;feedback;haptic interfaces;telemedicine","haptic augmented reality interface;interaction system;haptic based remote palpation examination;3D representation;haptic feedback;interaction natureleness;interaction transparency","","7","","10","","25 Jun 2015","","","IEEE","IEEE Conferences"
"Explore Voice and Foot-based Interaction Techniques to Navigate 2D Radiological Images in the Virtual Reality Operation Theatre","A. Zaman; A. Roy; K. Fatema; N. J. Farin; T. Doring; R. Malaka","University of Bremen,Digital Media Lab,Bremen,Germany; Stamford University Bangladesh,Department of Computer Science and Engineering,Dhaka,Bangladesh; Stamford University Bangladesh,Department of Computer Science and Engineering,Dhaka,Bangladesh; Stamford University Bangladesh,Department of Computer Science and Engineering,Dhaka,Bangladesh; TZI, University of Bremen,Digital Media Lab,Bremen,Germany; TZI, University of Bremen,Digital Media Lab,Bremen,Germany","2019 22nd International Conference on Computer and Information Technology (ICCIT)","19 Mar 2020","2019","","","1","7","During surgery, the surgeons often need to interact with the 2D radiological images. Due to sterility, surgeons are unable to interact with the system and depend on duty assistant. However, communication with the substitute might be complicated and error-prone if the operators and the surgeons do not have an equal level of communication skills which might interrupt the workflow. To get rid of the dependency on substitute operator, two hands-free modalities i.e. voice and foot- based interactions have been analyzed and investigated for the surgeon to interact with 2D images. To feel like a real Operation Theatre, the Virtual Reality environment has been designed, where the hands-free modalities are deployed. 16 participants had evaluated the interaction systems. To analysis, the system usability, the qualitative and quantitative studies have been considered. Our finding indicates that both systems reached general usability rating. Wherein the observation indicated that, the voice command system is comfortable to use whereas foot- based interaction is more efficient. Nevertheless, for the short and longer interaction foot-based interaction is preferable than a voice command, however, for short interaction, voice command is more acceptable.","","978-1-7281-5842-6","10.1109/ICCIT48885.2019.9038175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9038175","Foot gesture;Voice Command;Virtual Reality;Human-Computer Interaction","","interactive systems;medical image processing;radiology;speech processing;surgery;virtual reality","system usability;voice command system;foot-based interaction techniques;error-prone;communication skills;hands-free modalities;surgeon;voice-based interaction systems;virtual reality operation theatre;2D radiological images","","","","28","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Toward a model-based approach to the specification of virtual reality environments","D. Fogli; P. Mussio; A. Celentano; F. Pittarello","Dipt. di Elettronica per l'Automazione, Universita degli Studi di Brescia, Italy; Dipt. di Elettronica per l'Automazione, Universita degli Studi di Brescia, Italy; NA; NA","Fourth International Symposium on Multimedia Software Engineering, 2002. Proceedings.","25 Feb 2003","2002","","","148","155","An approach to the specification of a virtual reality (VR) interactive environment is presented, which merges and generalizes two recently proposed methods: the PCL characteristic pattern approach to WIMP system design and the interaction locus approach to interactive navigation in 3D virtual spaces. Merging the two points of view allows refinement of the model of interaction of a user with a virtual environment and leads to the definition of ""real"" and ""virtual"" characteristic patterns, which the discussion shows to be important for the designer to properly undertake the design of complex virtual reality systems.","","0-7695-1857-5","10.1109/MMSE.2002.1181607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181607","","Virtual reality;Computer interfaces;Layout;Human computer interaction;Navigation;Merging;Virtual environment;Proposals;Laboratories;Process design","virtual reality;user interfaces;formal specification","virtual reality interactive environment;specification;PCL characteristic pattern approach;WIMP system design;interaction locus approach;interactive navigation;3D virtual spaces;user interaction model;virtual characteristic pattern;real characteristic pattern","","2","","19","","25 Feb 2003","","","IEEE","IEEE Conferences"
"Implementation of General Motion Recognition Library for Smart Devices","T. Kim; S. Kim; Y. Kim; C. Kim","Dept. of Software Design & Manage., Gachon Univ., Seongnam, South Korea; Dept. of Software Design & Manage., Gachon Univ., Seongnam, South Korea; Dept. of Software Design & Manage., Gachon Univ., Seongnam, South Korea; Dept. of Software Design & Manage., Gachon Univ., Seongnam, South Korea","2013 International Conference on Information Science and Applications (ICISA)","15 Aug 2013","2013","","","1","2","Apple's iPhone has opened the market of various smart devices and changed the way of computing in daily life. In traditional PCs, mouse and keyboard are representative input devices, but, in smart devices, touch screen and motion recognition become major input methods. Especially, in game applications, motion recognition is more important to dynamically interact with users. In this paper, we propose a library of motion recognition for general interactive applications on smart devices. The proposed library provides the functions of registering motions in the development phase and recognizing a real-time motion in the running phase. By using the proposed library, any application can create and register its customized motions and recognize user's real-time motion from the registered motions in the common interface.","2162-9048","978-1-4799-0604-8","10.1109/ICISA.2013.6579452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579452","","Libraries;Real-time systems;Context;Registers;Games;Heuristic algorithms;Gesture recognition","human computer interaction;interactive systems;motion estimation;touch sensitive screens","general motion recognition library;smart devices;Apple iPhone;representative input device;touch screen;input method;game application;dynamic interaction;interactive application;motion registration;real-time motion recognition","","","","5","","15 Aug 2013","","","IEEE","IEEE Conferences"
"Distributed simulation and virtual reality visualization of multi-robot distributed receding horizon control systems","Yan Zhao; Liang Bai; B. W. Gordon","Control and Information Systems (CIS) Laboratory, Department of Mechanical and Industrial Engineering, Concordia University, Montreal, Quebec, H3G IM8, Canada; Control and Information Systems (CIS) Laboratory, Department of Mechanical and Industrial Engineering, Concordia University, Montreal, Quebec, H3G IM8, Canada; Control and Information Systems (CIS) Laboratory, Department of Mechanical and Industrial Engineering, Concordia University, Montreal, Quebec, H3G IM8, Canada","2007 IEEE International Conference on Robotics and Biomimetics (ROBIO)","16 May 2008","2007","","","1290","1295","This paper presents a new framework and experimental setup for distributed simulation and virtual reality visualization of multi-robot receding horizon control (RHC) systems. A distributed RHC algorithm for formation control of multi-robot systems is implemented on multiple computers using a UDP/IP network and synchronization functions for real-time implementation. An object oriented virtual reality (VR) environment is also developed which can be interfaced with the distributed simulation. Several types of VR ground and air vehicles can be simulated and combined with the distributed RHC system for verification of the controller in an unstructured VR environment. Furthermore, a 6 degree of freedom motion platform is integrated with the system to allow pilot vehicle interaction studies. Together this work provides a general and useful tool for development and testing of multi-robot distributed RHC control systems.","","978-1-4244-1761-2","10.1109/ROBIO.2007.4522350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522350","Real-time distributed RHC;Virtual Reality;Formation Control;6-DOF platform","Virtual reality;Visualization;Control system synthesis;Distributed control;Computational modeling;Object oriented modeling;Control systems;Multirobot systems;Computer networks;Distributed computing","control engineering computing;data visualisation;distributed control;multi-robot systems;predictive control;synchronisation;virtual reality","distributed simulation;virtual reality visualization;multirobot distributed receding horizon control systems;multirobot receding horizon control systems;formation control;UDP/IP network;synchronization functions;real-time implementation;object oriented virtual reality environment","","3","","26","","16 May 2008","","","IEEE","IEEE Conferences"
"Hybrid Control with Multi-Contact Interactions for 6DOF Haptic Foot Platform on a Cable-Driven Locomotion Interface","M. J. -. Otis; M. Mokhtari; C. du Tremblay; D. Laurendeau; F. de Rainville; C. M. Gosselin","Laval University, LVSN, e-mail: martin.otis.2@ulaval.ca; Defence Research and Development Canada; Defence Research and Development Canada; Laval University, LVSN, e-mail: laurend@gel.ulaval.ca; Defence Research and Development Canada; Laval University, Groupe de robotique, e-mail: gosselin@gmc.ulaval.ca","2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems","31 Mar 2008","2008","","","161","168","The control of haptic devices is a challenging task due to some inherent limitations and disadvantages that must first be taken into account. The difficulties are heightened when considering a locomotion interface where a walker produces large wrenches and where one must simulates the tactile sensation of any virtual object, be it rigid or soft. A Cable-Driven Locomotion Interface (CDLI), embedded as a peripheral in a virtual environment, is thus designed to address some of the aforementioned issues, since the use of cables as a mechanical transmission is known to provide many advantages such as low inertia, high speed, high acceleration and large workspace, among others. A CDLI walker would then navigate in a virtual environment with the aid of two haptic platforms (one for each foot) that can be regarded as two independent parallel robots sharing a common workspace as well as constraints in six degrees of freedom (DOF). The architecture of the CDLI framework has two components: the virtual environment manager and the controller manager. The former contains the definition of the environment in which the user navigates, as expressed by a graphic rendering engine and a communication interface. The second component computes and controls the wrenches from two physical models to accurately simulate soft and rigid virtual objects, thereby allowing haptic simulation of virtual objects using hybrid admittance/impedance with multi-contact interactions.","2324-7355","978-1-4244-2005-6","10.1109/HAPTICS.2008.4479937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479937","CDLI;cable;wire;haptic;parallel;robot;locomotion;interface;virtual;environment;H.1.1 [Models and principles]: Systems and Information Theory??General systems theory;H.5.2 [Information interfaces and presentation (e.g., HCI)]: User Interfaces??Haptic I/O","Haptic interfaces;Foot;Virtual environment;Mechanical cables;Navigation;Environmental management;Communication system control;Computational modeling;Acceleration;Parallel robots","haptic interfaces;legged locomotion;path planning;rendering (computer graphics)","multicontact interaction;haptic foot platform;cable-driven locomotion interface;tactile sensation;virtual environment;path planning;parallel robot;graphic rendering engine;communication interface","","11","","24","","31 Mar 2008","","","IEEE","IEEE Conferences"
"Interactive dynamic simulation using haptic interaction","Wookho Son; Kyunghwan Kim; N. M. Amato; J. C. Trinkle","Texas A&M Univ., TX, USA; NA; NA; NA","Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)","6 Aug 2002","2000","1","","145","150 vol.1","Describes an interactive dynamic simulator for virtual environments which allows user interaction via a haptic interface. The interactive simulation is performed in our testbed dynamic simulator I-GMS (Interactive Generalized Motion Simulator), which has been developed in an object-oriented framework for simulating motions of free bodies and complex linkages such as those needed for robotic systems or human body simulation. User interaction is achieved by performing push and pull operations via the PHANToM haptic device which runs as on integrated part of I-GMS. We demonstrate the user interaction capability of I-GMS through online editing of trajectories for a 6-DOF robot manipulator.","","0-7803-6348-5","10.1109/IROS.2000.894596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=894596","","Haptic interfaces;Object oriented modeling;Biological system modeling;Virtual environment;Performance evaluation;System testing;Couplings;Human robot interaction;Imaging phantoms;Manipulators","virtual reality;dynamics;digital simulation;mechanical engineering computing;haptic interfaces;object-oriented methods;manipulator dynamics;biomechanics;biology computing;control system analysis computing","interactive dynamic simulation;haptic interaction;virtual environments;user interaction;haptic interface;I-GMS dynamic simulator;Interactive Generalized Motion Simulator;object-oriented framework;free body motions;complex linkages;robotic systems;human body simulation;push operations;pull operations;PHANToM haptic device;online trajectory editing;robot manipulator","","1","","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Metrics for the Evaluation of Tracking Systems for Virtual Environments","E. Luckett; T. Key; N. Newsome; J. A. Jones",University of Mississippi; Rust College; Clemson University; University of Mississippi,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1711","1716","In this paper, we present three generalizable metrics by which tracking systems for virtual environments can be evaluated. These metrics include positional accuracy, rotational accuracy, and tracking resolution. Additionally, we present methods for acquiring these measurements using components commonly available at hardware and hobby shops. The methods are tested using a consumer-grade virtual reality system but are widely generalizable to most tracking systems, both professional-and consumer-grade.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798374","Human-centered computing;Interaction paradigms;Virtual reality","Resists;Position measurement;Measurement by laser beam;Three-dimensional displays;Testing;Surface emitting lasers","virtual reality","consumer-grade virtual reality system;tracking systems;virtual environments;positional accuracy;rotational accuracy;tracking resolution","","2","","11","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Virtual experiments for introduction of computing: Using virtual reality technology","F. Li; D. Li; J. Zheng; S. Zhao","School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China","2015 IEEE Frontiers in Education Conference (FIE)","7 Dec 2015","2015","","","1","5","Introduction to Computing is a public course for the first-year non-major undergraduate students, aiming at training students for the abilities in computer science and technology with computational thinking. However, as new computer technologies emerge continuously and rapidly, it is required for this course to accommodate more and more knowledge. Therefore the teaching contents are growing enormously, which makes it very difficult to cover all of them in limited hours, and therefore sets an obstacle in understanding computing principles and building up a clear and general picture of computing, especially for non-major students. As computer science and technology are becoming more and more essential for various disciplines and majors, it is urgent for the education community to find out an effective and propagable way to solve this problem. In this regard, we employ virtual reality technology to the experiment teaching of this course, and have developed 18 virtual experiments to support the whole teaching process. For example, Turing machine is a basic model for computer science and technology. However, since it is not a real machine, it is not easy for the students to imagine the working process of Turing machine and understand the related concepts. Another example, the execution of an instruction is very important to understand the principles of computer organization. However, as the information flow is invisible, it is difficult and time-consuming for the teachers to explain how an instruction is executed inside a computer. Therefore, 3D modeling and animation techniques are used to demonstrate the invisible micro-structure of computers, and human-machine interaction and visualization techniques are used to present the internal process of information evolution, thus constructing a complete virtual experiment system of this course, including demonstration experiments, verification experiments and interaction experiments. Our virtual experiments have applied software copyrights and served more than 12,000 students from five universities of China since 2013. The evaluation demonstrates that the virtual experiments have produced excellent results in both teaching effectiveness and learning efficiency, relieved the conflicts between limited hours and vast knowledge, and helped students understand and build up the knowledge of computing.","","978-1-4799-8454-1","10.1109/FIE.2015.7344376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344376","virtual experiments;Introduction to Computing;virtual reality","Computers;Random access memory;Education;Turing machines;Computational modeling;Computer science;Animation","computer aided instruction;computer animation;computer science education;solid modelling;student experiments;training;Turing machines;virtual reality","virtual experiments;virtual reality technology;introduction to computing;public course;student training;computer science;computational thinking;teaching contents;Turing machine;computer organization;3D modeling;animation;human-machine interaction;demonstration experiments;verification experiments;interaction experiments;software copyrights;teaching effectiveness;learning efficiency","","5","","10","","7 Dec 2015","","","IEEE","IEEE Conferences"
"An innovative virtual reality system for mild cognitive impairment: Diagnosis and evaluation","S. Yeh; Y. Chen; C. Tsai; A. Rizzo","Computer Science and Information Engineering Dept., National Central University, Taiwan; Computer Science and Information Engineering Dept., National Central University, Taiwan; Taipei Veterans General Hospital, Taiwan; Institute for Creative Technologies, University of Southern California, USA","2012 IEEE-EMBS Conference on Biomedical Engineering and Sciences","15 Apr 2013","2012","","","23","27","In advanced countries throughout the world, the population of Alzheimer's Disease(AD) patients has been gradually increasing with the aging of the society. As a result, it has become an important research topic how to diagnose AD early and give necessary treatment and training to AD patients, especially those with mild cognitive impairment(MCI), whose executive functions such as response inhibition, cognitive flexibility, attention switching and planning may display evident disorder and impairment. Unlike traditional paper tests and subjective assessments by the patient's relatives, this study adopts virtual reality(VR) technology to develop a novel diagnosis & assessment system, which uses head mounted display(HMD), game technology and sensors to generate an interactive and panoramic scenario-a virtual convenience store-for assessment of executive functions and memory. A variety of tasks of multi-layered difficulty-level hierarchy, such as memorizing a shopping list, looking for certain goods, and checking out, has been designed for customized and adaptive assessment, training, and treatment of MD. In the meantime, the study also records test-takers' performance data (including path and central-vision movement) in the process of all tasks for the development of a novel diagnosis & assessment method. Moreover, test-takers' technology acceptance is measured for assessing the elderly's subjective perception of new technology and discussing the topic of human-machine interaction. In the study, tests on 2 healthy adults have been completed, the system's functionality has been preliminarily verified, and test-takers' subjective perception of the system has been investigated.","","978-1-4673-1666-8","10.1109/IECBES.2012.6498023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498023","virtual realit;mild cognitive impairment;Alzheimer's Disease;executive function","","computer games;diseases;helmet mounted displays;patient diagnosis;patient treatment;sensors;virtual reality","innovative virtual reality system;mild cognitive impairment;Alzheimer disease patient;AD diagnosis;AD patient treatment;AD patient training;MCI;response inhibition;cognitive flexibility;attention switching;planning;VR;head mounted display;HMD;game technology;sensors;virtual convenience store;multilayered difficulty-level hierarchy;test-taker performance data;path movement;central-vision movement;test-taker technology acceptance;elderly subjective perception;human-machine interaction;system functionality;test-taker subjective perception","","7","","7","","15 Apr 2013","","","IEEE","IEEE Conferences"
"Implementation Study of Parachute Training Simulator","C. Duran Aygun; H. Gozde; M. Dursun; M. M. Aygun; M. C. Taplamacioglu",University of Gazi; National Defense University; University of Gazi; TOBB University of Economics and Technology; University of Gazi,"2019 6th International Conference on Electrical and Electronics Engineering (ICEEE)","12 Aug 2019","2019","","","307","311","Parachute training simulator is a powerful simulation tool that gives training starting from the position of free fall to jumpers with scenarios and graphics very close to reality and can perform this education close to reality. Parachute simulator is a cost-effective solution for paratroopers to enhance training, planning, practicing skills and mission readiness in a short notice and safe environment. Parachute training simulator designed parachute training with 3D virtual images are an effective addition to mastering all the necessary parachute dynamics and control by providing a real jump to the staff. The system provides real-time practicing the crucial skills under a wide range of weather conditions, wind and emergency situations while improving mission readiness and performance by using virtual reality with high resolution visual database. Parachute simulator training starts with free fall motion. After the parachute is turned on by the user, it is aimed to be controlled by the parachute control rivers (ripcord) to land at the specified point. A parachute simulator training system generally includes parachute training unit, visual system, instructor console, environmental voice and intercom are the basic units. In this study, the example design procedure is explained for a parachute simulator.","","978-1-7281-3910-4","10.1109/ICEEE2019.2019.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792509","parachute simulator;control applications;man machine interactions;precision motion control","Training;Solid modeling;Virtual reality;Brushless motors;Atmospheric modeling;Actuators;Real-time systems","aerodynamics;aerospace computing;aerospace testing;computer based training;design engineering;parachutes;virtual reality","mission readiness;necessary parachute dynamics;parachute control rivers;parachute simulator training system;parachute training unit;3D virtual images;weather conditions;high resolution visual database;ripcord;instructor console;environmental voice;intercom","","","","9","","12 Aug 2019","","","IEEE","IEEE Conferences"
"Effective manipulation of virtual objects within arm's reach","M. Moehring; B. Froehlich","Group Research Virtual Technologies, Volkswagen AG; Virtual Reality Systems Group, Bauhaus-Universita¨t Weimar","2011 IEEE Virtual Reality Conference","29 Apr 2011","2011","","","131","138","We present a study that compares finger-based direct interaction to controller-based ray interaction in a CAVE as well as in head-mounted displays. We focus on interaction tasks within reach of the users' arms and hands and explore various feedback methods including visual, pressure-based tactile and vibro-tactile feedback. Furthermore, we enhanced a precise finger tracking device with a direct pinch-detection mechanism to improve the robustness of grasp detection. Our results indicate that finger-based interaction is generally preferred if the functionality and ergonomics of manually manipulated virtual artifacts has to be assessed. However, controller-based interaction is often faster and more robust. In projection-based environments finger-based interaction almost reaches the task completion times and the subjective robustness of controller-based interaction if the grasping heuristics relies on our direct pinch detection. It also improves significantly by adding tactile feedback, while visual feedback proves sufficient in head-mounted displays. Our findings provide a guideline for the design of fine grain finger-based interfaces.","2375-5334","978-1-4577-0038-5","10.1109/VR.2011.5759451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759451","","Grasping;Visualization;Tactile sensors;Robustness;Performance evaluation;Optical feedback","ergonomics;force feedback;haptic interfaces;helmet mounted displays;virtual reality","effective virtual object manipulation;finger-based direct interaction;controller-based ray interaction;CAVE;head-mounted display;interaction task;user arms;user hands;visual feedback;pressure-based tactile feedback;vibro-tactile feedback;finger tracking device;direct pinch-detection mechanism;grasp detection;ergonomics;projection-based environment;grasping heuristics;fine grain finger-based interface","","27","1","24","","29 Apr 2011","","","IEEE","IEEE Conferences"
"Automatic generation of world in miniatures for realistic architectural immersive virtual environments","A. Bonsch; S. Freitag; T. W. Kuhlen","Visual Computing Institute, RWTH Aachen University, JARA - High Performance Computing; Visual Computing Institute, RWTH Aachen University, JARA - High Performance Computing; Visual Computing Institute, RWTH Aachen University, JARA - High Performance Computing","2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","155","156","Orientation and wayfinding in architectural Immersive Virtual Environments (IVEs) are non-trivial, accompanying tasks which generally support the users' main task. World in Miniatures (WIMs) - essentially 3D maps containing a scene replica - are an established approach to gain survey knowledge about the virtual world, as well as information about the user's relation to it. However, for large-scale, information-rich scenes, scaling and occlusion issues result in diminishing returns. Since there typically is a lack of standardized information regarding scene decompositions, presenting the inside of self-contained scene extracts is challenging. Therefore, we present an automatic WIM generation workflow for arbitrary, realistic in- and outdoor IVEs in order to support users with meaningfully selected and scaled extracts of the IVE as well as corresponding context information. Additionally, a 3D user interface is provided to manually manipulate the represented extract.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504700","I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques","Three-dimensional displays;Context;Data mining;Solid modeling;Buildings;Runtime;Geometry","graphical user interfaces;virtual reality","3D user interface;indoor IVE;outdoor IVE;automatic WIM generation workflow;self-contained scene extraction;scene decompositions;large-scale scenes;information-rich scenes;3D maps;automatic world in miniature generation;realistic architectural immersive virtual environments","","3","","5","","7 Jul 2016","","","IEEE","IEEE Conferences"
"A Low cost Augmented Reality system for Wide Area Indoor Navigation","V. Dosaya; S. Varshney; V. K. Parameshwarappa; A. Beniwal; S. Tak","Ramaiah Institute of Technology,Department of Information Science and Engineering,Bangalore,India; Ramaiah Institute of Technology,Department of Information Science and Engineering,Bangalore,India; Ramaiah Institute of Technology,Department of Information Science and Engineering,Bangalore,India; Ramaiah Institute of Technology,Department of Information Science and Engineering,Bangalore,India; Ramaiah Institute of Technology,Department of Information Science and Engineering,Bangalore,India","2020 International Conference on Decision Aid Sciences and Application (DASA)","15 Jan 2021","2020","","","190","195","In today's world, there are a lot of outdoor navigation apps for visually challenged people, but there are none that can precisely tell a user's location inside a large structure. Indoor navigation is a complex task for the visually challenged as well as for the general public, especially in large structures like malls, airports, museums, factories, etc. Present solutions and technologies are not cost-effective as well as complex. Hence, we are proposing a low-cost model that uses Augmented Reality to place virtual anchors across a structure, so a person can navigate from one location to another with the help of these anchors. The model doesn't use technologies like GPS. Machine Learning, and Artificial Intelligence but here, the anchors placed are pervasive and persistent across the indoor environment for smooth navigation. Once placed, these virtual anchors remain at their location and can be used at any time by any person registered on our app. This model can be extended for the general public in any indoor space and also can be enhanced by gamification for better user interaction and retention. This model can also be extended to collaborate with the Aarogya Setu app, which can help us identify routes that go through spaces through which covid positive patients have passed which in turn helps us avoid those routes in real-time navigation.","","978-1-7281-9677-0","10.1109/DASA51403.2020.9317014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317014","Augmented reality;GPS;Blind Navigation;Indoor Navigation;Mixed Reality;Unity;Covid-19;Vector Mapping","Augmented reality;Smart phones;Mobile applications;Indoor navigation;Global Positioning System;Cloud computing;COVID-19","augmented reality;computer games;handicapped aids;learning (artificial intelligence);mobile computing","wide area indoor navigation;outdoor navigation apps;visually challenged people;complex task;general public;low-cost model;virtual anchors;machine learning;artificial intelligence;indoor environment;smooth navigation;indoor space;user interaction;Aarogya Setu app;real-time navigation;low cost augmented reality system","","","","10","","15 Jan 2021","","","IEEE","IEEE Conferences"
"Development of a driving VR prototype for distraction awareness employing eye tracking","R. Blanco; J. Calle; A. Uribe-Quevedo","Universidad Militar Nueva Granada, Bogota, Colombia; Universidad Militar Nueva Granada, Bogota, Colombia; Universidad Militar Nueva Granada, Bogota, Colombia","2017 IEEE 6th Global Conference on Consumer Electronics (GCCE)","21 Dec 2017","2017","","","1","4","According to the World Health Organization, car accidents are amongst the top ten worldwide causes of death as a result of speeding, driving while under the influence of alcohol or other psychoactive substances, non-use of safety restraints, and distractions. The current appropriation and impact of mobile devices are changing how people perform daily activities, which is leading to distractions that are increasing the risk of traffic accidents. Attention skills play an important role while driving and have become a field of study that is employing professional driving simulators to create awareness amongst drivers about the dangers of distractions. However, high-end simulators are not available to the general population due to costs and technical requirements, and often used in industry and research. In this paper, we present the development of a virtual simulator that employs consumer grade eye-tracking for measuring attention during driving while receiving incoming calls and texts on a mobile device. The eye tracking metrics provide gaze data that allows determining what caused a distraction during the simulation.","","978-1-5090-4045-2","10.1109/GCCE.2017.8229447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229447","","Mobile handsets;Automobiles;Roads;Accidents;Visualization;Keyboards","eye;human computer interaction;medical signal detection;neurophysiology;road accidents;road safety;virtual reality","mobile device;traffic accidents;attention skills;professional driving simulators;virtual simulator;consumer grade eye-tracking;eye tracking metrics;driving VR prototype;distraction awareness;World Health Organization;car accidents;psychoactive substances;safety restraints","","","","13","","21 Dec 2017","","","IEEE","IEEE Conferences"
"Verbal/Nonverbal Communication Permitting User to Communicate with Virtual Environment without Special Instrument","T. Yoshikawa; N. Abe; K. Tanaka; H. Taki; T. Yagi; Shoujie He","Kyushu Institute of Technology; Kyushu Inst. of Technol., Kazuaki; Kyushu Inst. of Technol., Kazuaki; NA; NA; NA","11th International Conference on Parallel and Distributed Systems (ICPADS'05)","21 Nov 2005","2005","2","","191","195","In this research, a dialog environment between human and virtual environment has been constructed. At preset, if a person wants to interact with virtual environment, special device such a data glove is required, but it makes difficult for general users to manipulate virtual objects. When we cannot manipulate objects directly, it is natural that we ask someone with a privilege to do the operation in place of us. In the case, it is convenient if methods used in daily life are allowed. That is, the method to be proposed here is based on the integration between verbal information through the utterances and non-verbal information by the gestures such as finger pointing. The experimental results have proved the effectiveness of this approach in terms of facilitating man-machine interaction and communication. The environment constructed in this research allows a user to communicate by talking and showing gestures to a personified agent in virtual environment. A user can use his/her finger to point at a virtual object and ask him to manipulate it","1521-9097","0-7695-2281-5","10.1109/ICPADS.2005.293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524285","","Virtual environment;Instruments;Fingers;Assembly systems;Data gloves;Speech recognition;Systems engineering and theory;Yagi-Uda antennas;Helium;Humans","gesture recognition;graphical user interfaces;human computer interaction;virtual reality","verbal-nonverbal communication;dialog environment;gesture recognition;man-machine interaction","","","","8","","21 Nov 2005","","","IEEE","IEEE Conferences"
"Effects of an in-car augmented reality system on improving safety of younger and older drivers","Wai-Tat Fu; J. Gasper; S. Kim","Beckman Institute of Science and Technology, University of Illinois at Urbana-Champaign, USA; Beckman Institute of Science and Technology, University of Illinois at Urbana-Champaign, USA; Beckman Institute of Science and Technology, University of Illinois at Urbana-Champaign, USA","2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","23 Dec 2013","2013","","","59","66","We designed and tested the effects of an in-car augmented reality system (ARS) on younger and older drivers, with and without a secondary distraction task. When potential danger is detected, the ARS alerts the driver by progressively indicating the time to collision to the lead vehicle as well as merging vehicles from side lanes by an AR display that overlaps with the lead or merging vehicles. We tested the ARS with younger (18-30) and older (65-75) drivers in a high-fidelity driving simulator. Results showed that the ARS could significantly reduce collisions caused by hazard events such as sudden slowing of the lead vehicle or merging of vehicles from sides lanes. Consistent with previous results, older drivers, despite age-related decline in cognitive and motor abilities, could leverage their driving experience to avoid forward collisions with the lead vehicle as much as younger drivers. However, older drivers were poorer in avoiding collisions caused by sudden merging events than younger drivers. The ARS was found to be most useful in helping older adults to avoid collision caused by sudden hazard events, especially with the presence of a distraction task. The ARS was also more effective for older than younger drivers to encourage a safe driving distance with the lead vehicle. Interestingly, there seemed to be differential effects of the ARS on the general driving behavior of younger and older drivers. While older drivers in general became more careful and safer in how they drive with the ARS, younger drivers seemed to rely on the ARS to alert them to potential hazard events without adopting safer driving behavior.","","978-1-4799-2869-9","10.1109/ISMAR.2013.6671764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671764","","Vehicles;Augmented reality;Merging;Hazards;Visualization;Accidents;Analysis of variance","accident prevention;augmented reality;human computer interaction;traffic engineering computing","in-car augmented reality system;younger driver;older driver;distraction task;high-fidelity driving simulator;hazard event;cognitive ability;motor ability;forward collision;driving behavior","","3","","30","","23 Dec 2013","","","IEEE","IEEE Conferences"
"Altering the Stiffness, Friction, and Shape Perception of Tangible Objects in Virtual Reality Using Wearable Haptics","S. V. Salazar; C. Pacchierotti; X. de Tinguy; A. Maciel; M. Marchal","University Rennes, CNRS, Inria, IRISA, Rennes, France; University Rennes, CNRS, Inria, IRISA, Rennes, France; University Rennes, INSA, IRISA, Inria, CNRS, Rennes, France; Federal University of Rio Grande do Sul, Porto Alegre, Brazil; University Rennes, INSA, IRISA, Inria, CNRS, Rennes, France","IEEE Transactions on Haptics","11 Mar 2020","2020","13","1","167","174","Tangible objects are used in virtual reality (VR) and augmented reality (AR) to enhance haptic information on the general shape of virtual objects. However, they are often passive or unable to simulate rich varying mechanical properties. This article studies the effect of combining simple passive tangible objects and wearable haptics for improving the display of varying stiffness, friction, and shape sensations in these environments. By providing timely cutaneous stimuli through a wearable finger device, we can make an object feel softer or more slippery than it really is, and we can also create the illusion of encountering virtual bumps and holes. We evaluate the proposed approach carrying out three experiments with human subjects. Results confirm that we can increase the compliance of a tangible object by varying the pressure applied through a wearable device. We are also able to simulate the presence of bumps and holes by providing timely pressure and skin stretch sensations. Altering the friction of a tangible surface showed recognition rates above the chance level, albeit lower than those registered in the other experiments. Finally, we show the potential of our techniques in an immersive medical palpation use case in VR. These results pave the way for novel and promising haptic interactions in VR, better exploiting the multiple ways of providing simple, unobtrusive, and inexpensive haptic displays.","2329-4051","","10.1109/TOH.2020.2967389","European Union's Horizon 2020 Research and Innovation Programme; Fapergs-Brazil PqG 2017 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961106","Haptic interfaces;tactile feedback;human computer interaction.","Haptic interfaces;Shape;Pistons;Skin;Friction;End effectors;Force","augmented reality;haptic interfaces;human computer interaction;image resolution;medical image processing;skin","wearable finger device;wearable device;VR;haptic interactions;virtual reality;augmented reality;haptic information;virtual objects;mechanical properties;tangible object friction;tangible object stiffness;tangible object shape perception;immersive medical palpation","Adult;Biomechanical Phenomena;Equipment Design;Feedback, Sensory;Female;Friction;Humans;Male;Surface Properties;Touch Perception;Virtual Reality;Wearable Electronic Devices;Young Adult","1","","29","IEEE","16 Jan 2020","","","IEEE","IEEE Journals"
"Stability of Nonlinear Time-Delay Systems Describing Human–Robot Interaction","F. Müller; J. Jäkel; J. Suchý; U. Thomas","Chair of Robotics and Human-Machine-Interaction Lab, Chemnitz University of Technology, Chemnitz, Germany; Department of Electrical Engineering and Information Technology, Leipzig University of Applied Science, Leipzig, Germany; Chair of Robotics and Human-Machine-Interaction Lab, Chemnitz University of Technology, Chemnitz, Germany; Chair of Robotics and Human-Machine-Interaction Lab, Chemnitz University of Technology, Chemnitz, Germany","IEEE/ASME Transactions on Mechatronics","1 Jan 2020","2019","24","6","2696","2705","In this paper, we present sufficient conditions for the stability analysis of a stationary point for a special type of nonlinear time-delay systems. These conditions are suitable for analyzing systems describing physical human-robot interaction (pHRI). For this stability analysis a new human model consisting of passive and active elements is introduced and validated. The stability conditions describe parameterization bounds for the human model and an impedance controller. The results of this paper are compared to stability conditions based on passivity, approximated time-delays and to numerical approaches. As a result of the comparison, it is shown that our conditions are more general than the passivity condition of Colgate and Schenkel. This includes the consideration of negative stiffness and nonlinear virtual environments. As an example, a pHRI including a nonlinear virtual environment with a polynomial structure is introduced and also successfully analyzed. These theoretical results could be used in the design of robust controllers and stability observers in pHRI.","1941-014X","","10.1109/TMECH.2019.2939907","Federal Ministry for Economic Affairs and Energy of Germany; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851257","Impedance control;Lyapunov–Krasovskii functional;nonlinear time-delay systems;physical human–robot interaction (pHRI)","Numerical stability;Impedance;Biological system modeling;Stability criteria;Human-robot interaction;Analytical models","control system analysis;control system synthesis;delay systems;human-robot interaction;nonlinear control systems;robust control;stability","stability analysis;physical human-robot interaction;pHRI;human model;stability conditions;approximated time-delays;nonlinear virtual environment;stability observers;nonlinear time-delay systems stability;robust controllers","","","","35","CCBY","27 Sep 2019","","","IEEE","IEEE Journals"
"Grasp Recognition with Uncalibrated Data Gloves - A Comparison of Classification Methods","G. Heumer; H. B. Amor; M. Weber; B. Jung","VR and Multimedia Group, Institute of Informatics, TU Bergakademie Freiberg, Germany. e-mail: guido.heumer@informatik.tu-freiberg.de; VR and Multimedia Group, Institute of Informatics, TU Bergakademie Freiberg, Germany. e-mail: amor@informatik.tu-freiberg.de; VR and Multimedia Group, Institute of Informatics, TU Bergakademie Freiberg, Germany. e-mail: matthias.weber@informatik.tu-freiberg.de; VR and Multimedia Group, Institute of Informatics, TU Bergakademie Freiberg, Germany. e-mail: jung@informatik.tu-freiberg.de","2007 IEEE Virtual Reality Conference","23 Apr 2007","2007","","","19","26","This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out, where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was analyzed with 28 classifiers including different types of neural networks, decision trees, Bayes nets, and lazy learners. Each classifier was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.","2375-5334","1-4244-0905-5","10.1109/VR.2007.352459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4161001","Data Glove;Calibration;Grasp Recognition;Classification Methods;I.3.6 [Computer Graphics]: Methodology and Techniques¿Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism¿[Virtual Reality]","Data gloves;Calibration;Shape;Sensor phenomena and characterization;Testing;Taxonomy;Data analysis;Classification tree analysis;Neural networks;Decision trees","calibration;data gloves;pattern classification;virtual prototyping;virtual reality","grasp recognition;uncalibrated data gloves;classification methods;object manipulation;Schlesinger taxonomy;cumbersome calibration","","31","","24","","23 Apr 2007","","","IEEE","IEEE Conferences"
"Design of general object terminals for measure & control systems based on embedded platform","Zhigang Bing; Jian Zhang; Yan Li","School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Jinnan District, China; TEDA Cable Television Network Co., Ltd, Binhai New Area, Tianjin, China; Tianjin Handsbrains Sci-Tech Co., Ltd, Jinnan District, China","Proceeding of the 11th World Congress on Intelligent Control and Automation","5 Mar 2015","2014","","","5864","5868","In measure & control related education and training, the specifications such as cost, flexibility, user experience, and the actual operability are impossible combinations for traditional solutions and products of general object (including control object, sensor and actuator, etc.). In order to resolve this problem, a general object terminal was designed, which combined real electrical signal with virtual 3D scene. The mechanical field and other unmanageable fields are transformed to electrical field through mathematical modeling and process channel techniques, which is easier to be controlled. The embedded system provides both electrical signals representing the input and output characteristics of a variety of general objects and 3D interaction to enhance the real experience for users. In the design and implementation of the object terminal, the real time performance, design ability and expansibility are fully considered. Furthermore, combined with the business service software on the server and the standard optional modules, the designed object terminal constructs a novel practical training and evaluation platform which complements virtuality with reality, and combines software with hardware.","","978-1-4799-5825-2","10.1109/WCICA.2014.7053722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053722","measure and control system;general object;embedded system;virtual reality","Three-dimensional displays;Graphics processing units;Field programmable gate arrays;Automation;Control systems;Training","control engineering computing;embedded systems;solid modelling;virtual reality","general object terminal design;measure & control systems;embedded platform;user experience;electrical signal;virtual 3D scene;mechanical field;unmanageable fields;electrical field;mathematical modeling;process channel techniques;3D interaction;real time performance;design ability;business service software;standard optional modules;object terminal;training platform;evaluation platform","","","","","","5 Mar 2015","","","IEEE","IEEE Conferences"
"Sapporo world window Urban interaction through public and private screens","J. H. Choi; J. Seeburger","Institute for Creative Industries and Innovation, Queensland University of Technology, Brisbane, Australia; Institute for Creative Industries and Innovation, Queensland University of Technology, Brisbane, Australia","2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)","12 May 2011","2011","","","508","512","This paper introduces Sapporo World Window, a screen-based application that is currently under development for the new underway passage at the centre of Sapporo City. There are ten large public screens installed in the space, displaying user-generated videos about various aspects of the city and a real-time map that visualises users' interaction with the city. The application aims to engage the general public by functioning as a unique `point of connection' for socio-cultural and technological interactions, making the space a lively social place where people can have meaningful experiences of interacting with people and places of Sapporo through mobile phones (keitai) and the public screens in the space. This paper first outlines the contextual background and key concept for the application's design. Then the paper discusses the user interaction processes, technical specifications, and interface design, followed by the conclusions and outlook.","","978-1-61284-937-9","10.1109/PERCOMW.2011.5766942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766942","social networking;urban screens;locative media;mobile interaction;urban informatics","Cities and towns;Mobile handsets;Visualization;Media;Mobile communication;User interfaces;Servers","mobile computing;public information systems;screens (display);social sciences computing;user interfaces","Sapporo World Window;screen based application;underway passage;public screens;user generated video;real-time map;user interaction;socio-cultural interaction;technological interaction;mobile computing;interface design","","4","","11","","12 May 2011","","","IEEE","IEEE Conferences"
"An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality","D. Monteiro; H. -N. Liang; J. Wang; H. Chen; N. Baghaei",Xi’an Jiaotong-Liverpool University; Xi’an Jiaotong-Liverpool University; Xi’an Jiaotong-Liverpool University; Xi’an Jiaotong-Liverpool University; Massey University,"2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","14 Dec 2020","2020","","","713","724","The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness.","1554-7868","978-1-7281-8508-8","10.1109/ISMAR50242.2020.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284718","Virtual Reality;2D/3D Views;Controller types;First Person Shooter;Gaming;Head-Mounted Displays","Performance evaluation;Visualization;Three-dimensional displays;Two dimensional displays;Keyboards;Games;Turning","computer games;data visualisation;helmet mounted displays;virtual reality","virtual reality;input techniques;VR experience;VR FPS;typical 2D display;controller type;2D visual technique;VR headset;first person shooter games;commercial VR head-mounted displays;2D-3D views;PlaneFrame","","","","54","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Poor Thing! Would You Feel Sorry for a Simulated Robot? A comparison of empathy toward a physical and a simulated robot","S. H. Seo; D. Geiskkovitch; M. Nakane; C. King; J. E. Young","University of Manitoba, Winnipeg, Manitoba, Canada; University of Manitoba, Winnipeg, Manitoba, Canada; University of Manitoba, Winnipeg, Manitoba, Canada; ZenFri Inc., Winnipeg, Manitoba, Canada; University of Manitoba, Winnipeg, Manitoba, Canada","2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","4 Nov 2018","2015","","","125","132","In designing and evaluating human-robot interactions and interfaces, researchers often use a simulated robot due to the high cost of robots and time required to program them. However, it is important to consider how interaction with a simulated robot differs from a real robot; that is, do simulated robots provide authentic interaction? We contribute to a growing body of work that explores this question and maps out simulated-versus-real differences, by explicitly investigating empathy: how people empathize with a physical or simulated robot when something bad happens to it. Our results suggest that people may empathize more with a physical robot than a simulated one, a finding that has important implications on the generalizability and applicability of simulated HRI work. Empathy is particularly relevant to social HRI and is integral to, for example, companion and care robots. Our contribution additionally includes an original and reproducible HRI experimental design to induce empathy toward robots in laboratory settings, and an experimentally validated empathy-measuring instrument from psychology for use with HRI. Categories and Subject Descriptors H.5.2 [User Interfaces]: evaluation/methodology General Terms Experimentation and Human Factors.","2167-2121","978-1-4503-2882-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520653","Human-robot interaction;simulated interaction;robot embodiment;empathy","Robots;Psychology;Instruments;Videos;Human-robot interaction;Programming;Computational modeling","human factors;human-robot interaction","empathy;human-robot interactions;physical robot;simulated HRI work;simulated robot","","","","40","","4 Nov 2018","","","IEEE","IEEE Conferences"
"Seated Immersive Exergaming for Fall Prevention of Older Adults","S. Rings; F. Steinicke; T. Picker; C. Prasuhn",Universitäat Hamburg; Universitäat Hamburg; Hochschule Düsseldorf; Universitäat Hamburg,"2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","11 May 2020","2020","","","289","290","Virtual reality (VR) exergames provide a unique opportunity for developing safe and effective therapies for older adults at assisted living facilities. This group of people has an increased risk of falls, which can lead to severe injuries and increase the risk of falls even further. An early detailed individualized assessment and treatment intervention for older adults with preexisting conditions is recommend, but the workload for physicians is high and training is often perceived as boring because exercises generally do not change that often. In this paper we introduce a seated VR exergame for fall prevention of older adults. The game has been designed and developed together with clinical experts and therapists to provide adequate fall prevention exercises, which can be implemented in patients daily schedules and administered through a VR exergame. Movements that control the exergame match the motions suggested by our partner physicians and improve balance by shifting the players center of mass.","","978-1-7281-6532-5","10.1109/VRW50115.2020.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090601","Human-centered computing;Human-computer interaction (HCI);Interaction Paradigms;virtual reality","Training;Cameras;Medical treatment;Assisted living;Task analysis;Torso","biomechanics;biomedical equipment;computer games;diseases;geriatrics;patient care;patient diagnosis;patient treatment;virtual reality","safe therapies;assisted living facilities;treatment intervention;seated VR exergame;fall prevention exercises","","","","10","","11 May 2020","","","IEEE","IEEE Conferences"
"Survivor Buddy and SciGirls: Affect, outreach, and questions","R. Murphy; V. Srinivasan; N. Rashidi; B. Duncan; A. Rice; Z. Henkel; M. Garza; C. Nass; V. Groom; T. Zourntos; R. Daneshwar; S. Prasad","Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; Computer Science and Engineering, Texas A and M University, College Station, TX- 77843; CHIME Lab, Stanford University, Stanford, CA. 94305-2050; CHIME Lab, Stanford University, Stanford, CA. 94305-2050; Electrical & Computer Engineering, Texas A& M University, College Station, TX 77843; Electrical & Computer Engineering, Texas A& M University, College Station, TX 77843; Electrical & Computer Engineering, Texas A& M University, College Station, TX 77843","2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","22 Apr 2010","2010","","","127","128","This paper describes the Survivor Buddy human-robot interaction project and how it was used by four middle-school girls to illustrate the scientific process for an episode of ""SciGirls"", a Public Broadcast System science reality show. Survivor Buddy is a four degree of freedom robot head, with the face being a MIMO 740 multi-media touch screen monitor. It is being used to explore consistency and trust in the use of robots as social mediums, where robots serve as intermediaries between dependents (e.g., trapped survivors) and the outside world (doctors, rescuers, family members). While the SciGirl experimentation was neither statistically significant nor rigorously controlled, the experience makes three contributions. It introduces the Survivor Buddy project and social medium role, it illustrates that human-robot interaction is an appealing way to make robotics more accessible to the general public, and raises interesting questions about the existence of a minimum set of degrees of freedom for sufficient expressiveness, the relative importance of voice versus non-verbal affect, and the range and intensity of robot motions.","2167-2148","978-1-4244-4892-0","10.1109/HRI.2010.5453233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453233","assistive robots;human-robot interaction;gaze and gestures;interaction styles;robots;user interfaces","Human robot interaction;Cognitive robotics;TV;Computer science;Broadcasting;MIMO;Monitoring;Robot motion;User interfaces;Testing","human-robot interaction;user interfaces","SciGirls;survivor buddy human-robot interaction;middle-school girls;Public Broadcast System science reality show;freedom robot head;MIMO;multimedia touch screen monitor;social medium role;robotics","","","","11","","22 Apr 2010","","","IEEE","IEEE Conferences"
"Turning Augmented Reality into a media: Design exploration to build a dedicated visual language","N. Henchoz; V. Lepetit; P. Fua; J. Miles",EPFL+ECAL Lab; EPFL CVLab; EPFL CVLab; Space3D Solutions,"2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities","1 Dec 2011","2011","","","83","89","This work collects the explorations conducted within the EPFL+ECAL Lab by several designers to interpret the various spheres of action of Augmented Reality in order to derive visual principles. These principles seek to contribute to developing a specific visual grammar, which is essential if Augmented Reality is to go beyond technological performance to acquire the status of a true media, like all other vision-based media. This visual grammar constitutes a reference based on which designers may then build projects whose narrative and/or emotional content captivates the end-user. The research presented starts from an artistic approach through design and derives generic principles. The starting point is the development of actual AR installations and their presentation to the general public in the form of an evolving exhibition entitled Give Me More. Following two initial presentations in Lausanne and San Francisco, Give Me More was the winner of the DMY International Design Festival Berlin 2010. The designers' work relies on the software and research of the EPFL Computer Vision Laboratory (CVLab) and Dr Julien Pilet. The results have made it possible to define certain rules for visual links between the physical and virtual worlds, simple narrative principles for the animations and a global approach for the design of representation devices. The results also show that there is much work yet to be done in order to devise an initial global visual grammar for Augmented Reality - an absolute prerequisite to turn this into a fully-fledged media.","2381-8360","978-1-4673-0059-9","10.1109/ISMAR-AMH.2011.6093661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093661","Art;augmented reality;computer vision;interaction design;image recognition;media design;object recognition;perception;pervasive computing","Augmented reality;Visualization;Animation;Media;Computer vision;Art;Lighting","augmented reality;computer animation;computer vision;grammars;human computer interaction;multimedia computing;visual languages","augmented reality;design exploration;dedicated visual language;EPFL+ECAL Lab;visual principles;visual grammar;vision-based media;generic principles;Give Me More;DMY International Design Festival Berlin 2010;EPFL Computer Vision Laboratory;Dr Julien Pilet;CVLab","","1","","16","","1 Dec 2011","","","IEEE","IEEE Conferences"
"Detection of 3D position of eyes through a consumer RGB-D camera for stereoscopic mixed reality environments","M. Chessa; M. Garibotti; G. Maiello; L. Caroggio; H. Huang; S. Sabatini; F. Solari","Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy; Dept. of Informatics, Bioengineering, Robotics and System Engineering, University of Genoa, Italy","2014 International Conference on 3D Imaging (IC3D)","9 Feb 2015","2014","","","1","8","A novel approach to track the 3D position of the user's eyes in stereoscopic virtual environments, where stereo glasses are worn, is proposed. Such an approach improves a state-of-the-art real-time face tracking algorithm by addressing the occlusion due the stereo glasses and providing estimation of eye position based on biométrie features. More generally, our solution can be seen as a proof of concept for a more robust approach to improving motion tracking techniques. In particular, the proposed technique yields accurate and stable estimates of the 3D position of the user's eyes, while the user moves in front of the stereoscopic display. The correct tracking of both eyes' 3D position is a crucial step in order to achieve a more natural human-computer interaction which diminishes visual fatigue. The proposed approach is validated through quantitative tests: (i) we assessed the accuracy of our algorithm for tracking the 3D position of users' eyes with and without stereo glasses; (ii) we have performed a perceptual assessment of the natural interaction in the virtual environments through experimental sessions with several users.","2379-1780","978-1-4799-8023-9","10.1109/IC3D.2014.7032592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032592","Human-computer interactions;Virtual reality;Augmented reality;stereoscopic 3D entertainment system;computer vision","Face;Glass;Three-dimensional displays;Stereo image processing;Virtual reality;Solid modeling;Tracking","computer graphics;face recognition;human computer interaction;image colour analysis;stereo image processing","3D position detection;consumer RGB-D camera;stereoscopic mixed reality environments;users eyes 3D position tracking;stereoscopic virtual environments;stereo glasses;face tracking algorithm;occlusion;biometric features;motion tracking techniques;stereoscopic display;human computer interaction;visual fatigue","","","","13","","9 Feb 2015","","","IEEE","IEEE Conferences"
"Immersive and non-immersive virtual reality system to learn relative motion concepts","M. Kozhevnikov; J. Gurlitt","Department of Engineering, Norfolk State University, VA USA; Department of Educational Science, University of Freiburg, Germany","2013 3rd Interdisciplinary Engineering Design Education Conference","10 Jun 2013","2013","","","168","172","The focus of the current study is to understand the strength and limits of immersive virtual environments as a new media for learning and teaching relative motion concepts. Our results show that while training in both Immersive Virtual Environment (IVE) and Desktop (non-immersive) Virtual Environment (DVE) resulted in a significant improvement on relative motion problem solving test in general, the IVE group performed significantly better than the DVE group on solving two-dimensional relative motion problems after training in the simulations. This result supports our hypothesis that egocentric encoding of the scene in IVE (where the learner constitutes a part of a scene being immersed in it) as compared to allocentric encoding on a computer screen in DVI (where the earner is looking on the scene from “outside”) is beneficial for studying two-dimensional problems.","","978-1-4673-5112-6","10.1109/IEDEC.2013.6526781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526781","Virtual Environment;Educational Technology;Immersivity;Relative Motion","Computational modeling;Solid modeling;Switches;Abstracts;Reliability;Real-time systems;Virtual reality","computer aided instruction;human computer interaction;teaching;user interfaces;virtual reality","immersive virtual reality system;non immersive virtual reality system;relative motion concept learning;relative motion concept teaching;desktop virtual environment;IVE;DVE;motion problem solving test;educational technology","","2","","14","","10 Jun 2013","","","IEEE","IEEE Conferences"
"Research on VRI: Virtual World and Real World Interface","M. R. Syamsuddin; C. H. Lee; Y. Kwon","Korea Inst. of Sci. & Technol., Seoul, South Korea; Korea Inst. of Sci. & Technol., Seoul, South Korea; Korea Inst. of Sci. & Technol., Seoul, South Korea","2009 International Symposium on Ubiquitous Virtual Reality","4 Sep 2009","2009","","","72","75","Many virtual worlds and interaction devices (in Real World) have been developed and used. However,each virtual world developed specific interface layer for each device. This paper aims to discuss about general interface layer that can bridge between Virtual Worlds and Real World, so that a virtual world can communicate with many devices, also in vice versa.The main idea of this paper is to make virtual worlds or interaction devices can be configured in the standard format.","","978-1-4244-4437-3","10.1109/ISUVR.2009.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232259","VRI;Virtual World Interface;Interaction Device Interface","Avatars;Computer networks;Virtual reality;Bridges;Second Life;Virtual environment;Delay;Communication system control;Humans;Computer network management","user interfaces;virtual reality","VRI;virtual world interface;real world interface;interaction devices","","1","","18","","4 Sep 2009","","","IEEE","IEEE Conferences"
"An Interaction Tool for Immersive Environments Using Mobile Devices","D. P. d. S. Medeiros; F. G. d. Carvalho; A. B. Raposo; I. H. F. d. Santos","Tecgraf - Comput. Graphics Group PUC-Rio (Pontificia Univ. Catolica do Rio de Janeiro), Rio de Janeiro, Brazil; Tecgraf - Comput. Graphics Group PUC-Rio (Pontificia Univ. Catolica do Rio de Janeiro), Rio de Janeiro, Brazil; Tecgraf - Comput. Graphics Group PUC-Rio (Pontificia Univ. Catolica do Rio de Janeiro), Rio de Janeiro, Brazil; Petrobras, CENPES, Rio de Janeiro, Brazil","2013 XV Symposium on Virtual and Augmented Reality","7 Nov 2013","2013","","","90","96","Interaction in engineering virtual environments differs by the necessity of the high precision level needed for the execution of specifics tasks for this kind of environment. Generally this kind of task uses specific interaction devices with 4 or more DOF. Current applications involving 3D interaction use interaction devices for object modelling or for the implementation of navigation, selection and manipulation tecniques in a virtual environment. A related problem is the necessity of controlling tasks that are naturally non-immersive, such as symbolic input (e.g., text, photos). Another problem is the large learning curve to handle such non-conventional devices. The addition of sensors and the popularization of smartphones and tablets, allowed the use of such devices in virtual engineering environments. Thes devices, besides their popularity and sensors, differs by the possibility of including additional information and performing naturally non-immersive tasks. This work presents a 3D interaction tablet-based tool, which allows the aggregation of all major 3D interaction topics, such as navigation, selection, manipulation, system control and symbolic input.","","978-0-7695-5001-5","10.1109/SVR.2013.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655766","3D Interaction;virtual reality;mobile devices;virtual environments","Three-dimensional displays;Smart phones;Solid modeling;Virtual environments;Navigation;Sensors","mobile computing;notebook computers;smart phones;task analysis;virtual reality","interaction tool;immersive environment;mobile devices;task execution;interaction device;object modelling;navigation;selection technique;manipulation technique;virtual environment;task control;symbolic input;smartphones;virtual engineering environment;naturally nonimmersive task;3D interaction tablet-based tool;system control","","2","","22","","7 Nov 2013","","","IEEE","IEEE Conferences"
"Instrument-Tissue Segment Interaction Using Finite Element Modeling","A. Alsaraira; I. Brown; R. McColl; F. Lim","Monash University Centre for Biomedical Engineering, Department of Electrical and Computer Systems Engineering, PO BOX 72, Monash University, 3800, Australia. Phone: +61 3 9905 1821; fax: +61 3 9905 3454; e-mail: Amer.Alsaraira@eng.monash.edu.au; Assoc. Prof., Member, IEEE, Monash University Centre for Biomedical Engineering, Department of Electrical and Computer Systems Engineering, PO BOX 72, Monash University, 3800, Australia; Monash University Centre for Biomedical Engineering, Department of Electrical and Computer Systems Engineering, PO BOX 72, Monash University, 3800, Australia; Monash University Centre for Biomedical Engineering, Department of Electrical and Computer Systems Engineering, PO BOX 72, Monash University, 3800, Australia","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","22 Oct 2007","2007","","","2760","2763","A virtual reality based laparoscopic surgery simulator is an important training option for laparoscopic surgeons. It has significant advantages over other training methods. Instruments-anatomy interactions are one of the main features of these simulators. In this paper we present the deformation of the uterine tube using three dimensional finite element methods with finite element software. The work examines the feasibility of incorporating the finite element (FE) model within the visual graphic model to achieve high degree of realism of instrument-tissue interactions.","1558-4615","978-1-4244-0787-3","10.1109/IEMBS.2007.4352900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4352900","","Instruments;Finite element methods;Solid modeling;Deformable models;Computational modeling;Laparoscopes;Minimally invasive surgery;Humans;Biological system modeling;Computational geometry","biological organs;biological tissues;biomechanics;biomedical education;computer based training;computer graphics;deformation;finite element analysis;medical computing;physiological models;surgery;virtual reality","instrument-tissue segment interaction model;three dimensional finite element modeling;virtual reality;laparoscopic surgery simulator;laparoscopic surgery training;uterine tube deformation;finite element software;visual graphic model","Computer-Assisted Instruction;Elasticity;Finite Element Analysis;General Surgery;Laparoscopy;Models, Anatomic;Robotics;Software","1","","14","","22 Oct 2007","","","IEEE","IEEE Conferences"
"A generalized perception filter for distributed virtual environments","Jiang Du; S. J. Turner; Bu Sung Lee","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore","2002 IEEE Region 10 Conference on Computers, Communications, Control and Power Engineering. TENCOM '02. Proceedings.","28 Feb 2003","2002","1","","297","300 vol.1","A distributed virtual environment (DVE) allows a group of geographically dispersed users to interact in real time. To provide a usable system, interactions with objects and other users must be observed by participants with sufficient consistency and within a limited delay. However in a wide area network (WAN), there can be considerable communication delays between different users. The main effects will be in the visual mode of the environment, where delays can cause discontinuous state changes and jumps in the positions of objects. This paper presents a generalized perception filter for hiding the effects of latency due to communication delays. It extends previous work in this area by considering both periodic updates and updates due to a dead reckoning model. The algorithm is also suitable for DVE that are implemented using the High Level Architecture.","","0-7803-7490-8","10.1109/TENCON.2002.1181273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181273","","Filters;Delay effects;Virtual environment;Wide area networks;Military standards;Dead reckoning;Standards development;Protocols;Joining processes;Computational efficiency","virtual reality;wide area networks;real-time systems;delays;IEEE standards;open systems;simulation","distributed virtual environment;generalized perception filter;DVE;real time interaction;wide area network;WAN;communication delays;visual mode;latency hiding;periodic updates;dead reckoning model;High Level Architecture","","","","6","","28 Feb 2003","","","IEEE","IEEE Conferences"
"Augmented Reality for the Abstract Paintings: Application Scenarios, Semantic Similarity Analysis and Case Study","O. Golembovska; V. Kharchenko; I. Shostak; M. Danova; O. Feoktystova; V. Plietnov","“Carte Blanche” Magazine,Kyiv,Ukraine; National Aerospace University “KhAI”,Kharkiv,Ukraine; National Aerospace University “KhAI”,Kharkiv,Ukraine; National Aerospace University “KhAI”,Kharkiv,Ukraine; National Aerospace University “KhAI”,Kharkiv,Ukraine; National Aerospace University “KhAI”,Kharkiv,Ukraine","2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","5 Dec 2019","2019","2","","1007","1011","The approach to the analysis of the perception by visitors of abstract visual objects of modern art with elements of augmented reality is presented. The methodological basis of the approach are: a set of private scenarios of interaction between individual visitors and their groups with the objects exhibited; generalized scenario of lexico-semantic analysis of the proximity of the visitors verbal assessments to the name of the object; method of verbal semantic differential. Is considered an example of a quantitative assessment obtaining of the semantic proximity verbal assessments of visitors to the name of a visual object. The application of the discussed approach will provide the possibility of automatic adjustment (based on the technology of the Internet of Things) of augmented reality elements in abstract visual objects of art, with a view to a deeper understanding of their audience.","","978-1-7281-4069-8","10.1109/IDAACS.2019.8924411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924411","augmented reality;verbal semantic differential method;abstract paintings;semantic similarity analysis;scenarios approach","Semantics;Visualization;Art;Augmented reality;Internet of Things;Painting;Information systems","art;augmented reality;data visualisation;Internet of Things","abstract paintings;semantic similarity analysis;abstract visual objects;modern art;generalized scenario;lexico-semantic analysis;visitors verbal assessments;quantitative assessment;semantic proximity verbal assessments;visual object;augmented reality elements","","","","10","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality","M. Feick; S. Bateman; A. Tang; A. Miede; N. Marquardt","University College London,London,UK; University of New Brunswick,Fredericton,Canada; University of Toronto,Toronto,Canada; htw saar,Saarbruecken,Germany; University College London,London,UK","2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","14 Dec 2020","2020","","","195","206","Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers and were generally favored by participants.","1554-7868","978-1-7281-8508-8","10.1109/ISMAR50242.2020.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284771","Virtual Reality;Tangible Interfaces;VR Object Exploration and Manipulation;Tangible Proxy Objects","Shape;Manipulators;Augmented reality","manipulators;virtual reality","complex virtual objects;free-hand interaction techniques;TanGi toolkit;physical proxy objects;object exploration;object embodiment;manipulators;virtual reality;composable shape primitives","","","","66","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Gesture3DFramework: A Generic Gesture-Based Interaction Middleware Applied to 3D Environments","D. Passos Costa; P. N. M. Sampaio; V. F. Martins","Computing and Systems Graduate Program Salvador University (UNIFACS) Salvador, Bahia, Brazil; Computing and Systems Graduate Program Salvador University (UNIFACS) Salvador, Bahia, Brazil; School of Computing and Informatics Marckenzie Presbiterian University São Paulo, São Paulo, Brazil","2018 XLIV Latin American Computer Conference (CLEI)","5 Aug 2019","2018","","","590","598","The technological advances provide the development and wide adoption of different kinds of humanmachine interfaces, which leads to the creation of new applications such as those based on multimedia and virtual reality (3D). In particular, the proposal of interaction metaphors applied to 3D environments which aim at replicating real world concepts into the virtual environment, facilitates user's interaction. The utilization of gestural interaction metaphors within a 3D environment can turn the user experience more familiar and concrete, making the training curb smaller. However, in order to apply interaction metaphors it is necessary their classification and generalization, so that they can be widely deployed in different applications. This paper introduces the development of a generic and customizable solution for the mediation of user gestural interaction (selection, manipulation and navigation) with heterogeneous rendering engines for virtual reality environments. This solution, called Gesture3DFramework, allows the users context and gesture-metaphors configuration to be easily customized so that it can be adaptable to multiple 3D virtual environments. With Gesture3DFramework, the final user (and developer) will be provided with a higher level of abstraction when it comes to the development of interactive virtual reality applications, since once the configuration directives have been described, the system will adapt itself to the specific interaction routines of the applied rendering engine.","","978-1-7281-0437-9","10.1109/CLEI.2018.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786315","Virtual Reality;Gesture Interaction;Selection;Manipulation;Navigation;Immersion;3D environment","Three-dimensional displays;Casting;Navigation;Middleware;Virtual environments;Rendering (computer graphics)","gesture recognition;graphical user interfaces;human computer interaction;middleware;rendering (computer graphics);virtual reality","generic gesture-based interaction middleware;user experience;user gestural interaction;multiple 3D virtual environments;interactive virtual reality applications;Gesture3DFramework;rendering engine;interaction routines","","","","","","5 Aug 2019","","","IEEE","IEEE Conferences"
