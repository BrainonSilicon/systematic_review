"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"VIVR: Presence of Immersive Interaction for Visual Impairment Virtual Reality","J. Kim","Division of Computer Engineering, Hansung University, Seoul, South Korea","IEEE Access","6 Nov 2020","2020","8","","196151","196159","The immersive virtual reality (VR) to provide a realistic walking experience for the visually impaired is proposed in this study. To achieve this, a novel immersive interaction using a walking aid, i.e., a white cane, is designed. The key structure of the proposed interaction consists of a walking process that enables users with visual impairments to process the ground recognition and inference processes realistically by connecting the white cane to the VR controller. Additionally, a decision-making model using deep learning is proposed to design interactions that can be applied to real-life situations instead of being limited to virtual environment experiences. A learning model is designed that can accurately and efficiently process sensing of braille block, which is an important process in the walking of visually impaired people using a white cane assistance tool. The goal is to implement a white cane walking system that can be used in the real world in addition to a virtual environment. Finally, a survey is conducted to confirm that the proposed immersive interaction provides a walking experience with high presence in virtual reality when compared with the real-world experience. The applicability of the proposed deep-learning-based decision-making model in the real world is verified by its high accuracy in recognition of braille block.","2169-3536","","10.1109/ACCESS.2020.3034363","Hansung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241733","Immersive virtual reality;presence;immersive interaction;visual impairment;deep learning","Legged locomotion;Visualization;Haptic interfaces;Solid modeling;Virtual environments;Deep learning","decision making;gait analysis;handicapped aids;learning (artificial intelligence);neural nets;virtual reality","realistic walking experience;immersive interaction;walking aid;walking process;ground recognition;inference processes;VR controller;decision-making;deep learning;virtual environment experiences;braille block;visually impaired people;white cane assistance tool;white cane walking system;real-world experience;visual impairment virtual reality;immersive virtual reality;VIVR","","","","34","CCBY","28 Oct 2020","","","IEEE","IEEE Journals"
"Polyvalent display framework to control virtual navigations by 6DOF tracking","P. Bourdot; D. Touraine","Lab. d'Informatique pour la Mecanique et les Sci. de l'Ingenieur, CNRS, Orsay, France; NA","Proceedings IEEE Virtual Reality 2002","7 Aug 2002","2002","","","277","278","Many applications using Virtual Reality (VR) require free hand navigation control. This forbids the use of devices such as Joystick, 3D mouse, ... That is generally because hand-based gesture interaction must be dedicated to specific tasks: sculpting or grasping the objects, which compose a virtual scene. With this aim, we present a polyvalent display approach, which allows the system to determine the desired focus and the moving intention of the user for virtual navigations. After we point out the limits of the classical solutions used to control virtual navigations, we describe the conceptual and mathematical implementation of our approach. Some of our considerations depend on physiological and ergonomic constraints. We conclude on the first applications of our fully head tracking navigation controller.","1087-8270","0-7695-1492-8","10.1109/VR.2002.996537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996537","","Displays;Virtual reality;Layout;Aircraft navigation;Mice;Equations;Grasping;Taxonomy;Virtual environment;Keyboards","virtual reality;navigation;tracking","Virtual Reality;free hand navigation control;gesture interaction;polyvalent display;virtual scene;head tracking;virtual environments","","8","","8","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Development of Eye-Gaze Interface System and Its Application to Virtual Reality Controller","H. F. Putra; K. Ogata","Graduate School of Science and Technology (GSST), Kumamoto University, Kumamoto, Japan; Graduate School of Science and Technology (GSST), Kumamoto University, Kumamoto, Japan","2018 International Conference on Computer Engineering, Network and Intelligent Multimedia (CENIM)","13 May 2019","2018","","","208","213","Eye-gaze tracking systems are a type of human-machine interface application. These systems have been widely applied in many fields, such as medical, interface systems, and studies on human behavior. In this study, an eye-gaze tracking system is designed to control the view of a virtual reality application. As is generally known, gyroscopes and game controllers are popular devices to control the view of virtual reality applications. To control the view of a virtual reality application, we measure the vertical and horizontal movements of the eye-gaze. Using ring-shaped pattern matching, we find the position of the iris and estimate the direction of the eye-gaze. The vertical and horizontal movement of the eye-gaze are transformed into the rotation of virtual reality camera view in the x and y axes, respectively. The ease of use of the system was evaluated in terms of the distance traveled by the user and the time taken to complete the task.","","978-1-5386-7509-0","10.1109/CENIM.2018.8710926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710926","eye-gaze;tracking;interface;virtual reality","Virtual reality;Cameras;Iris;Image color analysis;Games;Pattern matching;Lenses","cameras;computer vision;eye;gaze tracking;human computer interaction;user interfaces;virtual reality","horizontal movement;vertical movement;virtual reality camera view;eye-gaze interface system;virtual reality controller;eye-gaze tracking system;human-machine interface application;interface systems;virtual reality application;game controllers","","1","","10","","13 May 2019","","","IEEE","IEEE Conferences"
"Genesys: A Virtual Reality scene builder","J. D. O. De Leon; R. P. Tavas; R. A. Aranzanso; R. O. Atienza","Ubiquitous Computing Laboratory, Electrical and Electronics Engineering Institute, University of the Philippines Diliman, Quezon City, Philippines; Ubiquitous Computing Laboratory, Electrical and Electronics Engineering Institute, University of the Philippines Diliman, Quezon City, Philippines; Ubiquitous Computing Laboratory, Electrical and Electronics Engineering Institute, University of the Philippines Diliman, Quezon City, Philippines; Ubiquitous Computing Laboratory, Electrical and Electronics Engineering Institute, University of the Philippines Diliman, Quezon City, Philippines","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","3708","3711","Virtual Reality (VR) is a computer technology which simulates real world environments and a user's interactions within it. Current trends in VR, however, are focused on the gaming industry with limited work on VR creation tools. Thus, we propose a VR application that allows users to create 3-dimensional scenes in virtual reality named Genesys. Scenes created are viewed with the use of VR head mounted displays which provide the user with a more immersive work environment compared to traditional 2D monitors. Interactions within this workspace are done through motion controllers that allow users to manipulate with objects and spawn more from an online repository. Scenes created can be shared to the Genesys community of artists, creators and users through an online website. This feature widens the project's applications in education, business, and entertainment as a world builder and presentation tool. To measure the effectiveness of the application, a scene composition test was conducted at the end of the experiment. The results showed that Genesys is generally easy to use for people with limited experience in VR. However, improvements can still be made on the object manipulation controls such as rotation and scaling.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848751","","Virtual reality;Games;Three-dimensional displays;Engines;Industries;Google;Software","helmet mounted displays;human computer interaction;user interfaces;virtual reality","Genesys;virtual reality scene builder;computer technology;VR creation tools;user interactions;gaming industry;three-dimensional scenes;VR head mounted displays;immersive work environment;motion controllers;online Web site;scene composition test;object manipulation controls","","1","","17","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Continuous automatic calibration for optical see-through displays","K. R. Moser; Y. Itoh; J. E. Swan",Mississippi State University; Technical University of Munich; Mississippi State University,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","241","242","The current advent of consumer level optical see-through (OST) head-mounted displays (HMD's) has greatly broadened the accessibility of Augmented Reality (AR) to not only researchers but also the general public as well. This increased user base heightens the need for robust automatic calibration mechanisms suited for nontechnical users. We are developing a fully automated calibration system for two stereo OST HMD's, a consumer level and prototype model, based on the recently introduced interaction free display calibration (INDICA) method. Our current efforts are also focused on the development of an evaluation process to assess the performance of the system during use by non-expert subjects.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223385","Calibration;OST HMD;Augmented Reality","Calibration;Cameras;Hardware;Augmented reality;Robustness;Prototypes;Head","augmented reality;helmet mounted displays","continuous automatic calibration system;consumer level optical see-through head-mounted displays;augmented reality;robust automatic calibration mechanisms;interaction free display calibration method;INDICA method","","1","","6","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Evaluating Multiple Levels of an Interaction Fidelity Continuum on Performance and Learning in Near-Field Training Simulations","A. Bhargava; J. W. Bertrand; A. K. Gramopadhye; K. C. Madathil; S. V. Babu",Clemson University; Clemson University; Clemson University; Clemson University; Clemson University,"IEEE Transactions on Visualization and Computer Graphics","13 Mar 2018","2018","24","4","1418","1427","With costs of head-mounted displays (HMDs) and tracking technology decreasing rapidly, various virtual reality applications are being widely adopted for education and training. Hardware advancements have enabled replication of real-world interactions in virtual environments to a large extent, paving the way for commercial grade applications that provide a safe and risk-free training environment at a fraction of the cost. But this also mandates the need to develop more intrinsic interaction techniques and to empirically evaluate them in a more comprehensive manner. Although there exists a body of previous research that examines the benefits of selected levels of interaction fidelity on performance, few studies have investigated the constituent components of fidelity in a Interaction Fidelity Continuum (IFC) with several system instances and their respective effects on performance and learning in the context of a real-world skills training application. Our work describes a large between-subjects investigation conducted over several years that utilizes bimanual interaction metaphors at six discrete levels of interaction fidelity to teach basic precision metrology concepts in a near-field spatial interaction task in VR. A combined analysis performed on the data compares and contrasts the six different conditions and their overall effects on performance and learning outcomes, eliciting patterns in the results between the discrete application points on the IFC. With respect to some performance variables, results indicate that simpler restrictive interaction metaphors and highest fidelity metaphors perform better than medium fidelity interaction metaphors. In light of these results, a set of general guidelines are created for developers of spatial interaction metaphors in immersive virtual environments for precise fine-motor skills training simulations.","1941-0506","","10.1109/TVCG.2018.2794639","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8260967","Bimanual Interaction;Interaction Fidelity;Empirical Evaluation;Educational Virtual Reality","Training;Aerospace electronics;Solid modeling;Metrology;Mice;Virtual environments","computer based training;helmet mounted displays;virtual reality","Interaction Fidelity Continuum;near-field training simulations;virtual reality applications;education;real-world interactions;safe risk-free training environment;intrinsic interaction techniques;near-field spatial interaction task;learning outcomes;discrete application points;performance variables;medium fidelity interaction metaphors;spatial interaction metaphors;immersive virtual environments;fine-motor skills training simulations;bimanual interaction metaphors;restrictive interaction metaphors;head-mounted displays;tracking technology","Adolescent;Adult;Cognition;Female;High Fidelity Simulation Training;Humans;Male;Surveys and Questionnaires;Task Performance and Analysis;User-Computer Interface;Virtual Reality;Young Adult","1","","33","Traditional","17 Jan 2018","","","IEEE","IEEE Journals"
"A Physiology-Based QoE Comparison of Interactive Augmented Reality, Virtual Reality and Tablet-Based Applications","C. Keighrey; R. Flynn; S. Murray; N. Murray","Athlone Institute of Technology, Athlone, Ireland; Athlone Institute of Technology, Athlone, Ireland; Health Service Executive, Primary Care Centre, Longford, Ireland; Athlone Institute of Technology, Athlone, Ireland","IEEE Transactions on Multimedia","17 Dec 2020","2021","23","","333","341","The availability of affordable head-mounted display technology has facilitated new, potentially more immersive, interactive multimedia experiences. These technologies were traditionally focused on entertainment; however, academia and industry are now exploring applications in other domains such as health, learning and training. Key to the success of these new multimedia experiences is the understanding of a user's perceived quality of experience (QoE). Subjective user ratings have been the primary mechanism to capture insights into a user's experience. Such ratings have generally been captured post experience and reflected using a mean opinion score (MOS). However, user perception is multifactorial and subjective ratings alone do not express the true measure of an experience. As a result, recent efforts to capture QoE have included exploring the use of implicit metrics (e.g., physiological measures). This article presents the results of an experimental QoE evaluation and comparison of immersive applications delivered across three multimedia platforms. The platforms compared were augmented reality, tablet and virtual reality. The QoE methodology employed considered explicit (post-test questionnaire) and implicit (heart rate and electrodermal activity) assessment methods. The results indicate comparatively higher levels of QoE for users of the augmented reality and tablet platforms.","1941-0077","","10.1109/TMM.2020.2982046","Irish Research Council - Government of Ireland Postgraduate Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042281","Quality of experience;augmented reality;virtual reality;physiological;speech language pathology;aphasia","Quality of experience;Semantics;Multimedia systems;Augmented reality;Biomedical monitoring","augmented reality;helmet mounted displays;human computer interaction;mobile computing;multimedia systems;quality of experience;user interfaces","user experience;perceived quality of experience;tablet based applications;physiology based QoE comparison;physiological measures;mean opinion score;interactive multimedia experiences;immersive multimedia experiences;head mounted display;virtual reality;interactive augmented reality","","7","","35","IEEE","19 Mar 2020","","","IEEE","IEEE Journals"
"A novel menu interaction method using head-mounted display for smartphone-based virtual reality","C. Sheng; L. Jiang; B. Tang; X. Tang","College of Electronic Science and Engineering, NUDT, Changsha 410073, China; College of Electronic Science and Engineering, NUDT, Changsha 410073, China; College of Electronic Science and Engineering, NUDT, Changsha 410073, China; College of Electronic Science and Engineering, NUDT, Changsha 410073, China","2017 Progress In Electromagnetics Research Symposium - Spring (PIERS)","18 Jan 2018","2017","","","2384","2388","The menu interaction design method for smartphone-based VR is currently one of the most popular research domains. The existing menu interaction method can be generally divided into two categories: the interaction devices based method and ray selection based method. However, both of them have some inevitable defects, which lead to bad user experience. In this paper, a novel interaction method is proposed, combining the advantages of the ray selection technology and HMD technology. And a smartphone-based VR application demonstration named “Target Arena” is developed. In addition, the user experience is evaluated by conducting a survey on several aspects of the demonstration system such as portability, controllability, comfort degree, and overall user experience. Experimental results show that the menu interaction method proposed in the paper performs better than other existing methods.","","978-1-5090-6269-0","10.1109/PIERS.2017.8262151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8262151","","Resists;Games;Virtual environments;Head;Smart phones;Electromagnetics;Springs","helmet mounted displays;interactive devices;smart phones;user interfaces;virtual reality","head-mounted display;smartphone-based virtual reality;menu interaction design method;existing menu interaction method;interaction devices;ray selection based method;ray selection technology;smartphone-based VR application demonstration","","","","4","","18 Jan 2018","","","IEEE","IEEE Conferences"
"Towards a linguistically motivated model for selection in virtual reality","T. Pfeiffer","A.I. Group, Faculty of Technology, Bielefeld University","2012 IEEE Virtual Reality Workshops (VRW)","12 Apr 2012","2012","","","89","90","Swiftness and robustness of natural communication is tied to the redundancy and complementarity found in our multimodal communication. Swiftness and robustness of human-computer interaction (HCI) is also a key to the success of a virtual reality (VR) environment. The interpretation of multimodal interaction signals has therefore been considered a high goal in VR research, e.g. following the visions of Bolt's put-that-there in 1980 [1]. It is our impression that research on user interfaces for VR systems has been focused primarily on finding and evaluating technical solutions and thus followed a technology-oriented approach to HCI. In this article, we argue to complement this by a human-oriented approach based on the observation of human-human interaction. The aim is to find models of human-human interaction that can be used to create user interfaces that feel natural. As the field of Linguistics is dedicated to the observation and modeling of human-human communication, it could be worthwhile to approach natural user interfaces from a linguistic perspective. We expect at least two benefits from following this approach. First, the human-oriented approach substantiates our understanding of natural human interactions. Second, it brings about a new perspective by taking the interaction capabilities of a human addressee into account, which are not often explicitly considered or compared with that of the system. As a consequence of following both approaches to create user interfaces, we expect more general models of human interaction to emerge.","2375-5334","978-1-4673-1246-2","10.1109/VR.2012.6180896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180896","H.5.2 [Information Interfaces and Presentation]: User Interfaces — Natural Language;I.3.6 [Computer Graphics]: Methodology and Techniques — Interaction Techniques;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial;Augmented and Virtual Realities","Pragmatics;Humans;Virtual reality;Human computer interaction;Solid modeling;Context","human computer interaction;user interfaces;virtual reality","linguistically motivated model;virtual reality environment;natural communication;multimodal communication;human-computer interaction;HCI;multimodal interaction signals;VR research;Bolt put-that-there;technology-oriented approach;human-oriented approach;human-human interaction;human-human communication;natural user interfaces;human addressee","","1","","13","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Research and Application of Access Control Technique in 3D Virtual Reality System OpenSim","Y. Wei; Y. Lu; X. Hu; B. Sun","Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China; Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China; Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China; Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China","2013 Sixth International Symposium on Computational Intelligence and Design","24 Apr 2014","2013","2","","65","68","Access control in 3-D virtual reality systems is a wide and still growing topic. A good access control model is a premise for data security, and makes the whole system play its functions reliably. We compare access control techniques in 3D system OpenSim with that of other virtual reality systems. By using a general extended scheme, we analyze the model and the rule of access control in OpenSim. In this scheme, we provide a method of expanding network services for special proposes. Meanwhile, it verifies the feasibility of developing OpenSim's services on the basis of data security.","","978-0-7695-5079-4","10.1109/ISCID.2013.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804829","Access Control;Capability;Virtual Reality;OpenSim","Access control;Servers;Virtual reality;Three-dimensional displays;Animation;Solid modeling","authorisation;human computer interaction;public domain software;virtual reality","access control technique;3D virtual reality system;OpenSim;data security;system functions;general extended scheme;network service expansion","","","","13","","24 Apr 2014","","","IEEE","IEEE Conferences"
"An infrastructure for realizing custom-tailored augmented reality user interfaces","W. Broll; I. Lindt; J. Ohlenburg; I. Herbst; M. Wittkamper; T. Novotny","Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany; Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany; Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany; Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany; Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany; Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany","IEEE Transactions on Visualization and Computer Graphics","26 Sep 2005","2005","11","6","722","733","Augmented reality (AR) technologies are rapidly expanding into new application areas. However, the development of AR user interfaces and appropriate interaction techniques remains a complex and time-consuming task. Starting from scratch is more common than building upon existing solutions. Furthermore, adaptation is difficult, often resulting in poor quality and limited flexibility with regard to user requirements. In order to overcome these problems, we introduce an infrastructure for supporting the development of specific AR interaction techniques and their adaptation to individual user needs. Our approach is threefold: a flexible AR framework providing independence from particular input devices and rendering platforms, an interaction prototyping mechanism allowing for fast prototyping of new interaction techniques, and a high-level user interface description, extending user interface descriptions into the domain of AR. The general usability and applicability of the approach is demonstrated by means of three example AR projects.","1941-0506","","10.1109/TVCG.2005.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512022","Index Terms- Distributed systems;distributed applications;multimedia information systems;artificial;augmented;and virtual realities;user interfaces;collaborative computing;graphics systems;distributed/network graphics;methodology and techniques;device independence;graphics data structures and data types;interaction techniques;three-dimensional graphics and realism;virtual reality.","Augmented reality;User interfaces;Virtual reality;Application software;Graphics;Prototypes;Computer interfaces;Wearable computers;Computer displays;Iris","augmented reality;data structures;groupware;human computer interaction;multimedia computing;rendering (computer graphics);user interfaces","custom-tailored augmented reality;user interface;rendering platform;interaction prototyping mechanism;distributed system;distributed application;multimedia information systems;artificial reality;augmented reality;virtual reality;collaborative computing;graphics system;distributed-network graphics;device independence;graphics data structure;data type;3D graphics","Algorithms;Computer Simulation;Computer Systems;Cybernetics;Data Display;Environment;Imaging, Three-Dimensional;Man-Machine Systems;Models, Theoretical;Online Systems;User-Computer Interface","18","13","44","","26 Sep 2005","","","IEEE","IEEE Journals"
"Brain Activity in Virtual Reality: Assessing Signal Quality of High-Resolution EEG While Using Head-Mounted Displays","S. Hertweck; D. Weber; H. Alwanni; F. Unruh; M. Fischbach; M. E. Latoschik; T. Ball","Translational Neurotechnology Lab, University Medical Center Freiburg; Translational Neurotechnology Lab, University Medical Center Freiburg; Translational Neurotechnology Lab, University Medical Center Freiburg; Human-Computer Interaction Lab, University of Wuerzburg; Human-Computer Interaction Lab, University of Wuerzburg; Human-Computer Interaction Lab, University of Wuerzburg; Translational Neurotechnology Lab, University Medical Center Freiburg","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","970","971","Biometric measures such as the electroencephalogram (EEG) promise to become viable alternatives to subjective questionnaire ratings for the evaluation of psychophysical effects associated with Virtual Reality (VR) systems, as they provide objective and continuous measurements without breaking the exposure. The extent to which the EEG signal can be disturbed by the presence of VR systems, however, has been barely investigated. This study outlines how to evaluate the compatibility of a given EEG-VR setup on the example of two commercial head-mounted displays (HMDs), the Oculus Rift and the HTC Vive Pro. We use a novel experimental protocol to compare the spectral composition between conditions with and without an HMD present during an eyes-open vs. eyes-closed task. We found general artifacts at the line hum of 50 Hz, and additional HMD refresh rate artifacts (90 Hz) for the Oculus rift exclusively. Frequency components typically most interesting to non-invasive EEG research and applications , however, remained largely unaffected. We observed similar topographies of visually-induced modulation of alpha band power for both HMD conditions in all subjects. Hence, the study introduces a necessary validation test for HMDs in combination with EEG and further promotes EEG as a potential biometric measurement method for psychophysical effects in VR systems.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798369","","Electroencephalography;Frequency measurement;Resists;Electrodes;Virtual reality;Task analysis;Physiology","electroencephalography;helmet mounted displays;medical signal processing;neurophysiology;virtual reality","signal quality;addHMD refresh rate artifacts;biometric measurement method;EEG-VR setup;head-mounted displays;HTC Vive Pro;Oculus Rift;EEG signal;Virtual Reality systems;electroencephalogram;high-resolution EEG;brain activity;spectral composition;frequency 50.0 Hz;frequency 90.0 Hz","","1","","9","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Using relative head and hand-target features to predict intention in 3D moving-target selection","J. S. Casallas; J. H. Oliver; J. W. Kelly; F. Merienne; S. Garbaya","Iowa State University Arts et Metiers ParisTech; Virtual Reality Applications Center, Iowa State University; Department of Psychology, Iowa State University; Institut Image Arts et Métiers ParisTech; Institut Image Arts et Métiers ParisTech","2014 IEEE Virtual Reality (VR)","24 Apr 2014","2014","","","51","56","Selection of moving targets is a common, yet complex task in human-computer interaction (HCI) and virtual reality (VR). Predicting user intention may be beneficial to address the challenges inherent in interaction techniques for moving-target selection. This article extends previous models by integrating relative head-target and hand-target features to predict intended moving targets. The features are calculated in a time window ending at roughly two-thirds of the total target selection time and evaluated using decision trees. With two targets, this model is able to predict user choice with up to ~ 72% accuracy on general moving-target selection tasks and up to ~ 78% by also including task-related target properties.","2375-5334","978-1-4799-2871-2","10.1109/VR.2014.6802050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802050","H.5.2 [Information interfaces and presentation]: User Interfaces — Interaction Styles, Theory and methods;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism — Virtual Reality;I.5.4 [Pattern Recognition]: Applications","Accuracy;Predictive models;Decision trees;Three-dimensional displays;Solid modeling;Virtual reality;Human computer interaction","decision trees;human computer interaction;virtual reality","3D moving-target selection;human-computer interaction;HCI;virtual reality;VR;user intention prediction;decision trees;user choice prediction;task-related target properties;relative hand-target features;relative head-target features","","","","42","","24 Apr 2014","","","IEEE","IEEE Conferences"
"Ginput: a tool for fast hi-fi prototyping of gestural interactions in virtual reality","J. R. Fonseca; J. Abreu; L. Figueredo; J. G. Neto; V. Teichrieb; J. P. Quintino; F. Q. B. da Silva; A. L. M. Santos; H. Pinho","Universidade Federal de,Voxar Labs / CIn,Pernambuco; Universidade Federal de,Voxar Labs / CIn,Pernambuco; Universidade Federal de,Voxar Labs / CIn,Pernambuco; Universidade Federal de,Voxar Labs / CIn,Pernambuco; Universidade Federal de,Voxar Labs / CIn,Pernambuco; Universide Federal de,Projeto CIn-Samsung,Pernambuco; CIn / Universidade Federal de,Pernambuco; CIn / Universidade Federal de,Pernambuco; Desenvolvimento para a Informática,Samsung Instituto de","2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","16 Dec 2020","2020","","","63","64","Gestural interfaces in virtual reality (VR) expand the design space for user interaction, allowing spatial metaphors with the environment and more natural and immersive experiences. Typically, machine learning approaches recognize gestures with models that rely on a large number of samples for the training phase, which is an obstacle for rapidly prototyping gestural interactions. In this paper, we propose a solution designed for hi-fi prototyping of gestures within a virtual reality environment through a high-level Domain-Specific Language (DSL), as a subset of the natural language. The proposed DSL allows non-programmer users to intuitively describe a broad domain of poses and connect them for compound gestures. Our DSL was designed to be general enough for multiple input classes, such as body tracking, hand tracking, head movement, motion controllers, and buttons. We tested our solution for wands with VR designers and developers. Results showed that the tool gives non-programmers the ability to prototype gestures with ease and refine its recognition within a few minutes.","","978-1-7281-7675-8","10.1109/ISMAR-Adjunct51615.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288432","Human-centered computing;Visualization;Visualization techniques;Treemaps; Human-centered computing;Visualization;Visualization design and evaluation methods","Training;Solid modeling;Tracking;Natural languages;Tools;DSL;Task analysis","gesture recognition;high level languages;human computer interaction;learning (artificial intelligence);motion control;object tracking;software prototyping;virtual reality","gestural interfaces;design space;user interaction;spatial metaphors;immersive experiences;machine learning;virtual reality environment;high-level domain-specific language;DSL;natural language;nonprogrammer users;compound gestures;prototype gestures;fast hi-fi prototyping;natural experience;gesture recognition;gestural interaction rapid prototyping;body tracking;hand tracking;head movement;motion controllers;buttons;Ginput","","","","7","","16 Dec 2020","","","IEEE","IEEE Conferences"
"Progressive feedback point cloud rendering for virtual reality display","R. Tredinnick; M. Broecker; K. Ponto",Wisconsin Institute for Discovery - University of Wisconsin-Madison; Wisconsin Institute for Discovery - University of Wisconsin-Madison; Wisconsin Institute for Discovery - University of Wisconsin-Madison,"2016 IEEE Virtual Reality (VR)","7 Jul 2016","2016","","","301","302","Previous approaches to rendering large point clouds on immersive displays have generally created a trade-off between interactivity and quality. While these approaches have been quite successful for desktop environments when interaction is limited, virtual reality systems are continuously interactive, which forces users to suffer through either low frame rates or low image quality. This paper presents a novel approach to this problem through a progressive feedback-driven rendering algorithm. This algorithm uses reprojections of past views to accelerate the reconstruction of the current view. The presented method is tested against previous methods, showing improvements in both rendering quality and interactivity.","2375-5334","978-1-5090-0836-0","10.1109/VR.2016.7504773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504773","Point-based graphics;virtual reality;3D scanning","Rendering (computer graphics);Three-dimensional displays;Graphics processing units;Measurement;Virtual reality;Feedback loop;Geometry","rendering (computer graphics);virtual reality","progressive feedback point cloud rendering;virtual reality display;immersive displays;interactivity;desktop environments;virtual reality systems;progressive feedback-driven rendering algorithm","","6","","3","","7 Jul 2016","","","IEEE","IEEE Conferences"
"Work-in-Progress—A Generalizable Virtual Reality Training and Intelligent Tutor for Additive Manufacturing","M. Mogessie; S. D. Wolf; M. Barbosa; N. Jones; B. M. McLaren","Carnegie Mellon University,Human-Computer Interaction Institute,Pittsburgh,PA,United States; Carnegie Mellon University,NextManufacturing Center,Pittsburgh,PA,United States; Carnegie Mellon University,Human-Computer Interaction Institute,Pittsburgh,PA,United States; Carnegie Mellon University,NextManufacturing Center,Pittsburgh,PA,United States; Carnegie Mellon University,Human-Computer Interaction Institute,Pittsburgh,PA,United States","2020 6th International Conference of the Immersive Learning Research Network (iLRN)","4 Aug 2020","2020","","","355","358","There is currently significant demand for training in how to use metals additive manufacturing (AM) machines. Such training is important not only for the technicians who run and maintain the machines, but also for engineers and strategic decision makers who need to support AM part fabrication. Furthermore, there are a variety of AM machines, each with different details to be learned and potential hazards to overcome, and it is difficult to train more than a handful of users at one time. To address these challenges, a prototype training system has been developed, the AM Training Tutor, which uses interactive virtual reality (VR) to train users on a specific AM machine - the EOS M290. To make the training technology more widely available and expand its use across a variety of different AM machines, efforts are underway to develop a modularized and generic version of the AM Training Tutor that can be customized with relatively little effort to train users to operate other AM machines. This work-in-progress paper details the progress to-date, challenges and proposed solutions with the aim to demonstrate how standalone VR-based training systems can be redesigned for relatively easy repurposing and generalization.","","978-1-7348995-0-4","10.23919/iLRN47897.2020.9155119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155119","generalized VR;VR-based training;workforce training;cognitive tutor;advanced manufacturing;additive manufacturing;3D printing","Training;Tutorials;Three-dimensional printing;Three-dimensional displays;Solid modeling;Earth Observing System","computer based training;decision making;human computer interaction;intelligent tutoring systems;interactive systems;production engineering computing;rapid prototyping (industrial);virtual reality","intelligent tutor;strategic decision makers;AM machines;prototype training system;AM Training Tutor;interactive virtual reality;specific AM machine;training technology;standalone VR-based training systems;generalizable virtual reality training;metals additive manufacturing machines;EOS M290","","","","17","","4 Aug 2020","","","IEEE","IEEE Conferences"
"3D Interaction assistance through context-awareness","Y. Dennemont; G. Bouyer; S. Otmane; M. Mallem","IBISC Laboratory, Evry University, France; IBISC Laboratory, Evry University, France; IBISC Laboratory, Evry University, France; IBISC Laboratory, Evry University, France","2012 IEEE Virtual Reality Workshops (VRW)","12 Apr 2012","2012","","","103","104","This work focuses on enabling 3D interaction assistance by adding adaptivity depending on the tasks, objectives, and the general interaction context. We model the context using Conceptual Graphs (CG) based on an ontology. Including CG in our scene manager (Virtools) allows us to add semantic information and to describe the available tools. We handle rules leading to adaptation with a logic programming layer (Prolog+CG) included in the Amine platform. This project is a step towards Intelligent Virtual Environments, which proposes a hybrid solution by adding a separate semantic reasoning to classic environments. The first case study automatically manages few modalities depending on the distance to objects, user movement, available tools and modality risks.","2375-5334","978-1-4673-1246-2","10.1109/VR.2012.6180903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180903","I.3.6 [Methodology and Techniques Subjects]: Interaction Techniques;[I.3.7]: Computer Graphics — Virtual reality;I.2.4 [Computing Methodologies]: Knowledge Representation Formalism and Methods","Context;Engines;Semantics;Cognition;Three dimensional displays;Usability;Visualization","human computer interaction;ontologies (artificial intelligence);PROLOG;ubiquitous computing;virtual reality","3D interaction assistance;context-awareness;conceptual graphs;ontology;scene manager;Virtools;logic programming layer;Prolog+CG;Amine platform;intelligent virtual environments;semantic reasoning","","","","7","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Redirected Walking in Irregularly Shaped Physical Environments with Dynamic Obstacles","H. Chen; S. Chen; E. S. Rosenberg","USC Institute for Creative Technologies, Los Angeles, CA; USC Institute for Creative Technologies, Los Angeles, CA; USC Institute for Creative Technologies, Los Angeles, CA","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","523","524","Redirected walking (RDW) is a virtual reality (VR) locomotion technique that enables the exploration of a large virtual environment (VE) within a small physical space via real walking. Thus far, the physical environment has generally been assumed to be rectangular, static, and free of obstacles. However, it is unlikely that real-world locations that may be used for VR fulfill these constraints. In addition, accounting for dynamic obstacles such as people helps increase user safety when the view of the physical world is occluded by a head-mounted display. In this work, we present the design and initial implementation of a RDW planning algorithm that can redirect the user in an irregularly shaped physical environment with dynamically moving obstacles. This represents an important step towards the use of RDW in more dynamic, real-world environments.","","978-1-5386-3365-6","10.1109/VR.2018.8446563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446563","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality","Heuristic algorithms;Legged locomotion;Planning;Prediction algorithms;Safety;Virtual environments","collision avoidance;helmet mounted displays;user interfaces;virtual reality","irregularly shaped physical environment;real-world environments;dynamic obstacles;redirected walking;virtual reality locomotion technique;virtual environment;physical space;physical world;RDW planning algorithm;user safety;head-mounted display","","2","","4","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Enhancing the Stiffness Perception of Tangible Objects in Mixed Reality Using Wearable Haptics","X. de Tinguy; C. Pacchierotti; M. Marchal; A. Lécuyer","Inria CNRS IRISA, Univ Rennes INSA, Rennes, France; Inria CNRS IRISA, Univ Rennes INSA, Rennes, France; Inria CNRS IRISA, Univ Rennes INSA, Rennes, France; Inria CNRS IRISA, Univ Rennes INSA, Rennes, France","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","81","90","This paper studies the combination of tangible objects and wearable haptics for improving the display of stiffness sensations in virtual environments. Tangible objects enable to feel the general shape of objects, but they are often passive or unable to simulate several varying mechanical properties. Wearable haptic devices are portable and unobtrusive interfaces able to generate varying tactile sensations, but they often fail at providing convincing stiff contacts and distributed shape sensations. We propose to combine these two approaches in virtual and augmented reality (VR/AR), becoming able of arbitrarily augmenting the perceived stiffness of real/tangible objects by providing timely tactile stimuli at the fingers. We developed a proof-of-concept enabling to simulate varying elasticity/stiffness sensations when interacting with tangible objects by using wearable tactile modules at the fingertips. We carried out a user study showing that wearable haptic stimulation can well alter the perceived stiffness of real objects, even when the tactile stimuli are not delivered at the contact point. We illustrated our approach both in VR and AR, within several use cases and different tangible settings, such as when touching surfaces, pressing buttons and pistons, or holding an object. Taken together, our results pave the way for novel haptic sensations in VR/AR by better exploiting the multiple ways of providing simple, unobtrusive, and low-cost haptic displays.","","978-1-5386-3365-6","10.1109/VR.2018.8446280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446280","Human-centered computing-Human computer interaction-Interaction devices-Haptic devices","Haptic interfaces;Skin;Shape;Belts;Virtual environments;Mechanical factors;Augmented reality","augmented reality;elasticity;haptic interfaces","virtual reality;augmented reality;elasticity sensations;stiff contacts;mixed reality;distributed shape sensations;tactile sensations;wearable haptic devices;stiffness perception;low-cost haptic displays;wearable haptic stimulation;wearable tactile modules","","5","","32","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Acting Together, Acting Stronger? Interference between Participants during Face-to-face Cooperative Interception Task","C. Faure; A. Limballe; B. Bideau; T. Perrin; R. Kulpa","Inria, M2S, Univ Rennes, Rennes, France; Inria, M2S, Univ Rennes, Rennes, France; Inria, M2S, Univ Rennes, Rennes, France; Inria, M2S, Univ Rennes, Rennes, France; Inria, M2S, Univ Rennes, Rennes, France","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","918","919","People generally coordinate their action to be more effective. However, in some cases, interference between them occur, resulting in an inefficient collaboration. The main goal of this study is to explore the way two persons regulate their actions when performing a cooperative task of ball interception, and how interference between them may occur. Starting face to face, twenty-four participants (twelve teams of two) had to physically intercept balls moving down from the roof to the floor in a virtual room. To this end, they controlled a virtual paddle attached to their hand moving along the anterior-posterior axis, and were not allowed to communicate. Results globally showed participants were often able to intercept balls without collision by dividing the interception space in two equivalent parts. However, an area of uncertainty (where many trials were not intercepted) appeared in the center of the scene highlighting the presence of interference between participants. The width of this area increased when situation became more complex and when less information was available. Moreover, participants often interpreted balls starting above them as balls they should intercept, even when these balls were in fine intercepted by their partner. Overall, results showed that team coordination emerges from between-participants interactions in this ball interception task and that interference between them depends on task complexity (uncertainty on partner's action and visual information available).","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797967","Virtual reality;Collaborative interactions;Ball interception;Perception-action;Team interference","Task analysis;Interference;Uncertainty;Visualization;Sports;Complexity theory;Collaboration","groupware;multi-robot systems;virtual reality","virtual room;virtual paddle;anterior-posterior axis;interception space;team coordination;between-participants interactions;ball interception task;task complexity;face-to-face cooperative interception","","","","5","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Mixed reality simulation with physical mobile display devices","M. Rodrigue; A. Waranis; T. Wood; T. Höllerer","University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara","2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","105","110","This paper presents the design and implementation of a system for simulating mixed reality in setups combining mobile devices and large backdrop displays. With a mixed reality simulator, one can perform usability studies and evaluate mixed reality systems while minimizing confounding variables. This paper describes how mobile device AR design factors can be flexibly and systematically explored without sacrificing the touch and direct unobstructed manipulation of a physical personal MR display. First, we describe general principles to consider when implementing a mixed reality simulator, enumerating design factors. Then, we present our implementation which utilizes personal mobile display devices in conjunction with a large surround-view display environment. Standing in the center of the display, a user may direct a mobile device, such as a tablet or head-mounted display, to a portion of the scene, which affords them a potentially annotated view of the area of interest. The user may employ gesture or touch screen interaction on a simulated augmented camera feed, as they typically would in video-see-through mixed reality applications. We present calibration and system performance results and illustrate our system's flexibility by presenting the design of three usability evaluation scenarios.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223331","Augmented reality;virtual reality;large displays;immersive displays;mobile device;input device;interaction techniques","Virtual reality;Cameras;Lenses;Smart phones;Servers;Mobile communication","augmented reality;helmet mounted displays;human computer interaction;mobile computing;touch sensitive screens","mixed reality simulation;physical mobile display devices;backdrop displays;usability studies;mixed reality systems;mobile device AR design factors;touch manipulation;direct unobstructed manipulation;physical personal MR display;personal mobile display devices;surround-view display environment;tablet;head-mounted display;gesture interaction;touch screen interaction;augmented camera feed;video-see-through mixed reality applications;calibration;system performance;system flexibility;usability evaluation scenarios","","3","","17","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Web Assisted Online Children's Rural Intelligent Travel Framework based on Virtual Cloud Computing and VR","J. Zhang","Chongqing University of Education,School of Tourism Service and Management,Chongqing,China,400065","2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC)","23 Apr 2020","2020","","","395","398","Web assisted online Children's rural intelligent travel framework based on virtual cloud computing and VR is designed in this manuscript. Virtual reality technology refers to the computer, sensor technology, artificial intelligence, multimedia and some other technologies combined to generate general human-computer interaction technology. Based on this technology, this paper proposes the novel rural intelligent travel framework. The green mountains and rivers in the countryside make urban residents yearn endlessly. The data mining, VR and the intelligent system are combined for the systematic design. The experimental results have proven the effectiveness.","","978-1-7281-4889-2","10.1109/ICCMC48092.2020.ICCMC-00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076490","Web system;rural intelligent;virtual computing;cloud computing;VR technology","","cloud computing;data mining;human computer interaction;town and country planning;travel industry;virtual reality","virtual cloud computing;virtual reality technology;artificial intelligence;human-computer interaction technology;Web assisted online children rural intelligent travel framework;data mining;intelligent system;VR technology","","","","21","","23 Apr 2020","","","IEEE","IEEE Conferences"
"Visual Data Mining by Virtual Reality for Protein-Protein Interaction Networks","N. Aouaa; R. Gherbi; A. Meziane; H. Hadjar; I. Setitra","CERIST, Research Centre For Scientific And Technical Information, Algiers, Algeria, Université Abderrahmane Mira Béjaia - Rue Targa Ouzemour Béjaia, Algeria; Paris-Sud XI University, Orsay, France; CERIST, Research Centre For Scientific And Technical Information, Algiers, Algeria; CERIST, Research Centre For Scientific And Technical Information, Algiers, Algeria; CERIST, Research Centre For Scientific And Technical Information, Algiers, Algeria","2018 IEEE/ACS 15th International Conference on Computer Systems and Applications (AICCSA)","17 Jan 2019","2018","","","1","8","Currently, visualization techniques in the genetic field require a very important modeling phase in terms of resources. 2D based projections of traditional visualization techniques are rarely adapted to manage and process such huge mass of information. To overcome such limitation, we propose to use a new graph modeling technique. This, when used in conjunction with virtual reality technology, allows biologists to have a wide visibility and fluent exploration through several points of view and user interaction, thus enabling what we can call visual data mining of big scientific data. The general principle of our approach is to build a biological network model in the form of a graph with a spatial representation adapted to the visualization of biological networks in a virtual environment. The results show that the improvement of the node distribution algorithm allows a better and more intuitive visualization, compared to the equivalent two-dimensional visualization.","2161-5330","978-1-5386-9120-5","10.1109/AICCSA.2018.8612849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8612849","virtual reality;visual data mining;big scientific data;protein interaction networks;user interaction","Proteins;Data visualization;Solid modeling;Virtual reality;Visualization;Biological system modeling","biology computing;data mining;data visualisation;distributed algorithms;graph theory;network theory (graphs);proteins;virtual reality","node distribution algorithm;two-dimensional visualization;virtual environment;biological network model;big scientific data;user interaction;virtual reality technology;graph modeling technique;2D based projections;genetic field;protein-protein interaction networks;visual data mining","","1","","21","","17 Jan 2019","","","IEEE","IEEE Conferences"
"BlenderVR: Open-source framework for interactive and immersive VR","B. F. G. Katz; D. Q. Felinto; D. Touraine; D. Poirier-Quinot; P. Bourdot","LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France","2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","203","204","BlenderVR is an open-source project framework for interactive and immersive applications based on an extension of the Blender Game Engine to Virtual Reality applications. BlenderVR is a generalization of the BlenderCAVE project, accounting for alternate platforms (e.g., HMD, video-walls). The goal is to provide a flexible and easy to use framework for the creation of VR applications for various platforms, making use of the existing power of the BGE's graphics rendering and physics engine. Compatible with 3 major Operating Systems, BlenderVR has been developed by VR researchers with support from the Blender Community. BlenderVR currently handles multi-screen/multi-user tracked stereoscopic rendering through efficient low-level master/slave synchronization process with multimodal interactions via OSC and VRPN protocols.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223366","H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;I.3.2 [Graphics Systems]: Distributed/network graphics","Rendering (computer graphics);Synchronization;Virtual reality;Engines;Games;Navigation","protocols;public domain software;rendering (computer graphics);synchronisation;virtual reality","BlenderVR;virtual reality;open-source framework;interactive application;immersive application;Blender game engine;graphics rendering;physics engine;synchronization process;OSC protocol;VRPN protocol","","5","","8","","27 Aug 2015","","","IEEE","IEEE Conferences"
"[DC] Case-studies of Contemporary Presence Theory: Towards More Objective and Reliable Measures of Presence","J. Schirm","Sheffield Hallam University, Reutlingen University","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1363","1364","A large body of literature is concerned with models of presence-the sensory illusion of being part of a virtual scene-but there is still no general agreement on how to measure it in an objective and reliable way. When it comes to virtual reality, presence is often considered as one of the main factors contributing to quality of experience, yet existing methods either rely on subjective assessments of users or on specifics of the virtual environment they are applied in, making it difficult for experimental procedures to be generalized. This paper presents ideas for research into promising measures of presence, based on first experiments with novel behavioral measures inside a rich environment which users can feel present in more naturally.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798203","Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality","Atmospheric measurements;Particle measurements;Virtual environments;Games;Real-time systems;Reliability","behavioural sciences;virtual reality","contemporary presence theory;objective measures;reliable measures;general agreement;virtual reality;quality of experience;virtual environment;behavioral measures;sensory illusion;virtual scene","","","","12","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Knowledge Spaces in VR: Intuitive Interfacing with a Multiperspective Hypermedia Environment","P. Gerjets; M. Lachmair; M. V. Butz; J. Lohmann",Leibniz-Institut für Wissensmedien; Leibniz-Institut für Wissensmedien; University of Tübingen; University of Tübingen,"2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","555","556","Virtual reality technologies, along with motion based input devices allow for the design of innovative interfaces between learners and digital knowledge resources. These interfaces might facilitate knowledge work in educational and scientific contexts. Compared to 2D interfaces, immersive 3D environments provide greater flexibility regarding the interface design, however, so far no general, theory-driven and validated design principles are available. Seeing that complex learning environments can foster the development of various cognitive abilities, like multiperspective reasoning skills (MPRS), such design principles are highly desirable. Using multiperspective hypermedia environments (MHEs) as a testbed, the presented project aims to identify and evaluate design principles, derived from cognitive science. We will create and study interactive, immersive 3D-interface to MHEs using virtual reality technology. To evaluate the developed system, we will contrast the acquisition of MPRS in 2D and 3D learning environments. We expect that the developed design principles will be directly applicable for enhancing the accessibility of other knowledge environments.","","978-1-5386-3365-6","10.1109/VR.2018.8446137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446137","VR;Hypermedia Environment;HCI;Interaction Design;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Theory and Methods","Conferences;Virtual reality;Three-dimensional displays;User interfaces","cognition;computer aided instruction;hypermedia;interactive systems;virtual reality","knowledge spaces;intuitive interfacing;multiperspective hypermedia environment;virtual reality technology;motion based input devices;innovative interfaces;digital knowledge resources;knowledge work;educational contexts;scientific contexts;immersive 3D environments;interface design;general theory-driven;complex learning environments;cognitive abilities;multiperspective reasoning skills;MPRS;MHEs;3D learning environments;developed design principles;knowledge environments;VR","","1","","15","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Simulator sickness in immersive virtual environment","C. Jinjakam; K. Hamamoto","Graduate School of Science and Technology, Course of Science and Technology, Tokai University, Tokyo, JAPAN; Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai University, Tokyo, JAPAN","The 5th 2012 Biomedical Engineering International Conference","21 Feb 2013","2012","","","1","4","The simulator sickness in immersive virtual environment was studied for future questionnaire improvement. The top four sickness scores are general discomfort, eyestrain, difficulty concentrating, and fatigue. These experimental results suggest future questionnaire for immersive virtual environment to concentrate on problems about eyes and seeing. Furthermore, the least four simulator sickness are burping, increase salivation, sweating, and stomach awareness. These sickness symptoms might be reduced to shorten future simulator sickness questionnaire.","","978-1-4673-4892-8","10.1109/BMEiCon.2012.6465465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6465465","Virtual reality;immersive virtual environment;simulator sickness;simulator sickness questionnaire","Virtual environments;Fatigue;Stomach;Educational institutions;Focusing;Electronic mail","digital simulation;human computer interaction;human factors;medical computing;virtual reality","simulator sickness;immersive virtual environment;sickness scores;general discomfort;eyestrain;concentration difficulty;fatigue;burping;salivation;sweating;stomach awareness;sickness symptoms;virtual reality","","3","","9","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Annotation-Based Development of Explorable Immersive VR/AR Environments","J. Flotyński; A. Nowak","Poznań University of Economics and Business,61-875 Poznań,Poland; Poznań University of Economics and Business,61-875 Poznań,Poland","2019 International Conference on 3D Immersion (IC3D)","30 Jan 2020","2019","","","1","9","Virtual and augmented reality environments consist of objects that typically interact with other objects and users, leading to evolution of 3D objects and scenes over time. In multiple VR/AR applications in different domains, interactions and temporal properties of 3D content may be represented using general or domain knowledge, which makes them comprehensible to average users or domain experts without an expertise in IT. Logging interactions and their results can be especially useful in VR/AR environments that are intended to monitor and gain knowledge about the system behavior as well as users' behavior and preferences. However, the available approaches to development of VR/AR environments do not enable logging interactions in an explorable way. The main contribution of this paper is a method of developing explorable VR/AR environments on the basis of existing environments developed using well established tools, such as game engines and imperative programming languages. In the approach, interactions can be represented with general or domain knowledge. The method is discussed in the context of an immersive car showroom, which enables acquisition of knowledge about customers' interests and preferences for marketing and merchandising purposes.","2379-1780","978-1-7281-5189-2","10.1109/IC3D48390.2019.8975907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8975907","annotations;exploration;interaction;virtual reality;augmented reality;semantics;ontologies","","augmented reality","logging interactions;domain knowledge;annotation-based development;temporal properties;domain experts;system behavior;virtual reality environments;augmented reality environments;VR/AR applications;immersive VR/AR environments","","","","35","","30 Jan 2020","","","IEEE","IEEE Conferences"
"A role for general purpose simulation in campaign-level operational modeling","M. E. Lehocky","Operations Research and Cost Analysis Solutions, LLC, PO Box 184, Ledyard, Connecticut 06339, USA","2017 Winter Simulation Conference (WSC)","8 Jan 2018","2017","","","4462","4464","Military operational and other system-of-systems analysis efforts are usually facilitated by using highly complex and expensive capability-focused simulation models. For studies in which the principal elements of analysis focus on readiness, responsiveness or operational presence rather than kinetic effects and entity interactions, models derived from general purpose simulation applications may provide a better balance of cost and stochastic fidelity than detailed high-resolution simulations or spreadsheet-based analytics. This case study explores USCG cutter operational presence modeling in use, from large-scale campaign simulation and point-solution models to a general-purpose simulation implementation.","1558-4305","978-1-5386-3428-8","10.1109/WSC.2017.8248164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248164","","Industries","digital simulation;military computing","system-of-systems analysis;Military operational analysis;capability-focused simulation models;stochastic fidelity;spreadsheet-based analytics;point-solution models;large-scale campaign simulation;USCG cutter operational presence modeling;high-resolution simulations;general purpose simulation applications;entity interactions;kinetic effects;campaign-level operational modeling","","","","","","8 Jan 2018","","","IEEE","IEEE Conferences"
"Developing and Validating Virtual Reality Tool for the Evaluation of Cognitive and Physical Performance During Simulated lengthy field March","S. K. Naor; R. Yanovich; Y. Bahat; M. Plotnik; I. Ketko; A. Gottlieb; O. Ben-Gal; Y. Heled","Sheba Medical Center,The Center of Advanced Technologies in Rehabilitation,Tel Hashomer,Israel; Israel Defense Forces Medical Corps,Heller Institute of Medical Research,Tel Hashomer,Israel; Sheba Medical Center,The Center of Advanced Technologies in Rehabilitation,Tel Hashomer,Israel; Sheba Medical Center,The Center of Advanced Technologies in Rehabilitation,Tel Hashomer,Israel; Israel Defense Forces Medical Corps,Heller Institute of Medical Research,Tel Hashomer,Israel; Sheba Medical Center,The Center of Advanced Technologies in Rehabilitation,Tel Hashomer,Israel; Sheba Medical Center,The Center of Advanced Technologies in Rehabilitation,Tel Hashomer,Israel; Tel Aviv,Heller Institute of Medical Research,Tel Hashomer,Israel","2019 International Conference on Virtual Rehabilitation (ICVR)","13 Feb 2020","2019","","","1","7","Athletes, soldiers, and rescue personnel are often required to perform intensive and prolonged physically demanding activities while remaining cognitively focused. The combined effect of physical and cognitive tasks is of great interest, as both efforts share central nervous system reserves. Amid a larger study that is aimed to create an ecologically validated virtual reality (VR) - based experimental protocol to explore the effect of high-load physical and cognitive efforts on young individuals, the present report focuses on comparing new cognitive tasks presented in the context of simulated military missions with physical load to already established cognitive testing battery. Twelve young participants performed a 10 Km loaded march on a treadmill in VR settings with or without additional cognitive tasks (VR-COG). Each experimental day, subjects underwent pre-and post-evaluation, in which cognitive (trail making test - the color trail test CTT version, and SYNWIN battery for multitasking evaluation) and physical tests (time to exhaustion test - TTE) were conducted. In general, strong or moderate correlations were found between VR-COG performances and the cognitive tests. The VR-COG tasks, together with CTT components, were able to successfully predict the effect of the combined physical and cognitive load on the multitasking performance. Multitasking was evaluated by the SYNWIN score. We believe that our protocol allows optimal conditions for measurement of the effect of high-load physical and cognitive efforts for an extended period of time, thus contributing to the motor-cognitive interaction model knowledge base. It is apparent that virtual environments are ideal set ups for studying military activities, as they enable the participants to experience a particular situation within a controlled area.","2331-9569","978-1-7281-1285-5","10.1109/ICVR46560.2019.8994473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994473","virtual reality;ecologic environments;cognitive load;SYNWIN;CTT;Arm forces;military","Task analysis;Protocols;Visualization;Legged locomotion;Virtual reality;Navigation;Batteries","cognition;human factors;neurophysiology;psychology;virtual reality","virtual reality tool;physical performance;simulated lengthy field march;rescue personnel;central nervous system reserves;ecologically validated virtual reality;experimental protocol;young individuals;simulated military missions;physical load;cognitive testing battery;young participants;10 Km loaded march;VR settings;additional cognitive tasks;post-evaluation;trail making test;color trail test CTT version;multitasking evaluation;exhaustion test - TTE;VR-COG performances;cognitive tests;VR-COG tasks;multitasking performance;motor-cognitive interaction model knowledge base;virtual environments","","","","27","","13 Feb 2020","","","IEEE","IEEE Conferences"
"Exploring Virtual Environments by Visually Impaired Using a Mixed Reality Cane Without Visual Feedback","L. Zhang; K. Wu; B. Yang; H. Tang; Z. Zhu","The City University of New York,Borough of Manhattan Community College,New York,NY,USA; The City University of New York,Borough of Manhattan Community College,New York,NY,USA; The City University of New York,Borough of Manhattan Community College,New York,NY,USA; The City University of New York,Borough of Manhattan Community College,New York,NY,USA; The City University of New York,The City College Visual Computing Lab,New York,NY,USA","2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","16 Dec 2020","2020","","","51","56","Though virtual reality (VR) has been advanced to certain levels of maturity in recent years, the general public, especially the population of the blind and visually impaired (BVI), still cannot enjoy the benefit provided by VR. Current VR accessibility applications have been developed either on expensive head-mounted displays or with extra accessories and mechanisms, which are either not accessible or inconvenient for BVI individuals. In this paper, we present a mobile VR app that enables BVI users to access a virtual environment on an iPhone in order to build their skills of perception and recognition of the virtual environment and the virtual objects in the environment. The app uses the iPhone on a selfie stick to simulate a long cane in VR, and applies Augmented Reality (AR) techniques to track the iPhone's real-time poses in an empty space of the real world, which is then synchronized to the long cane in the VR environment. Due to the use of mixed reality (the integration of VR & AR), we call it the Mixed Reality cane (MR Cane), which provides BVI users auditory and vibrotactile feedback whenever the virtual cane comes in contact with objects in VR. Thus, the MR Cane allows BVI individuals to interact with the virtual objects and identify approximate sizes and locations of the objects in the virtual environment. We performed preliminary user studies with blind-folded participants to investigate the effectiveness of the proposed mobile approach and the results indicate that the proposed MR Cane could be effective to help BVI individuals in understanding the interaction with virtual objects and exploring 3D virtual environments. The MR Cane concept can be extended to new applications of navigation, training and entertainment for BVI individuals without more significant efforts.","","978-1-7281-7675-8","10.1109/ISMAR-Adjunct51615.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288373","Virtual Reality;Mixed Reality;Visually Impaired;Spatial Exploration","Training;Visualization;Navigation;Virtual environments;Tools;Space exploration;Augmented reality","augmented reality;handicapped aids;haptic interfaces;helmet mounted displays;mobile computing","3D virtual environment;blind and visually impaired;virtual reality;BVI;mobile VR app;virtual objects;augmented reality;VR environment;mixed reality cane;VR accessibility applications;MR cane;head-mounted displays;vibrotactile feedback;auditory feedback","","","","24","","16 Dec 2020","","","IEEE","IEEE Conferences"
"[DC] Self-Adaptive Technologies for Immersive Trainings","J. Heyse","Ghent University - imec, IDLab","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1381","1382","Online learning is the preferred option for professional training, e.g. Industry 4.0 or e-health, because it is more cost efficient than on-site organisation of realistic training sessions. However, current online learning technologies are limited in terms of personalisation, interactivity and immersiveness that are required by applications such as surgery and pilot training. Virtual Reality (VR) technologies have the potential to overcome these limitations. However, due to its early stage of research, VR requires significant improvements to fully unlock its potential. The focus of this PhD is to tackle research challenges to enable VR for online training in three dimensions: (1) dynamic adaptation of the training content for personalised trainings, by incorporating prior knowledge and context data into self-learning algorithms; (2) mapping of sensor data onto what happens in the VR environment, by focusing on motion prediction techniques that use past movements of the users, and (3) investigating immersive environments with intuitive interactions, by gaining a better understanding of human motion in order to improve interaction. The designed improvements will be characterised though a prototype VR training platform for multiple use cases. This work will not only advance the state of the art on VR training, but also on online e-learning applications in general.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798207","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Applied computing;Life and medical sciences;Health informatics","Training;Virtual reality;Tracking;Resists;Haptic interfaces;Surgery;Prototypes","computer based training;sensors;virtual reality","online training;training content;personalised trainings;context data;self-learning algorithms;sensor data;VR environment;motion prediction techniques;immersive environments;intuitive interactions;prototype VR training platform;online e-learning applications;immersive trainings;on-site organisation;immersiveness;surgery;pilot training;training sessions;online learning technologies;self-adaptive technologies;virtual reality technologies","","","","6","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Logging Interactions in Explorable Immersive VR/AR Applications","J. Flotyński; P. Sobociński","Poznań University of Economics and Business, 61-875 Poznań, Poland; Poznań University of Economics and Business, 61-875 Poznań, Poland","2018 International Conference on 3D Immersion (IC3D)","7 Mar 2019","2018","","","1","8","3D content of immersive virtual and augmented reality applications typically consists of objects that can interact with users and other objects. Interactions may be followed by changes of objects' properties, including their geometry, structure and appearance as well as high-level, domain-specific semantics. Logging interactions and temporal 3D content properties in a uniform, explorable way can be useful in various domains that involve multiple collaborating users and interacting objects, and can benefit from analysis of the users' and objects' behavior, e.g., in design, training, education, e-commerce, marketing and merchandising. However, the available approaches do not enable creation of explorable immersive VR/AR applications with regards to interactions and temporal 3D content properties. The main contribution of this paper is an approach to generating explorable interaction logs in immersive applications. The logs are ontologies compliant with the semantic web. Hence, the approach may be used with general or domain knowledge in a way intelligible to average users or domain specialists without expertise in IT. The approach is discussed in the context of a heterogeneous immersive environment, which combines diverse VR presentation and interaction devices to enable collaboration of multiple users within urban design.","2379-1780","978-1-5386-7590-8","10.1109/IC3D.2018.8657830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657830","3D content exploration;interaction;semantic web;urban design;Unity","Three-dimensional displays;Ontologies;Semantics;Solid modeling;Urban areas;Tools;Semantic Web","augmented reality;groupware;ontologies (artificial intelligence);semantic Web","logging interactions;immersive virtual reality applications;augmented reality applications;domain-specific semantics;temporal 3D content properties;multiple collaborating users;interacting objects;e-commerce;explorable interaction logs;immersive applications;general domain knowledge;domain specialists;heterogeneous immersive environment;interaction devices;explorable immersive VR-AR applications","","","","20","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Augmented reality @ Siemens: ""The Workflow Designer Project"" & ""Augmented reality PDA""","A. Raczynski; C. Reimann; P. Gussmann; C. Matysczok; A. Grow; W. Rosenbach",NA; NA; NA; NA; NA; NA,"The First IEEE International Workshop Agumented Reality Toolkit,","6 Jan 2003","2002","","","2 pp.","","In this paper we describe two AR projects at Siemens: ""The Workflow Designer Project"" and ""AR-PDA - A Personal Digital Assistant for VR/AR Content"". The Workflow Designer makes use of pattern recognition and tracking to augment real-time video with 3D models, allowing a remote expert to guide a worker through a complicated workflow step-by-step. The general idea of the AR-PDA project to the development of applications for new mobile devices like 3/sup rd/ generation mobile telephones, organizer or PDA for the consumer market. In this context augmented reality is used to support efficiently final users during their daily tasks and especially in service situations. The AR-PDA enhances real camera images by virtual objects (3D-animations, 2D-graphics or text) and allows personalized user interactions with the augmented scene.","","0-7803-7680-3","10.1109/ART.2002.1106974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1106974","","Augmented reality;Layout;Pattern recognition;Wireless networks;Video compression;Image coding;Prototypes;Animation;Displays;Personal digital assistants","3G mobile communication;augmented reality;workflow management software;notebook computers;pattern recognition;real-time systems;computer animation","augmented reality;Siemens;The Workflow Designer Project;Augmented reality PDA;A Personal Digital Assistant for VR/AR Content;AR-PDA;pattern recognition;pattern tracking;real-time video;3D models;3rd generation mobile telephones;virtual objects;3D animations;2D graphics;text;personalized user interactions;augmented scene","","","3","3","","6 Jan 2003","","","IEEE","IEEE Conferences"
"3D Positioning System Based on One-handed Thumb Interactions for 3D Annotation Placement","S. Tashiro; H. Uchiyama; D. Thomas; R. -i. Taniguchi",Kyushu University; Kyushu University; Kyushu University; Kyushu University,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1181","1182","This paper presents a 3D positioning system based on one-handed thumb interactions for simple 3D annotation placement with a smart-phone. To place an annotation at a target point in the real environment, the 3D coordinate of the point is computed by interactively selecting the corresponding points in multiple views by users while performing SLAM. Generally, it is difficult for users to precisely select an intended pixel on the touchscreen. Therefore, we propose to compute the 3D coordinate from multiple observations with a robust estimator to have the tolerance to the inaccurate user inputs. In addition, we developed three pixel selection methods based on one-handed thumb interactions. A pixel is selected at the thumb position at a live view in FingAR, the position of a reticle marker at a live view in SnipAR, or that of a movable reticle marker at a freezed view in FreezAR. In the preliminary evaluation, we investigated the 3D positioning accuracy of each method.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797979","Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computer vision;Computer vision problems;Reconstruction;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Pointing","Three-dimensional displays;Thumb;Augmented reality;Cameras;Simultaneous localization and mapping;Task analysis","control engineering computing;mobile robots;SLAM (robots);touch sensitive screens;user interfaces","pixel selection methods;one-handed thumb interactions;thumb position;3D positioning system;3D annotation placement;smart-phone;SLAM;robust estimator;FingAR;reticle marker;SnipAR;FreezAR","","","","6","","15 Aug 2019","","","IEEE","IEEE Conferences"
"A General Reactive Algorithm for Redirected Walking Using Artificial Potential Functions","J. Thomas; E. S. Rosenberg",University of Minnesota; University of Minnesota,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","56","62","Redirected walking enables users to locomote naturally within a virtual environment that is larger than the available physical space. These systems depend on steering algorithms that continuously redirect users within limited real world boundaries. While a majority of the most recent research has focused on predictive algorithms, it is often necessary to utilize reactive approaches when the user's path is unconstrained. Unfortunately, previously proposed reactive algorithms assume a completely empty space with convex boundaries and perform poorly in complex real world spaces containing obstacles. To overcome this limitation, we present Push/Pull Reactive (P2R), a novel algorithm that uses an artificial potential function to steer users away from potential collisions. We also introduce three new reset strategies and conducted an experiment to evaluate which one performs best when used with P2R. Simulation results demonstrate that the proposed approach outperforms the previous state-of-the-art reactive algorithm in non-convex spaces with and without interior obstacles.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797983","Redirected Walking;Virtual Reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality","Prediction algorithms;Legged locomotion;Force;Virtual environments;Heuristic algorithms;Layout","collision avoidance;control engineering computing;mobile robots;path planning;virtual reality","artificial potential function;redirected walking;virtual environment;predictive algorithms;reactive algorithms;convex boundaries;complex real world spaces;P2R;nonconvex spaces;general reactive algorithm;push/pull reactive","","1","","22","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Visualisation of a simple beam under a load in virtual environment","S. Jankovic; L. Jankovic; A. H. C. Chan; G. H. Little","Sch. of Civil Eng., Birmingham Univ., UK; NA; NA; NA","Proceedings Fifth International Conference on Information Visualisation","7 Aug 2002","2001","","","589","591","Visualisation is generally considered to be a problem in structural design. Being an iterative process, structural design is time consuming for engineers, and results are difficult for clients to visualize. Current numerical methods have slow dynamic visualisation capability, which is usually separate from the calculation phase. The paper investigates whether an analogue virtual reality (VR) model of a structure can integrate calculation and visual representation. Early results of this research show that it is possible to have analogue virtual reality models of structures capable of real time user interaction. The paper shows how the user can interact with a VR analogue model of a simple beam, move a concentrated load along the length of the beam, change the value of the force, and visualise deflection of the beam using the analytical solution of the problem.","","0-7695-1195-3","10.1109/IV.2001.942115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942115","","Visualization;Virtual environment;Virtual reality;Solid modeling;Steel;Civil engineering;Postal services;Process design;Design engineering;Geometry","virtual reality;data visualisation;structural engineering computing;real-time systems","simple beam visualisation;load;virtual environment;structural design;iterative process;engineers;numerical methods;slow dynamic visualisation capability;calculation phase;analogue virtual reality model;visual representation;real time user interaction;VR analogue model;concentrated load;beam deflection;analytical solution","","","","5","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Multi-Window 3D Interaction for Collaborative Virtual Reality","A. Kunert; T. Weissker; B. Froehlich; A. Kulik","Virtual Reality and Visualization Research Group, Bauhaus-Universität Weimar, Weimar, Germany; Virtual Reality and Visualization Research Group, Bauhaus-Universität Weimar, Weimar, Germany; Virtual Reality and Visualization Research Group, Bauhaus-Universität Weimar, Weimar, Germany; Virtual Reality and Visualization Research Group, Bauhaus-Universität Weimar, Weimar, Germany","IEEE Transactions on Visualization and Computer Graphics","30 Sep 2020","2020","26","11","3271","3284","We present a novel collaborative virtual reality system that offers multiple immersive 3D views at large 3D scenes. The physical setup consists of two synchronized multi-user 3D displays: a tabletop and a large vertical projection screen. These displays afford different presentations of the shared 3D scene. The wall display lends itself to the egocentric exploration at 1:1 scale, while the tabletop affords an allocentric overview. Additionally, handheld 3D portals facilitate the personal exploration of the scene, the comparison of views, and the exchange with others. Our developments enable seamless 3D interaction across these independent 3D views. This requires the simultaneous representation of user input in the different viewing contexts. However, the resulting interactions cannot be executed independently. The application must coordinate the interactions and resolve potential ambiguities to provide plausible effects. We analyze and document the challenges of seamless 3D interaction across multiple independent viewing windows, propose a high-level software design to realize the necessary functionality, and apply the design to a set of interaction tools. Our setup was tested in a formal user study, which revealed general advantages of collaborative 3D data exploration with multiple views in terms of user preference, comfort, and task performance.","1941-0506","","10.1109/TVCG.2019.2914677","European Unions Horizon 2020 Framework Programme for Research and Innovation; German Federal Ministry of Education and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705329","Collaborative virtual environment;multi-display setups;3D interaction techniques;input disambiguation","Three-dimensional displays;Collaboration;Windows;Microsoft Windows;Navigation;Virtual environments;Two dimensional displays","data visualisation;groupware;interactive systems;three-dimensional displays;virtual reality","novel collaborative virtual reality system;multiple immersive 3D views;synchronized multiuser 3D;vertical projection screen;wall display;egocentric exploration;tabletop affords;personal exploration;seamless 3D interaction;independent 3D views;multiple independent viewing windows;interaction tools;collaborative 3D data exploration;multiwindow 3D interaction;3D portals","","9","","52","IEEE","3 May 2019","","","IEEE","IEEE Journals"
"VR Sickness Prediction for Navigation in Immersive Virtual Environments using a Deep Long Short Term Memory Model","Y. Wang; J. -R. Chardonnet; F. Merienne","Arts et Metiers, LISPEN EA7515, HESAM, UBFC, Institut Image; Arts et Metiers, LISPEN EA7515, HESAM, UBFC, Institut Image; Arts et Metiers, LISPEN EA7515, HESAM, UBFC, Institut Image","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1874","1881","This paper proposes a new objective metric of visually induced motion sickness (VIMS) in the context of navigation in virtual environments (VEs). Similar to motion sickness in physical environments, VIMS can induce many physiological symptoms such as general discomfort, nausea, disorientation, vomiting, dizziness and fatigue. To improve user satisfaction with VR applications, it is of great significance to develop objective metrics for VIMS that can analyze and estimate the level of VR sickness when a user is exposed to VEs. One of the well-known objective metrics is the postural instability. In this paper, we trained a LSTM model for each participant using a normal-state postural signal captured before the exposure, and if the postural sway signal from post-exposure was sufficiently different from the pre-exposure signal, the model would fail at encoding and decoding the signal properly; the jump in the reconstruction error was called loss and was proposed as the proposed objective measure of simulator sickness. The effectiveness of the proposed metric was analyzed and compared with subjective assessment methods based on the simulator sickness questionnaire (SSQ) in a VR environment, achieving a Pearson correlation coefficient of. 89. Finally, we showed that the proposed method had the potential to be deployed within a closed-loop system and get real-time performance to predict VR sickness, opening new insights to develop user-centered and customized VR applications based on physiological feedback.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798213","Human-centered computing;Virtual reality;Walkthrough evaluations;User interface design;Interaction devices;Computing methodologies;Machine learning;Machine learning approaches;Neural networks","Navigation;Virtual environments;Physiology;Real-time systems;Three-dimensional displays;Logic gates;Deep learning","closed loop systems;human factors;physiology;recurrent neural nets;virtual reality","VR sickness prediction;navigation;immersive virtual environments;deep long short term memory model;visually induced motion sickness;VIMS;VEs;physical environments;physiological symptoms;general discomfort;dizziness;fatigue;user satisfaction;VR applications;postural instability;LSTM model;normal-state postural signal;postural sway signal;post-exposure;pre-exposure signal;objective measure;simulator sickness questionnaire;VR environment;user-centered VR applications","","1","","34","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Visualizing Ecological Data in Virtual Reality","J. Huang; M. S. Lucash; R. M. Scheller; A. Klippel","Pennsylvania State University, ChoroPhronesis; Portland State University; North Carolina State University; Pennsylvania State University, ChoroPhronesis","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1311","1312","Visualizing complex scientific data and models in 2D can be challenging. The result can be hard to interpret and understand for the general audience, and the model accuracy hard to evaluate even for the experts. To address these problems, we created a workflow that translates data of an ecological model, LANDIS-II, into a high-fidelity 3D model in virtual reality (VR). We combined ecological modeling, analytical modeling, procedural modeling, and VR, to allow users to experience a forest in northern Wisconsin (WI), United States, under two climate scenarios. Users can explore and interact with the forest under different climate scenarios, explore the impacts of climate change on different tree species, and retrieve information from a 3D tree database. The VR application can be used as an educational tool for the general public, and as a model checking tool by researchers.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797771","Visualization;Scientific visualization;Geographic visualization;Virtual reality;Human computer interaction;Human centered computing;Interaction paradigms;Software and its engineering;Software organization and properties;Virtual worlds software;Virtual worlds training simulations;Physical sciences and engineering;Earth and atmospheric sciences;Environmental sciences;Visualization application domains","Forestry;Data visualization;Solid modeling;Biological system modeling;Three-dimensional displays;Data models","data visualisation;ecology;environmental science computing;formal verification;vegetation;virtual reality","virtual reality;ecological model;high-fidelity 3D model;VR;ecological modeling;analytical modeling;procedural modeling;forest;model checking tool;climate scenarios;ecological data visualization;LANDIS-II;climate change;tree species","","1","","10","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Acceptance and User Experience of Driving with a See-Through Cockpit in a Narrow-Space Overtaking Scenario","P. Lindemann; D. Eisl; G. Rigoll","Technical University of Munich Munich, Germany; Technical University of Munich Munich, Germany; Technical University of Munich Munich, Germany","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1040","1041","In this work, we examine the implications of driving with transparent cockpits (TCs) in a narrow-space overtaking scenario. We utilize a virtual environment to simulate two possible manifestations: a user-controlled head-mounted system and a static projection-based system. We conducted a user study with an overtaking task and present results for acceptance and a comparison of both systems regarding user experience. Participants preferred the static TC and evaluated it as the solution with higher pragmatic quality and attractiveness. The TC generally scored highly in hedonic quality and was rated positively regarding perceived safety and ease of use.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798069","Human-centered computing;Human computer interaction (HCI);Mixed / augmented reality, Virtual reality","Resists;Vehicles;Solid modeling;Task analysis;Virtual reality;User experience;Safety","human factors;road safety;traffic engineering computing;user interfaces;virtual reality","user experience;narrow-space overtaking scenario;transparent cockpits;user-controlled head-mounted system;static projection-based system;overtaking task;see-through cockpit;acceptance","","","","2","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Opportunities for Virtual and Mixed Reality Knowledge Demonstration","R. Horst; R. Dörner",RheinMain University of Applied Sciences; RheinMain University of Applied Sciences,"2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","29 Apr 2019","2018","","","381","385","Universities are more and more expected to disseminate research findings to the general public and specific stakeholders especially in the economy (“third mission”). Since this third mission also affects third party research funding within recent decades, j scientists do need to be able to service both educational and marketing related purposes within their presentations. Moreover, scientists need to address heterogenous audiences. To meet such demanding requirements, Virtual Reality (VR) and Mixed Reality (MR) technology could serve as a valuable medium. In this position paper, we explore opportunities concerning VR and MR for what we refer to as `Knowledge Demonstration' (KD), based on a state of the art literature analysis. Furthermore, we propose aspects to consider when planning virtually augmented KD systems.","","978-1-5386-7592-2","10.1109/ISMAR-Adjunct.2018.00110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8699173","Human-centered computing—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Interaction paradigms—Virtual reality;Applied computing—Education—Computer-assisted instruction;Applied computing—Operations research—Marketing","Education;Collaboration;Augmented reality;Face;Tools;Videos","educational institutions;technical presentation;virtual reality","universities;virtually augmented KD systems;virtual reality technology;marketing related purposes;educational related purposes;mixed reality knowledge demonstration;mixed reality technology","","","","59","","29 Apr 2019","","","IEEE","IEEE Conferences"
"Towards understanding scene transition techniques in immersive 360 movies and cinematic experiences","K. R. Moghadam; E. D. Ragan","Texas A&M University, United States; Texas A&M University, United States","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","375","376","Many researchers have studied methods of effective travel in virtual environments, but little work has considered scene transitions, which may be important for virtual reality experiences like immersive 360 degree movies. In this research, we designed and evaluated three different scene transition techniques in two environments, conducted a pilot study, and collected metrics related to sickness, spatial orientation, and preference. Our preliminary results indicate that faster techniques are generally preferred by gamers and more gradual transitions are preferred by participants with less experience with 3D gaming and virtual reality.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892333","H.5.1 [Information interfaces and presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities","Interpolation;Teleportation;Motion pictures;Three-dimensional displays;Virtual environments;Google","human computer interaction;virtual reality","cinematic experiences;virtual reality;immersive 360 degree movies;scene transition techniques","","8","","3","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Hand-based interactions in Virtual Reality: No better feeling than the real thing!","D. Panzoli; P. Royeres; M. Fedou","IRIT UMR 5505, Toulouse, F-31400, France; INU Jean-Francois Champollion, Albi, France; INU Jean-Francois Champollion, Albi, France","2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)","14 Oct 2019","2019","","","1","2","Manipulating tools or crafting objects using one's bare hands is a recurring activity in many immersive training applications. Yet, high technical challenge and costly haptic hardware confine such tasks to surgery training and specialised simulators of the like, while more general immersive applications settle for less natural experiences using standard game controllers and/or relying on more abstract interactions. We designed a low-cost prototype were a physical object is used to provide a natural haptic feedback to a barehanded virtual manipulation task, hypothesising that nothing better than a real object would provide an accurate feeling of touch and control. With VR and AR high-end technologies now reaching out to broad audiences, we wondered if it was yet possible to design a compelling experience using only consumer market products and APIs. In this paper we demonstrate indeed that these technologies are mature enough to build a working prototype, but we failed to reach a level of quality suitable for more widespread usage.","2474-0489","978-1-7281-4540-2","10.1109/VS-Games.2019.8864546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864546","Mixed Reality;Hand-based interaction;haptic feedback;object recognition","Haptic interfaces;Cameras;Virtual environments;Training;Hardware;Task analysis","haptic interfaces;virtual reality","hand-based interactions;virtual reality;manipulating tools;crafting objects;bare hands;immersive training applications;haptic hardware;surgery training;general immersive applications;natural experiences;standard game controllers;abstract interactions;low-cost prototype;physical object;natural haptic feedback;barehanded virtual manipulation task;high-end technologies;consumer market products;augmented reality;API","","","","6","","14 Oct 2019","","","IEEE","IEEE Conferences"
"Empowering Young Job Seekers with Virtual Reality","E. Prasolova-Førland; M. Fominykh; O. I. Ekelund","IMTEL, Norwegian University of Science and Technology, Trondheim, Norway; IMTEL, Norwegian University of Science and Technology, Trondheim, Norway; IMTEL, Norwegian University of Science and Technology, Trondheim, Norway","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","295","302","This paper presents the results of the Virtual Internship project that aims to help young job seekers get insights of different workplaces via immersive and interactive experiences. We designed a concept of `Immersive Job Taste' that provides a rich presentation of occupations with elements of workplace training, targeting a specific group of young job seekers, including high-school students and unemployed. We developed several scenarios and applied different virtual and augmented reality concepts to build prototypes for different types of devices. The intermediary and the final versions of the prototypes were evaluated by several groups of primary users and experts, including over 70 young job seekers and high school students and over 45 various professionals and experts. The data were collected using questionnaires and interviews. The results indicate a generally very positive attitude towards the concept of immersive job taste, although with significant differences between job seekers and experts. The prototype developed for room-scale virtual reality with controllers was generally evaluated better than those including cardboard with 360 videos or with animated 3D graphics and augmented reality glasses. In the paper, we discuss several aspects, such as the potential of immersive technologies for career guidance, fighting youth unemployment by better informing the young job seekers, and various practical and technology considerations.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798179","Virtual Reality;Career guidance;unemployment","Task analysis;Employment;Interviews;Fish;Training;Three-dimensional displays;Videos","augmented reality;computer based training;human computer interaction;unemployment","room-scale virtual reality;immersive job taste;high-school students;augmented reality;empowering young job seekers;virtual internship project;workplace training;career guidance;youth unemployment","","","","16","","15 Aug 2019","","","IEEE","IEEE Conferences"
"In-Situ Labeling for Augmented Reality Language Learning","B. Huynh; J. Orlosky; T. Höllerer","University of California, Santa Barbara; Osaka University; University of California, Santa Barbara","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1606","1611","Augmented Reality is a promising interaction paradigm for learning applications. It has the potential to improve learning outcomes by merging educational content with spatial cues and semantically relevant objects within a learner's everyday environment. The impact of such an interface could be comparable to the method of loci, a well known memory enhancement technique used by memory champions and polyglots. However, using Augmented Reality in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is a significant challenge, and interaction with arbitrary (unmodeled) physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a framework for in-situ object labeling and selection in Augmented Reality, with a particular focus on language learning applications. Our framework uses a generalized object recognition model to identify objects in the world in real time, integrates eye tracking to facilitate selection and interaction within the interface, and incorporates a personalized learning model that dynamically adapts to student's growth. We show our current progress in the development of this system, including preliminary tests and benchmarks. We explore challenges with using such a system in practice, and discuss our vision for the future of AR language learning applications.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798358","Human-centered computing;Mixed and augmented reality;Theory and algorithms for application domains;Semi-supervised learning","Three-dimensional displays;Real-time systems;Two dimensional displays;Labeling;Augmented reality;Object recognition;Cameras","augmented reality;computer aided instruction;linguistics;natural languages","language learning applications;generalized object recognition model;personalized learning model;augmented reality language learning;semantically relevant objects;memory champions;polyglots;scalable object recognition;arbitrary physical objects;in-situ object;memory enhancement technique","","2","","33","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Estimating the motion-to-photon latency in head mounted displays","J. Zhao; R. S. Allison; M. Vinnikov; S. Jennings","Department of Electrical Engineering and Computer Science, York University, Canada; Department of Electrical Engineering and Computer Science, York University, Canada; Flight Research Laboratory, National Research Council, Canada; Flight Research Laboratory, National Research Council, Canada","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","313","314","We present a method for estimating the Motion-to-Photon (End-to-End) latency of head mounted displays (HMDs). The specific HMD evaluated in our study was the Oculus Rift DK2, but the procedure is general. We mounted the HMD on a pendulum to introduce damped sinusoidal motion to the HMD during the pendulum swing. The latency was estimated by calculating the phase shift between the captured signals of the physical motion of the HMD and a motion-dependent gradient stimulus rendered on the display. We used the proposed method to estimate both rotational and translational Motion-to-Photon latencies of the Oculus Rift DK2.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892302","Motion-to-Photon Latency;End-to-End Latency;Head-Mounted Displays","Resists;Potentiometers;Photodiodes;Virtual reality;Cameras;Estimation;Frequency-domain analysis","helmet mounted displays;human computer interaction;virtual reality","head mounted displays;HMD;Oculus Rift DK2;pendulum;damped sinusoidal motion;phase shift;motion-dependent gradient stimulus;translational motion-to-photon latencies;rotational motion-to-photon latencies;virtual reality","","18","","6","","6 Apr 2017","","","IEEE","IEEE Conferences"
"A Structure to Integrate Natural Interaction into VR Systems for Education in Health","D. d. S. Ferreira; L. S. Machado","NA; Lab. de Tecnol. para o Ensino Virtual e Estatistica (LabTEVE), Univ. Fed. da Paraiba (UFPB), Joao Pessoa, Brazil","2013 XV Symposium on Virtual and Augmented Reality","7 Nov 2013","2013","","","208","211","The use of virtual reality (VR) systems for education can improve the learning process in health, providing more motivation and realism to users. In general, specific devices are used to increase immersion and, in general, demand an adaptation period. Natural Interaction (NI) can provide an intuitive way to interact in VR systems since it can improve and make the communication between users and the system easier. The present work investigated NI techniques provided by VR frameworks for health. As a result, was designed a structure to integrate NI techniques into a framework.","","978-0-7695-5001-5","10.1109/SVR.2013.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655781","Natural Interaction;Virtual Reality;Optical Tracking;Health Education","Visualization;Education;Virtual reality;Three-dimensional displays;Mice;Nickel;Multimedia communication","biomedical education;computer aided instruction;health care;human computer interaction;medical computing;virtual reality","VR frameworks;NI techniques;user communication;natural Interaction;adaptation period;user realism;user motivation;learning process;health education;VR systems;virtual reality","","","","21","","7 Nov 2013","","","IEEE","IEEE Conferences"
"Jedi ForceExtension: Telekinesis as a Virtual Reality interaction metaphor","R. M. S. Clifford; N. M. B. Tuanquin; R. W. Lindeman","HIT Lab NZ, University of Canterbury, New Zealand; HIT Lab NZ, University of Canterbury, New Zealand; HIT Lab NZ, University of Canterbury, New Zealand","2017 IEEE Symposium on 3D User Interfaces (3DUI)","6 Apr 2017","2017","","","239","240","Virtual Reality (VR) enables us to freely operate in a space that is unconstrained by physical laws and limitations. To take advantage of this aspect, we have developed a technique for pseudo-telekinetic object manipulation in VR using slight downward tilt of the head to simulate Jedi concentration. This telekinetic ability draws inspiration from The Force abilities exhibited in the Star Wars universe, and is particularly well suited to VR because it provides the ability to interact with and manipulate objects at a distance. We implemented force translate, force rotate, force push and force pull behaviours as examples of the general concept of force extension. We conducted exploratory user testing to assess telekinesis as a suitable interaction metaphor. Subject performance and feedback varied between participants but were generally encouraging.","","978-1-5090-6716-9","10.1109/3DUI.2017.7893360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893360","Virtual Reality;3D interaction;3D Selection;object displacement","Three-dimensional displays;Force;Two dimensional displays;User interfaces;Virtual environments;Laser transitions","virtual reality","virtual reality interaction metaphor;pseudotelekinetic object manipulation;VR;Jedi concentration;Star Wars universe;force translate behaviours;force rotate behaviours;force push behaviours;force pull behaviours;force extension","","2","","5","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Simple augmented reality system for 3D ultrasonic image by see-through HMD and single camera and marker combination","S. Tano; K. Suzuki; K. Miki; N. Watanabe; M. Iwata; T. Hashiyama; J. Ichino; K. Nakayama","University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN; University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN; Showa General Hospital, Kodaira-shi, Tokyo, 187-8510 JAPAN; University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN; Tokyo Metropolitan College of Industrial Technology, Shinagawa-ku, 140-0011, JAPAN; University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN; University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN; University of Electro-Communications, Chofu-shi, Tokyo, 182-8585 JAPAN","Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics","7 Jun 2012","2012","","","464","467","Thanks to the rapid progress of ICT, significant progress has been made in both the “generation” and “display” of the advanced medical information. However, serious problems still remain in both the “generation” and “display”. Therefore, we propose a simple augmented reality system that can display an ultrasonic image of exactly the same plane in the body of the patient that a doctor is looking at. The key idea is to utilize the fact that an ultrasonic probe moves inside the doctor's field of view and within the accessible range of the arm in order to simplify the augmented reality system. The prototype has been developed using only a see-through HMD and single camera and marker combination. Three simple interaction methods compensate for the limitations in 3D position sensing. We worked with a medical doctor to test the prototype system and found it to be effective.","2168-2208","978-1-4577-2177-9","10.1109/BHI.2012.6211617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6211617","","","augmented reality;biomedical ultrasonics;cameras;helmet mounted displays;medical image processing;medical information systems","augmented reality system;3D ultrasonic image;see-through HMD;ICT;medical information display;medical information generation;ultrasonic probe;interaction methods;3D position sensing;medical doctor;single camera-marker combination;information and communication technology;head mounted display","","2","","12","","7 Jun 2012","","","IEEE","IEEE Conferences"
"Relief Mapping on facade of Sino Portuguese Architecture in Virtual Reality","K. Kalarat","Multimedia Technology and Animation, Walailak University, Nakhonsithammarat, Thailand","2014 Fourth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)","29 May 2014","2014","","","333","336","Virtual Reality (VR) is an artificial environment created for presented for users to interact to virtual world and to be experienced in the way that users believe and accept it such a real environment. The environment is created to be three dimensional world using computer technology which is called Virtual Environment (VE). Virtual Environment makes user experiences immersion and realize to be inside and the part of the virtual world. Therefore, Virtual Environment is very important factor for making user immersive effectively, user must be able to explore what appears in the virtual environment and be able to interact by changing perspectives seamlessly. Generally, Virtual Environment (VE) should be made to be reality as much as possible by using high resolution of 3D model to increase more information and detail to the model in the environment. However, high resolution model, high polygon, really needs high performance computing to calculate rendering process to support user's interaction in real-time. For this paper, we focus on implementing Relief Mapping technique to reduce the polygonal bas-relief built in 3D world which is applied to VR of Sino Portuguese Architecture due to the facade of buildings consist of many bas-relief pattern via game engine, Unity3D, for real-time rendering and VR walkthrough. The result of this uses a single polygon/1 bas-relief and the quality of visualization after rendering process is compatible to the conventional process.","","978-1-4799-3724-0","10.1109/DICTAP.2014.6821706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821706","Virtual Reality (VR);Virtual Environment (VE);immersion;Facade;Bas-relief;Relief Mapping;Unity3D","Solid modeling;Three-dimensional displays;Real-time systems;Computer architecture;Virtual environments;Rendering (computer graphics)","data visualisation;parallel processing;real-time systems;rendering (computer graphics);solid modelling;user interfaces;virtual reality","facade;Sino Portuguese architecture;virtual reality;artificial environment;computer technology;virtual environment;3D model;high resolution model;high performance computing;rendering process;user interaction;relief mapping technique;polygonal bas-relief;bas-relief pattern;game engine;Unity3D;real-time rendering;VR walkthrough;visualization","","","","15","","29 May 2014","","","IEEE","IEEE Conferences"
"NASA Telexploration Project demo","J. Norris; S. Davidoff",NASA Jet Propulsion Laboratory; NASA Jet Propulsion Laboratory,"2014 IEEE Virtual Reality (VR)","24 Apr 2014","2014","","","183","184","NASA's Telexploration Project seeks to make us better explorers by building immersive environments that feel like we are really there. The Mission Operations Innovation Office and its Operations Laboratory at the NASA Jet Propulsion Laboratory (JPL) founded the Telexploration Project, and is researching how immersive visualization and natural human-robot interaction can enable mission scientists, engineers, and the general public to interact with NASA spacecraft and alien environments in a more effective way. These efforts have been accelerated through partnerships with many different companies, especially in the video game industry. These demos will exhibit some of the progress made at NASA and its commercial partnerships by allowing attendees to experience Mars data acquired from NASA spacecraft in a head mounted display using several rendering and interaction techniques.","2375-5334","978-1-4799-2871-2","10.1109/VR.2014.6802112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802112","virtual reality;space exploration;robotics","NASA;Mars;Laboratories;Propulsion;Robots;Three-dimensional displays;Space vehicles","astronomy computing;data visualisation;helmet mounted displays;human-robot interaction;Mars;rendering (computer graphics);virtual reality","NASA Telexploration Project demo;immersive environment;Mission Operations Innovation Office;Operations Laboratory;NASA Jet Propulsion Laboratory;NASA JPL;immersive visualization;natural human-robot interaction;mission science;mission engineering;NASA spacecraft;alien environment;Mars data;head mounted display;rendering technique;interaction technique","","3","","","","24 Apr 2014","","","IEEE","IEEE Conferences"
"Interaction Patterns of Spatial Navigation in VR Workspaces","A. sudár; Á. B. Csapó","Széchenyi István University,Multidisciplinary Doctoral School of Engineering Sciences,Hungary; Széchenyi István University,Multidisciplinary Doctoral School of Engineering Sciences,Hungary","2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","11 May 2020","2019","","","615","618","During the discovery of any new environment - be it real or virtual - the first step is usually that of exploration. The goal exploration in this context is to find the location that provides the most informative, or otherwise useful viewpoint in the space. Depending on the complexity of the space and the task at hand, this can be a difficult challenge. Nevertheless, the finding of useful viewpoints can be facilitated, at least in virtual reality, by changing the layout of the space, as well as by changing other visual features to lower the cognitive load of navigation. In this paper, we present a set of experiments using the MaxWhere desktop virtual environment which suggest that besides the arrangement of a space, its key points of discovery can also influence the time it takes to accomplish a task. This conclusion is supported both by data on key points of navigation and on the activation mode that was utilized on the various smartboards laid out in 3D space. In this research, we hope to show how specific nodes can be found, based on a data-oriented approach, which can lead users to focus on the most important information and obtain the best overview of any virtual space in general.","2380-7350","978-1-7281-4793-2","10.1109/CogInfoCom47531.2019.9089998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089998","virtual reality;MaxWhere;spatial cognition;spatial preferences;spatial navigation","Navigation;Task analysis;Virtual environments;Three-dimensional displays;Cognition;Atmospheric measurements","virtual reality","virtual reality;visual features;MaxWhere desktop virtual environment;activation mode;data-oriented approach;virtual space;interaction patterns;spatial navigation;VR workspaces;goal exploration;informative viewpoint","","6","","25","","11 May 2020","","","IEEE","IEEE Conferences"
"Real-time VR Simulation of Laparoscopic Cholecystectomy based on Parallel Position-based Dynamics in GPU","J. Pan; L. Zhang; P. Yu; Y. Shen; H. Wang; H. Hao; H. Qin","Beihang University Peng Cheng Lab,State Key Lab of VR Tech & Syst; Beihang University,State Key Lab of VR Tech & Syst; Beihang University,State Key Lab of VR Tech & Syst; Beijing Normal University,Faculty of Education; Beijing Aerospace General Hospital; Beihang University,Peng Cheng Lab; Stony Brook University,Department of Computer Science","2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","11 May 2020","2020","","","548","556","In recent years, virtual reality (VR) based training has greatly changed surgeons learning mode. It can simulate the surgery from the visual, auditory, and tactile aspects. VR medical simulator can greatly reduce the risk of the real patient and the cost of hospitals. Laparoscopic cholecystectomy is one of the typical representatives in minimal invasive surgery (MIS). Due to the large incidence of cholecystectomy, the application of its VR-based simulation is vital and necessary for the residents' surgical training. In this paper, we present a VR simulation framework based on position-based dynamics (PBD) for cholecystectomy. To further accelerate the deformation of organs, PBD constraints are solved in parallel by a graph coloring algorithm. We introduce a bio-thermal conduction model to improve the realism of the fat tissue electrocautery. Finally, we design a hybrid multi-model connection method to handle the interaction and simulation of the liver-gallbladder separation. This simulation system has been applied to laparoscopic cholecystectomy training in several hospitals. From the experimental results, users can operate in real-time with high stability and fidelity. The simulator is also evaluated by a number of digestive surgeons through preliminary studies. They believed that the system can offer great help to the improvement of surgical skills.","2642-5254","978-1-7281-5608-8","10.1109/VR46266.2020.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089572","Human-centered computing—Human computer interaction—Interactive systems and tools—User interface programming;Computer systems organization—Real-time systems—Real- time system architecture","Surgery;Biological system modeling;Strain;Biological tissues;Image color analysis;Deformable models;Solid modeling","biological tissues;computer based training;graphics processing units;liver;medical computing;surgery;virtual reality","minimal invasive surgery;VR-based simulation;PBD constraints;graph coloring algorithm;hybrid multimodel connection method;laparoscopic cholecystectomy;parallel position-based dynamics;virtual reality based training;visual aspects;tactile aspects;VR medical simulator;GPU;auditory aspects;biothermal conduction model;fat tissue electrocautery;liver-gallbladder separation","","","","30","","11 May 2020","","","IEEE","IEEE Conferences"
"The Library: A Non-Intrusive Gaze Directed Virtual Reality Animation","K. A. Yu",NA,"2019 IEEE 2nd Workshop on Animation in Virtual and Augmented Environments (ANIVAE)","2 Apr 2020","2019","","","1","4","In recent years, Cinematic Virtual Reality (CVR) has emerged as a unique form of VR storytelling. While narrative strategies for this new medium are still being explored, one often discussed issue is in guiding the viewer to look where the filmmaker intended. Conventional cinematic techniques such as sound and visual cues have been adopted to direct the viewer's attention. However, this dispersed viewer's attention might suggest an unexplored territory for storytelling in VR and immersive media in general. My current creative practice-led research explores how dispersed viewer's attention could be used to navigate a branching and spatial narrative structure. The Library is a work in progress, an animated VR experience that reimagines Jorge Luis Borges' Library of Babel. The story progresses according to how the viewer's gaze reacts to the narrative events that are organised both temporally and spatially, all the while without the viewer being aware of the gaze interaction mechanism. This paper presents some of the strategies that have emerged in the production of the creative work, which could lead to further explorations of a non-intrusive gaze directed interactive immersive cinema.","","978-1-7281-3229-7","10.1109/ANIVAE47543.2019.9050930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050930","Cinematic Virtual Reality;Animation;Computer Graphics","","computer animation;data visualisation;human computer interaction;humanities;interactive systems;virtual reality","cinematic virtual reality;VR storytelling;narrative strategies;sound cues;visual cues;dispersed viewer;branching;spatial narrative structure;animated VR experience;narrative events;gaze interaction mechanism;cinematic techniques;nonintrusive gaze directed virtual reality","","","","9","","2 Apr 2020","","","IEEE","IEEE Conferences"
"Muscleblazer: Force-Feedback Suit for Immersive Experience","Y. Kishishita; S. Das; A. V. Ramirez; C. Thakur; R. Tadayon; Y. Kurita","Hiroshima University; Hiroshima University; Hiroshima University; Hiroshima University; Arizona State University, CuBiC; Hiroshima University, JST PRESTO","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1813","1818","The increasing use of virtual reality (VR) and augmented reality (AR) systems has opened the possibility of providing immersive experiences to the general population around the world. However, most of the existing systems do not provide highly effective force-feedback experiences to the user. To provide such augmented systems in combination with force-feedback, novel ideas must be introduced that can be easily integrated with the existing VR and AR technologies. This work proposes a first-person VR game integrated with a soft exoskeleton that enhances the quality of interaction between the subject and the virtual environment (VE) through an additional force-feedback element. The effect of introducing the force-feedback element on the user is analyzed by using biosensors and questionnaire feedback. The biosensors are used to measure the level of anxiety induced in the subject during interaction with the virtual environment. Conditions in which this interaction occurs with and without force-feedback are compared. The questionnaire analyzes the perceived change in emotions of users as a result of the introduction of the force-feedback element. This game can be played by most individuals, regardless of age and physical fitness.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797962","Virtual reality;Force-feedback;Pneumatic actuators;Wearable technology;Biosensors;Galvanic skin response (GSR)","Games;Virtual environments;Muscles;Valves;Solenoids;Force;Biosensors","augmented reality;biosensors;force feedback;haptic interfaces;human factors","force-feedback suit;immersive experience;highly effective force-feedback experiences;augmented systems;first-person VR game;virtual environment;additional force-feedback element;questionnaire feedback;virtual reality;augmented reality systems;AR systems;soft exoskeleton;biosensors","","1","","12","","15 Aug 2019","","","IEEE","IEEE Conferences"
"3rd Virtual and Augmented Reality for Good (VAR4Good) Workshop","A. Dey; M. Billinghurst; G. Welch; E. Rojas-Muñoz",NA; NA; NA; NA,"2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","29 Apr 2019","2018","","","364","364","Virtual Reality (VR) and Augmented Reality (AR) are becoming mainstream. With the research and technological advances, it is now possible to use these technologies in almost all domains and places. This provides a bigger opportunity to create applications that intend to impact society in greater ways than beyond just entertainment. Today the world is facing different challenges including healthcare, environment, and education. Now is the time to explore how VR/AR might be used to solve widespread societal challenges. The third Virtual and Augmented Reality for Good (VAR4Good) workshop will bring together researchers, developers, and industry partners in presenting and promoting research that intends to solve real-world problems using VR/AR. The workshop will provide a platform to grow a research community that discusses challenges and opportunities to create Virtual and Augmented Reality for Good. We invite application and position papers (2-4 pages, excluding references), that address the way that VR/AR technologies can solve real-world problems in various application domains including, but not limited to, health, the environment, education, sports, the arts, and applications in support of special needs such as assistive, adaptive, and rehabilitative applications. Our focus and preference will be on applications that are beyond general uses of VR/AR. Please see full CFP on our website.","","978-1-5386-7592-2","10.1109/ISMAR-Adjunct.2018.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8699256","","Augmented reality;Conferences;Australia;Education;Entertainment industry;Medical services","augmented reality;human computer interaction;technology management","VAR4Good;technological advances;Virtual and Augmented Reality for Good workshop;VR/AR technologies","","","","","","29 Apr 2019","","","IEEE","IEEE Conferences"
"Interacting with Distant Objects in Augmented Reality","M. Whitlock; E. Harnner; J. R. Brubaker; S. Kane; D. A. Szafir",University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder,"2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","41","48","Augmented reality (AR) applications can leverage the full space of an environment to create immersive experiences. However, most empirical studies of interaction in AR focus on interactions with objects close to the user, generally within arms reach. As objects move farther away, the efficacy and usability of different interaction modalities may change. This work explores AR interactions at a distance, measuring how applications may support fluid, efficient, and intuitive interactive experiences in room-scale augmented reality. We conducted an empirical study (N = 20) to measure trade-offs between three interaction modalities-multimodal voice, embodied freehand gesture, and handhelds devices-for selecting, rotating, and translating objects at distances ranging from 8 to 16 feet (2.4m-4.9m). Though participants performed comparably with embodied freehand gestures and handheld remotes, they perceived embodied gestures as significantly more efficient and usable than device-mediated interactions. Our findings offer considerations for designing efficient and intuitive interactions in room-scale AR applications.","","978-1-5386-3365-6","10.1109/VR.2018.8446381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446381","Human-centered computing-Interaction design-Interaction design process and methods-User interface design","Task analysis;Usability;Visualization;Pervasive computing;Augmented reality;Electronic mail","augmented reality;gesture recognition;user interfaces","distant objects;augmented reality;immersive experiences;AR interactions;fluid experiences;intuitive interactive experiences;room-scale;empirical study;interaction modalities-multimodal voice;embodied freehand gesture;handhelds devices-for selecting;device-mediated interactions;efficient interactions;intuitive interactions;interaction modalities;room-scale AR applications","","2","","44","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Building semi-immersing human-computer interaction environment of virtual teleconferencing based on multi-screen general projection","Sun Lifeng; Zhong Yuzhuo; Yang Bing","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; NA","IEEE 2002 International Conference on Communications, Circuits and Systems and West Sino Expositions","25 Feb 2003","2002","1","","639","643 vol.1","Building a human-computer interaction environment which supports natural communication and interaction among participant group is the key issue of virtual teleconferencing system, which needs to reestablish the correct space relationship of participants and synthesizing real size video into the virtual environment. We introduce a method that uses multiple projection screens driven by network-connected PCs to construct the spatial and realism semi-immerse virtual teleconferencing environment based on general projection transformation. The experiment shows that participants can interact and collaborate with natural interaction model in there.","","0-7803-7547-5","10.1109/ICCCAS.2002.1180699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1180699","","Teleconferencing;Collaborative work;Virtual environment;Space technology;Personal communication networks;Videoconference;Delay;Resource management;Computer science;Systems engineering and theory","teleconferencing;man-machine systems;virtual reality;groupware;computer networks","semi-immersing human-computer interaction environment;multi-screen general projection;natural communication;natural interaction;virtual teleconferencing system;multiple projection screens;network-connected PC;projection transformation;collaborate model;collaborative working environment;centralized control;distributed control;high-speed rendering","","","1","6","","25 Feb 2003","","","IEEE","IEEE Conferences"
"Interactive Modeling of Trees using VR Devices","Z. Liu; C. Shen; Z. Li; T. Weng; O. Deussen; Z. Cheng; D. Wang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences","2019 International Conference on Virtual Reality and Visualization (ICVRV)","6 Oct 2020","2019","","","69","75","Conventional interactive 3D tree modeling systems are generally based on 2D input devices, and it's not convenient to generate desired 3D tree shape from 2D inputs due to the complexity of 3D tree structures. In this paper, we present a system for modeling trees interactively using a 3D gesture-based VR platform. The system contains a head-mounted display (HMD) and a 6-DOF motion controller for interaction. We propose an improved procedural modeling method to generate trees faster for VR platform. Using the 6-DOF motion controller, users can manipulate tree structures by a set of 3D interactive operations, including geometric editing using 3D gestures, sketching, brushing and silhouette-guided growth. Our interactions are more flexible and convenient than using traditional 2D input devices, e.g., we allow the user to simultaneously rotate and translate parts of a tree using a 3D gesture.","2375-141X","978-1-7281-4752-9","10.1109/ICVRV47840.2019.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212896","Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Shape modeling","Three-dimensional displays;Vegetation;Solid modeling;Two dimensional displays;Computational modeling;Input devices;Skeleton","geometry;gesture recognition;helmet mounted displays;interactive systems;solid modelling;virtual reality","interactive modeling;VR devices;tree structures;3D gesture-based VR platform;head-mounted display;3D interactive operations;2D input devices;interactive 3D tree modeling systems;geometric editing","","","","25","","6 Oct 2020","","","IEEE","IEEE Conferences"
"A Desktop VR-based HCI framework for programming instruction","M. Chandramouli; J. Heffron",Purdue University Calumet; Purdue University Calumet,"2015 IEEE Integrated STEM Education Conference","11 Jun 2015","2015","","","129","134","Programming skills are becoming increasingly important in both academia and industry. While this signifies numerous opportunities for students, it also inherently involves the challenge of preparing students suitably for these opportunities. Students, especially those at the beginner level, encounter difficulties when learning to program and the lack of efficient tools to overcome such difficulties can affect students' motivation. Over time, this creates to a drastic and negative impact in their attitude towards `learning programming', which is undesirable for student success in engineering education. To rectify this, a suitable approach that can motivate students needs to be developed to change students' mindset towards learning programming. To this end, to facilitate interactive and fun-filled learning, this research employs a learner-centric, user-friendly Virtual Environment (VE) to teach programming concepts. The impact of this research extends beyond engineering and technology education as this framework can serve as a tool to strengthen STEM education and enhance general programming literacy.","","978-1-4799-1829-4","10.1109/ISECon.2015.7119905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119905","HCI;Desktop VR;Programing Instruction","Programming profession;Containers;Conferences;Visualization;Human computer interaction;Indexes","computer aided instruction;computer science education;engineering education;human computer interaction;human factors;programming;virtual reality","desktop VR-based HCI framework;programming instruction;student motivation;engineering education;interactive fun-filled learning;learner-centric user-friendly virtual environment;programming concepts;teaching;technology education;programming literacy;programming skills;STEM education","","2","","7","","11 Jun 2015","","","IEEE","IEEE Conferences"
"Haptically enabled interactive virtual reality prototype for general assembly","A. Bhatti; Yong Bing Khoo; D. Creighton; J. Anticev; S. Nahavandi; Mingwei Zhou","Deakin University, Australia; CSIRO, Australia; Deakin University, Australia; CSIRO, Australia; Deakin University, Australia; CSIRO, Australia","2008 World Automation Congress","9 Dec 2008","2008","","","1","6","Desktop computers based virtual training systems are attracting paramount attention from manufacturing industries due to their potential advantages over the conventional training practices. Significant cost savings can be realized due to the shorter training-scenarios development times and reuse of existing engineering models. In addition, by using computer based virtual reality (VR) training systems, the time span from the product design to commercial production can be shortened due to non-reliance on hardware parts. Within the aforementioned conceptual framework, a haptically enabled interactive and immersive virtual reality (HIIVR) system is presented. Unlike existing VR systems, the presented idea tries to imitate real physical training scenarios by providing comprehensive user interaction, constrained within the physical limitations of the real world imposed by the haptics devices within the virtual environment. As a result, in contrast to the existing VR systems, capable of providing knowledge generally about assembly sequences only, the proposed system helps in procedural learning and procedural skill development as well, due to its high physically interactive nature.","2154-4824","978-1-889335-38-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4699035","Haptics;virtual assembly;interactive and immersive","Virtual reality;Virtual prototyping;Assembly;Industrial training;Computer aided manufacturing;Manufacturing industries;Costs;Design engineering;Product design;Production systems","assembling;computer based training;haptic interfaces;industrial training;production engineering computing;virtual reality","haptically enabled interactive virtual reality prototype;general assembly;manufacturing industries;computer-based virtual reality training systems;immersive virtual reality","","","","9","","9 Dec 2008","","","IEEE","IEEE Conferences"
"Natural Interaction to Support Teaching Activities in Health","D. Dos Santos Ferreira; L. S. Machado","Lab. de Tecnol. para o Ensino Virtual e Estatistica (LabTEVE), Univ. Fed. da Paraiba (UFPB), João Pessoa, Brazil; Lab. de Tecnol. para o Ensino Virtual e Estatistica (LabTEVE), Univ. Fed. da Paraiba (UFPB), João Pessoa, Brazil","2014 XVI Symposium on Virtual and Augmented Reality","2 Oct 2014","2014","","","254","257","In the field of human-computer interaction, Natural Interaction methods has been gaining importance because they provide communication between user and machine through an easy and intuitive way, by interpreting the natural actions of persons. In the development of Virtual Reality systems for health, there is a need to produce forms of interaction that are similar to the commonly performed by people in daily activities. Because of its intuitive features, the Natural Interaction can perform an important role in this scenario. It is important to emphasize that some natural behaviors can be of cultural origin, creating the need to identify the specific features target audience in interaction with applications that provide natural interfaces. The general goal of paper is to discuss a set of techniques for Natural Interaction by gestures for use in tracking systems by optical devices to support professors in health teaching activities.","","978-1-4799-4261-9","10.1109/SVR.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913100","Natural Interaction;Virtual Reality;Optical Tracking;Health Education","Three-dimensional displays;Mice;Education;Visualization;Augmented reality;Cultural differences","biomedical education;computer aided instruction;health care;human computer interaction;medical computing;teaching;virtual reality","human-computer interaction;natural interaction methods;virtual reality systems;daily activities;natural interfaces;tracking systems;optical devices;health teaching activities","","","","8","","2 Oct 2014","","","IEEE","IEEE Conferences"
"The Matter of Attention and Motivation – Understanding Unexpected Results from Auditory Localization Training Using Augmented Reality","S. H. Chon; S. Kim",University of Colorado Boulder; Rochester Institute of Technology,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1503","1506","We present the results from a seven-week auditory localization training using Microsoft HoloLens. Eight participants were divided into two groups. Both groups showed a generally declining pattern in localization performance over the eight tests, unlike the results from our previous study. The decreasing slope was smaller for the train group than for the control group, which might reflect some mild effect of training. There was a one-time performance improvement after two trainings, which was not observed from subsequent tests. The training program might have been too simple to maintain participants' attention for weeks. Possible extraneous factors such as the academic calendar are discussed that might have had an impact on this decreasing pattern against the hypotheses.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797683","Localization;Training effect;Augmented reality;HRTFs;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied Computing;Education;Interactive learning environments","Training;Visualization;Augmented reality;Task analysis;Ear;Atmospheric measurements;Particle measurements","augmented reality;behavioural sciences computing;human factors","training program;augmented reality;seven-week auditory localization training;Microsoft HoloLens;generally declining pattern;localization performance;train group;control group;one-time performance improvement;motivation;academic calendar","","","","16","","15 Aug 2019","","","IEEE","IEEE Conferences"
"A Gas Turbine Virtual Reality Application Migration to Mixed Reality: Development Experience","H. Sulaiman; A. M. Yusof; N. Ibrahim; R. A. Latif","Universiti Tenaga Nasional,Department of Informatics,Malaysia; Universiti Tenaga Nasional,Department of Informatics,Malaysia; Universiti Tenaga Nasional,Institute of Informatics and Computing in Energy,Malaysia; Universiti Tenaga Nasional,Department of Informatics,Malaysia","2020 IEEE Graphics and Multimedia (GAME)","15 Jan 2021","2020","","","19","24","This paper discusses several technical issues and difficulties faced while converting a Gas Turbine Virtual Reality (VR) application to a Mixed Reality (MR) application. Direct conversion from a VR to MR is feasible. However, it is found that several techniques or methods employed in VR are ineffective in MR. This is due to the Head Mounted Device used in the MR application allows users to see the real world while visualising computer-generated image, whereby the same function is not applicable in VR. Among the functions in VR which are not compatible with MR are teleporting and interaction. These issues were discovered during a prototype testing of the MR system. Based on the observation, it is found that in an MR system, the view inside HMD is overcrowded when a virtual background is visible along with the actual background. In addition to that, in an MR system, users prefer to physically walk towards the virtual object rather than teleporting using a controller. The discussion is not only highlighting the problem faced but also the solutions. In general, the objective of this paper is to share the technical issues experienced during the development process. It is expected that this knowledge can be helpful to future researchers or developers.","","978-1-7281-9244-4","10.1109/GAME50158.2020.9315123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315123","Virtual Reality (VR);Mixed Reality (MR);teleportation;interaction","Virtual reality;Turbines;Three-dimensional displays;Real-time systems;Solids;Navigation;Mixed reality","data visualisation;gas turbines;helmet mounted displays;mechanical engineering computing;mobile computing;virtual reality;wearable computers","mixed reality application;head mounted device;virtual background;gas turbine virtual reality application;computer generated image visualisation;VR device;mobile devices","","","","10","","15 Jan 2021","","","IEEE","IEEE Conferences"
"The design and implementation of the multi-screen interaction service architecture for the Real-Time streaming media","X. Xie; Z. Yu; K. Wang","The Shandong Provincial Engineering Center of Health Informatics, Qingdao University, Qingdao, China; The Shandong Provincial Engineering Center of Health Informatics, Qingdao University, Qingdao, China; The Shandong Provincial Engineering Center of Health Informatics, Qingdao University, Qingdao, China","2013 Ninth International Conference on Natural Computation (ICNC)","19 May 2014","2013","","","1600","1604","With the development of the Networks Convergence in China, the multi-screen interaction has been nearer and nearer to us. In order to build a unified service platform which supports the multi-screen interaction, A general application architecture is proposed for the Real-Time streaming media, which supports various platform and terminal. By employing virtual reality, MPEG-4 CODEC, interprocess communication technology and Real-Time streaming media transmission protocol, the Real-Time streaming media system is implemented, which verified the feasibility of the application architecture for multi-screen interaction service.","2157-9563","978-1-4673-4714-3","10.1109/ICNC.2013.6818237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818237","Real-Time;Streaming Media;Multi-Screen Interaction;Virtual-Reality;Encoding","Streaming media;Servers;Real-time systems;Protocols;Games;Transform coding;Virtual reality","computerised instrumentation;media streaming;virtual reality","multiscreen interaction service architecture;networks convergence;China;unified service platform;virtual reality;MPEG-4 CODEC;interprocess communication technology;real-time streaming media transmission protocol;real-time streaming media system","","","","12","","19 May 2014","","","IEEE","IEEE Conferences"
"Getting around in google cardboard – exploring navigation preferences with low-cost mobile VR","W. Powell; V. Powell; P. Brown; M. Cook; J. Uddin",University of Portsmouth; University of Portsmouth; University of Portsmouth; University of Portsmouth; University of Portsmouth,"2016 IEEE 2nd Workshop on Everyday Virtual Reality (WEVR)","23 Feb 2017","2016","","","5","8","In recent years there has been a paradigm shift in the uptake and use of Virtual Reality (VR). Advances in graphics rendering, and the introduction of low-cost VR headsets has brought VR into the reach of ordinary consumers. Google Cardboard VR viewers cost just a few dollars and work with most smart phones, enabling mobile VR to truly enter the domain of the everyday. However, these headsets are currently generally used for passive entertainment or viewing 360 degree media, and are not ideally suited to active exploration of a virtual space. In this paper we present our preliminary evaluation of three approaches to travel and navigation.","","978-1-5090-0840-7","10.1109/WEVR.2016.7859536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859536","Interaction;travel techniques;HMD;mobile VR;Google Cardboard;navigation","Navigation;Headphones;Switches;Google;Bluetooth;Mobile communication","mobile computing;rendering (computer graphics);virtual reality","Google cardboard;navigation preferences;low-cost mobile VR;virtual reality;graphics rendering;low-cost VR headsets;smart phones;360 degree media","","9","","14","","23 Feb 2017","","","IEEE","IEEE Conferences"
"The Transreality Interaction Platform: Enabling Interaction across Physical and Virtual Reality","K. A. Martin; J. J. Laviola","Dept. of Comput. Sci., Univ. of Central Florida, Orlando, FL, USA; Dept. of Comput. Sci., Univ. of Central Florida, Orlando, FL, USA","2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)","4 May 2017","2016","","","177","186","The convergence of the Internet of Things (IoT) and interactive systems will enable future interactive environments which transcend physical and virtual reality. Embedded Things provide sensors and actuators to virtualize the physical environment, while Interactive Things extend the virtualized environment with modalities for human interaction, ranging from tangible and wearable interfaces to immersive virtual and augmented reality interfaces. We introduce the Transreality Interaction Platform (TrIP) to enable service ecosystems which situate virtual objects alongside virtualized physical objects and allow for novel ad-hoc interactions between humans, virtual, and physical objects in a transreality environment. TrIP provides a generalized middleware platform addressing the unique challenges that arise in complex transreality systems which have yet to be fully explored in current IoT or HCI research. We describe the system architecture, data model, and query language for the platform and present a proof-of-concept implementation. We evaluate the performance of the implementation and demonstrate its use integrating embedded and interactive things for seamless interaction across physical and virtual realities.","","978-1-5090-5880-8","10.1109/iThings-GreenCom-CPSCom-SmartData.2016.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917082","Internet of Things;Interactive Things;Transreality;Interactive Systems;Augmented Reality;Virtual Reality;Tangible User Interface","Solid modeling;Virtual reality;Human computer interaction;Ecosystems;Databases;Sensor systems and applications","interactive systems;Internet of Things;middleware;virtual reality","transreality interaction platform;virtual reality;Internet of Things;IoT;interactive systems;interactive environments;embedded things;actuators;physical environment;interactive things;human interaction;wearable interfaces;tangible interfaces;service ecosystems;virtual objects;generalized middleware platform","","4","","50","","4 May 2017","","","IEEE","IEEE Conferences"
"Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data","B. Laha; D. A. Bowman; J. D. Schiffbauer","Center for Human-Computer Interaction and the Department of Computer Science, Virginia Tech; Center for Human-Computer Interaction and the Department of Computer Science, Virginia Tech; Department of Geological Sciences, University of Missouri, Columbia, MO","IEEE Transactions on Visualization and Computer Graphics","13 Mar 2013","2013","19","4","529","538","In our research agenda to study the effects of immersion (level of fidelity) on various tasks in virtual reality (VR) systems, we have found that the most generalizable findings come not from direct comparisons of different technologies, but from controlled simulations of those technologies. We call this the mixed reality (MR) simulation approach. However, the validity of MR simulation, especially when different simulator platforms are used, can be questioned. In this paper, we report the results of an experiment examining the effects of field of regard (FOR) and head tracking on the analysis of volume visualized micro-CT datasets, and compare them with those from a previous study. The original study used a CAVE-like display as the MR simulator platform, while the present study used a high-end head-mounted display (HMD). Out of the 24 combinations of system characteristics and tasks tested on the two platforms, we found that the results produced by the two different MR simulators were similar in 20 cases. However, only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study. Our observations provide evidence both for and against the validity of MR simulation, and give insight into the differences caused by different MR simulator platforms. The present experiment also examined new conditions not present in the original study, and produced new significant results, which confirm and extend previous existing knowledge on the effects of FOR and head tracking. We provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications.","1941-0506","","10.1109/TVCG.2013.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479179","MR simulator;immersion;micro-CT;volume visualization;virtual reality;3D visualization;HMD;virtual environments.","Virtual reality;Visualization;Mice;Solid modeling;Head;Training;Computational modeling","data analysis;data visualisation;digital simulation;helmet mounted displays;virtual reality","MR simulation approach;immersion effects;visual volume data analysis;virtual reality systems;VR;mixed reality simulation approach;field of regard;FOR;head tracking;volume visualized microCT datasets;CAVE-like display;MR simulator platform;high-end head-mounted display;HMD;display systems;volume visualization applications","Adolescent;Adult;Computer Graphics;Cues;Equipment Design;Equipment Failure Analysis;Female;Humans;Imaging, Three-Dimensional;Imaging, Three-Dimensional;Male;Task Performance and Analysis;Tomography, X-Ray Computed;User-Computer Interface;Visual Perception;Young Adult","15","","31","","13 Mar 2013","","","IEEE","IEEE Journals"
"Batmen Forever: Unified Virtual Hand Metaphor for Consumer VR Setups","A. Montes Rodrigues; M. Nagamura; L. G. Freire da Costa; M. K. Zuffo",Interdisciplinary Center in Interactive Technologies - Polytechnic School - University of São Paulo; Interdisciplinary Center in Interactive Technologies - Polytechnic School - University of São Paulo; Interdisciplinary Center in Interactive Technologies - Polytechnic School - University of São Paulo; Interdisciplinary Center in Interactive Technologies - Polytechnic School - University of São Paulo,"2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","854","855","In this work, we present a hand-based natural interaction that allows performing fundamental actions such as moving or controlling objects and climbing ladders. The setup was restricted to available consumer VR technology, aiming to advance towards a practical unified framework for 3D interaction. The strategy was syncing the closest natural movement allowed by the device with primary task actions, either directly or indirectly, creating hypernatural UIs. The prototype allowed successful completion of the three challenges proposed by the 2018 3DUI Contest, as validated by a preliminary user study with participants from the target audience and also from the general public.","","978-1-5386-3365-6","10.1109/VR.2018.8446277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446277","I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques","Three-dimensional displays;User interfaces;Conferences;Virtual reality;Industrial engineering","human computer interaction;user interfaces;virtual reality","unified virtual hand metaphor;consumer VR setups;hand-based natural interaction;primary task actions;hypernatural UIs;ladder climbing;consumer VR technology","","1","","4","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Design and Implementation of the Interactive Virtual Reality Touring System - A Case Study of Shulin Ji'an Temple in Taiwan","J. -H. Lo; M. -J. You","Fo-Guang University,Department of Applied Informatics,Yilan,Taiwan,26247; Fo-Guang University,Department of Applied Informatics,Yilan,Taiwan,26247","202020 3rd IEEE International Conference on Knowledge Innovation and Invention (ICKII)","18 Jan 2021","2020","","","115","117","Most current tour guiding methods for Taiwanese temples employ graphic webpage frameworks combined with captioned pictures for introduction. This type of tour guiding lacks interactive presence. In addition, the audience may not be able to focus on browsing webpages or learn essential information from the introduction. This study adopted the Delphi method to evaluate the current developed system. This system was aimed at designing VR-based interaction that differs from conventional tour guiding methods to aid users in viewing the display space from their viewpoints. Users can not only control camera view angles but also select the paths and guiding information as if they were walking in the temple. The analysis results revealed that in general, the users perceived Web VR tour guiding as convenient and easy to use. The display and content of the tour guiding system presented clear information to the users, aiding them in gaining further understanding of the introduced item. Finally, the study results can serve as a reference for design research on VR applications in tour guiding.","","978-1-7281-9333-5","10.1109/ICKII50300.2020.9318862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318862","A-Frame;Delphi method;Questionnaire for User Interface Satisfaction (QUIS);Virtual Reality (VR)","Internet;Technological innovation;Social networking (online);Graphics;Statistical analysis;Cultural differences;Usability","interactive systems;Internet;virtual reality","interactive virtual reality touring system;Taiwanese temples;graphic webpage frameworks;captioned pictures;interactive presence;webpages;Delphi method;VR-based interaction;conventional tour;camera view angles;guiding information;Web VR tour;tour guiding system;design research;Shulin Ji'an Temple","","","","8","","18 Jan 2021","","","IEEE","IEEE Conferences"
"Human-robot interaction by information sharing","Y. Anzai","Japan Society for the Promotion of Science, Tokyo, Japan","2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","21 Mar 2013","2013","","","65","66","In the plenary talk addressed at ROMAN92, considered as the first scientific meeting for human-robot interaction [1], I noted that `one of the noticeable features of the current work on human-robot communication is that it generally lacks attention to computer science, particularly to human-computer interaction' [2]. Until that time the field of HRI had been covered by industrial robotics, remote control and augmented reality, and much less attention had been paid to interaction per se, or the design of interactive systems supported by computers and computer network technology.","2167-2148","978-1-4673-3101-2","10.1109/HRI.2013.6483503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6483503","","Educational institutions;Robots;Human-robot interaction;Information management;Human computer interaction;Interactive systems;Computers","augmented reality;human-robot interaction;industrial robots;interactive systems;telerobotics","information sharing;human-robot interaction;ROMAN92;scientific meeting;human-robot communication;computer science;HRI;industrial robotics;remote control;augmented reality;interactive system design;computer network technology","","3","","3","","21 Mar 2013","","","IEEE","IEEE Conferences"
"VR Dodge-ball: Application of Real-time Gesture Detection from Wearables to ExerGaming","S. Ishii; M. Luimula; A. Yokokubo; G. Lopez","Aoyama Gakuin University 5-10-1 Fuchinobe,Chuo-ku, Sagamihara-shi, Kanagawa,Japan; Turku University of Applied Sciences Joukahaisenkatu 3C,Turku,Finland,20520; Aoyama Gakuin University 5-10-1 Fuchinobe,Chuo-ku, Sagamihara-shi, Kanagawa,Japan; Aoyama Gakuin University 5-10-1 Fuchinobe,Chuo-ku, Sagamihara-shi, Kanagawa,Japan","2020 11th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","2 Nov 2020","2020","","","000081","000082","Wearable technologies are applied to various areas nowadays, including health care and physical training. In previous work, we developed ExerSense, an algorithm to segment, classify, and count different predefined motions in real-time using correlation of acceleration. Using ExerSense, we developed a Virtual Reality (VR) Dodge-ball game. A watch-like wearable device is mounted to the right wrist and connected wireless to a head-mounted display (HMD). ExerSense enables to recognize real-time ball throwing gesture. Dodging opponent's ball in 3-Dimensional virtual space is detected using the sensors embedded in the HMD. We found that wearable devices have an advantage for virtual exergames against the camera-based approaches: there is no space and place limitation. Additionally, the study shows that exergames could be implemented with only general usage devices.","2380-7350","978-1-7281-8213-1","10.1109/CogInfoCom50765.2020.9237824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237824","","Wrist;Wireless communication;Wireless sensor networks;Wearable computers;Resists;Virtual reality;Real-time systems","computer games;gesture recognition;helmet mounted displays;human computer interaction;mobile computing;motion sensors;pattern classification;real-time systems;sensor fusion;virtual reality;wearable computers","health care;physical training;ExerSense;motion segmentation;head mounted display;wearable devices;virtual exergames;3-dimensional virtual space;virtual reality dodge ball game;real time gesture detection;VR Dodge ball;motion classification;gesture recognition;embedded sensors","","","","8","","2 Nov 2020","","","IEEE","IEEE Conferences"
"Uncertainty Boundaries for Complex Objects in Augmented Reality","J. Chen; B. MacIntyre","School of Interactive Computing, Georgia Institute of Technology, johnchen@cc.gatech.edu; School of Interactive Computing, Georgia Institute of Technology, blair@cc.gatech.edu","2008 IEEE Virtual Reality Conference","4 Apr 2008","2008","","","247","248","Registration errors between the physical world and computer- generated objects are a central problem in Augmented Reality (AR) systems. Some existing AR systems have demonstrated how to dynamically estimate registration errors based on estimates of spatial errors in the system. Using these error estimates, these systems also demonstrated a number of ways of ameliorating the effects of registration error. One central part of this previous work was the creation and use of error regions around objects; unfortunately, the analytic methods used only created accurate regions for simple convex objects. In this paper, we present a simple and stable algorithm for generating the uncertainty regions for complex objects, including non-convex objects and objects with interior holes. We demonstrate how our approach can be used to create a set of more accurate error-based highlights in the presence of registration error, and also be used as a general highlighting mechanism.","2375-5334","978-1-4244-1971-5","10.1109/VR.2008.4480784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480784","AR;registration error;non-convex objects;spatial uncertainty;I.3.3 [Computer Graphics]: Picture/Image Generation - Viewing Algorithms;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques","Uncertainty;Augmented reality;Computer errors;Computer graphics;Sensor systems;Physics computing;Image generation;Shape;Application software;Delay","augmented reality;uncertainty handling","uncertainty boundaries;complex objects;augmented reality;computer-generated objects;registration errors","","2","","6","","4 Apr 2008","","","IEEE","IEEE Conferences"
"A Game Theoretic Approach for Modeling User-System Interaction in Networked Virtual Environments","S. Lazem; D. Gracanin; A. Abdel-Hamid","Department of Computer Science, Virginia Tech, US, e-mail: shlazem@vt.edu; Department of Computer Science, Virginia Tech, US, e-mail:gracanin@vt.edu; Arab Academy for Science, Technology, and Maritime Transport Alexandria, Egypt, e-mail:hamid@aast.edu","2009 IEEE Virtual Reality Conference","7 Apr 2009","2009","","","277","278","Networked Virtual Environments (NVEs) are distributed 3D simulations shared among geographically dispersed users. Adaptive resource allocation is a key issue in NVEs, since user interactions affect system resources which in turn affect the user's experience. Such interplay between the users and the system can be modeled using game theory. Game theory is an analytical tool that studies decision-making between interacting agents (players), where player decisions impact greatly other players. We propose a basic structure for a Game Theory model that describes the interaction between the users and the system in NVEs based on an exploratory study of mobile virtual environments.","2375-5334","978-1-4244-3943-0","10.1109/VR.2009.4811053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811053","C.2.4 [Computer-Communication Networks]: Distributed Systems¿Distributed Applications;H.5.1 [Information Interfaces AND Presentation]: Multimedia Information Systems¿Artificial, augmented, and virtual realities;G.2.0 [Mathematics of Computing]: Discrete Mathematics¿General","Game theory;Virtual environment;Rendering (computer graphics);Delay;Prefetching;Computer science;Resource management;Computer interfaces;Computer networks;Distributed computing","decision making;game theory;human computer interaction;mobile computing;resource allocation;virtual reality","game theoretic approach;user-system interaction modeling;networked virtual environment;distributed 3D simulation;adaptive resource allocation;decision-making;mobile virtual environment","","2","","4","","7 Apr 2009","","","IEEE","IEEE Conferences"
"Underwater integral photography","N. Maki; K. Yanaka",Kanagawa Institute of Technology; Kanagawa Institute of Technology,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","343","344","A novel integral photography (IP) system in which the amount of popping out is more than three times larger than usual is demonstrated in this study. If autostereoscopic display is introduced into virtual reality, IP is an ideal candidate because not only the horizontal but also the vertical parallax can be obtained. However, the amount of popping out obtained by IP is generally far less than that obtained by head-mounted display because the ray density decreases when the viewer is distant from the fly's eye lens. Although a solution is to extend the focal length of the fly's eye lens, this lens is difficult to manufacture. We address this problem by simply immersing the fly's eye lens into water to extend the effective focal length.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223436","3D display;autostereoscopic display;integral photography","Lenses;IP networks;Photography;Virtual reality;Three-dimensional displays;Rendering (computer graphics);Human computer interaction","digital photography;geophysics computing;human computer interaction;oceanographic techniques;three-dimensional displays;virtual reality","underwater integral photography system;IP system;virtual reality;autostereoscopic display;vertical parallax;head-mounted display;fly eye lens;focal length;ray density","","1","","4","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Latency and User Performance in Virtual Environments and Augmented Reality","S. R. Ellis","NASA Ames Res. Center, Moffett Field, CA, USA","2009 13th IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications","28 Dec 2009","2009","","","69","69","System rendering latency has been recognized by senior researchers, such as Professor Fredrick Brooks of UNC (Turing Award 1999), as a major factor limiting the realism and utility of head-referenced display systems. Latency has been shown to reduce the user's sense of immersion within a virtual environment, to disturb user interaction with virtual objects, and to contribute to motion sickness during some simulation tasks. Latency, however, is not just an issue for external display systems since finite nerve conduction rates and variation in transduction times in the human body's sensors also pose problems for latency management within the nervous system. Some of the phenomena arising from the brain's handling of sensory asynchrony due to latency will be discussed as a prelude to consideration of the effects of latency in interactive displays. The causes and consequences of the erroneous movement that appears in displays due to latency will be illustrated with examples of the user performance impact provided by several experiments. These experiments will review the generality of user sensitivity to latency when users judge either object or environment stability. Hardware and signal processing countermeasures will also be discussed. In particular the tuning of a simple extrapolative predictive filter not using a dynamic movement model will be presented. Results show that it is possible to adjust this filter so that the appearance of some latencies may be hidden without the introduction of perceptual artifacts such as overshoot. Several examples of the effects of user performance will be illustrated by three-dimensional tracking and tracing tasks executed in virtual environments. These experiments demonstrate classic phenomena known from work on manual control and show the need for very responsive systems if they are intended to support precise manipulation. The practical benefits of removing interfering latencies from interactive systems will be emphasized with some classic final examples from surgical telerobotics and human-computer interaction.","1550-6525","978-0-7695-3868-6","10.1109/DS-RT.2009.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361782","","Delay;Virtual environment;Augmented reality;Displays;Filters;Limiting;Brain modeling;Humans;Sensor phenomena and characterization;Sensor systems","augmented reality;interactive systems","user performance;virtual environments;augmented reality;system rendering latency;motion sickness;nervous system;sensory asynchrony;extrapolative predictive filter;interactive systems","","","","","","28 Dec 2009","","","IEEE","IEEE Conferences"
"The role of latency in the validity of AR simulation","C. Lee; S. Bonebrake; D. A. Bowman; T. Höllerer","University of California, Santa Barbara; University of California, Santa Barbara; Virginia Tech; University of California, Santa Barbara","2010 IEEE Virtual Reality Conference (VR)","8 Apr 2010","2010","","","11","18","It is extremely challenging to run controlled studies comparing multiple Augmented Reality (AR) systems. We use an AR simulation approach, in which a Virtual Reality (VR) system is used to simulate multiple AR systems. To investigate the validity of this approach, in our first experiment we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results. We include a discussion on general issues we encountered with replicating a prior study. In our second experiment further exploring the validity of AR simulation, we investigated the effects of simulator latency on the results from experiments conducted in an AR simulator. We found simulator latency to have a significant effect on 3D tracing, however there was no interaction between simulator latency and artificial latency. Based on the results from these two experiments, we conclude that simulator latency is not inconsequential in determining task performance. Simulating visual registration is not sufficient to simulate the overall perception of registration errors in an AR system. We also need to keep simulator latency at a minimum. We discuss the impact of these results on the use of the AR simulation approach.","2375-5334","978-1-4244-6238-4","10.1109/VR.2010.5444820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444820","I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality-AR Simulation;I.3.6 [Methodology and Techniques]: Device independence-Replication","Delay;Virtual reality;Computational modeling;Hardware;Graphics;Augmented reality;Computer displays;Degradation;Software performance;Jitter","augmented reality;simulation","multiple augmented reality simulation;virtual reality;simulator latency;3D tracing;artificial latency;registration errors","","26","","14","","8 Apr 2010","","","IEEE","IEEE Conferences"
"Multi-dimensional Interactive City Exploration through Mixed Reality","I. Herbst; A. Braun; R. McCall; W. Broll","Collaborative Virtual and Augmented Environments Department, Fraunhofer FIT, Sankt Augustin, Germany; Collaborative Virtual and Augmented Environments Department, Fraunhofer FIT, Sankt Augustin, Germany; Collaborative Virtual and Augmented Environments Department, Fraunhofer FIT, Sankt Augustin, Germany; Collaborative Virtual and Augmented Environments Department, Fraunhofer FIT, Sankt Augustin, Germany","2008 IEEE Virtual Reality Conference","4 Apr 2008","2008","","","259","260","In this paper we present a pervasive outdoor mixed reality edutainment game for exploring the history of a city in the spatial and the temporal dimension, which will closely couple the real environment with the virtual content. The game provides a new and unique user experience, which links rich interactive content to time and places. We introduce the development of such a game, including a universal mechanism to define and setup multi-modal user interfaces for game challenges.","2375-5334","978-1-4244-1971-5","10.1109/VR.2008.4480790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4480790","Augmented Reality (AR);Multimodal Interfaces;Pervasive Gaming;Mixed Reality (MR);Presence;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities - Audio input/output;H.5.2 [User Interfaces]: Input devices and strategies;I.3.1 [Hardware Architecture]: Input devices;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction Techniques;K.8.0 [Personal Computing]: General - Games","Cities and towns;Virtual reality;User interfaces;Displays;Multimedia systems;Bluetooth;Optical receivers;Optical sensors;Mice;Iris","augmented reality;computer games;interactive systems;ubiquitous computing;user interfaces","multidimensional interactive city exploration;pervasive outdoor mixed reality edutainment game;virtual content;multimodal user interface;augmented reality;multimodal interface;pervasive gaming","","1","","3","","4 Apr 2008","","","IEEE","IEEE Conferences"
"The effect of trails on first-time and subsequent navigation in a virtual environment","R. A. Ruddle","Sch. of Comput., Leeds Univ., UK","IEEE Proceedings. VR 2005. Virtual Reality, 2005.","15 Aug 2005","2005","","","115","122","Trails are a little-researched type of aid that offers great potential benefits for navigation, especially in virtual environments (VEs). An experiment was performed in which participants repeatedly searched a virtual building for target objects assisted by: (1) a trail, (2) landmarks, (3) a trail and landmarks, or (4) neither. The trail was displayed as a white line that showed exactly where a participant had previously traveled. The trail halved the distance that participants traveled during first-time searches, indicating the immediate benefit to users if even a crude form of trail were implemented in a variety of VE applications. However, the general clutter or ""pollution"" produced by trails reduced the benefit during subsequent navigation and, in the later stages of these searches, caused participants to travel more than twice as far as they needed to, often accidentally bypassing targets even when a trail led directly to them. The proposed solution is to use gene alignment techniques to extract a participant's primary trail from the overall, polluted trail, and graphically emphasize the primary trail to aid navigation.","2375-5334","0-7803-8929-8","10.1109/VR.2005.1492761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1492761","","Navigation;Virtual environment;Virtual reality;Data mining;Pollution;Chromium;Graphics;Human computer interaction;Displays;Performance analysis","virtual reality;navigation;user interfaces","trails;virtual environments;virtual building;target object searching;landmarks;clutter;gene alignment technique;polluted trail;navigation aid","","9","","38","","15 Aug 2005","","","IEEE","IEEE Conferences"
"Robot Competition to Evaluate Guidance Skill for General Users in VR Environment","T. Inamura; Y. Mizuchi","The Graduate University for Advanced Studies, National Institute of Informatics SOKENDAI, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan","2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","25 Mar 2019","2019","","","552","553","Robot competition such as RoboCup@Home is one of the most effective ways to evaluate the performance of human-robot interaction; however, it takes a lot of costs for real robot maintenance and the practice of evaluation sessions. We have proposed a simulation software to evaluate human-robot interaction in daily life environment based on immersive virtual reality. In this paper, we design a task named `human navigation' in which the evaluation requires a subjective impression by the users. Through a substantiative experiment, we confirmed that the proposed task and system reduced the cost for the practice of the competition.","2167-2148","978-1-5386-8555-6","10.1109/HRI.2019.8673218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673218","Natural Language Generation;Guidance;Robot Competition","Robots;Task analysis;Human-robot interaction;Software;Solid modeling;Navigation;Natural languages","control engineering computing;human-robot interaction;robot programming;virtual reality","robot competition;general users;VR environment;RoboCup@Home;human-robot interaction;robot maintenance;human navigation;guidance skill evaluation;software simulation","","","","3","","25 Mar 2019","","","IEEE","IEEE Conferences"
"A Framework for Tangible User Interfaces within Projector-based Mixed Reality","Y. Yuan; X. Yang; S. Xiao","School of Software, Shanghai Jiao Tong University, China. e-mail: andy1789@sjtu.edu.cn; School of Software, Shanghai Jiao Tong University, China. e-mail: yangxubo@cs.sjtu.edu.cn; School of Software, Shanghai Jiao Tong University, China. e-mail: xsjiu99@cs.sjtu.edu.cn","2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality","6 Jun 2008","2007","","","283","284","This paper proposes a framework named TIPMR, for designing tangible user interfaces (TUIs) within projector-based mixed reality applications. The framework divides the target application into three parts: GUI-based application, TUI and an assistant. The assistant is employed as an adapter to translate between TUI operations and general GUI commands like mouse or keyboard events. This architecture makes it easier to focus on designing GUI-based applications and TUI separately. We built a tourist guidance system with two different tangible interaction modes based on TIPMR to demonstrate its usefulness and efficiency.","","978-1-4244-1749-0","10.1109/ISMAR.2007.4538868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538868","tangible user interface;graphical user interface;projector-based mixed reality;extended MCRpd model","User interfaces;Virtual reality;Graphical user interfaces;Application software;Mice;Electronic mail;Keyboards;Computer architecture;Computer interfaces;Joining processes","graphical user interfaces;user interfaces;virtual reality","tangible user interfaces;projector-based mixed reality;GUI commands;graphical user interfaces","","2","","4","","6 Jun 2008","","","IEEE","IEEE Conferences"
"Development and Analysis of Simulation Virtual Online Guiding Resources based on VR Technology","C. Hu; C. Li","Guangzhou College of Technology and Business,Guangzhou,China,510850; Guangzhou College of Technology and Business,Guangzhou,China,510850","2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)","18 Jan 2021","2020","","","29","33","Development and the analysis of simulation virtual online guiding resources based on VR technology is conducted in this paper. MR can easily complete human body modeling, as if it has a perspective eye, directly hitting the anatomical structure of the human body, and can interact with holographic images in real time through gestures, voice, peripherals, etc., and is in the theoretical study and practical exercise of general education both have positive and broad application prospects. In this research work, the face recognition, data mining and intelligent interaction models are combined to construct the efficient system. The experiment results have reflected the high accuracy and the core robustness. This proves the efficiency of the proposed model.","","978-1-7281-7089-3","10.1109/ICISS49785.2020.9316109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316109","Virtual Reality;Data Simulation;Data Mining;Online Guiding;Resource Mining","Face recognition;Solid modeling;Feature extraction;Training;Biological system modeling;Real-time systems;Face detection","data mining;face recognition;solid modelling;virtual reality","face recognition;intelligent interaction models;data mining;holographic images;anatomical structure;perspective eye;human body modeling;VR technology;simulation virtual online guiding resources","","","","19","","18 Jan 2021","","","IEEE","IEEE Conferences"
"AI for Toggling the Linearity of Interactions in AR","J. Qian; L. Denoue; J. Biehl; D. A. Shamma","Brown Univ., Providence, RI, USA; FXPAL, Palo Alto, CA, USA; FXPAL, Palo Alto, CA, USA; FXPAL, Palo Alto, CA, USA","2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","17 Jan 2019","2018","","","185","186","Interaction in augmented reality (AR) or mixed reality environments is generally classified into two modalities: linear (relative to object) or non-linear (relative to camera). Switching between these modes tailors the AR experience to different scenarios. Such interactions can be arduous in cases when on-board touch interaction is limited or restricted as is often the case in medical or industrial applications that require sterility. To solve this, we present Sound-to-Experience where the modality can be effectively toggled by noise or sound which is detected using a modern Artificial Intelligence deep-network classifier.","","978-1-5386-9269-1","10.1109/AIVR.2018.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613659","AR, AI, Augmented Reality, Artificial Intelligence, Mixed Reality, Interaction, Modality","Linearity;Switches;Cameras;Tracking;Augmented reality;Real-time systems","artificial intelligence;augmented reality;human computer interaction;neural nets;pattern classification","medical applications;industrial applications;AI;mixed reality environments;AR experience;on-board touch interaction;modern artificial intelligence deep-network classifier;sound-to-experience;linearity interactions;augmented reality environments","","","","6","","17 Jan 2019","","","IEEE","IEEE Conferences"
"You Shall Not Pass: Non-Intrusive Feedback for Virtual Walls in VR Environments with Room-Scale Mapping","M. Boldt; M. Bonfert; I. Lehne; M. Cahnbley; K. Korschinq; L. Bikas; S. Finke; M. Hanci; V. Kraft; B. Liu; T. Nguyen; A. Panova; R. Singh; A. Steenbergen; R. Malaka; J. Smeddinck","University of the Arts Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; Univ. of Bremen, Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany; University of Bremen, Germany","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","143","150","Room-scale mapping facilitates natural locomotion in virtual reality (VR), but it creates a problem when encountering virtual walls. In traditional video games, player avatars can simply be prevented from moving through walls. This is not possible in VR with room-scale mapping due to the lack of physical boundaries. Game design is either limited by avoiding walls, or the players might ignore them, which endangers the immersion and the overall game experience. To prevent players from walking through walls, we propose a combination of auditory, visual, and vibrotactile feedback for wall collisions. This solution can be implemented with standard game engine features, does not require any additional hardware or sensors, and is independent of game concept and narrative. A between-group study with 46 participants showed that a large majority of players without the feedback did pass through virtual walls, while 87% of the participants with the feedback refrained from walking through walls. The study found no notable differences in game experience.","","978-1-5386-3365-6","10.1109/VR.2018.8446177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446177","Virtual reality;virtual walls;tactile feedback;haptic feedback;visual feedback;auditory feedback;locomotion;game design;K.8.0 [Personal Computing]: General - games;H.5.2 [Information interfaces and presentation]: User Interfaces. - Interaction styles","Games;Legged locomotion;Visualization;Hardware;Haptic interfaces;Resists;Vibrations","avatars;computer games;virtual reality","game concept;narrative;players;virtual walls;game experience;nonintrusive feedback;VR environments;virtual reality;traditional video games;player avatars;game design;avoiding walls;wall collisions;standard game engine features;room-scale mapping;natural locomotion","","","","29","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Corrective feedback for depth perception in CAVE-like systems","A. K. T. Ng; L. K. Y. Chan; H. Y. K. Lau","The University of Hong Kong, Hong Kong, China; The University of Hong Kong, Hong Kong, China; The University of Hong Kong, Hong Kong, China","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","293","294","The perceived distance estimation in an immersive virtual reality system is generally underestimated to the actual distance. Approaches had been found to provide users with better dimensional perception. One method used in head-mounted displays is to interact by walking with visual feedback, but it is not suitable for a CAVE-like system, like imseCAVE, with confined spaces for walking. A verbal corrective feedback mechanism is proposed. The result shows that estimation accuracy generally improves after eight feedback trials although some estimations become overestimated. One possible explanation is the need of more verbal feedback trials. Further research on top-down approach for improvement in depth perception is suggested.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892292","Distance estimation;error correction;training from feedback;CAVE-like systems;imseCAVE","Estimation;Training;Legged locomotion;Virtual environments;Three-dimensional displays;Visualization","helmet mounted displays;human computer interaction;virtual reality","depth perception;CAVE-like systems;perceived distance estimation;immersive virtual reality system;dimensional perception;head-mounted displays;visual feedback;imseCAVE;verbal corrective feedback;cave automatic virtual environment","","2","","7","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Interactive, immersive visualization for indoor environments: use of augmented reality, human-computer interaction and building simulation","A. Malkawi; R. Srinivasan; B. Jackson; Yun Yi; Kin Chan; S. Angelov","Dept. of Archit., Pennsylvania Univ., Philadelphia, PA, USA; Dept. of Archit., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA; NA; NA","Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.","9 Aug 2004","2004","","","833","838","We present an interactive gesture recognition-based, immersive augmented reality system visualizing computational fluid dynamics (CFD) datasets of indoor environments. CFD simulation is used to predict the indoor environments and assess their response to specific internal and external conditions. To enable efficient visualization of CFD datasets in actual-space, an augmented reality system was integrated with a CFD simulation engine. To facilitate efficient data manipulation of the simulated post-processed CFD data and to increase the user-control of the immersive environment, a new intuitive method of human-computer interaction (HCI) has been incorporated. A gesture recognition system was integrated with the augmented reality-CFD structure to transform hand-postural data into a general description of hand-shape, through forward kinematics and computation of hand segment positions and their joint angles. This enabled real-time interactions between users and simulated CFD results in actual-space.","1093-9547","0-7695-2177-0","10.1109/IV.2004.1320237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1320237","","Indoor environments;Augmented reality;Data visualization;Computational fluid dynamics;Buildings;Computational modeling;Human computer interaction;Computer simulation;Virtual reality;Pipelines","human computer interaction;data visualisation;digital simulation;gesture recognition;augmented reality;computational fluid dynamics;real-time systems","interactive immersive visualization;augmented reality;human-computer interaction;building simulation;interactive gesture recognition;computational fluid dynamics simulation;computational fluid dynamics visualization;hand-postural data transformation;hand segment positions;real-time interactions","","4","","26","","9 Aug 2004","","","IEEE","IEEE Conferences"
"wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation","M. Kono; J. Rekimoto","The University of Tokyo; The University of Tokyo, Sony Computer Science Laboratories, Inc.","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1529","1532","There has been a long history in electrical muscle stimulation (EMS), which has been used for medical and interaction purposes. Human-computer interaction (HCI) researchers are now working on various applications, including virtual reality (VR), notification, and learning. For the electric signals applied to the human body, various types of waveforms have been considered and tested. In typical applications, pulses with short duration are applied, however, many perspectives are required to be considered. In addition to the duration and polarity of the pulse/waves, the wave shapes can also be an essential factor to consider. A problem of conventional EMS toolkits and systems are that they have a limitation to the variety of signals that it can produce. For example, some may be limited to monophonic pulses. Furthermore, they are usually limited to rectangular pulses and a limited range of frequencies, and other waveforms cannot be produced. These kinds of limitations make us challenging to consider variations of EMS signals in HCI research and applications. The purpose of “wavEMS” is to encourage testing of a variety of waveforms for EMS, which can be manipulated through audio output. We believe that this can help improve HCI applications, and to open up new application areas.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798102","Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;General and reference;Document types;General literature","Energy management;Human computer interaction;Medical services;Muscles;Safety;History","electromyography;human computer interaction;medical signal processing;virtual reality","EMS signals;wavEMS;HCI applications;electrical muscle stimulation;medical interaction purposes;virtual reality;electric signals;human body;wave shapes;monophonic pulses;rectangular pulses;human computer interaction researchers;signal variation freedom;EMS toolkits;VR;waves polarity","","","","34","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Robust optical see-through head-mounted display calibration: Taking anisotropic nature of user interaction errors into account","E. Azimi; L. Qian; P. Kazanzides; N. Navab","Johns Hopkins Univ., USA; Johns Hopkins Univ., USA; Johns Hopkins Univ., USA; Johns Hopkins Univ., USA, TU München, Germany","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","219","220","Uncertainty in measurement of point correspondences negatively affects the accuracy and precision in the calibration of head-mounted displays (HMD). In general, the distribution of alignment errors for optical see-through calibration are not isotropic, and one can estimate its distribution based on interaction requirements of a given calibration process and the user's measurable head motion and hand-eye coordination characteristics. Current calibration methods, however, mostly utilize the Direct Linear Transformation (DLT) method which minimizes Euclidean distances for HMD projection matrix estimation, disregarding the anisotropicity in the alignment errors. We utilize the error covariance in order to take the anisotropic nature of error distribution into account. The main hypothesis of this study is that using Mahalonobis distance within the nonlinear optimization can improve the accuracy of the HMD calibration. The simulation results indicate that our new method outperforms the standard DLT method both in accuracy and precision, and is more robust against user alignment errors. To the best of our knowledge, this is the first time that anisotropic noise has been accommodated in the optical see-through HMD calibration.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892255","augmented reality;HMD;calibration;error propagation;Mahalonobis distance","Calibration;Resists;Robustness;Adaptive optics;Nonlinear optics;Standards;Uncertainty","augmented reality;calibration;helmet mounted displays;matrix algebra;nonlinear programming;user interfaces","robust optical see-through head-mounted display calibration;user interaction errors;augmented reality;point correspondence measurement;alignment error distribution;user measurable head motion;hand-eye coordination characteristics;direct linear transformation method;Euclidean distances;HMD projection matrix estimation;alignment error anisotropicity;error covariance;Mahalonobis distance;nonlinear optimization;standard DLT method;anisotropic noise","","5","","4","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Depth Perception and Action in Wearable Augmented Reality: A Pilot Study","S. Baldassi",NA,"2015 IEEE International Symposium on Mixed and Augmented Reality Workshops","3 Dec 2015","2015","","","12","14","Wearable Augmented Reality (AR) is arguably closing the gaps of interaction of user with computers, and computers with the real world. This generates a strong need to understand the neural, cognitive and perceptual mechanisms leveraged by this technology. Because the most advanced wearable AR technologies support gestural interaction with real content, in our study we exploit the well known dissociation between vision-for-perception and vision-for-action to understand how the user's cognitive systems encode the AR space, to introduce novel methodologies to support UI design in AR, and to provide general guidelines for designing visual and interactive spaces in AR. In two experiments we find a dissociation between visual-only estimates of depth and motor finger-reaching to similar elements, supporting the idea that AR space leverages similar mechanisms as the real world, and can be designed accordingly.","","978-1-4673-8471-1","10.1109/ISMARW.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344748","Mixed/Augmented Reality;Psychophysics;Depth Perception;Gestural Interactions","Visualization;Sensitivity;Three-dimensional displays;Context;Yttrium;Augmented reality;Presses","augmented reality;cognitive systems;gesture recognition;wearable computers","depth perception;wearable augmented reality;AR;user interaction;neural mechanisms;cognitive mechanisms;perceptual mechanisms;gestural interaction;vision-for-perception;vision-for-action;UI design","","","","5","","3 Dec 2015","","","IEEE","IEEE Conferences"
"In your hand computing: tangible interfaces for mixed reality","J. M. S. Dias; N. Barata; P. Santos; A. Correia; P. Nande; R. Bastos","ADETTI, ISCTE, Lisbon, Portugal; ADETTI, ISCTE, Lisbon, Portugal; ADETTI, ISCTE, Lisbon, Portugal; ADETTI, ISCTE, Lisbon, Portugal; ADETTI, ISCTE, Lisbon, Portugal; ADETTI, ISCTE, Lisbon, Portugal","2003 IEEE International Augmented Reality Toolkit Workshop","9 Aug 2004","2003","","","29","31","In this paper, mixed reality (MR) is proposed as a new framework that supplements the definition of augmented reality (AR). MR includes the continuum transitions from real environments (RE), to AR, passing through augmented virtuality (AV) towards fully virtual environments (VE). In the context of MR and according to H. Kato et al. (2001), tangible interfaces are those in which: 1) each virtual object is registered to a (tangible) physical object; and 2) the user interacts with virtual objects by manipulating the corresponding tangible object. Tangible interfaces are described in the literature as being intuitive, since physical object manipulations are mapped to virtual object operations. Users generally require intuitive real-time interaction in MR. Having this in mind, we have come up with a series of novel sensor-less and cable-less tangible interfaces. In the present work, we add novel uses for these interfaces since they are the only physical means of user interaction that triggers the functionalities of the system.","0953-5683","0-7803-8240-4","10.1109/ART.2003.1320422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1320422","","Computer interfaces;Virtual reality;Augmented virtuality;Visualization;Fingers;Trajectory;Cameras;Augmented reality;Virtual environment;User interfaces","augmented reality;gesture recognition","mixed reality;augmented reality;real environments;augmented virtuality;virtual environments;tangible interfaces;user interaction;virtual objects;real-time interaction;in-your-hand computing","","","","6","","9 Aug 2004","","","IEEE","IEEE Conferences"
"Marker localization for educational game in virtual environment","H. Han-wu; D. Xian-yin; W. Yue-ming","Faculty of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, 510006, China; Faculty of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, 510006, China; Faculty of Electromechanical Engineering, Guangdong University of Technology, Guangzhou, 510006, China","2010 International Conference on Audio, Language and Image Processing","10 Jan 2011","2010","","","881","885","A computer vision based marker localization and identification approach for educational game interaction was studied. The main idea of this approach is that the position and poses information of the marker in the handheld device is tracked by the static camera. Then the operation such as pick up or put down a virtual objects is based on the position and poses information of the marker. In this way, player can carry or place virtual objects with a simple handheld device which provides more intuitive human-computer interaction than using mouse and keyboard. Furthermore, a general architecture of the educational game which uses marker localization approach as the main interaction way is presented. Last, a feeding animal game for children is developed to verify the presented approach. Children can carry different food to different animal in the game so that they can learn what food the animals like. The running result illustrated that the presented approach in this paper is effective, and it can provide a natural interaction for game in virtual environment.","","978-1-4244-5858-5","10.1109/ICALIP.2010.5685198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685198","","Games;Cameras;Handheld computers;Human computer interaction;Mice","computer aided instruction;computer games;computer vision;human computer interaction;interactive devices;virtual reality","virtual environment;computer vision based marker localization;educational game interaction;static camera;handheld device;human computer interaction","","","","7","","10 Jan 2011","","","IEEE","IEEE Conferences"
"Identifying Accessibility Conditions for Children with Multiple Disabilities: A Virtual Reality Wheelchair Simulator","N. Rodriguez",L1RMM - University of Montpellier - CNRS,"2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","29 Apr 2019","2018","","","370","372","Training is one of the main domain applications of Virtual Reality (VR). Simulation and visual realism provide training situations very close to practice with real systems while reducing cost and with greater safety. Furthermore, VR offers the possibility of change time or space scales, visualize from different perspectives, experience inaccessible real environments, all under the user's control, without risks, at her own pace. This allows to develop skills and to have confidence to work thereafter in real conditions with real equipment. Interaction technologies are now more widely available and affordable. But generally devices are conceived for “standard” people leaving behind people with impairments and further accentuating the digital gap. In this paper, we present our work in the development of an accessible wheelchair simulator designed to allow children with multiple disabilities to familiarize themselves with the wheelchair, and practitioners to better understand children capabilities.","","978-1-5386-7592-2","10.1109/ISMAR-Adjunct.2018.00107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8699276","virtual reality;simulator;disability;multiple disabilities;wheelchair;learning;augmented and alternative communication;interaction devices;I.3.1 [Computer Graphics]: Hardware Architecture — Input devices;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism – Virtual reality;H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces And Presentation]: User Interfaces — Input devices and strategies","Wheelchairs;Virtual reality;Tools;Solid modeling;Visualization;Games;Adaptation models","computer based training;computer simulation;handicapped aids;virtual reality;wheelchairs","accessibility conditions;VR;visual realism;interaction technologies;accessible wheelchair simulator;virtual reality wheelchair simulator;children with multiple disabilities","","","","12","","29 Apr 2019","","","IEEE","IEEE Conferences"
"Travel Your Desk? An Office Desk Substitution and its Effects on Cybersickness, Presence and Performance in an HMD-based Exploratory Analysis Task","D. Zielasko; B. Weyers; T. W. Kuhlen","JARA-HPC, Visual Computing Institute, RWTH Aachen University, Aachen, Germany; University of Trier, Germany; JARA-HPC, Visual Computing Institute, RWTH Aachen University, Aachen, Germany","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1285","1286","In this work, we evaluate the feasibility of an office desk substitution in the context of a visual data analysis task involving travel. We measure the impact on cybersickness as well as the general task performance and presence. In the conducted user study (n=52), surprisingly, and partially in contradiction to existing work, we found no significant differences for those core measures between the control condition without a virtual table and the condition containing a virtual table.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798068","Human-centered concepts [Human computer interaction (HCI)]: Interaction paradigms;Virtual reality;Human-centered concepts [Human computer interaction (HCI)]: Visualization;Empirical studies in visualization","Task analysis;Data analysis;Virtual environments;Visualization;Haptic interfaces;Three-dimensional displays","data analysis;data visualisation;helmet mounted displays;human factors;office automation;virtual reality","office desk substitution;visual data analysis task;cybersickness;HMD-based exploratory analysis task;virtual table","","","","8","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Effects of Image Size and Structural Complexity on Time and Precision of Hand Movements in Head Mounted Virtual Reality","A. U. Batmaz; M. de Mathelin; B. Dresp-Langley","University of Strasbourg CNRS, ICube Laboratory, UMR 7357 Strasbourg, France; University of Strasbourg CNRS, ICube Laboratory, UMR 7357 Strasbourg, France; University of Strasbourg CNRS, ICube Laboratory, UMR 7357 Strasbourg, France","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","167","174","The effective design of virtual reality (VR) simulators requires a deeper understanding of VR mediated human actions such as hand movements, with specifically tailored experiments testing how different design parameters affect performance. The present experiment investigates the time and precision of hand (index finger) movements under varying conditions of structural complexity and image size in VR without tactile feed-back from object to hand/finger. 18 right-handed subjects followed a complex and a simple physiological structure of small, medium and large size in VR, with the index finger of one of their two hands, from right to left, and from left to right. The results show that subjects performed best with small-size-simple structures and large-size-complex structures in VR. Movement execution was generally faster and more precise on simple structures. Performance was less precise when the dominant hand was used to follow the complex structures and small object size in VR. It is concluded that both size and structural complexity critically influence task execution in VR when no tactile feed-back from object to finger is generated. Individual learning curves should be monitored from the beginning of the training as suggested by the individual speed-precision analyses.","","978-1-5386-3365-6","10.1109/VR.2018.8446217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446217","Computing methodologies-Computer Graphics-Graphic systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices;Human-centered computing-Interaction design;Software and its engineering-Software organization and properties-Virtual worlds software-Virtual worlds training simulations","Indexes;Complexity theory;Three-dimensional displays;Head;Software;Image color analysis;Virtual reality","biomechanics;feedback;helmet mounted displays;image processing;physiological models;virtual reality","structural complexity;image size;hand movements;virtual reality simulators;VR mediated human actions;index finger;tactile feedback;physiological structure;complex structures;speed-precision analyses;head mounted virtual reality;learning curves","","2","","41","","30 Aug 2018","","","IEEE","IEEE Conferences"
"A Non-Stationary Office Desk Substitution for Desk-Based and HMD-Projected Virtual Reality","D. Zielasko; B. Weyers; T. W. Kuhlen","Visual Computing Institute, RWTH Aachen University, JARA-HPC, Aachen, Germany; University of Trier, Germany; Visual Computing Institute, RWTH Aachen University, JARA-HPC, Aachen, Germany","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1884","1889","The ongoing migration of HMDs to the consumer market also allows the integration of immersive environments into analysis workflows that are often bound to an (office) desk. However, a critical factor when considering VR solutions for professional applications is the prevention of cybersickness. In the given scenario the user is usually seated and the surrounding real world environment is very dominant, where the most dominant part is maybe the desk itself. Including this desk in the virtual environment could serve as a resting frame and thus reduce cybersickness next to a lot of further possibilities. In this work, we evaluate the feasibility of a substitution like this in the context of a visual data analysis task involving travel, and measure the impact on cybersickness as well as the general task performance and presence. In the conducted user study ( n=52), surprisingly, and partially in contradiction to existing work, we found no significant differences for those core measures between the control condition without a virtual table and the condition containing a virtual table. However, the results also support the inclusion of a virtual table in desk-based use cases.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8797837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8797837","Human-centered concepts [Human computer interaction (HCI)]: Interaction paradigms—Virtual reality;Human-centered concepts [Human computer interaction (HCI)]: Visualization—Empirical studies in visualization","Task analysis;Data analysis;Virtual environments;Data visualization;Visualization;Three-dimensional displays","data analysis;helmet mounted displays;user interfaces;virtual reality","nonstationary office desk substitution;HMD-projected virtual reality;consumer market;VR solutions;professional applications;cybersickness;virtual environment;visual data analysis task;virtual table;desk-based use cases","","1","","32","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Multiuser Interaction with Hybrid VR and AR for Cultural Heritage Objects","Y. Li; E. Ch’ng; S. Cai; S. See","International Doctoral Innovation Centre & NVIDIA Joint-Lab on Mixed Reality, University of Nottingham, Ningbo, China; NVIDIA Joint-Lab on Mixed Reality, University of Nottingham, Ningbo, China; Faculty of Humanities and Social Sciences & & NVIDIA Joint-Lab on Mixed Reality, University of Nottingham, Ningbo, China; NVIDIA AI Technology Centre, NVIDIA, Singapore","2018 3rd Digital Heritage International Congress (DigitalHERITAGE) held jointly with 2018 24th International Conference on Virtual Systems & Multimedia (VSMM 2018)","26 Aug 2019","2018","","","1","8","This research investigates the factors and ways in which users initiate conversations and engage in interactions in a hybrid virtual environment using a combination of Virtual Reality (VR) and Augmented Reality (AR) devices. The research was done in the `spirit of the ancient Silk Road' where trade brought in exchange of ideas, cultural influence and cross-border communications. The notion of a 21st century Silk Road is necessarily digital, over the Internet and based around 3D cultural heritage objects. Digi-Capital's Report forecasts the revenue of AR and VR to be US$150b by 2020. We projected that VR and AR will become pervasive, much like the Social Web and the universal ubiquity of mobile devices such as smartphones and wearables. Here, we conducted a user study exploring users' acceptance of the use of hybrid VR and AR for cultural heritage, and investigated the social nature of multiple co-located user interaction. We adapted the UTAUT questionnaire in our experiment and found that social influence has positive effects on performance expectancy and effort expectancy, which generate positive effects on user behavioural intention. This study pioneers the future design and use of hybrid VR and AR technology in cultural heritage specifically, and in other application areas generally by highlighting the significant role that social influence plays in enhancing users' behavioural intention facilitated by different immersive devices.","","978-1-7281-0292-4","10.1109/DigitalHeritage.2018.8810126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8810126","Virtual reality;augmented reality;hybrid VR AR;technology acceptance;interaction design;social interaction;cultural heritage;heritage objects","Cultural differences;Virtual environments;Roads;Global communication;Augmented reality;History","augmented reality;cultural aspects;history;human computer interaction;Internet;mobile computing;user interfaces","hybrid virtual environment;cross-border communications;mobile devices;user behavioural intention;multiuser interaction;social Web;immersive devices;hybrid VR;hybrid AR;virtual reality;augmented reality devices;3D cultural heritage objects;digi-capital's report;smartphones;multiple co-located user interaction;UTAUT questionnaire","","","","25","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Latency Measurement in Head-Mounted Virtual Environments","J. A. Jones; E. Luckett; T. Key; N. Newsome",University of Mississippi; University of Mississippi; Rust College; Clemson University,"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1000","1001","In this paper, we discuss a generalizable method to measure end-to-end latency. This is the length of time that elapses between when a real-world movement occurs and when the pixels within a head-mounted display are updated to reflect this movement. The method described here utilizes components commonly available at electronics and hobby shops. We demonstrate this measurement method using an HTC Vive and discuss the influence of its low-persistence display on latency measurement.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798361","Human-centered computing;Interaction paradigms;Virtual reality;Visualization;Visualization design and evaluation methods","Photodiodes;Resists;Oscilloscopes;Liquid crystal displays;Virtual environments;Cameras","helmet mounted displays;virtual reality","real-world movement;head-mounted display;latency measurement;head-mounted virtual environments;generalizable method;HTC Vive","","2","","3","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Three Haptic Shape-Feedback Controllers for Virtual Reality","M. Sinclair; E. Ofek; C. Holz; I. Choi; E. Whitmire; E. Strasnick; H. Benko","Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA","2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","30 Aug 2018","2018","","","777","778","We present three new novel haptic controllers that render shape force feedback during interaction. 1) CLAW is a multi-purpose controller that renders tactile forces for common hand interactions, such as grasping, touching, and triggering grasped objects. 2) Haptic Revolver is a general-purpose handheld VR controller that renders touch contact with virtual surfaces, motion shear along a surface, textures, and shapes using interchangeable wheels. 3) Haptic Links haptic render shape feedback between two controllers using variable-stiffness locking mechanisms to provide force feedback for grasping and interacting with two-handed objects such as wind instruments, steering wheels, handle bars, or bow and arrow.","","978-1-5386-3365-6","10.1109/VR.2018.8446399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446399","Haptics;Virtual Reality;Controller;#K.6.1 [Management of Computing and Information Systems];Project and People Management;Life Cycle;K.7.m [The Computing Profession];Miscellaneous-Ethics","Conferences;Virtual reality;Three-dimensional displays;User interfaces","force feedback;haptic interfaces;rendering (computer graphics);virtual reality","virtual reality;multipurpose controller;common hand interactions;grasped objects;general-purpose handheld VR controller;virtual surfaces;shape feedback;two-handed objects;tactile force;haptic shape-feedback controllers;shape force feedback;Haptic Revolver;CLAW controller;rendering","","1","","9","","30 Aug 2018","","","IEEE","IEEE Conferences"
"The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?","B. Bach; R. Sicat; J. Beyer; M. Cordeil; H. Pfister",Harvard University; Harvard University; Harvard University; Monash University; Harvard University,"IEEE Transactions on Visualization and Computer Graphics","4 Dec 2017","2018","24","1","457","467","We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user's real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.","1941-0506","","10.1109/TVCG.2017.2745941","NIH; King Abdullah University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019876","Augmented Reality;3D Interaction;User Study;Immersive Displays","Three-dimensional displays;Data visualization;Two dimensional displays;Augmented reality;Stereo image processing;Visualization;Mice","augmented reality;data visualisation;helmet mounted displays","spatial positioning;tangible markers;visualization environments;human perceptual;interaction modalities;spatial proximity;cognitive proximity;spatial perception;desktop environment;interactive exploration;immersive tangible augmented reality;natural human perception;Microsoft HoloLens;handheld tablet;HoloLens display;virtual content;spatial position;3D exploration;augmented-reality head-mounted display;3D hologram;3D content;3D visualization;2D surface","Computer Graphics;Female;Holography;Humans;Imaging, Three-Dimensional;Male;Perception;Task Performance and Analysis;User-Computer Interface;Virtual Reality","32","","67","Traditional","29 Aug 2017","","","IEEE","IEEE Journals"
