Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
"Fristedt S., Smith F., Grynne A., Browall M.","55084313400;55346672700;57221701745;13102729200;","Digi-Do: a digital information tool to support patients with breast cancer before, during, and after start of radiotherapy treatment: an RCT study protocol",2021,"BMC Medical Informatics and Decision Making","21","1", 76,"","",,,"10.1186/s12911-021-01448-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101771341&doi=10.1186%2fs12911-021-01448-3&partnerID=40&md5=20d63504c6ce82e8a6c6276d22fd80f8","Jönköping Academy For Improvement of Health and Welfare and IMPROVE, School of Health and Welfare, Jönköping University, Jönköping, Sweden; Department of Health Sciences, Faculty of Medicine, Lund University, Lund, Sweden; Regional Cancer Centre West, Gothenburg, Sweden; Department of Technology Management and Economics, Chalmers University of Technology, Gothenburg, Sweden; Department of Nursing and IMPROVE, School of Health and Welfare, Jönköping University, Jönköping, Sweden; Affiliated with the Department of Oncology, Institute of Clinical Sciences, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden","Fristedt, S., Jönköping Academy For Improvement of Health and Welfare and IMPROVE, School of Health and Welfare, Jönköping University, Jönköping, Sweden, Department of Health Sciences, Faculty of Medicine, Lund University, Lund, Sweden; Smith, F., Regional Cancer Centre West, Gothenburg, Sweden, Department of Technology Management and Economics, Chalmers University of Technology, Gothenburg, Sweden; Grynne, A., Department of Nursing and IMPROVE, School of Health and Welfare, Jönköping University, Jönköping, Sweden; Browall, M., Department of Nursing and IMPROVE, School of Health and Welfare, Jönköping University, Jönköping, Sweden, Affiliated with the Department of Oncology, Institute of Clinical Sciences, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden","Background: Radiation Therapy (RT) is a common treatment after breast cancer surgery and a complex process using high energy X-rays to eradicate cancer cells, important in reducing the risk of local recurrence. The high-tech environment and unfamiliar nature of RT can affect the patient’s experience of the treatment. Misconceptions or lack of knowledge about RT processes can increase levels of anxiety and enhance feelings of being unprepared at the beginning of treatment. Moreover, the waiting time is often quite long. The primary aim of this study will be to evaluate whether a digital information tool with VR-technology and preparatory information can decrease distress as well as enhance the self-efficacy and health literacy of patients affected by breast cancer before, during, and after RT. A secondary aim will be to explore whether the digital information tool increase patient flow while maintaining or increasing the quality of care. Method: The study is a prospective and longitudinal RCT study with an Action Research participatory design approach including mixed-methods data collection, i.e., standardised instruments, qualitative interviews (face-to-face and telephone) with a phenomenological hermeneutical approach, diaries, observations, and time measurements, and scheduled to take place from autumn 2020 to spring 2022. The intervention group (n = 80), will receive standard care and information (oral and written) and the digital information tool; and the control group (n = 80), will receive standard care and information (oral and written). Study recruitment and randomisation will be completed at two centres in the west of Sweden. Discussion: Research in this area is scarce and, to our knowledge, only few previous studies examine VR as a tool for increasing preparedness for patients with breast cancer about to undergo RT that also includes follow-ups six months after completed treatment. The participatory approach and design will safeguard the possibilities to capture the patient perspective throughout the development process, and the RCT design supports high research quality. Digitalisation brings new possibilities to provide safe, person-centred information that also displays a realistic picture of RT treatment and its contexts. The planned study will generate generalisable knowledge of relevance in similar health care contexts. Trial registration: ClinicalTrials.gov Identifier: NCT04394325. Registered May 19, 2020. Prospectively registered. © 2021, The Author(s).","Evaluation; Health literacy; Participatory design; Self-efficacy; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85101771341
"Monteiro P., Melo M., Valente A., Vasconcelos-Raposo J., Bessa M.","57201131572;7102354924;7102410111;36070012200;14031038800;","Delivering Critical Stimuli for Decision Making in VR Training: Evaluation Study of a Firefighter Training Scenario",2021,"IEEE Transactions on Human-Machine Systems","51","2", 9247430,"65","74",,,"10.1109/THMS.2020.3030746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103091552&doi=10.1109%2fTHMS.2020.3030746&partnerID=40&md5=2ab0568260ec97afacd284c5f8dae21d","Inesc Tec, Porto, 4200-465, Portugal; Universidade de Trás-os-Montes e Alto Douro, Vila Real, 5000-801, Portugal","Monteiro, P., Inesc Tec, Porto, 4200-465, Portugal; Melo, M., Inesc Tec, Porto, 4200-465, Portugal; Valente, A., Inesc Tec, Porto, 4200-465, Portugal, Universidade de Trás-os-Montes e Alto Douro, Vila Real, 5000-801, Portugal; Vasconcelos-Raposo, J., Inesc Tec, Porto, 4200-465, Portugal, Universidade de Trás-os-Montes e Alto Douro, Vila Real, 5000-801, Portugal; Bessa, M., Inesc Tec, Porto, 4200-465, Portugal, Universidade de Trás-os-Montes e Alto Douro, Vila Real, 5000-801, Portugal","The goal for a virtual reality (VR) training system is to enable trainees to acquire all the knowledge they need to perform effectively in a real environment. Such a system should provide an experience so authentic that no further real-world training is necessary, meaning that it is sufficient to train in VR. We evaluate the impact of a haptic thermal stimulus, which is of paramount importance to decision making, on trainees performance and knowledge acquisition. A thermal device was created to deliver the stimulus. As a proof of concept, a procedure from firefighter training is selected, in which sensing the temperature of a door with one's hand is essential. The sample consisted of 48 subjects divided among three experimental scenarios: one in which a virtual thermometer is used (visual stimulus), another in which the temperature is felt with the hand (thermal stimulus) and a third in which both methods are used (visual + thermal stimuli). For the performance evaluation, we measured the total time taken, the numbers of correctly executed procedures and identified neutral planes, the deviation from the target height, and the responses to a knowledge transfer questionnaire. Presence, cybersickness, and usability are measured to evaluate the impact of the haptic thermal stimulus. Considering the thermal stimulus condition as the baseline, we conclude that the significantly different results in the performance among the conditions indicate that the better performance in the visual-only condition is not representative of the real-life performance. Consequently, VR training applications need to deliver the correct stimuli for decision making. © 2013 IEEE.","Firefighters; haptic cues; multisensory displays; thermal; training; virtual reality (VR)","Fire extinguishers; Knowledge acquisition; Knowledge management; Virtual reality; Evaluation study; Knowledge transfer; Proof of concept; Real environments; Real-life performance; Training applications; Training scenario; Training Systems; Decision making",Article,"Final","",Scopus,2-s2.0-85103091552
"Baceviciute S., Terkildsen T., Makransky G.","55441702500;57205338475;50361371800;","Remediating learning from non-immersive to immersive media: Using EEG to investigate the effects of environmental embeddedness on reading in Virtual Reality",2021,"Computers and Education","164",, 104122,"","",,,"10.1016/j.compedu.2020.104122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100105880&doi=10.1016%2fj.compedu.2020.104122&partnerID=40&md5=0e064f3f47964fe4caef507e98daa3ff","University of Copenhagen, Department of Psychology, Denmark","Baceviciute, S., University of Copenhagen, Department of Psychology, Denmark; Terkildsen, T., University of Copenhagen, Department of Psychology, Denmark; Makransky, G., University of Copenhagen, Department of Psychology, Denmark","Virtual Reality (VR) has the potential to enrich education but little is known about how unique affordances of immersive technology might influence leaning and cognition. This study investigates one particular affordance of VR, namely environmental embeddedness, which enables learners to be situated in simulated or imagined settings that contextualize their learning. A sample of 51 university students were administered written learning material in a between-subjects design study, wherein one group read text about sarcoma cancer on a physical pamphlet in the real world, and the other group read identical text on a virtual pamphlet embedded in an immersive VR environment which resembled a hospital room. The study combined advanced EEG measurement techniques, learning tests, and cognitive load measures to compare conditions. Results show that the VR group performed significantly better on a knowledge transfer post-test. However, reading in VR was found to be more cognitively effortful and less time-efficient. Findings suggest the significance of environmental embeddedness for learning, and provide important considerations for the design of educational VR environments, as we remediate learning content from non-immersive to immersive media. © 2021 Elsevier Ltd","EEG; Embeddedness; Learning; Remediation; Virtual reality environments","E-learning; Knowledge management; Remediation; Cognitive loads; Immersive media; Immersive technologies; Knowledge transfer; Learning contents; Learning materials; Measurement techniques; University students; Virtual reality",Article,"Final","",Scopus,2-s2.0-85100105880
"Araiza-Alba P., Keane T., Chen W.S., Kaufman J.","57191667278;55582336900;57188973057;7403022831;","Immersive virtual reality as a tool to learn problem-solving skills",2021,"Computers and Education","164",, 104121,"","",,,"10.1016/j.compedu.2020.104121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099353627&doi=10.1016%2fj.compedu.2020.104121&partnerID=40&md5=82079d081459ac2caf3cea01c4ad7087","Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia","Araiza-Alba, P., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Keane, T., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Chen, W.S., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia; Kaufman, J., Swinburne University of Technology, PO Box 218, Hawthorn, Victoria, Australia","Immersive virtual reality (IVR) technology has demonstrated positive educational outcomes related to its use and is gaining traction in educational and training settings; IVR is expected to have widespread adoption within the classroom in the upcoming years. However, the educational potential of IVR with children has not been thoroughly investigated, especially as a tool for problem-solving skills. Therefore, this study was designed to answer the following questions: (1) Is IVR a useful tool to learn and practice problem-solving skills? More specifically, do children using IVR solve a game better than those using a tablet application or a board game? (2) Does IVR provide a more engaging experience for children to practice problem-solving skills than on a tablet or a board game? (3) Do problem-solving skills learned with IVR technology transfer to real-life (physical game)? Children (n = 120) aged 7–9.9 years were randomly assigned to a problem-solving game in one of three conditions: board game, tablet, or IVR. The results showed that, overall, the percentage of children who completed the problem-solving game was higher in the IVR condition (77.5%), compared with those in the tablet (32.5%) or board game (30%) conditions. We also found that the interest and enjoyment scores of participants using IVR were significantly higher than participants in the other two conditions, and that the children in the IVR condition were able to learn how to solve the problem and transfer their learning to the physical game. IVR is a technology capable of engaging interest and motivating the user, as well as having the potential to assist in cognitive processing and knowledge transfer. © 2021 Elsevier Ltd","21st century abilities; Elementary education; Games; Problem-solving skills; Virtual reality","Knowledge management; Technology transfer; Transfer learning; Board games; Cognitive processing; Educational potential; Immersive virtual reality; Knowledge transfer; Physical games; Problem solving skills; Tablet applications; Virtual reality",Article,"Final","",Scopus,2-s2.0-85099353627
"Bejjani W., Leonetti M., Dogar M.R.","57193493231;36802120200;22939838000;","Learning image-based Receding Horizon Planning for manipulation in clutter",2021,"Robotics and Autonomous Systems","138",, 103730,"","",,,"10.1016/j.robot.2021.103730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100235606&doi=10.1016%2fj.robot.2021.103730&partnerID=40&md5=983057a274cb9fe1ff204644295e294e","School of Computing, University of Leeds, Leeds, LS2 9JT, United Kingdom","Bejjani, W., School of Computing, University of Leeds, Leeds, LS2 9JT, United Kingdom; Leonetti, M., School of Computing, University of Leeds, Leeds, LS2 9JT, United Kingdom; Dogar, M.R., School of Computing, University of Leeds, Leeds, LS2 9JT, United Kingdom","The manipulation of an object into a desired location in a cluttered and restricted environment requires reasoning over the long-term consequences of an action while reacting locally to the multiple physics-based interactions. We present Visual Receding Horizon Planning (VisualRHP) in a framework which interleaves real-world execution with look-ahead planning to efficiently solve a short-horizon approximation to a multi-step sequential decision making problem. VisualRHP is guided by a learned heuristic that acts on an abstract colour-labelled image-based representation of the state. With this representation, the robot can generalize its behaviours to different environment setups, that is, different number and shape of objects, while also having transferable manipulation skills that can be applied to a multitude of real-world objects. We train the heuristic with imitation and reinforcement learning in discrete and continuous actions spaces. We detail our heuristic learning process for environments with sparse rewards, and non-linear, non-continuous, dynamics. In particular, we introduce necessary changes for improving the stability of existing reinforcement learning algorithms that use neural networks with shared parameters. In a series of simulation and real-world experiments, we show the robot performing prehensile and non-prehensile actions in synergy to successfully manipulate a variety of real-world objects in real-time. © 2021 Elsevier B.V.","Abstract state representation; Heuristic learning; Imitation and reinforcement learning; Manipulation in clutter; Physics-based manipulation; Receding Horizon Planning","Agricultural robots; Decision making; Radar clutter; Reinforcement learning; Robots; Continuous actions; Heuristic learning; Image-based representation; Physics-based; Real world experiment; Real-world objects; Receding horizon; Sequential decision making; Learning algorithms",Article,"Final","",Scopus,2-s2.0-85100235606
"Shakarami A., Shahidinejad A., Ghobaei-Arani M.","57216524748;54586030700;57189577393;","An autonomous computation offloading strategy in Mobile Edge Computing: A deep learning-based hybrid approach",2021,"Journal of Network and Computer Applications","178",, 102974,"","",,1,"10.1016/j.jnca.2021.102974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099309176&doi=10.1016%2fj.jnca.2021.102974&partnerID=40&md5=1aec444973f6ca247778577029f7dd2d","Department of Computer Engineering, Qom Branch, Islamic Azad University, Qom, Iran","Shakarami, A., Department of Computer Engineering, Qom Branch, Islamic Azad University, Qom, Iran; Shahidinejad, A., Department of Computer Engineering, Qom Branch, Islamic Azad University, Qom, Iran; Ghobaei-Arani, M., Department of Computer Engineering, Qom Branch, Islamic Azad University, Qom, Iran","The fast growth of under developing internet-based technologies has been leading to propose promising methods to handle the heterogeneous massive volume of data produced by pervasive smart equipments such as handy mobile devices. Thanks to the mentioned technologies, these mobile devices can run critical business/entertainment applications such as Augmented Reality, Virtual Reality, vehicular networks, and media streaming. However, due to such devices' inherent limitations, some emerging computation environments such as Mobile Edge Computing have been introduced to achieve some essential requirements such as low latency, low energy consumption, and low cost. In the literature, offloading is a technique to transfer the burden of the mobile devices' work incurred by running applications' requests to these computation environments. On the other hand, exploring the computation environment to find the most efficient place to execute such requests is challenging work to achieve. In addition, different researches have been proposed to cope with the management problems of the offloading criterion. In this paper, an autonomous computation offloading framework is proposed to address some challenges related to time-intensive and resource-intensive applications. However, to the best of the authors’ knowledge, the proposed autonomous framework has not been explored as a control model for self-management in the computation offloading criterion. Besides, to cope with the large dimension of the offloading decision-making problem, different simulations including Deep Neural Networks, multiple linear regression, hybrid model, and Hidden Markov Model as the planning module of the mentioned autonomous methodology have been conducted. Simulation results show that the proposed hybrid model can appropriately fit the problem with near-optimal accuracy regarding the offloading decision-making, the latency, and the energy consumption predictions in the proposed self-management framework. © 2021 Elsevier Ltd","Hidden markov model; Machine learning; Mobile edge computing; Neural networks; Offloading; Regression","Augmented reality; Decision making; Deep neural networks; Edge computing; Energy utilization; Green computing; Hidden Markov models; Linear regression; Media streaming; Computation environments; Computation offloading; Decision-making problem; Emerging computation; Energy consumption prediction; Internet based technology; Low energy consumption; Multiple linear regressions; Deep learning",Article,"Final","",Scopus,2-s2.0-85099309176
"Liubogoshchev M., Korneev E., Khorov E.","57200496493;57222333639;34869950000;","Everest: Bitrate adaptation for cloud VR",2021,"Electronics (Switzerland)","10","6", 678,"1","17",,,"10.3390/electronics10060678","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102367580&doi=10.3390%2felectronics10060678&partnerID=40&md5=b8109a2394052392181695bfc8233c23","Kharkevich Institute for Information Transmission Problems of the Russian Academy of Sciences, Moscow, 127051, Russian Federation; Moscow Institute of Physics and Technology, Moscow, 141701, Russian Federation","Liubogoshchev, M., Kharkevich Institute for Information Transmission Problems of the Russian Academy of Sciences, Moscow, 127051, Russian Federation, Moscow Institute of Physics and Technology, Moscow, 141701, Russian Federation; Korneev, E., Kharkevich Institute for Information Transmission Problems of the Russian Academy of Sciences, Moscow, 127051, Russian Federation, Moscow Institute of Physics and Technology, Moscow, 141701, Russian Federation; Khorov, E., Kharkevich Institute for Information Transmission Problems of the Russian Academy of Sciences, Moscow, 127051, Russian Federation, Moscow Institute of Physics and Technology, Moscow, 141701, Russian Federation","Cloud Virtual Reality (VR) technology is expected to promote VR by providing a higher Quality of Experience (QoE) and energy efficiency at lower prices for the consumer. In cloud VR, the virtual environment is rendered on the remote server and transmitted to the headset as a video stream. To guarantee real-time experience, networks need to transfer huge amounts of data with much stricter delays than imposed by the state-of-the-art live video streaming applications. To reduce the burden imposed on the networks, cloud VR applications shall adequately react to the changing network conditions, including the wireless channel fluctuations and highly variable user activity. For that, they need to adjust the quality of the video stream adaptively. This paper studies video quality adaptation for cloud VR and improves the QoE for cloud VR users. It develops a distributed, i.e., with no assistance from the network, bitrate adaptation algorithm for cloud VR, called the Enhanced VR bitrate Estimator (EVeREst). The algorithm aims to optimize the average bitrate of cloud VR video flows subject to video frame delay and loss constraints. For that, the algorithm estimates both the current network load and the delay experienced by separate frames. It anticipates the changes in the users’ activity and limits the bitrate accordingly, which helps prevent excess interruptions of the playback. With simulations, the paper shows that the developed algorithm significantly improves the QoE for the end-users compared to the state-of-the-art adaptation algorithms developed for MPEG DASH live streaming, e.g., BOLA. Unlike these algorithms, the developed algorithm satisfies the frame loss requirements of multiple VR sessions and increases the network goodput by up to 10 times. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Bitrate adaptation; Cloud VR; Quality of experience; Real-time adaptive video; Video traffic; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85102367580
"Papke D.J., Jr., Lohmann S., Downing M., Hufnagl P., Mutter G.L.","57194010887;56273785700;57192249730;7007182921;7005780100;","Computational augmentation of neoplastic endometrial glands in digital pathology displays",2021,"Journal of Pathology","253","3",,"258","267",,,"10.1002/path.5586","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097382540&doi=10.1002%2fpath.5586&partnerID=40&md5=c86bc3dad5ffc499d7bf69b16c25b507","Department of Pathology, Brigham and Women's Hospital, Boston, MA, United States; Harvard Medical School, Boston, MA, United States; Institut für Pathologie, Charité – Universitätsmedizin, Berlin, Germany","Papke, D.J., Jr., Department of Pathology, Brigham and Women's Hospital, Boston, MA, United States, Harvard Medical School, Boston, MA, United States; Lohmann, S., Institut für Pathologie, Charité – Universitätsmedizin, Berlin, Germany; Downing, M., Department of Pathology, Brigham and Women's Hospital, Boston, MA, United States; Hufnagl, P., Institut für Pathologie, Charité – Universitätsmedizin, Berlin, Germany; Mutter, G.L., Department of Pathology, Brigham and Women's Hospital, Boston, MA, United States, Harvard Medical School, Boston, MA, United States","The pathologic diagnosis of neoplasia requires localization and classification of lesional tissue, a process that depends on the recognition of an abnormal spatial distribution of neoplastic elements relative to admixed normal background tissue. In endometrial intraepithelial neoplasia (EIN), a pre-cancer usually managed by hysterectomy, a clonally mutated proliferation of cytologically altered glands (‘neoplastic-EIN’) aggregates in clusters that also contain background non-neoplastic glands (‘background-NL’). Here, we used image analysis to classify individual glands within endometrial tissue fragments as neoplastic-EIN or background-NL, and we used the distribution of predictions to localize foci diagnostic of EIN. Nuclear coordinates were automatically assigned and were used as vertices to generate Delaunay triangulations for each gland. Graph statistical variables were used to develop random forest algorithms to classify glands as neoplastic-EIN or background-NL. Individual glands in an independent validation set were scored by a ‘ground truth’ biomarker (PAX2 immunohistochemistry). We found that exclusion of small glands led to improvement in classification accuracy. Using an inclusion threshold of 200 nuclei per gland, our final model classification accuracy was 77.5% in the validation set, with a positive predictive value of 0.81. We leveraged this high positive predictive value in a point cloud overlay display to assist end-user identification of EIN foci. This study demonstrates that graph theory approaches applied to small-scale anatomic elements in the endometrium allow biologic classification by machine learning, and that spatial superimposition over large-scale tissue expanses can have practical diagnostic utility. We expect this augmented diagnostic approach to be generalizable to commonly encountered problems in other organ systems. © 2020 The Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd. © 2020 The Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","augmented diagnosis; augmented reality; digital pathology; endometrial intraepithelial neoplasia; endometrium; graph theory; image analysis; pathology; pre-cancer","cytokeratin AE1; cytokeratin AE3; transcription factor PAX2; Article; augmented reality; clinical classification; computational augmentation; controlled study; diagnostic accuracy; diagnostic test accuracy study; endometrial intraepithelial neoplasia; endometrium; endometrium biopsy; endometrium tumor; female; histopathology; human; human tissue; image analysis; image segmentation; immunohistochemistry; machine learning; mathematical computing; precancer; predictive value; priority journal; random forest; tissue section; triangulation",Article,"Final","",Scopus,2-s2.0-85097382540
"Parong J., Mayer R.E.","56520185200;7403065717;","Cognitive and affective processes for learning science in immersive virtual reality",2021,"Journal of Computer Assisted Learning","37","1",,"226","241",,3,"10.1111/jcal.12482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089751070&doi=10.1111%2fjcal.12482&partnerID=40&md5=8ad3aa7f4645bc62203c9c281872a614","Department of Psychological and Brain Sciences, University of California, Santa Barbara, Santa Barbara, CA, United States; Department of Psychology, University of Wisconsin–Madison, Madison, WI, United States","Parong, J., Department of Psychological and Brain Sciences, University of California, Santa Barbara, Santa Barbara, CA, United States, Department of Psychology, University of Wisconsin–Madison, Madison, WI, United States; Mayer, R.E., Department of Psychological and Brain Sciences, University of California, Santa Barbara, Santa Barbara, CA, United States","As immersive virtual reality (IVR) systems proliferate in classrooms, it is important to understand how they affect learning outcomes and the underlying affective and cognitive processes that may cause these outcomes. Proponents argue that IVR could improve learning by increasing positive affective and cognitive processing, thereby supporting improved performance on tests of learning outcome, whereas opponents of IVR contend that it could hurt learning by increasing distraction, thereby disrupting cognitive learning processes and leading to poorer learning outcomes. In a media comparison study, students viewed a biology lesson either as an interactive animated journey in IVR or as a slideshow on a desktop monitor. Those who viewed the IVR lesson performed significantly worse on transfer tests, reported higher emotional arousal, reported more extraneous cognitive load and showed less engagement based on EEG measures than those who viewed the slideshow lesson, with or without practice questions added to the lessons. Mediational analyses showed that the lower retention scores for the IVR lesson were related to an increase in self-reported extraneous cognitive load and emotional arousal. These results support the notion that immersive environments create high affective and cognitive distraction, which leads to poorer learning outcomes than desktop environments. © 2020 John Wiley & Sons Ltd","affective processing; cognitive processing; multimedia; science learning; virtual reality",,Article,"Final","",Scopus,2-s2.0-85089751070
"Harris D.J., Hardcastle K.J., Wilson M.R., Vine S.J.","57192429891;57221862684;55574207642;36811509000;","Assessing the learning and transfer of gaze behaviours in immersive virtual reality",2021,"Virtual Reality",,,,"","",,,"10.1007/s10055-021-00501-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100476971&doi=10.1007%2fs10055-021-00501-w&partnerID=40&md5=ad4779acdc2d1563f331327866d64c86","School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Counter Terrorism Protective Security Operations, Metropolitan Police Service, Lambeth HQ, London, SE1 7LP, United Kingdom","Harris, D.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Hardcastle, K.J., Counter Terrorism Protective Security Operations, Metropolitan Police Service, Lambeth HQ, London, SE1 7LP, United Kingdom; Wilson, M.R., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Vine, S.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom","Virtual reality (VR) has clear potential for improving simulation training in many industries. Yet, methods for testing the fidelity, validity and training efficacy of VR environments are, in general, lagging behind their adoption. There is limited understanding of how readily skills learned in VR will transfer, and what features of training design will facilitate effective transfer. Two potentially important elements are the psychological fidelity of the environment, and the stimulus correspondence with the transfer context. In this study, we examined the effectiveness of VR for training police room searching procedures, and assessed the corresponding development of perceptual-cognitive skill through eye-tracking indices of search efficiency. Participants (n = 54) were assigned to a VR rule-learning and search training task (FTG), a search only training task (SG) or a no-practice control group (CG). Both FTG and SG developed more efficient search behaviours during the training task, as indexed by increases in saccade size and reductions in search rate. The FTG performed marginally better than the CG on a novel VR transfer test, but no better than the SG. More efficient gaze behaviours learned during training were not, however, evident during the transfer test. These findings demonstrate how VR can be used to develop perceptual-cognitive skills, but also highlight the challenges of achieving transfer of training. © 2021, The Author(s).","Fidelity; Police; Policing; Training; Validity; VR","E-learning; Eye tracking; Cognitive skill; Gaze behaviours; Immersive virtual reality; Search behaviours; Search efficiency; Simulation training; Training design; Transfer of trainings; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85100476971
"Harvey C., Selmanović E., O’Connor J., Chahin M.","48761392600;35218206600;57197835447;57204813574;","A comparison between expert and beginner learning for motor skill development in a virtual reality serious game",2021,"Visual Computer","37","1",,"3","17",,1,"10.1007/s00371-019-01702-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067692541&doi=10.1007%2fs00371-019-01702-w&partnerID=40&md5=bc2d985815aa255af972888f2dbf8fff","Birmingham City University, West Midlands, B4 7XG, United Kingdom; University of Sarajevo, Zmaja od Bosne 35, B&H, Sarajevo, Bosnia and Herzegovina","Harvey, C., Birmingham City University, West Midlands, B4 7XG, United Kingdom; Selmanović, E., University of Sarajevo, Zmaja od Bosne 35, B&H, Sarajevo, Bosnia and Herzegovina; O’Connor, J., Birmingham City University, West Midlands, B4 7XG, United Kingdom; Chahin, M., University of Sarajevo, Zmaja od Bosne 35, B&H, Sarajevo, Bosnia and Herzegovina","In order to be used for skill development and skill maintenance, virtual environments require accurate simulation of the physical phenomena involved in the process of the task being trained. The accuracy needs to be conveyed in a multimodal fashion with varying parameterisations still being quantified, and these are a function of task, prior knowledge, sensory efficacy and human perception. Virtual reality (VR) has been integrated from a didactic perspective in many serious games and shown to be effective in the pedological process. This paper interrogates whether didactic processes introduced into a VR serious game, by taking advantage of augmented virtuality to modify game attributes, can be effective for both beginners and experts to a task. The task in question is subjective performance in a clay pigeon shooting simulation. The investigation covers whether modified game attributes influence skill and learning in a complex motor task and also investigates whether this process is applicable to experts as well as beginners to the task. VR offers designers and developers of serious games the ability to provide information in the virtual world in a fashion that is impossible in the real world. This introduces the question of whether this is effective and transfers skill adoption into the real world and also if a-priori knowledge influences the practical nature of this information in the pedagogic process. Analysis is conducted via a between-subjects repeated measure ANOVA using a 2 × 2 factorial design to address these questions. The results show that the different training provided affects the performance in this task (N= 57). The skill improvement is still evidenced in repeated measures when information and guidance is removed. This effect does not exist under a control condition. Additionally, we separate by an expert and non-expert group to deduce if a-priori knowledge influences the effect of the presented information, it is shown that it does not. © 2019, The Author(s).","Learning; Serious game; Training; Virtual reality","E-learning; Personnel training; Sensory perception; Virtual reality; Augmented virtualities; Human perception; Learning; Physical phenomena; Priori knowledge; Repeated measures; Skill development; Subjective performance; Serious games",Article,"Final","",Scopus,2-s2.0-85067692541
"Militello M., Tredway L., Hodgkins L., Simon K.","25223738500;18438543700;57191982516;57222469178;","Virtual reality classroom simulations: how school leaders improve instructional leadership capacity",2021,"Journal of Educational Administration",,,,"","",,,"10.1108/JEA-10-2020-0219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102766946&doi=10.1108%2fJEA-10-2020-0219&partnerID=40&md5=e8138c82fa723f09ba433c11c0c4e374","Educational Leadership, East Carolina University, Greenville, NC, United States; Institute for Educational Leadership, Washington, District of Columbia, United States","Militello, M., Educational Leadership, East Carolina University, Greenville, NC, United States; Tredway, L., Institute for Educational Leadership, Washington, District of Columbia, United States; Hodgkins, L., Educational Leadership, East Carolina University, Greenville, NC, United States; Simon, K., Institute for Educational Leadership, Washington, District of Columbia, United States","Purpose: The purpose of this study was to explore the utility of a virtual reality (VR) classroom experience for improving the capacity of instructional leaders. Specifically, school leaders used VR to build their classroom observation and analysis skills to prepare to have more effective post-observation conversations with teachers. The authors provide insights from multiple data points that highlight the affordances of the virtual setting for improving classroom observation skills. Design/methodology/approach: Drawing on the application of simulations to practice classroom observations, the authors developed a VR experience in which participants tag observable elements of academic discourse using codes from two observation protocols. The protocols identify elements of equitable student access: how teachers call on students and how they design questions. Seventy-five school leaders used the VR platform to observe a classroom scenario and code evidence of equitable classroom access. The authors analyzed data from tagging in the virtual reality scenario and triangulated these data with survey data focused on observation practices from participants' schools. A reflection component is included on the platform to collect these qualitative data. Findings: The study results indicate that the virtual reality platform provides an innovative process for leadership professional development focused on building school leaders' capacity to identify elements of academic discourse during classroom observations. Participants reported that the opportunity to practice classroom observations in a risk-free environment was useful. However, for school leaders to fully transfer the data to using in conversations with teachers, they benefit from leadership coaching. Originality/value: This study ascertains the potential effectiveness of an advanced technology for enhancing instructional leadership by using evidence-based classrooms observations to drive improvements in teaching practice. Beyond the utility of the virtual reality tool, this study provides a proof of concept for the next generation of instructional leadership through teacher observations with augmented reality. © 2021, Emerald Publishing Limited.","Coaching; Instructional leadership; Teacher observation; Virtual reality",,Article,"Article in Press","",Scopus,2-s2.0-85102766946
"Nebytova L.A., Katrenko M.V., Savin D.I., Zhuravleva Y.I.","57222574015;57222571577;57222569797;57222571063;","Augmented reality in the training process of children with hearing disorders",2021,"CEUR Workshop Proceedings","2834",,,"330","339",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103254538&partnerID=40&md5=7ad9948ae8c3fd0ca35798c25963be43","Stavropol Reg. Inst. for the Development of Education, Advanced Training and Retraining of Educators, Stavropol, 355000, Russian Federation; North-Caucasian Federal University, Stavropol, 355009, Russian Federation; Volgograd State Medical University, Pyatigorsk, 357528, Russian Federation","Nebytova, L.A., Stavropol Reg. Inst. for the Development of Education, Advanced Training and Retraining of Educators, Stavropol, 355000, Russian Federation; Katrenko, M.V., North-Caucasian Federal University, Stavropol, 355009, Russian Federation; Savin, D.I., North-Caucasian Federal University, Stavropol, 355009, Russian Federation; Zhuravleva, Y.I., Volgograd State Medical University, Pyatigorsk, 357528, Russian Federation","The relevance of the present research stems from the fact that the structure of sports activity of hearing-impaired athletes has objective features and depends on their psychophysiological potential. To modernize educational and training activities, to demonstrate practice exercises in detail, and to intensify the mastering of the movement technique enables the technology of augmented reality (AR). The purpose of this research is to prove the effectiveness of the technology of augmented reality to enhance learning in the educational and training process on track and field athletics by the hearing-impaired children of the basic level. In the course of the writing of the present work the following learning methodology was used: theoretical methods of the generalization of scientific and methodological literature, testing, analysis method, data processing, and interpretation. For the processing of quantitative results obtained the methods of mathematical statistics were used. In our research the main tests for the evaluation of the training effect with the use of AR technology are a 20-meters run at flying start, 100-meter race, standing long jump, jumping rope time trial. The effectiveness of the use of AR technology has shown positive dynamics by all criteria of the tests applied in this study. This technology is quite simple in application and use. Apart from the above hearing-impaired athletes, the results of this research can be applied to some other special needs children engaged in sports. © 2021 CEUR-WS. All rights reserved.","AR technology; Augmented reality; Deaf and hearing-impaired athletes; Education; Educational and training process; Health limitations; Hearing-impaired patients; Information technologies; Sports and game method; Track and field athletics; Training by visual aids","Augmented reality; Data handling; Distance education; Sports; Statistics; Hearing disorders; Hearing impaired; Movement techniques; Quantitative result; Standing long jumps; Theoretical methods; Track and fields; Training activities; Audition",Conference Paper,"Final","",Scopus,2-s2.0-85103254538
"Liu P., Li C., Xiao C., Zhang Z., Ma J., Gao J., Shao P., Valerio I., Pawlik T.M., Ding C., Yilmaz A., Xu R.","57109570400;57208301188;57194183923;56452276600;57217084915;57212121943;56085690300;14623552900;7006249269;57214826251;57219084145;56984933900;","A Wearable Augmented Reality Navigation System for Surgical Telementoring Based on Microsoft HoloLens",2021,"Annals of Biomedical Engineering","49","1",,"287","298",,,"10.1007/s10439-020-02538-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086131311&doi=10.1007%2fs10439-020-02538-5&partnerID=40&md5=e3b5799ca3ae72fb69fe834a82a5b7e1","Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China; Department of Biomedical Engineering, The Ohio State University, Columbus, United States; Photogrammetric Computer Vision Laboratory, The Ohio State University, Columbus, United States; Department of Surgery, The Ohio State University, Columbus, United States; Department of Rehabilitation Medicine, The Second Hospital of Anhui Medical University, Hefei, Anhui, China","Liu, P., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China; Li, C., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China, Department of Biomedical Engineering, The Ohio State University, Columbus, United States; Xiao, C., Photogrammetric Computer Vision Laboratory, The Ohio State University, Columbus, United States; Zhang, Z., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China, Department of Biomedical Engineering, The Ohio State University, Columbus, United States; Ma, J., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China; Gao, J., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China; Shao, P., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China; Valerio, I., Department of Surgery, The Ohio State University, Columbus, United States; Pawlik, T.M., Department of Surgery, The Ohio State University, Columbus, United States; Ding, C., Department of Rehabilitation Medicine, The Second Hospital of Anhui Medical University, Hefei, Anhui, China; Yilmaz, A., Photogrammetric Computer Vision Laboratory, The Ohio State University, Columbus, United States; Xu, R., Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, Anhui, China, Department of Biomedical Engineering, The Ohio State University, Columbus, United States","This paper reports a new type of augmented reality (AR) system that integrates a Microsoft HoloLens device with a three-dimensional (3D) point tracking module for medical training and telementored surgery. In this system, a stereo camera is used to track the 3D position of a scalpel and transfer its coordinates wirelessly to a HoloLens device. In the scenario of surgical training, a virtual surgical scene with pre-recorded surgical annotations is superimposed with the actual surgical scene so that the surgical trainee is able to operate following virtual instructions. In the scenario of telementored surgery, the virtual surgical scene is co-registered with the actual surgical scene so that the virtual scalpel remotely mentored by an experienced surgeon provides the AR guidance for the inexperienced on-site operator. The performance characteristics of the proposed AR telementoring system are verified by benchtop experiments. The clinical applicability of the proposed system in telementored skin grafting surgery and fasciotomy is validated in a New Zealand rabbit model. Our benchtop and in vivo experiments demonstrate the potential to improve surgical performance and reduce healthcare disparities in remote areas with limited resources. © 2020, Biomedical Engineering Society.","Augment reality surgical navigation; Medical training; Microsoft HoloLens; Surgical telementoring","Augmented reality; E-learning; Navigation systems; Stereo image processing; Wearable computers; Augmented reality systems; In-vivo experiments; Medical training; New Zealand rabbits; Performance characteristics; Stereo cameras; Surgical training; Threedimensional (3-d); Transplantation (surgical); adult; animal experiment; animal tissue; Article; controlled study; fasciotomy; female; health care disparity; human; human tissue; image recording; in vivo study; medical education; medical student; New Zealand rabbit; nonhuman; priority journal; skin transplantation; surgeon; surgical training; telecommunication; virtual reality",Article,"Final","",Scopus,2-s2.0-85086131311
"Sakib M.N., Chaspari T., Behzadan A.H.","57213424763;55351228300;15073914400;","Physiological Data Models to Understand the Effectiveness of Drone Operation Training in Immersive Virtual Reality",2021,"Journal of Computing in Civil Engineering","35","1", 04020053,"","",,,"10.1061/(ASCE)CP.1943-5487.0000941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095978574&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000941&partnerID=40&md5=e4034444de1c6b70a770491142771400","Dept. of Multidisciplinary Engineering, Texas A and M Univ., 3127 TAMU, College Station, TX  77843, United States; Dept. of Computer Science and Engineering, Texas A and M Univ., 3127 TAMU, College Station, TX  77843, United States; Dept. of Construction Science, Texas A and M Univ., 3137 TAMU, College Station, TX  77843, United States","Sakib, M.N., Dept. of Multidisciplinary Engineering, Texas A and M Univ., 3127 TAMU, College Station, TX  77843, United States; Chaspari, T., Dept. of Computer Science and Engineering, Texas A and M Univ., 3127 TAMU, College Station, TX  77843, United States; Behzadan, A.H., Dept. of Construction Science, Texas A and M Univ., 3137 TAMU, College Station, TX  77843, United States","Data collection using unmanned aerial vehicles (UAVs) in construction and heavy civil projects is subject to compliance with strict operational rules and safety regulations. Both the US Federal Aviation Administration (FAA) and the European Union Aviation Safety Agency (EASA) require drone operators to keep the drone in sight and avoid flying near people or other objects. From the perspective of the operator, remaining in standing or sitting position while always looking up to monitor the drone movements can cause awkward body postures, stress, and fatigue. Coupled with the mental load resulting from delegated tasks, this could potentially put the drone mission, people, and property at risk. This research investigates the reliability of using the drone operator's physiological indexes and self-assessments to predict performance, mental workload (MWL), and stress in immersive virtual reality training and outdoor deployment. A user study was carried out to collect physiological data using wearable devices and design general population and group-specific prediction models. Results show that in 83% of cases, these models can predict performance, MWL, and stress levels accurately or within one level. This paper contributes to the core body of knowledge by providing a scalable approach to objectively quantifying performance, MWL, and stress that can be used to design adaptive training systems for drone operators. Personalized models of physiological signals are presented as reliable indexes to describing the outcome of interest. Scalability is achieved through the application of generalizable machine learning models that learn the interdependencies between physiological and self-assessment inputs and their association with corresponding outcomes. © 2020 American Society of Civil Engineers.","Human performance; Machine learning (ML); Mental workload; Physiological signals; Unmanned aerial vehicle (UAV); Virtual reality (VR); Wearable technology","Adaptive systems; Antennas; Automobile bodies; Data acquisition; Drones; E-learning; Forecasting; Personnel training; Physiology; Population statistics; Predictive analytics; Safety engineering; Virtual reality; Adaptive training system; General population; Immersive virtual reality; Machine learning models; Personalized model; Physiological indices; Physiological signals; Us federal aviation administrations; Physiological models",Article,"Final","",Scopus,2-s2.0-85095978574
"Lau K.W., Lee P.Y.","57211282004;36835177400;","Using virtual reality for professional training practices: exploring the factors of applying stereoscopic 3D technologies in knowledge transfer",2021,"Virtual Reality",,,,"","",,,"10.1007/s10055-021-00504-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101077739&doi=10.1007%2fs10055-021-00504-7&partnerID=40&md5=8289cf3850d93bac91546d72ef249966","The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","Lau, K.W., The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Lee, P.Y., The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","Current research has constantly highlighted the significance of developing a learning culture with a robust knowledge sharing and transfer process in knowledge-based societies. The emergence of stereoscopic 3D virtual technologies provides organizations with an opportunity to develop immersive and virtual professional training practices for employees’ transformative learning. This research gathered data from a survey of 326 respondents to investigate employees’ virtual learning experiences in a virtual learning environment (VLE). The results show that a successful VLE model could possibly enhance employees’ learning motivation, process and satisfaction. The empirical evidence of this research suggests that there are three key components of framework for actual implementation; they are (1) the careful selection of VLE; (2) the appropriate design of pedagogical strategies; and (3) the effective use of virtual stimuli which involves the factors of the stereoscopic 3D visualization, telepresence and multisensory interactions. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","Knowledge transfer; Learning science; Professional training; Stereoscopic 3D technology; Virtual reality","Computer aided instruction; E-learning; Knowledge based systems; Knowledge management; Personnel training; Stereo image processing; Surveys; Three dimensional computer graphics; Visual communication; Knowledge-based society; Multisensory interaction; Pedagogical strategies; Professional training; Stereoscopic 3-d technologies; Stereoscopic 3D visualization; Transformative learning; Virtual learning environments; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85101077739
"Ikbal M.S., Ramadoss V., Zoppi M.","57207710331;57192715243;35838978700;","Dynamic Pose Tracking Performance Evaluation of HTC Vive Virtual Reality System",2021,"IEEE Access","9",, 9309218,"3798","3815",,,"10.1109/ACCESS.2020.3047698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099084093&doi=10.1109%2fACCESS.2020.3047698&partnerID=40&md5=79bded2f82e5d3b22c34a9aa8c53bef2","Department of Mechanical, Energy, Management, and Transportation Engineering, PMAR Robotics Group, University of Genoa, Genoa, (GE), Italy","Ikbal, M.S., Department of Mechanical, Energy, Management, and Transportation Engineering, PMAR Robotics Group, University of Genoa, Genoa, (GE), Italy; Ramadoss, V., Department of Mechanical, Energy, Management, and Transportation Engineering, PMAR Robotics Group, University of Genoa, Genoa, (GE), Italy; Zoppi, M., Department of Mechanical, Energy, Management, and Transportation Engineering, PMAR Robotics Group, University of Genoa, Genoa, (GE), Italy","Virtual reality tracking devices are rapidly becoming the go-to system for cost-effective motion tracking solutions across different communities such as robotics, biomechanics, sports, rehabilitation, motion simulators, etc. This article focuses on the spatial tracking performance of HTC Vive's lighthouse tracking system (VLTS) devices (tracker, controller, and head mount display). A comprehensive literature survey on the performance analysis of VLTS on the various aspects is presented along with its shortcomings in terms of spatial tracking evaluation. The two key limitations have been identified: in static cases, there is a lack of standard procedures and criteria, and in dynamic cases, the entire study of spatial tracking. We address the first by assessing VLTS using the optical tracking system standard specified by ASTM International, and the latter by revising the standards to determine the upper-velocity limit for reliable tracking. The findings are substantiated with the trajectories of human wrist motion. Each evaluation's results are systematically analyzed with statistical hypothesis tests and criteria fulfillment. Comau NS16, an industrial serial robot, was used as the ground truth motion generator due to its repeatability and 6 degrees of workspace freedom. One of the major reasons for not having more generalized spatial tracking studies is that the tracking performance heavily depends on the configurations of the setup, work volume, environment, etc. Thus, the guidelines for configuring VLTS and the approach adapted from ASTM standards for evaluating VLTS for custom applications using our reported findings for both static and dynamic cases are included in the appendix. © 2013 IEEE.","Lighthouse tracking; motion tracking; performance evaluation and bench-marking; virtual reality and interfaces","ASTM standards; Cost effectiveness; Display devices; Gesture recognition; Industrial robots; Testing; Tracking (position); Virtual reality; Head-mount displays; Optical tracking systems; Performance analysis; Standard procedures; Statistical hypothesis test; Tracking performance; Tracking solutions; Virtual reality system; Motion tracking",Article,"Final","",Scopus,2-s2.0-85099084093
"Nguyen-Vo T., Riecke B.E., Stuerzlinger W., Pham D.-M., Kruijff E.","57021750000;6603396361;55902405400;57203971039;15022738100;","NaviBoard and NaviChair: Limited Translation Combined with Full Rotation for Efficient Virtual Locomotion",2021,"IEEE Transactions on Visualization and Computer Graphics","27","1", 8809840,"165","177",,2,"10.1109/TVCG.2019.2935730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096885584&doi=10.1109%2fTVCG.2019.2935730&partnerID=40&md5=85d46ecc0f5cefd72f65aac054e1267c","School of Interactive Arts + Technology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Institute of Visual Computing, Bonn-Rhein-Sieg University of Applied Sciences, Sankt Augustin, 53757, Germany","Nguyen-Vo, T., School of Interactive Arts + Technology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Riecke, B.E., School of Interactive Arts + Technology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Stuerzlinger, W., School of Interactive Arts + Technology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Pham, D.-M., School of Interactive Arts + Technology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Kruijff, E., Institute of Visual Computing, Bonn-Rhein-Sieg University of Applied Sciences, Sankt Augustin, 53757, Germany","Walking has always been considered as the gold standard for navigation in Virtual Reality research. Though full rotation is no longer a technical challenge, physical translation is still restricted through limited tracked areas. While rotational information has been shown to be important, the benefit of the translational component is still unclear with mixed results in previous work. To address this gap, we conducted a mixed-method experiment to compare four levels of translational cues and control: none (using the trackpad of the HTC Vive controller to translate), upper-body leaning (sitting on a 'NaviChair', leaning the upper-body to locomote), whole-body leaning/stepping (standing on a platform called NaviBoard, leaning the whole body or stepping one foot off the center to navigate), and full translation (physically walking). Results showed that translational cues and control had significant effects on various measures including task performance, task load, and simulator sickness. While participants performed significantly worse when they used a controller with no embodied translational cues, there was no significant difference between the NaviChair, NaviBoard, and actual walking. These results suggest that translational body-based motion cues and control from a low-cost leaning/stepping interface might provide enough sensory information for supporting spatial updating, spatial awareness, and efficient locomotion in VR, although future work will need to investigate how these results might or might not generalize to other tasks and scenarios. © 1995-2012 IEEE.","Adaptive control; cognitive informatics; human computer interaction; human factors; user interface; virtual reality","Computer graphics; Software engineering; Efficient locomotion; Sensory information; Simulator sickness; Spatial awareness; Spatial updating; Technical challenges; Translational component; Virtual locomotion; Controllers",Article,"Final","",Scopus,2-s2.0-85096885584
"Guarda A.F.R., Rodrigues N.M.M., Pereira F.","56337122500;7006052345;7201690397;","Constant Size Point Cloud Clustering: A Compact, Non-Overlapping Solution",2021,"IEEE Transactions on Multimedia","23",, 9000649,"77","91",,,"10.1109/TMM.2020.2974325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098157333&doi=10.1109%2fTMM.2020.2974325&partnerID=40&md5=88082449ccfda4fb0d35d8f41da0bd7a","Instituto Superior Tecnico, Universidade de Lisboa, Instituto de Telecomunicacoes, Lisbon, Portugal; ESTG, Instituto Politecnico de Leiria, Instituto de Telecomunicacoes, Leiria, Portugal","Guarda, A.F.R., Instituto Superior Tecnico, Universidade de Lisboa, Instituto de Telecomunicacoes, Lisbon, Portugal; Rodrigues, N.M.M., ESTG, Instituto Politecnico de Leiria, Instituto de Telecomunicacoes, Leiria, Portugal; Pereira, F., Instituto Superior Tecnico, Universidade de Lisboa, Instituto de Telecomunicacoes, Lisbon, Portugal","Point clouds have recently become a popular 3D representation model for many application domains, notably virtual and augmented reality. Since point cloud data is often very large, processing a point cloud may require that it be segmented into smaller clusters. For example, the input to deep learning-based methods like auto-encoders should be constant size point cloud clusters, which are ideally compact and non-overlapping. However, given the unorganized nature of point clouds, defining the specific data segments to code is not always trivial. This paper proposes a point cloud clustering algorithm which targets five main goals: i) clusters with a constant number of points; ii) compact clusters, i.e., with low dispersion; iii) non-overlapping clusters, i.e., not intersecting each other; iv) ability to scale with the number of points; and v) low complexity. After appropriate initialization, the proposed algorithm transfers points between neighboring clusters as a propagation wave, filling or emptying clusters until they achieve the same size. The proposed algorithm is unique since there is no other point cloud clustering method available in the literature offering the same clustering features for large point clouds at such low complexity. © 1999-2012 IEEE.","compactness; constant size; non-overlapping; Point cloud; point cloud clustering","3D modeling; Augmented reality; Deep learning; Learning systems; Three dimensional computer graphics; 3d representations; Clustering feature; Clustering methods; Learning-based methods; Low dispersions; Overlapping clusters; Point cloud data; Virtual and augmented reality; Clustering algorithms",Article,"Final","",Scopus,2-s2.0-85098157333
"Basak S., Corcoran P., Khan F., McDonnell R., Schukat M.","57219442368;57222327917;57206855808;24537203400;14016517800;","Learning 3D head pose from synthetic data: A semi-supervised approach",2021,"IEEE Access","9",,,"37557","37573",,,"10.1109/ACCESS.2021.3063884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102268238&doi=10.1109%2fACCESS.2021.3063884&partnerID=40&md5=a52ecb7406d35432276e60d6e13e40c6","School of Computer Science, National University of Ireland Galway, Galway, H91 TK33, Ireland; Department of Electronic Engineering, College of Science and Engineering, National University of Ireland Galway, Galway, H91 TK33, Ireland; School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, D02 PN40, Ireland","Basak, S., School of Computer Science, National University of Ireland Galway, Galway, H91 TK33, Ireland; Corcoran, P., Department of Electronic Engineering, College of Science and Engineering, National University of Ireland Galway, Galway, H91 TK33, Ireland; Khan, F., Department of Electronic Engineering, College of Science and Engineering, National University of Ireland Galway, Galway, H91 TK33, Ireland; McDonnell, R., School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, D02 PN40, Ireland; Schukat, M., School of Computer Science, National University of Ireland Galway, Galway, H91 TK33, Ireland","Accurate head pose estimation from 2D image data is an essential component of applications such as driver monitoring systems, virtual reality technology, and human-computer interaction. It enables a better determination of user engagement and attentiveness. The most accurate head pose estimators are based on Deep Neural Networks that are trained with the supervised approach and rely primarily on the accuracy of training data. The acquisition of real head pose data with a wide variation of yaw, pitch and roll is a challenging task. Publicly available head pose datasets have limitations with respect to size, resolution, annotation accuracy and diversity. In this work, a methodology is proposed to generate pixel-perfect synthetic 2D headshot images rendered from high-quality 3D synthetic facial models with accurate head pose annotations. A diverse range of variations in age, race, and gender are also provided. The resulting dataset includes more than 300k pairs of RGB images with corresponding head pose annotations. A wide range of variations in pose, illumination and background are included. The dataset is evaluated by training a state-of-the-art head pose estimation model and testing against the popular evaluation-dataset Biwi. The results show that training with purely synthetic data generated using the proposed methodology achieves close to state-of-the-art results on head pose estimation which are originally trained on real human facial datasets. As there is a domain gap between the synthetic images and real-world images in the feature space, initial experimental results fall short of the current state-of-the-art. To reduce the domain gap, a semisupervised visual domain adaptation approach is proposed, which simultaneously trains with the labelled synthetic data and the unlabeled real data. When domain adaptation is applied, a significant improvement in model performance is achieved. Additionally, by applying a data fusion-based transfer learning approach, better results are achieved than previously published work on this topic. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Face dataset; Head pose estimation; Synthetic face; Visual domain adaptation","Data fusion; Deep neural networks; Human computer interaction; Semi-supervised learning; Statistical tests; Transfer learning; Domain adaptation; Driver monitoring system; Head Pose Estimation; Model performance; Real-world image; State of the art; Synthetic images; Virtual reality technology; Deep learning",Article,"Final","",Scopus,2-s2.0-85102268238
"Cote-Allard U., Gagnon-Turcotte G., Phinyomark A., Glette K., Scheme E., Laviolette F., Gosselin B.","57201275431;56829549000;26639722300;13007091700;57202922022;12140687300;7101623799;","A Transferable Adaptive Domain Adversarial Neural Network for Virtual Reality Augmented EMG-Based Gesture Recognition",2021,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","29",, 9354785,"546","555",,,"10.1109/TNSRE.2021.3059741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100939669&doi=10.1109%2fTNSRE.2021.3059741&partnerID=40&md5=7b88f6ec57790867be092f2156544178","Department of Informatics, University of Oslo, Oslo, Norway; Laboratoire de Recherche sur les Microsystemes Biomedicaux, Universite Laval, Quebec, QC, Canada; Institute of Biomedical Engineering, University of New Brunswick, Fredericton, NB, Canada; Departement de Genie Electrique et de Genie Informatique, Universite Laval, Quebec, QC, Canada; Department of Electrical and Computer Engineering, Universite Laval, Quebec, QC, Canada","Cote-Allard, U., Department of Informatics, University of Oslo, Oslo, Norway; Gagnon-Turcotte, G., Laboratoire de Recherche sur les Microsystemes Biomedicaux, Universite Laval, Quebec, QC, Canada; Phinyomark, A., Institute of Biomedical Engineering, University of New Brunswick, Fredericton, NB, Canada; Glette, K., Department of Informatics, University of Oslo, Oslo, Norway; Scheme, E., Institute of Biomedical Engineering, University of New Brunswick, Fredericton, NB, Canada; Laviolette, F., Departement de Genie Electrique et de Genie Informatique, Universite Laval, Quebec, QC, Canada; Gosselin, B., Department of Electrical and Computer Engineering, Universite Laval, Quebec, QC, Canada","Within the field of electromyography-based (EMG) gesture recognition, disparities exist between the offline accuracy reported in the literature and the real-time usability of a classifier. This gap mainly stems from two factors: 1) The absence of a controller, making the data collected dissimilar to actual control. 2) The difficulty of including the four main dynamic factors (gesture intensity, limb position, electrode shift, and transient changes in the signal), as including their permutations drastically increases the amount of data to be recorded. Contrarily, online datasets are limited to the exact EMG-based controller used to record them, necessitating the recording of a new dataset for each control method or variant to be tested. Consequently, this paper proposes a new type of dataset to serve as an intermediate between offline and online datasets, by recording the data using a real-time experimental protocol. The protocol, performed in virtual reality, includes the four main dynamic factors and uses an EMG-independent controller to guide movements. This EMG-independent feedback ensures that the user is in-the-loop during recording, while enabling the resulting dynamic dataset to be used as an EMG-based benchmark. The dataset is comprised of 20 able-bodied participants completing three to four sessions over a period of 14 to 21 days. The ability of the dynamic dataset to serve as a benchmark is leveraged to evaluate the impact of different recalibration techniques for long-term (across-day) gesture recognition, including a novel algorithm, named TADANN. TADANN consistently and significantly ( $\text{p} < 0.05$ ) outperforms using fine-tuning as the recalibration technique. © 2001-2011 IEEE.","EMG; gesture recognition; leap motion; myoelectric control; transfer learning; virtual reality","Controllers; Neural networks; Virtual reality; Control methods; Dynamic factors; Experimental protocols; Fine tuning; Limb positions; Novel algorithm; Recalibrations; Transient changes; Gesture recognition; adult; article; clinical article; computer heuristics; electromyography; female; gesture; human; human experiment; male; motion; myoelectric control; transfer of learning; virtual reality; algorithm; controlled study; electromyography",Article,"Final","",Scopus,2-s2.0-85100939669
"Feng Y., Duives D., Daamen W., Hoogendoorn S.","57216936136;55630559900;6602658905;6701778851;","Data collection methods for studying pedestrian behaviour: A systematic review",2021,"Building and Environment","187",, 107329,"","",,1,"10.1016/j.buildenv.2020.107329","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095426073&doi=10.1016%2fj.buildenv.2020.107329&partnerID=40&md5=cdba405c4d43abf884b7f43d557c614d","Department of Transport & Planning, Delft University of Technology, Stevinweg 1, Delft, 2628 CN, Netherlands","Feng, Y., Department of Transport & Planning, Delft University of Technology, Stevinweg 1, Delft, 2628 CN, Netherlands; Duives, D., Department of Transport & Planning, Delft University of Technology, Stevinweg 1, Delft, 2628 CN, Netherlands; Daamen, W., Department of Transport & Planning, Delft University of Technology, Stevinweg 1, Delft, 2628 CN, Netherlands; Hoogendoorn, S., Department of Transport & Planning, Delft University of Technology, Stevinweg 1, Delft, 2628 CN, Netherlands","Collecting pedestrian behaviour data is vital to understand pedestrian behaviour. This systematic review of 145 studies aims to determine the capability of contemporary data collection methods in collecting different pedestrian behavioural data, identify research gaps and discuss the possibilities of using new technologies to study pedestrian behaviour. The review finds that there is an imbalance in the number of studies that feature various aspects of pedestrian behaviour, most importantly (1) pedestrian behaviour in large complex scenarios, and (2) pedestrian behaviour during new types of high-risk situations. Additionally, three issues are identified regarding current pedestrian behaviour studies, namely (3) little comprehensive data sets featuring multi-dimensional behaviour data simultaneously, (4) generalizability of most collected data sets is limited, and (5) costs of pedestrian behaviour experiments are relatively high. A set of new technologies offers opportunities to overcome some of these limitations. This review identifies three types of technologies that can become a valuable addition to pedestrian behaviour research methods, namely (1) applying VR experiments to study pedestrian behaviour in the environments that are difficult or cannot be mimicked in real-life, repeat experiments to determine the impact of factors on pedestrian behaviour and collect more accurate behavioural data to understand the decision-making process of pedestrian behaviour deeply, (2) applying large-scale crowd monitoring to study pedestrian movements in large complex environments and incident situations, and (3) utilising the Internet of Things to track pedestrian movements at various locations that are difficult to investigate at the moment. © 2020 The Authors","Crowd; Data collection method; IoT; Literature review; Pedestrian behaviour; Virtual reality","Data acquisition; Risk perception; Complex environments; Data collection method; Decision making process; High-risk situations; Multi dimensional; Pedestrian movement; Research gaps; Systematic Review; Behavioral research; decision making; Internet; literature review; pedestrian; travel behavior; virtual reality",Article,"Final","",Scopus,2-s2.0-85095426073
"Sloth S.B., Jensen R.D., Seyer-Hansen M., Christensen M.K., De Win G.","57195260098;56540615500;6507253337;35067747700;12779954100;","Remote training in laparoscopy: a randomized trial comparing home-based self-regulated training to centralized instructor-regulated training",2021,"Surgical Endoscopy",,,,"","",,,"10.1007/s00464-021-08429-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103224097&doi=10.1007%2fs00464-021-08429-7&partnerID=40&md5=4bc30bb2460df59cb59765a92acd428f","Centre for Educational Development, Aarhus University, Aarhus, Denmark; Department of Clinical Medicine, Aarhus University, Aarhus, Denmark; Corporate HR MidtSim, Central Denmark Region, Aarhus, Denmark; Department of Obstetrics & Gynecology, Aarhus University Hospital, Aarhus, Denmark; Antwerp Surgical Training, Anatomy and Research Center (ASTARC), Faculty of Medicine and Health Sciences, University of Antwerp, Antwerp, Belgium; Department of Urology, University Hospital Antwerp, Edegem, Belgium","Sloth, S.B., Centre for Educational Development, Aarhus University, Aarhus, Denmark; Jensen, R.D., Department of Clinical Medicine, Aarhus University, Aarhus, Denmark, Corporate HR MidtSim, Central Denmark Region, Aarhus, Denmark; Seyer-Hansen, M., Department of Obstetrics & Gynecology, Aarhus University Hospital, Aarhus, Denmark; Christensen, M.K., Centre for Educational Development, Aarhus University, Aarhus, Denmark; De Win, G., Antwerp Surgical Training, Anatomy and Research Center (ASTARC), Faculty of Medicine and Health Sciences, University of Antwerp, Antwerp, Belgium, Department of Urology, University Hospital Antwerp, Edegem, Belgium","Background: Simulation-based surgical training (SBST) is key to securing future surgical expertise. Proficiency-based training (PBT) in laparoscopy has shown promising results on skills transfer. However, time constraints and limited possibilities for distributed training constitute barriers to effective PBT. Home-based training may provide a solution to these barriers and may be a feasible alternative to centralized training in times of assembly constraints. Methods: We randomly assigned first-year trainees in abdominal surgery, gynecology, and urology to either centralized instructor-regulated training (CIRT) or home-based self-regulated training (HSRT) in laparoscopy. All participants trained on portable box trainers providing feedback on metrics and possibility for video reviewing. Training in both groups was structured as PBT with graded proficiency exercises adopted from the Fundamentals of Laparoscopic Surgery (FLS). The HSRT group trained at home guided by online learning materials, while the CIRT group attended two training sessions in the simulation center with feedback from experienced instructors. Performance tests consisted of hand–eye and bimanual coordination, suture and knot-tying, and FLS exercises. We analyzed passing rates, training time and distribution, and test performances. Results: Passing rates were 87% and 96% in the CIRT and HSRT group, respectively. HSRT facilitated distributed training and resulted in greater variation in training times. Task times for hand–eye and bimanual coordination were significantly reduced between pretest and posttest in both groups. Trainees maintained their posttest performances at the 6-month retention test. Our analyses revealed no significant inter-group differences in performances at pretest, posttest, or retention test. Performance improvements in the two groups followed similar patterns. Conclusion: CIRT and HSRT in laparoscopy result in comparable performance improvements. HSRT in laparoscopy is a feasible and effective alternative to CIRT when offered inside a supportive instructional design. Further research is needed to clarify trainees’ preferences and explore facilitators and barriers to HSRT. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Laparoscopy; Proficiency-based training; Remote learning; Self-regulated learning; Simulation; Surgery",,Article,"Article in Press","",Scopus,2-s2.0-85103224097
"Torres Calderon W., Roberts D., Golparvar-Fard M.","57219900490;57202510851;34868104300;","Synthesizing Pose Sequences from 3D Assets for Vision-Based Activity Analysis",2021,"Journal of Computing in Civil Engineering","35","1", 937,"","",,,"10.1061/(ASCE)CP.1943-5487.0000937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095970460&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000937&partnerID=40&md5=7f2d77939f7e91927fbaa9ebd445ca60","Dept. of Civil and Environmental Engineering, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States; Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States; Dept. of Civil and Environmental Engineering and Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States","Torres Calderon, W., Dept. of Civil and Environmental Engineering, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States; Roberts, D., Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States; Golparvar-Fard, M., Dept. of Civil and Environmental Engineering and Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana, IL  61801, United States","In recent years, computer vision algorithms have shown to effectively leverage visual data from jobsites for video-based activity analysis of construction equipment. However, earthmoving operations are restricted to site work and surrounding terrain, and the presence of other structures, particularly in urban areas, limits the number of viewpoints from which operations can be recorded. These considerations lower the degree of intra-activity and interactivity category variability to which said algorithms are exposed, hindering their potential for generalizing effectively to new jobsites. Secondly, training computer vision algorithms is also typically reliant on large quantities of hand-annotated ground truth. These annotations are burdensome to obtain and can offset the cost-effectiveness incurred from automating activity analysis. The main contribution of this paper is a means of inexpensively generating synthetic data to improve the capabilities of vision-based activity analysis methods based on virtual, kinematically articulated three-dimensional (3D) models of construction equipment. The authors introduce an automated synthetic data generation method that outputs a two-dimensional (2D) pose corresponding to simulated excavator operations that vary according to camera position with respect to the excavator and activity length and behavior. The presented method is validated by training a deep learning-based method on the synthesized 2D pose sequences and testing on pose sequences corresponding to real-world excavator operations, achieving 75% precision and 71% recall. This exceeds the 66% precision and 65% recall obtained when training and testing the deep learning-based method on the real-world data via cross-validation. Limited access to reliable amounts of real-world data incentivizes using synthetically generated data for training vision-based activity analysis algorithms. © 2020 American Society of Civil Engineers.",,"Computer vision; Cost effectiveness; Deep learning; Excavation; Excavators; Information analysis; Learning systems; Urban growth; Computer vision algorithms; Earthmoving operations; Learning-based methods; Pose corresponding; Synthetic data generations; Three-dimensional (3D) model; Training and testing; Two Dimensional (2 D); Construction equipment",Article,"Final","",Scopus,2-s2.0-85095970460
"Giesel M., Nowakowska A., Harris J.M., Hesse C.","23477051300;57191292034;7407315545;24824532100;","Perceptual uncertainty and action consequences independently affect hand movements in a virtual environment",2020,"Scientific Reports","10","1", 22307,"","",,,"10.1038/s41598-020-78378-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097750503&doi=10.1038%2fs41598-020-78378-z&partnerID=40&md5=c0f62f3784febd128933ff554e1c4fd4","School of Psychology, University of Aberdeen, Aberdeen, AB24 3FX, United Kingdom; School of Psychology and Neuroscience, University of St Andrews, St Andrews, KY16 9JP, United Kingdom","Giesel, M., School of Psychology, University of Aberdeen, Aberdeen, AB24 3FX, United Kingdom; Nowakowska, A., School of Psychology, University of Aberdeen, Aberdeen, AB24 3FX, United Kingdom; Harris, J.M., School of Psychology and Neuroscience, University of St Andrews, St Andrews, KY16 9JP, United Kingdom; Hesse, C., School of Psychology, University of Aberdeen, Aberdeen, AB24 3FX, United Kingdom","When we use virtual and augmented reality (VR/AR) environments to investigate behaviour or train motor skills, we expect that the insights or skills acquired in VR/AR transfer to real-world settings. Motor behaviour is strongly influenced by perceptual uncertainty and the expected consequences of actions. VR/AR differ in both of these aspects from natural environments. Perceptual information in VR/AR is less reliable than in natural environments, and the knowledge of acting in a virtual environment might modulate our expectations of action consequences. Using mirror reflections to create a virtual environment free of perceptual artefacts, we show that hand movements in an obstacle avoidance task systematically differed between real and virtual obstacles and that these behavioural differences occurred independent of the quality of the available perceptual information. This suggests that even when perceptual correspondence between natural and virtual environments is achieved, action correspondence does not necessarily follow due to the disparity in the expected consequences of actions in the two environments. © 2020, The Author(s).",,"article; artifact; augmented reality; avoidance behavior; expectation; hand movement; locomotion; motor performance; uncertainty; writing",Article,"Final","",Scopus,2-s2.0-85097750503
"López Ríos O., Lechuga López L.J., Lechuga López G.","6602320190;57219186980;57219186115;","A comprehensive statistical assessment framework to measure the impact of immersive environments on skills of higher education students: a case study",2020,"International Journal on Interactive Design and Manufacturing","14","4",,"1395","1410",,,"10.1007/s12008-020-00698-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091611562&doi=10.1007%2fs12008-020-00698-1&partnerID=40&md5=9ea2c3ee7e6c8f12ca21f48c463182f0","Instituto Tecnológico y de Estudios Superiores de Monterrey, Campus CDMX, Mexico City, CDMX, Mexico; Université de Paris, UFR Informatique, 5 Rue Thomas Mann, Paris, France","López Ríos, O., Instituto Tecnológico y de Estudios Superiores de Monterrey, Campus CDMX, Mexico City, CDMX, Mexico; Lechuga López, L.J., Université de Paris, UFR Informatique, 5 Rue Thomas Mann, Paris, France; Lechuga López, G., Instituto Tecnológico y de Estudios Superiores de Monterrey, Campus CDMX, Mexico City, CDMX, Mexico","Universities are facing the challenge of updating the contents of their current study programs and adopting novel education strategies in order to prepare the next generation of engineers who can adapt to the highly competitive labor market of Industry 4.0. This new industrial era requires skills and competencies in state-of-the-art technologies which are constantly and rapidly evolving. This research presents an alternative approach to the current teaching–learning methodologies, focusing on the use of Virtual Reality (VR) as an educational tool and its contribution towards the upcoming industrial challenges, via what we call interactive education for future engineers (IE). Our IE strategy is supported by interactive simulation using the latest VR technologies, currently being Facebook’s Oculus Rift hardware and software. With this approach, we seek to enhance the “know-how” of our students by training them in the practical skills they will need for the technological innovations of the evolving labor market. Our work has been implemented in engineering courses at Tecnologico de Monterrey in Mexico, aligning with the university’s educational model “TEC21” which aims to increase the students’ self-learning capabilities using interactive teaching and hands-on practical work to develop new abilities and competencies in an expanded variety of domains. So far, our results have shown that the use of IE not only improves the way professors teach, but reinforces skills, competencies, enhances creativity and strongly motivates the students in their daily learning. © 2020, Springer-Verlag France SAS, part of Springer Nature.","Assessment; Competencies; Higher education; Industry 4.0; Interactive education; Virtual Reality","Commerce; Computer software; Education computing; Educational technology; Employment; Industrial research; Learning systems; Technology transfer; Virtual reality; Hardware and software; Higher education students; Industrial challenges; Interactive simulations; Self-learning capability; State-of-the-art technology; Statistical assessment; Technological innovation; Students",Article,"Final","",Scopus,2-s2.0-85091611562
"Rojas-Muñoz E., Lin C., Sanchez-Tamayo N., Cabrera M.E., Andersen D., Popescu V., Barragan J.A., Zarzaur B., Murphy P., Anderson K., Douglas T., Griffis C., McKee J., Kirkpatrick A.W., Wachs J.P.","57205650789;57193616240;57195214444;56661822000;56661805900;7103266698;57218000158;57207592471;57219322088;57212093286;57210912536;57210919543;55233918200;7103368482;9241519000;","Evaluation of an augmented reality platform for austere surgical telementoring: a randomized controlled crossover study in cricothyroidotomies",2020,"npj Digital Medicine","3","1", 75,"","",,3,"10.1038/s41746-020-0284-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087848610&doi=10.1038%2fs41746-020-0284-9&partnerID=40&md5=68d255c42100215022ef3ba3cf678424","School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Department of Computer Science, Purdue University, West Lafayette, IN, United States; Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Naval Medical Center Portsmouth, Portsmouth, VA, United States; Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada; Department of Critical Care Medicine, University of Calgary, Calgary, AB, Canada; Canadian Forces Medical Services, Ottawa, ON, Canada","Rojas-Muñoz, E., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Lin, C., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Sanchez-Tamayo, N., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Cabrera, M.E., Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Andersen, D., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Popescu, V., Department of Computer Science, Purdue University, West Lafayette, IN, United States; Barragan, J.A., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States; Zarzaur, B., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Murphy, P., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Anderson, K., Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States; Douglas, T., Naval Medical Center Portsmouth, Portsmouth, VA, United States; Griffis, C., Naval Medical Center Portsmouth, Portsmouth, VA, United States; McKee, J., Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada; Kirkpatrick, A.W., Department of Surgery, and the Regional Trauma Services, University of Calgary, Calgary, AB, Canada, Department of Critical Care Medicine, University of Calgary, Calgary, AB, Canada, Canadian Forces Medical Services, Ottawa, ON, Canada; Wachs, J.P., School of Industrial Engineering, Purdue University, West Lafayette, IN, United States, Department of Surgery, School of Medicine, Indiana University, Indianapolis, IN, United States","Telementoring platforms can help transfer surgical expertise remotely. However, most telementoring platforms are not designed to assist in austere, pre-hospital settings. This paper evaluates the system for telementoring with augmented reality (STAR), a portable and self-contained telementoring platform based on an augmented reality head-mounted display (ARHMD). The system is designed to assist in austere scenarios: a stabilized first-person view of the operating field is sent to a remote expert, who creates surgical instructions that a local first responder wearing the ARHMD can visualize as three-dimensional models projected onto the patient’s body. Our hypothesis evaluated whether remote guidance with STAR could lead to performing a surgical procedure better, as opposed to remote audio-only guidance. Remote expert surgeons guided first responders through training cricothyroidotomies in a simulated austere scenario, and on-site surgeons evaluated the participants using standardized evaluation tools. The evaluation comprehended completion time and technique performance of specific cricothyroidotomy steps. The analyses were also performed considering the participants’ years of experience as first responders, and their experience performing cricothyroidotomies. A linear mixed model analysis showed that using STAR was associated with higher procedural and non-procedural scores, and overall better performance. Additionally, a binary logistic regression analysis showed that using STAR was associated to safer and more successful executions of cricothyroidotomies. This work demonstrates that remote mentors can use STAR to provide first responders with guidance and surgical knowledge, and represents a first step towards the adoption of ARHMDs to convey clinical expertise remotely in austere scenarios. © 2020, The Author(s).",,"adult; Article; augmented reality; controlled study; female; human; male; mentoring; priority journal; rating scale; surgeon; telementoring; tracheotomy; work experience",Article,"Final","",Scopus,2-s2.0-85087848610
"Rauscher M., Humpe A., Brehm L.","57212102453;57191606409;57125859600;","Virtual reality in tourism: Is it 'Real' enough?",2020,"Academica Turistica","13","2",,"127","138",,,"10.26493/2335-4194.13.127-138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099795988&doi=10.26493%2f2335-4194.13.127-138&partnerID=40&md5=f6a3055e8dfcd660b228e8aa2e759ef2","Munich University of Applied Sciences, Germany","Rauscher, M., Munich University of Applied Sciences, Germany; Humpe, A., Munich University of Applied Sciences, Germany; Brehm, L., Munich University of Applied Sciences, Germany","Virtual Reality Technology is increasingly becoming popular in the tourism sector. So far, the most researched application is the marketing of destinations. In contrast, the technology has also been mentioned as a means to limit or reduce the number of tourists at a specific sight or destination. In this respect vr is considered as a substitute for the actual trip. This paper addresses this issue by looking at the possibility to apply vr-technology to transfer the real-life experience into the digital world. In a qualitative research framework, visitor behaviour and experience are investigated when encountering vr sights in order to better understand items driving technology adoption. Structured content analysis is applied for data analysis where coding follows an adjusted Unified Theory of Acceptance and Use of Technology model. For interpretation purposes a pure qualitative framework was chosen. We find that enjoyment is an important driver for vr technology acceptance, whereas facilitating conditions and outcome expectations seem to be obstacles for it. Perceived usefulness is evaluated controversially. While the technology is not acknowledged as a substitute for a regular holiday trip, especially for travellers who take pleasure in active holidays or appreciate social interaction, it was recognised as an alternative for special occasions such as brief getaways from everyday life or short city trips. Overall, when appropriately implemented the technology might not only be useful to decrease visitor concentration in touristic hotspots or to decrease negative aspects associated with frequent travel but could further be applied to sites where visitors do not engage physically because sites are too distant, expensive, inhospitable, unsafe or fragile. © 2020 University of Primorska. All rights reserved.","Technology adoption; Tourism; Travel substitute; Utaut; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85099795988
"Nykänen M., Puro V., Tiikkaja M., Kannisto H., Lantto E., Simpura F., Uusitalo J., Lukander K., Räsänen T., Heikkilä T., Teperi A.-M.","57204180566;57193531916;51864912400;57210474667;57210417159;57210464151;57210465141;34976891300;7004111153;57219873265;36239398300;","Implementing and evaluating novel safety training methods for construction sector workers: Results of a randomized controlled trial",2020,"Journal of Safety Research","75",,,"205","221",,,"10.1016/j.jsr.2020.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095819636&doi=10.1016%2fj.jsr.2020.09.015&partnerID=40&md5=0f5de273bf90f1503b0331cdfff7f1e1","Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland","Nykänen, M., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Puro, V., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Tiikkaja, M., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Kannisto, H., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Lantto, E., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Simpura, F., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Uusitalo, J., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Lukander, K., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Räsänen, T., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Heikkilä, T., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland; Teperi, A.-M., Finnish Institute of Occupational Health, P.O. Box 40, Helsinki, FI-00032, Finland","Introduction: The construction industry is regarded as one of the most unsafe occupational fields worldwide. Despite general agreement that safety training is an important factor in preventing accidents in the construction sector, more studies are needed to identify effective training methods. To address the current research gap, this study evaluated the impact of novel, participatory safety training methods on construction workers’ safety competencies. Specifically, we assessed the efficacy of an immersive virtual reality (VR)-based safety training program and a participatory human factors safety training program (HFST) in construction industry workplaces. Method: In 2019, 119 construction sector workers from eight workplaces participated in a randomized controlled trial conducted in Finland. All the study participants were assessed using questionnaires at baseline, immediately after the intervention and at one-month follow-up. We applied generalized linear mixed modeling for statistical analysis. Results: Compared to lecture-based safety training, VR-based safety training showed a stronger impact on safety motivation, self-efficacy and safety-related outcome expectancies. In addition, the construction sector workers who participated in the VR-based safety training showed a greater increase in self-reported safety performance at one-month follow-up. Contrary to our study hypotheses, we found no significant differences between the study outcomes in terms of study participants in the HFST training condition and the comparison condition without HFST training. Conclusion: Our study indicates that VR technology as a safety training tool has potential to increase safety competencies and foster motivational change in terms of the safety performance of construction sector workers. In the future, the efficacy of participatory human factors safety training should be studied further using a version that targets both managerial and employee levels and is implemented in a longer format. Practical implications: Safety training in virtual reality provides a promising alternative to passive learning methods. Its motivating effect complements other safety training activities. © 2020","Human factors safety training; Safety locus of control; Safety motivation; Safety self-efficacy; Virtual reality","Accident prevention; Construction industry; Curricula; Human engineering; Learning systems; Motivation; Occupational risks; Safety factor; Surveys; Virtual reality; Construction sectors; Construction workers; Generalized linear mixed models; Immersive virtual reality; Randomized controlled trial; Safety performance; Safety training program; Training conditions; Personnel training; adult; article; building industry; comparative effectiveness; construction worker; controlled study; drug safety; employee; expectancy; female; Finland; follow up; human; human experiment; learning; locus of control; male; motivation; outcome assessment; questionnaire; randomized controlled trial; self concept; training; virtual reality; workplace",Article,"Final","",Scopus,2-s2.0-85095819636
"Rojtberg P., Pöllabauer T., Kuijper A.","36634805900;57216462428;56131137100;","Style-transfer GANs for bridging the domain gap in synthetic pose estimator training",2020,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2020",,, 9319045,"188","195",,,"10.1109/AIVR50618.2020.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100044677&doi=10.1109%2fAIVR50618.2020.00039&partnerID=40&md5=469f73452182699a7c7b6e0c1c8e9e66","Fraunhofer Igd, Tu Darmstadt, Darmstadt, Germany","Rojtberg, P., Fraunhofer Igd, Tu Darmstadt, Darmstadt, Germany; Pöllabauer, T., Fraunhofer Igd, Tu Darmstadt, Darmstadt, Germany; Kuijper, A., Fraunhofer Igd, Tu Darmstadt, Darmstadt, Germany","Given the dependency of current CNN architectures on a large training set, the possibility of using synthetic data is alluring as it allows generating a virtually infinite amount of labeled training data. However, producing such data is a nontrivial task as current CNN architectures are sensitive to the domain gap between real and synthetic data.We propose to adopt general-purpose GAN models for pixellevel image translation, allowing to formulate the domain gap itself as a learning problem. The obtained models are then used either during training or inference to bridge the domain gap. Here, we focus on training the single-stage YOLO6D [20] object pose estimator on synthetic CAD geometry only, where not even approximate surface information is available. When employing paired GAN models, we use an edge-based intermediate domain and introduce different mappings to represent the unknown surface properties.Our evaluation shows a considerable improvement in model performance when compared to a model trained with the same degree of domain randomization, while requiring only very little additional effort. © 2020 IEEE.",,"Computer aided design; Virtual reality; Image translation; Labeled training data; Learning problem; Model performance; Non-trivial tasks; Surface information; Synthetic data; Unknown surface; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85100044677
"Meka A., Pandey R., Häne C., Orts-Escolano S., Barnum P., David-Son P., Erickson D., Zhang Y., Taylor J., Bouaziz S., Legendre C., Ma W.-C., Overbeck R., Beeler T., Debevec P., Izadi S., Theobalt C., Rhemann C., Fanello S.","56376319400;57204394554;55365926600;53864128700;24766316800;57220544328;57208445887;57221363890;55365332000;36136449900;57190440499;57220828833;14822559900;36496499200;7003927551;16426108500;6507027272;8380954300;36170215300;","Deep Relightable Textures: Volumetric Performance Capture with Neural Rendering",2020,"ACM Transactions on Graphics","39","6", 259,"","",,1,"10.1145/3414685.3417814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097335033&doi=10.1145%2f3414685.3417814&partnerID=40&md5=acd92d40dd1e178499837faa99af3f8d","Google, MPI Informatics, Saarland Informatics Campus, Germany","Meka, A., Google, MPI Informatics, Saarland Informatics Campus, Germany; Pandey, R., Google, MPI Informatics, Saarland Informatics Campus, Germany; Häne, C., Google, MPI Informatics, Saarland Informatics Campus, Germany; Orts-Escolano, S., Google, MPI Informatics, Saarland Informatics Campus, Germany; Barnum, P., Google, MPI Informatics, Saarland Informatics Campus, Germany; David-Son, P., Google, MPI Informatics, Saarland Informatics Campus, Germany; Erickson, D., Google, MPI Informatics, Saarland Informatics Campus, Germany; Zhang, Y., Google, MPI Informatics, Saarland Informatics Campus, Germany; Taylor, J., Google, MPI Informatics, Saarland Informatics Campus, Germany; Bouaziz, S., Google, MPI Informatics, Saarland Informatics Campus, Germany; Legendre, C., Google, MPI Informatics, Saarland Informatics Campus, Germany; Ma, W.-C., Google, MPI Informatics, Saarland Informatics Campus, Germany; Overbeck, R., Google, MPI Informatics, Saarland Informatics Campus, Germany; Beeler, T., Google, MPI Informatics, Saarland Informatics Campus, Germany; Debevec, P., Google, MPI Informatics, Saarland Informatics Campus, Germany; Izadi, S., Google, MPI Informatics, Saarland Informatics Campus, Germany; Theobalt, C., Google, MPI Informatics, Saarland Informatics Campus, Germany; Rhemann, C., Google, MPI Informatics, Saarland Informatics Campus, Germany; Fanello, S., Google, MPI Informatics, Saarland Informatics Campus, Germany","The increasing demand for 3D content in augmented and virtual reality has motivated the development of volumetric performance capture systemsnsuch as the Light Stage. Recent advances are pushing free viewpoint relightable videos of dynamic human performances closer to photorealistic quality. However, despite significant efforts, these sophisticated systems are limited by reconstruction and rendering algorithms which do not fully model complex 3D structures and higher order light transport effects such as global illumination and sub-surface scattering. In this paper, we propose a system that combines traditional geometric pipelines with a neural rendering scheme to generate photorealistic renderings of dynamic performances under desired viewpoint and lighting. Our system leverages deep neural networks that model the classical rendering process to learn implicit features that represent the view-dependent appearance of the subject independent of the geometry layout, allowing for generalization to unseen subject poses and even novel subject identity. Detailed experiments and comparisons demonstrate the efficacy and versatility of our method to generate high-quality results, significantly outperforming the existing state-of-the-art solutions. © 2020 Owner/Author.","neural rendering; novel view synthesis; reflectance estimation; relighting; volumetric capture","3D modeling; Deep neural networks; Rendering (computer graphics); Surface scattering; Textures; Virtual reality; Visibility; Augmented and virtual realities; Dynamic performance; Global illumination; Performance capture; Photorealistic quality; Photorealistic rendering; Rendering algorithms; Sophisticated system; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85097335033
"Knierim P., Kiss F., Rauh M., Schmidt A.","55536806700;57194275301;57220211863;57204719065;","Tangibility is Overrated: Comparing Learning Experiences of Physical Setups and their Virtual Equivalent in Augmented Reality",2020,"ACM International Conference Proceeding Series",,,,"299","305",,1,"10.1145/3428361.3428379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097281134&doi=10.1145%2f3428361.3428379&partnerID=40&md5=f2a3600f9efb5b510bc9846efe9c88ee","Lmu Munich, Germany; University of Stuttgart, Germany","Knierim, P., Lmu Munich, Germany; Kiss, F., University of Stuttgart, Germany; Rauh, M., Lmu Munich, Germany; Schmidt, A., Lmu Munich, Germany","Augmented Reality (AR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of AR is blending the virtual and physical world into a coherent environment. In this paper, we examine the effect of substituting the physical components of lab experiments with tangible replicas and virtual representations. We conducted a user study with thirty participants who carried out the experiment in three different abstraction levels (original lab equipment, non-functional tangible props, virtual representation). We compared the users' performance regarding setup time, experienced workload, quality of measurements, and concept comprehension of the learning task. We found no effect on comprehension but significant differences in setup time and quality of measures. The results indicate that substitution reduces the experiment setup duration without affecting knowledge transfer. These results help to shape future AR learning environments, and we offer insights for creating complex mixed reality learning materials. © 2020 ACM.","Amplified Perception; Augmented Reality; Learning; Mixed Reality; Physical Substitution; Physics Lab Experiment; Thermal Vision","Augmented reality; Blending; Computer aided instruction; E-learning; Knowledge management; Abstraction level; Experiment set-up; Fundamental characteristics; Knowledge transfer; Learning environments; Learning experiences; Physical components; Virtual representations; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85097281134
"Ticala R., Ciupe A., Meza S., Orza B.","57221596383;57105700600;24829838400;24503681900;","Augmenting Learning through VR Storytelling",2020,"2020 14th International Symposium on Electronics and Telecommunications, ISETC 2020 - Conference Proceedings",,, 9301040,"","",,,"10.1109/ISETC50328.2020.9301040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607360&doi=10.1109%2fISETC50328.2020.9301040&partnerID=40&md5=3d616e9bbba1803ae2f7470ab54f8174","Technical University of Cluj-Napoca, Multimedia Systems and Applications Laboratory, Cluj-Napoca, Romania","Ticala, R., Technical University of Cluj-Napoca, Multimedia Systems and Applications Laboratory, Cluj-Napoca, Romania; Ciupe, A., Technical University of Cluj-Napoca, Multimedia Systems and Applications Laboratory, Cluj-Napoca, Romania; Meza, S., Technical University of Cluj-Napoca, Multimedia Systems and Applications Laboratory, Cluj-Napoca, Romania; Orza, B., Technical University of Cluj-Napoca, Multimedia Systems and Applications Laboratory, Cluj-Napoca, Romania","This paper presents a narrative instructional model as an interaction framework in a Virtual Reality environment. Virtual Reality solutions provide the means to present a storytelling scenario through visual interactive models. The proposed implementation consists of a VR prototype developed to assist kindergarten and primary school pupils, in assimilating knowledge about different types of musical instruments, while developing a sense of practice-based interaction. The application is structured on three levels; each level is represented by an element of nature (water, earth and air), corresponding to the know-how complexity: water or level 0 which reflects an elementary level of intrinsic knowledge; towards the air environment that reflects the level of cumulative knowledge that will be reached by the end of the game. Knowledge is assimilated by interacting with elements in the virtual environment and by following the annotated guidelines. © 2020 IEEE.","animated characters; musical instruments; primary and kindergarten education; progressive knowledge; storytelling; virtual reality","Technology transfer; Air environment; Elementary levels; Instructional model; Interaction framework; Interactive models; Primary schools; Virtual-reality environment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099607360
"Monteiro D., Liang H.-N., Wang J., Chen H., Baghaei N.","57144011000;8636386200;57207048907;57221155309;14020983900;","An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284718,"713","724",,,"10.1109/ISMAR50242.2020.00102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099294395&doi=10.1109%2fISMAR50242.2020.00102&partnerID=40&md5=9a9ee4dfccc97f04f5650a4b63a6eea1","Xi'an Jiaotong-Liverpool University, China; Massey University, New Zealand","Monteiro, D., Xi'an Jiaotong-Liverpool University, China; Liang, H.-N., Xi'an Jiaotong-Liverpool University, China; Wang, J., Xi'an Jiaotong-Liverpool University, China; Chen, H., Xi'an Jiaotong-Liverpool University, China; Baghaei, N., Massey University, New Zealand","The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness. © 2020 IEEE.","2D/3D Views; Controller types; First Person Shooter; Gaming; Head-Mounted Displays; Virtual Reality","Augmented reality; Controllers; Diseases; Helmet mounted displays; Contributing factor; First person shooter games; Head mounted displays; Input methods; Input techniques; Simulator sickness; Visual techniques; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099294395
"Barsom E.Z., Duijm R.D., Dusseljee-Peute L.W.P., Landman-van der Boom E.B., van Lieshout E.J., Jaspers M.W., Schijven M.P.","57140782000;57209450976;57206834659;57218800815;57218802771;7005754901;6602492995;","Cardiopulmonary resuscitation training for high school students using an immersive 360-degree virtual reality environment",2020,"British Journal of Educational Technology","51","6",,"2050","2062",,1,"10.1111/bjet.13025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090308641&doi=10.1111%2fbjet.13025&partnerID=40&md5=f346cbaaca0fa9416060f7e3d520cd22","Public Health research institute, Department of Medical Informatics of the Amsterdam UMC, Netherlands; Amsterdam UMC, Netherlands","Barsom, E.Z.; Duijm, R.D.; Dusseljee-Peute, L.W.P., Public Health research institute, Department of Medical Informatics of the Amsterdam UMC, Netherlands; Landman-van der Boom, E.B.; van Lieshout, E.J.; Jaspers, M.W., Amsterdam UMC, Netherlands; Schijven, M.P., Amsterdam UMC, Netherlands","Cardiopulmonary resuscitation (CPR) is a lifesaving emergency procedure. To increase survival rates, it is recommended to increase the number of high school students who know how to perform CPR. We have developed an immersive “Virtual Reality (VR) Resuscitation Training” to train the theoretical knowledge of CPR in which trainees must save the life of the patient in a virtual environment. This paper presents a randomized controlled study with a pre-posttest design to explore whether a VR enhanced curriculum improves high school students’ theoretical CPR knowledge. Forty students without previous CPR experience in the past year were randomly assigned to either the VR group or the standard group. The VR group had a significant higher increase of correct answers in comparison with the Standard group. More importantly, the gain in score on taking the correct sequence of CPR steps was significant favouring the VR-enhanced protocol over the Standard protocol. Therefore, the use of a VR training for CPR training appears to be an effective learning method for non-medical students and may be of great value skilling high school students in becoming adequate CPR providers. © 2020 British Educational Research Association","Cardiopulmonary resuscitation; immersive environment; medical education; virtual reality","E-learning; Learning systems; Resuscitation; Students; Technology transfer; Cardiopulmonary resuscitation; Effective learning; Emergency procedures; High school students; Medical students; Standard groups; Standard protocols; Virtual-reality environment; Virtual reality",Article,"Final","",Scopus,2-s2.0-85090308641
"Englmeier D., Fan F., Butz A.","57194090648;57221494100;55150450600;","Rock or Roll - Locomotion Techniques with a Handheld Spherical Device in Virtual Reality",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284676,"618","626",,1,"10.1109/ISMAR50242.2020.00089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099291639&doi=10.1109%2fISMAR50242.2020.00089&partnerID=40&md5=2f5568c3878ae3680a3168bbb39af3dc","Lmu Munich, Munich, Germany","Englmeier, D., Lmu Munich, Munich, Germany; Fan, F., Lmu Munich, Munich, Germany; Butz, A., Lmu Munich, Munich, Germany","We investigate the use of a handheld spherical object as a controller for locomotion in VR. Rotating the object controls avatar movement in two different ways: As a zero order controller, it is continuously rotated to the target position as if rolling a ball on the floor. As a first order controller, it is tilted like a joystick to determine the direction and speed of movement. We describe how our prototype was built from low-cost commercially available hardware and discuss our design decisions. Then we evaluate both locomotion techniques in a user study (N=20) and compare them to established methods using handheld VR controllers. Our prototype matched and in some cases outperformed these methods regarding task time and accuracy. All results were obtained without any usage instructions, indicating easy learnability. Some of our insights may transfer to interaction with other naturally shaped objects in VR experiences. © 2020 IEEE.","Haptic devices; Human computer interaction (HCI); Human computer interaction (HCI); Human-centered computing; Human-centered computing; Interaction devices; Interaction paradigms; Virtual reality","Augmented reality; Costs; Virtual reality; Design decisions; First-order controller; Learnability; Locomotion technique; Spherical objects; Target position; User study; Zero order; Controllers",Conference Paper,"Final","",Scopus,2-s2.0-85099291639
"Vakaliuk T.A., Shevchuk L.D., Shevchuk B.V.","57211133927;57202217000;57219950205;","Possibilities of using AR and VR technologies in teaching mathematics to high school students",2020,"Universal Journal of Educational Research","8","11B",,"6280","6288",,,"10.13189/ujer.2020.082267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096235416&doi=10.13189%2fujer.2020.082267&partnerID=40&md5=749ef820e7d73913b7f2a04b8e98ffe9","Department of Software Engineering, Faculty of Information and Computer Technologies, Zhytomyr Polytechnic State University, Ukraine; Mathematics Computer Science and Teaching Methods Department, Hryhorii Skovoroda University, Ukraine","Vakaliuk, T.A., Department of Software Engineering, Faculty of Information and Computer Technologies, Zhytomyr Polytechnic State University, Ukraine; Shevchuk, L.D., Mathematics Computer Science and Teaching Methods Department, Hryhorii Skovoroda University, Ukraine; Shevchuk, B.V., Mathematics Computer Science and Teaching Methods Department, Hryhorii Skovoroda University, Ukraine","Difficulties at the beginning of studying stereometry in 10 classes are well known from school practice. One of the main reasons for this is poorly developed spatial thinking and imagination. Therefore, the article describes some possibilities of using virtual and augmented reality in the process of teaching mathematics to students in 10-11 classes. It is established that pedagogical research in the field of theory of teaching mathematics should go this way. Undoubtedly, the lessons of mathematics with the addition of reality have a huge future in the field of education. The extensive use of virtual and augmented reality technologies in teaching mathematics proves the effectiveness and it is an attractive motivation for students. The use of augmented reality can be implemented in the teaching of mathematics from primary schools to universities. The object of research is the process of the formation of spatial imagination using virtual and augmented reality technologies in the process of teaching mathematics. The subject of the study is the environment of virtual and augmented reality. Research methods are analysis of publications on the problem, generalization of domestic and foreign experience, theoretical analysis, system analysis, systematization, and generalization of facts and patterns of research for the formation of spatial thinking using virtual and augmented reality environments, substantiation of the main conclusions. In our opinion, it is best to use free and well-made software such as ROAR AR for the Android operating system. Even in universities, it is advisable to use augmented reality systems in the training of future mathematics teachers, as they will be able to stop the decline in the popularity of mathematics studies in secondary and high schools by using such innovative teaching methods in their future pedagogical activities. © 2020 by authors.","Augmented Reality; Educational Technologies; Virtual Reality",,Article,"Final","",Scopus,2-s2.0-85096235416
"Souza V., Maciel A., Nedel L., Kopper R., Loges K., Schlemmer E.","57210972319;24329619800;8702986800;15022552000;57221603592;26424749700;","The Effect of Virtual Reality on Knowledge Transfer and Retention in Collaborative Group-Based Learning for Neuroanatomy Students",2020,"Proceedings - 2020 22nd Symposium on Virtual and Augmented Reality, SVR 2020",,, 9262701,"92","101",,,"10.1109/SVR51698.2020.00028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099538628&doi=10.1109%2fSVR51698.2020.00028&partnerID=40&md5=999be8e2249e7c159986b5ddaf469af4","Institute of Informatics (INF), Federal University of Rio Grande Do sul (UFRGS), Porto Alegre, Brazil; University of North Carolina at Greensboro (UNCG), Department of Computer Science, Greensboro, United States; University of Vale Do Rio Dos Sinos (UNISINOS), São Leopoldo, Brazil","Souza, V., Institute of Informatics (INF), Federal University of Rio Grande Do sul (UFRGS), Porto Alegre, Brazil; Maciel, A., Institute of Informatics (INF), Federal University of Rio Grande Do sul (UFRGS), Porto Alegre, Brazil; Nedel, L., Institute of Informatics (INF), Federal University of Rio Grande Do sul (UFRGS), Porto Alegre, Brazil; Kopper, R., University of North Carolina at Greensboro (UNCG), Department of Computer Science, Greensboro, United States; Loges, K., University of Vale Do Rio Dos Sinos (UNISINOS), São Leopoldo, Brazil; Schlemmer, E., University of Vale Do Rio Dos Sinos (UNISINOS), São Leopoldo, Brazil","There are many uses for virtual reality (VR) in education, and there is a consensus about its contribution in the teaching and learning processes. However, the majority of the studies assess the effectiveness of an individual learning in VR, and there is a need to explore more on the effects of VR using different levels of immersion and collaboration. This paper presents an experiment to investigate knowledge transfer in a group-based learning game. We introduce a VR serious game to support teaching and learning processes in neuroanatomy health education. A between-subjects experiment was conducted with 23 students to jointly assess learning, knowledge retention, and sense of presence. As a control condition, grouped students assembled a physical model of the human brain, while in the experimental condition, a virtual brain was assembled. In each group, one participant assembled the brain, while the others observed and verbally collaborated in a group-based learning strategy. Results shown high mean scores in the virtual condition. When comparing the knowledge test performance before and immediately after the experiment, we found significant difference only for the virtual condition. The same can be observed for retention. Because of the promising results achieved and motivated by the need of more engaging new tools for remote learning - fully used in quarantine conditions, such as the current one because of the Covid-19 pandemic - we conducted a pilot user study to evaluate the learning effect of a remote version of our collaborative VR game. © 2020 IEEE.","Presence; User Evaluation; Virtual Reality","Augmented reality; E-learning; Education computing; Knowledge management; Serious games; Teaching; Virtual reality; Collaborative groups; Experimental conditions; Individual learning; Knowledge retention; Knowledge transfer; Learning strategy; Sense of presences; Teaching and learning; Students",Conference Paper,"Final","",Scopus,2-s2.0-85099538628
"Lampen E., Lehwald J., Pfeiffer T.","57209683199;57202847650;14027435500;","Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095834964&doi=10.1145%2f3385956.3418974&partnerID=40&md5=6990b67981dda33559c0b31456034299","EvoBus GmbH, Neu-Ulm, Germany; Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Lampen, E., EvoBus GmbH, Neu-Ulm, Germany; Lehwald, J., EvoBus GmbH, Neu-Ulm, Germany; Pfeiffer, T., Faculty of Technology, University of Applied Sciences Emden/Leer, Emden, Germany","Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al. [21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks. © 2020 ACM.","Augmented Reality; Expert-Based Learning; Virtual Human","E-learning; Educational settings; Implementation cost; Industrial use case; Learning by observation; Learning scenarios; Sports and rehabilitations; State of the art; Threedimensional (3-d); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095834964
"Martin N., Mathieu N., Pallamin N., Ragot M., Diverrez J.-M.","57188745225;57221495248;13405769400;57073515600;56928312300;","Virtual reality sickness detection: An approach based on physiological signals and machine learning",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,, 9284654,"387","399",,,"10.1109/ISMAR50242.2020.00065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099321186&doi=10.1109%2fISMAR50242.2020.00065&partnerID=40&md5=eef96cc31598451773f458ec1c3fa5c9","Irt B<com, Cesson-Sevigne, France; Ubisoft, Montreuil, France","Martin, N., Irt B<com, Cesson-Sevigne, France; Mathieu, N., Ubisoft, Montreuil, France; Pallamin, N., Irt B<com, Cesson-Sevigne, France; Ragot, M., Irt B<com, Cesson-Sevigne, France; Diverrez, J.-M., Irt B<com, Cesson-Sevigne, France","Virtual Reality (VR) is spreading to the general public but still has a major issue: VR sickness. To take it into consideration and minimize its occurrence, evaluation methods are required. The current methods are mainly based on subjective measurements and therefore have several drawbacks (e.g., non-continuous, intrusive). Physiological signals combined with Machine Learning (ML) methods seem an interesting approach to go beyond these limits. In this paper, we present a large-scale experimentation (103 participants) where physiological data (cardiac and electrodermal activities) and subjective data (perceived VR sickness) were gathered during 30-minute VR video game sessions. Using ML methods, models were trained to predict VR sickness level (based on the physiological data labeled with the subjective data). Results showed an explained variance up to 75% (in a regression approach) and an accuracy up to 91% (in a classification approach). Despite generalization issues, this method seems promising and valuable for a real time, automatic and continuous evaluation of VR sickness, based on physiological signals and ML models. © 2020 IEEE.","Ergonomics; H.1.2 [Models and principles]: User/Machine Systems; Human factors; I.3.6 [Computer graphics]: Methodology and Techniques","Augmented reality; Diseases; E-learning; Machine learning; Physiology; Virtual reality; Classification approach; Electrodermal activity; Evaluation methods; General publics; Large-scale experimentations; Physiological data; Physiological signals; Subjective measurements; Physiological models",Conference Paper,"Final","",Scopus,2-s2.0-85099321186
"Buttussi F., Chittaro L., Valent F.","16229748200;7004119007;6701724324;","A virtual reality methodology for cardiopulmonary resuscitation training with and without a physical mannequin",2020,"Journal of Biomedical Informatics","111",, 103590,"","",,,"10.1016/j.jbi.2020.103590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092735575&doi=10.1016%2fj.jbi.2020.103590&partnerID=40&md5=442deb9abbf65196fa2e27093dd87c14","Human-Computer Interaction Lab, Department of Mathematics, Computer Science, and Physics, University of Udine, Udine, Italy; SOC Istituto di Igiene ed Epidemiologia Clinica, Azienda Sanitaria Universitaria Friuli Centrale, Udine, Italy","Buttussi, F., Human-Computer Interaction Lab, Department of Mathematics, Computer Science, and Physics, University of Udine, Udine, Italy, SOC Istituto di Igiene ed Epidemiologia Clinica, Azienda Sanitaria Universitaria Friuli Centrale, Udine, Italy; Chittaro, L., Human-Computer Interaction Lab, Department of Mathematics, Computer Science, and Physics, University of Udine, Udine, Italy; Valent, F., SOC Istituto di Igiene ed Epidemiologia Clinica, Azienda Sanitaria Universitaria Friuli Centrale, Udine, Italy","Background: Cardiopulmonary resuscitation (CPR) is an emergency procedure that can increase survival after a cardiac arrest. Performing CPR effectively requires both procedural knowledge and manual skills. Traditional CPR training methodology includes lessons led by instructors and supervised practice on mannequins, thus requiring considerable resources. Objective: This paper proposes a new methodology for low-cost CPR training based on virtual reality (VR) with and without the addition of a physical mannequin. Moreover, it describes an experimental evaluation of the methodology that assessed gain in manual skills during training, transfer of procedural knowledge and manual skills in a final assessment, and changes in self-efficacy with three measurements over time (pre-training, post-training, and post-assessment). Methods: We implemented a VR application that supports the proposed methodology, and can thus be used with or without a mannequin. The experimental evaluation involved 30 participants who tried CPR in VR twice, performing two repetitions of 30 chest compressions per trial. Half participants tried the VR application with the mannequin and half without it. Final assessment required all participants to perform CPR on the mannequin without the assistance of VR. To assess self-efficacy, participants filled in a questionnaire at the three times of measurement. Results: Mixed-design ANOVAs showed effects of repetition, effects of group, or interaction between the two variables on manual skills assessed during training. In the final assessment, participants in both groups correctly remembered most of the steps of the procedure. ANOVAs revealed differences between the two groups only in pressure-related skills (better with mannequin) and in the number of wrong steps added to the procedure (better without mannequin). Mixed-design ANOVA showed a self-efficacy increase in both groups after training, which was maintained after final assessment. Conclusions: The proposed VR methodology for CPR training has a positive effect on procedural knowledge, manual skills, and self-efficacy, with as well as without the physical mannequin. Trials on a mannequin are required to understand the correct pressure for chest compression. This supports the adoption of the proposed VR methodology to reduce instructor and mannequin time required to teach CPR to trainees. © 2020 Elsevier Inc.","Cardiopulmonary resuscitation; Experimental evaluation; Mannequin; Medical education; Training; Virtual reality","E-learning; Virtual reality; Cardiopulmonary resuscitation; Chest compressions; Considerable resources; Emergency procedures; Experimental evaluation; Post assessment; Procedural knowledge; VR applications; Resuscitation; adult; advanced life support; analysis of variance; Article; compression; computer simulation; controlled study; emergency health service; female; human; Likert scale; male; methodology; priority journal; protective glasses; questionnaire; resuscitation; skill; training; virtual reality; young adult",Article,"Final","",Scopus,2-s2.0-85092735575
"Pletz C., Zinn B.","57218546432;36770118100;","Evaluation of an immersive virtual learning environment for operator training in mechanical and plant engineering using video analysis",2020,"British Journal of Educational Technology","51","6",,"2159","2179",,1,"10.1111/bjet.13024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089538860&doi=10.1111%2fbjet.13024&partnerID=40&md5=f0549fc6714395b5221f74594500ae9d","Department of Vocational Education focused on Teaching Technology (BPT) at the, University of Stuttgart, Germany; Department of Vocational Education focused on Teaching Technology (BPT), University of Stuttgart, Germany","Pletz, C., Department of Vocational Education focused on Teaching Technology (BPT) at the, University of Stuttgart, Germany; Zinn, B., Department of Vocational Education focused on Teaching Technology (BPT), University of Stuttgart, Germany","A structural evaluation is imperative for developing an effective virtual learning environment. Understanding the extent to which content that has been learned virtually can be applied practically holds particular importance. A group of persons from the technical field of mechanical and plant engineering (N = 13) participated in a virtual operator training for a case application of additive manufacturing. To evaluate the virtual learning environment the participants answered quantitative questionnaires and were asked to apply what they had learned virtually to the real machine. Both the virtual training and testing phase on the real machine were recorded by video (800 minutes in total). The category system resulting from a structured qualitative video analysis with a total of 568 codes contains design-, instruction- and interaction-related optimisation potentials for further development of the virtual learning sequence. Mistakes, difficulties and other anomalies during the application on the real machine provide further revision options. The study uses video data for the first time to derive optimisation potentials and to investigate the learning transfer of virtually learned action knowledge to the real-world activity. © 2020 The Authors. British Journal of Educational Technology published by John Wiley & Sons Ltd on behalf of British Educational Research Association",,"Computer aided instruction; Engineering education; Learning systems; Personnel training; Surveys; Virtual reality; Learning Transfer; Operator training; Plant engineering; Real-world activities; Structural evaluation; Virtual learning; Virtual learning environments; Virtual operators; E-learning",Article,"Final","",Scopus,2-s2.0-85089538860
"Petersen G.B., Klingenberg S., Mayer R.E., Makransky G.","57205735672;57214077924;7403065717;50361371800;","The virtual field trip: Investigating how to optimize immersive virtual learning in climate change education",2020,"British Journal of Educational Technology","51","6",,"2098","2114",,6,"10.1111/bjet.12991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087505916&doi=10.1111%2fbjet.12991&partnerID=40&md5=95f409f7b19bb124fd571c95d712bfb7","Department of Psychology, University of Copenhagen, Denmark; University of Copenhagen, Denmark; University of California, Santa Barbara, United States; Department of Psychology at the University of Copenhagen, Denmark","Petersen, G.B., Department of Psychology, University of Copenhagen, Denmark; Klingenberg, S., University of Copenhagen, Denmark; Mayer, R.E., University of California, Santa Barbara, United States; Makransky, G., Department of Psychology at the University of Copenhagen, Denmark","Immersive Virtual Reality (IVR) is being used for educational virtual field trips (VFTs) involving scenarios that may be too difficult, dangerous or expensive to experience in real life. We implemented an immersive VFT within the investigation phase of an inquiry-based learning (IBL) climate change intervention. Students investigated the consequences of climate change by virtually traveling to Greenland and exploring albedo and greenhouse effects first hand. A total of 102 seventh and eighth grade students were randomly assigned to one of two instructional conditions: (1) narrated pretraining followed by IVR exploration or (2) the same narrated training material integrated within the IVR exploration. Students in both conditions showed significant increases in declarative knowledge, self-efficacy, interest, STEM intentions, outcome expectations and intentions to change behavior from the pre- to post-assessment. However, there was a significant difference between conditions favoring the pretraining group on a transfer test consisting of an oral presentation to a fictitious UN panel. The findings suggest that educators can choose to present important prerequisite learning content before or during a VFT. However, adding pretraining may lead to better transfer test performance, presumably because it helps reduce cognitive load while learning in IVR. © 2020 British Educational Research Association",,"Climate change; Students; Virtual reality; Declarative knowledge; Immersive virtual reality; Inquiry based learning (IBL); Learning contents; Oral presentations; Training material; Virtual field trips; Virtual learning; E-learning",Article,"Final","",Scopus,2-s2.0-85087505916
"Armstrong M., Tsuchiya K., Liang F., Kunze K., Pai Y.S.","57219866273;57194083287;57191504624;21743317500;56267209600;","Multiplex Vision: Understanding Information Transfer and F-Formation with Extended 2-Way FOV",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,,"10.1145/3385956.3418954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095814394&doi=10.1145%2f3385956.3418954&partnerID=40&md5=89c416380a230628f82185e756df2444","Keio University, Graduate School of Media Design, Tokyo, Japan; INNOCC Ignition Point Inc., Tokyo, Japan; University of Auckland, Auckland, New Zealand","Armstrong, M., Keio University, Graduate School of Media Design, Tokyo, Japan; Tsuchiya, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Liang, F., INNOCC Ignition Point Inc., Tokyo, Japan; Kunze, K., Keio University, Graduate School of Media Design, Tokyo, Japan; Pai, Y.S., University of Auckland, Auckland, New Zealand","Research in sociology shows that effective conversation relates to people's spatial and orientational relationship, namely the proxemics (distance, eye contact, synchrony) and the F-formation (orientation and arrangement). In this work, we introduce novel conversational paradigms that effects conventional F-formation by introducing the concept of multi-directional conversation. Multiplex Vision is a head-mounted device capable of providing a 360° field-of-view (FOV) and facilitating multi-user interaction multi-directionally, thereby providing novel methods on how people can interact with each other. We propose 3 possible new forms of interactions from our prototype: one-to-one, one-to-many, and many-to-many. To facilitate them, we manipulate 2 key variables, which are the viewing parameter and the display parameter. To gather feedback for our system, we conducted a study to understand information transfer between various modes, as well as a user study on how different proposed paradigms effect conversation. Finally, we discuss present and future use cases that can benefit from our system. © 2020 ACM.","360 field-of-view; conversation; F-formation; vision augmentation","Sociology; Display parameters; Field of views; Information transfers; Key variables; Multi-user interaction; Novel methods; Orientational relationship; Viewing parameters; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095814394
"Schwarz S., Regal G., Kempf M., Schatz R.","56785127800;55848939000;57204063955;35609979600;","Learning Success in Immersive Virtual Reality Training Environments: Practical Evidence from Automotive Assembly",2020,"ACM International Conference Proceeding Series",,,,"","",,,"10.1145/3419249.3420182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095803774&doi=10.1145%2f3419249.3420182&partnerID=40&md5=eeef469f019ee50a6111f03a2664300c","Austrian Institute of Technology GmbH, Vienna, Austria; Innovation.rocks GmbH, Vienna, Austria","Schwarz, S., Austrian Institute of Technology GmbH, Vienna, Austria; Regal, G., Austrian Institute of Technology GmbH, Vienna, Austria; Kempf, M., Innovation.rocks GmbH, Vienna, Austria; Schatz, R., Austrian Institute of Technology GmbH, Vienna, Austria","Learning success in assembly training using immersive virtual reality technologies depends on multiple factors ranging from quality levels of process and technical documentation, visual 3D rendering quality, maturity of didactic concepts to individual differences and attitudes of workers and trainers. In this paper, we present the results of a field study conducted in an automotive factory to evaluate an immersive virtual reality training environment (VTE). Set up under real training conditions, the VTE was operated by trainers to train novice assembly line workers on a specific task: the assembly of a vehicle center console. Using a between-subject design we compare training performance in terms of skill transfer and retention between workers being trained in the VTE to workers who had been trained conventionally on the physical car. Our results suggest positive transfer of the acquired procedures from the VTE to physical assembly and even performance improvements over time. We discuss learning success in the VTE in contrast to user experience feedback and the implemented sequence steps for mounting assembly parts, based on comprehensive behavioral data. Finally, reflections on trainer feedback lead the way to implications for the adaption of didactic strategies and operational procedures towards increasing overall effectiveness of VTE-supported assembly training. © 2020 ACM.","Automotive Assembly; Learning Transfer; Trainers.; Training; Virtual Reality; Workers","E-learning; Human computer interaction; Three dimensional computer graphics; User experience; Assembly-line workers; Automotive assemblies; Experience feedback; Immersive virtual reality; Individual Differences; Operational procedures; Overall effectiveness; Technical documentations; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85095803774
"Langer F., Milioto A., Haag A., Behley J., Stachniss C.","57219752510;57195937595;57222349912;35207956200;6507517732;","Domain Transfer for Semantic Segmentation of LiDAR Data using Deep Neural Networks",2020,"IEEE International Conference on Intelligent Robots and Systems",,, 9341508,"8263","8270",,,"10.1109/IROS45743.2020.9341508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102402694&doi=10.1109%2fIROS45743.2020.9341508&partnerID=40&md5=9feffa7e1e49f32b023d6caef2da0925","University of Bonn, Germany; Argo.ai, Munich, Germany","Langer, F., University of Bonn, Germany; Milioto, A., University of Bonn, Germany; Haag, A., Argo.ai, Munich, Germany; Behley, J., University of Bonn, Germany; Stachniss, C., University of Bonn, Germany","Inferring semantic information towards an understanding of the surrounding environment is crucial for autonomous vehicles to drive safely. Deep learning-based segmentation methods can infer semantic information directly from laser range data, even in the absence of other sensor modalities such as cameras. In this paper, we address improving the generalization capabilities of such deep learning models to range data that was captured using a different sensor and in situations where no labeled data is available for the new sensor setup. Our approach assists the domain transfer of a LiDAR-only semantic segmentation model to a different sensor and environment exploiting existing geometric mapping systems. To this end, we fuse sequential scans in the source dataset into a dense mesh and render semi-synthetic scans that match those of the target sensor setup. Unlike simulation, this approach provides a real-to-real transfer of geometric information and delivers additionally more accurate remission information. We implemented and thoroughly tested our approach by transferring semantic scans between two different real-world datasets with different sensor setups. Our experiments show that we can improve the segmentation performance substantially with zero manual re-labeling. This approach solves the number one feature request since we released our semantic segmentation library LiDAR-bonnetal [18]. © 2020 IEEE.",,"Agricultural robots; Data communication systems; Deep neural networks; Digital storage; Intelligent robots; Learning systems; Neural networks; Optical radar; Semantic Web; Semantics; Generalization capability; Geometric information; Learning-based segmentation; Real-world datasets; Segmentation performance; Semantic information; Semantic segmentation; Surrounding environment; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85102402694
"Lécuyer F., Gouranton V., Lamercerie A., Reuzeau A., Caillaud B., Arnaldi B.","57209417146;6506588443;57215823769;56741393700;55605009400;6603383416;","Unveiling the implicit knowledge, one scenario at a time",2020,"Visual Computer","36","10-12",,"1951","1963",,,"10.1007/s00371-020-01904-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087744022&doi=10.1007%2fs00371-020-01904-7&partnerID=40&md5=768db087b10889f5bf375586c542d6d8","INSA Rennes, Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Univ Rennes, Inria, CNRS, IRISA, Rennes, France; IRISA, Univ Rennes, Inria, CNRS, IRISA, Rennes, France","Lécuyer, F., INSA Rennes, Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Gouranton, V., INSA Rennes, Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Lamercerie, A., IRISA, Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Reuzeau, A., INSA Rennes, Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Caillaud, B., Univ Rennes, Inria, CNRS, IRISA, Rennes, France; Arnaldi, B., INSA Rennes, Univ Rennes, Inria, CNRS, IRISA, Rennes, France","When defining virtual reality applications with complex procedures, such as medical operations or mechanical assembly or maintenance procedures, the complexity and the variability of the procedures make the definition of the scenario difficult and time-consuming. Indeed, the variability complicates the definition of the scenario by the experts, and its combinatorics demand a comprehension effort for the developer, which is often out of reach. Additionally, the experts have a hard time explaining the procedures with a sufficient level of details, as they usually forget to mention some actions that are, in fact, important for the application. To ease the creation of scenario, we propose a complete methodology, based on (1) an iterative process composed of: (2) the recording of actions in virtual reality to create sequences of actions and (3) the use of mathematical tools that can generate a complete scenario from a few of those sequences, with (4) graphical visualization of the scenarios and complexity indicators. This process helps the expert to determine the sequences that must be recorded to obtain a scenario with the required variability. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Authoring; Generalization; Scenario; Virtual reality","Virtual reality; Complexity indicators; Graphical visualization; Implicit knowledge; Iterative process; Maintenance procedures; Mathematical tools; Mechanical assembly; Medical operations; Iterative methods",Article,"Final","",Scopus,2-s2.0-85087744022
"Tarkkanen K., Lehto A., Oliva D., Somerkoski B., Haavisto T., Luimula M.","25642288100;57208750395;42061986700;56297992300;57219971101;17435411000;","Research study design for teaching and testing fire safety skills with AR and VR Games",2020,"11th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2020 - Proceedings",,, 9237831,"167","172",,,"10.1109/CogInfoCom50765.2020.9237831","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096361007&doi=10.1109%2fCogInfoCom50765.2020.9237831&partnerID=40&md5=44575e2d89999555aca8e9be53dd6969","ICT, Turku University of Applied Sciences, Turku, Finland; RDI Services, Turku University of Applied Sciences, Turku, Finland; University of Turku, Department of Teacher Education, Turku, Finland","Tarkkanen, K., ICT, Turku University of Applied Sciences, Turku, Finland; Lehto, A., RDI Services, Turku University of Applied Sciences, Turku, Finland; Oliva, D., ICT, Turku University of Applied Sciences, Turku, Finland; Somerkoski, B., University of Turku, Department of Teacher Education, Turku, Finland; Haavisto, T., ICT, Turku University of Applied Sciences, Turku, Finland; Luimula, M., ICT, Turku University of Applied Sciences, Turku, Finland","Virtual and augmented reality (VR AR) games can provide innovative methods for teaching and learning important skills relating to fire safety. However, in an emergency context, testing the acquired knowledge and skills, i.e. verifying the learning, can be challenging. In this paper, we ask how the interplay between AR and VR could support learning verification. We describe two standalone games of both types, which interchangeably teach fire safety skills to children and verify their learning results. In particular, we describe the planned learning paths and research study designs for verification studies within and between these games to answer the above question. By operationalizing the two cases, the paper ends in proposing more generalized study design for AR and VR research in a fire safety context. © 2020 IEEE.","Augmented reality; Fire safety; Research design; Serious games; Virtual reality","Augmented reality; Fires; Emergency contexts; Innovative method; Planned learning; Research study design; Study design; Support learning; Teaching and learning; Virtual and augmented reality; Safety testing",Conference Paper,"Final","",Scopus,2-s2.0-85096361007
"Kaarlela T., Pieska S., Pitkaaho T.","57217675359;6507168349;36184573800;","Digital twin and virtual reality for safety training",2020,"11th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2020 - Proceedings",,, 9237812,"115","120",,,"10.1109/CogInfoCom50765.2020.9237812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096353792&doi=10.1109%2fCogInfoCom50765.2020.9237812&partnerID=40&md5=c6e4f43dd18d48bdcc09406601905405","Centria University of Applied Sciences, Ylivieska, Finland","Kaarlela, T., Centria University of Applied Sciences, Ylivieska, Finland; Pieska, S., Centria University of Applied Sciences, Ylivieska, Finland; Pitkaaho, T., Centria University of Applied Sciences, Ylivieska, Finland","In this paper, our latest research related to digital twins and virtual reality environments for safety training purposes will be described and evaluated. We will present three practical use cases to outline the current maturity level of virtual reality technology for industrial environments. Two of our use cases are virtual reality applications for safety and emergency training scenarios. In addition, one use case is the implementation of a digital twin for off-site safety training. This use case presents a seamless real-time transfer of data between the physical and virtual worlds. Use cases were developed based on the needs of, and in co-operation with, local small and medium-sized enterprises (SMEs). The proposed affordable and simple approaches provide virtual safety training solutions that can be utilized by SMEs of different industries. © 2020 IEEE.",,"Accident prevention; Data transfer; Digital twin; E-learning; Emergency training; Industrial environments; Maturity levels; Real-time transfer; Simple approach; Small and medium-sized enterprise; Virtual reality technology; Virtual-reality environment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85096353792
"Torda A.","55915596000;","CLASSIE teaching - Using virtual reality to incorporate medical ethics into clinical decision making",2020,"BMC Medical Education","20","1", 326,"","",,1,"10.1186/s12909-020-02217-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091550365&doi=10.1186%2fs12909-020-02217-y&partnerID=40&md5=a77d40ab6a43ee32177fa090d7265519","Faculty of Medicine, UNSW Sydney, Kensington, NSW  2052, Australia","Torda, A., Faculty of Medicine, UNSW Sydney, Kensington, NSW  2052, Australia","Background: Teaching medical ethics (ME) in the clinical environment is often difficult, uncalibrated and medical students get variable exposure to skilled educators. Explicit discussion of ethical dimensions of patient management is often neglected, as clinical teachers may feel inadequately skilled to do this. Methods: We developed a suite of online modules. Each consisted of a clinical scenario filmed using virtual reality (VR) technology, linked to an adaptive, interactive, online tutorial which explicitly discussed the relevant ethical issues and guidelines. These were embedded in clinical placements of students to encourage the transfer of knowledge from these modules to clinical skill competency. We conducted a pilot study to evaluate these modules which examined student engagement, knowledge gains (self-perceived and measured) and user experience. We also reviewed reflections to assess the incorporation of these modules and transfer of knowledge into the clinical learning and skill development of the students. Results: Engagement and self-perceived knowledge gains were extremely high. Students found these modules realistic, interesting and helpful. The measured knowledge gains (module exit quiz) were moderate. User experience was positive overall, although students were intolerant of any technical glitches. There was mixed feedback on whether the VR aspect of the clinical scenarios added value. Student reflections showed high level incorporation of these modules into clinical practice of the students and evidence of knowledge transfer (level 3 Kirkpatrick model of evaluation) in over ¾ of students. Conclusions: This study showed that the use VR clinical scenarios combined with interactive online learning modules resulted in demonstrable high-level student engagement and learning gains in medical ethics and transfer of knowledge to clinical application. It standardised and ensured the student experience of high-quality educational deliverables in clinical years of medical education. This use of VR and online technology can be adapted for use in many areas of the medical curricula where we need to ensure the delivery of well calibrated, high quality, educational deliverables at scale for students. © 2020 The Author(s).",,"article; clinical decision making; e-learning; human; human experiment; medical education; medical ethics; pilot study; practice guideline; skill; virtual reality",Article,"Final","",Scopus,2-s2.0-85091550365
"Mitchell T.J., Jones A.J., O'Connor M.B., Wonnacott M.D., Glowacki D.R., Hyde J.","8714214700;57209268511;56817180000;57209237066;16424472800;55322921100;","Towards molecular musical instruments: Interactive sonifications of 17-Alanine, graphene and carbon nanotubes",2020,"ACM International Conference Proceeding Series",,,,"214","221",,,"10.1145/3411109.3411143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092203603&doi=10.1145%2f3411109.3411143&partnerID=40&md5=83381831d05a23d7141d158078911641","Ct Lab, University of the West of England, Bristol, United Kingdom; Intangible Realities Laboratory, University of Bristol, United Kingdom; School of Music and Performing Arts, Bath Spa University, United Kingdom","Mitchell, T.J., Ct Lab, University of the West of England, Bristol, United Kingdom; Jones, A.J., Intangible Realities Laboratory, University of Bristol, United Kingdom; O'Connor, M.B., Intangible Realities Laboratory, University of Bristol, United Kingdom; Wonnacott, M.D., Intangible Realities Laboratory, University of Bristol, United Kingdom; Glowacki, D.R., Intangible Realities Laboratory, University of Bristol, United Kingdom; Hyde, J., School of Music and Performing Arts, Bath Spa University, United Kingdom","Scientists increasingly rely on computational models of atoms and molecules to observe, understand and make predictions about the microscopic world. Atoms and molecules are in constant motion, with vibrations and structural fluctuations occurring at very short time-scales and corresponding length-scales. But can these microscopic oscillations be converted into sound? And, what would they sound like? In this paper we present our initial steps towards a generalised approach for sonifying data produced by a real-Time molecular dynamics simulation. The approach uses scanned synthesis to translate real-Time geometric simulation data into audio. The process is embedded within a stand alone application as well as a variety of audio plugin formats to enable the process to be used as an audio synthesis method for music making. We review the relevant background literature before providing an overview of our system. Simulations of three molecules are then considered: 17-Alanine, graphene and a carbon nanotube. Four examples are then provided demonstrating how the technique maps molecular features and parameters onto the auditory character of the resulting sound. A case study is then provided in which the sonification/synthesis method is used within a musical composition. © 2020 ACM.","augmented reality; game audio; musicology; sonic interaction design; sonification; sound art; spatial audio; virtual reality","Carbon nanotubes; Graphene; Molecular dynamics; Molecules; Computational model; Geometric simulation; Molecular dynamics simulations; Molecular feature; Musical composition; Short time scale; Standalone applications; Structural fluctuations; Audio acoustics",Conference Paper,"Final","",Scopus,2-s2.0-85092203603
"Lu C., Ghoman S.K., Cutumisu M., Schmölzer G.M.","57211914244;57209659059;57203017694;6602638022;","Unsupervised Machine Learning Algorithms Examine Healthcare Providers' Perceptions and Longitudinal Performance in a Digital Neonatal Resuscitation Simulator",2020,"Frontiers in Pediatrics","8",, 544,"","",,1,"10.3389/fped.2020.00544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091597216&doi=10.3389%2ffped.2020.00544&partnerID=40&md5=80092a8331fd41d6014b610d129c1207","Department of Educational Psychology, Faculty of Education, Centre for Research in Applied Measurement and Evaluation, University of Alberta, EdmontonAB, Canada; Centre for the Studies of Asphyxia and Resuscitation, Neonatal Research Unit, Royal Alexandra Hospital, Edmonton, AB, Canada; Department of Pediatrics, Faculty of Medicine and Dentistry, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, Faculty of Science, University of Alberta, Edmonton, AB, Canada","Lu, C., Department of Educational Psychology, Faculty of Education, Centre for Research in Applied Measurement and Evaluation, University of Alberta, EdmontonAB, Canada; Ghoman, S.K., Centre for the Studies of Asphyxia and Resuscitation, Neonatal Research Unit, Royal Alexandra Hospital, Edmonton, AB, Canada, Department of Pediatrics, Faculty of Medicine and Dentistry, University of Alberta, Edmonton, AB, Canada; Cutumisu, M., Department of Educational Psychology, Faculty of Education, Centre for Research in Applied Measurement and Evaluation, University of Alberta, EdmontonAB, Canada, Centre for the Studies of Asphyxia and Resuscitation, Neonatal Research Unit, Royal Alexandra Hospital, Edmonton, AB, Canada, Department of Computing Science, Faculty of Science, University of Alberta, Edmonton, AB, Canada; Schmölzer, G.M., Centre for the Studies of Asphyxia and Resuscitation, Neonatal Research Unit, Royal Alexandra Hospital, Edmonton, AB, Canada, Department of Pediatrics, Faculty of Medicine and Dentistry, University of Alberta, Edmonton, AB, Canada","Background: Frequent simulation-based education is recommended to improve health outcomes during neonatal resuscitation but is often inaccessible due to time, resource, and personnel requirements. Digital simulation presents a potential alternative; however, its effectiveness and reception by healthcare professionals (HCPs) remains largely unexplored. Objectives: This study explores HCPs' attitudes toward a digital simulator, technology, and mindset to elucidate their effects on neonatal resuscitation performance in simulation-based assessments. Methods: The study was conducted from April to August 2019 with 2-month (June–October 2019) and 5-month (September 2019–January 2020) follow-up at a tertiary perinatal center in Edmonton, Canada. Of 300 available neonatal HCPs, 50 participated. Participants completed a demographic survey, a pretest, two practice scenarios using the RETAIN neonatal resuscitation digital simulation, a posttest, and an attitudinal survey (100% response rate). Participants repeated the posttest scenario in 2 months (86% response rate) and completed another posttest scenario using a low-fidelity, tabletop simulator (80% response rate) 5 months after the initial study intervention. Participants' survey responses were collected to measure attitudes toward digital simulation and technology. Knowledge was assessed at baseline (pretest), acquisition (posttest), retention (2-month posttest), and transfer (5-month posttest). Results: Fifty neonatal HCPs participated in this study (44 females and 6 males; 27 nurses, 3 nurse practitioners, 14 respiratory therapists, and 6 doctors). Most participants reported technology in medical education as useful and beneficial. Three attitudinal clusters were identified by a hierarchical clustering algorithm based on survey responses. Although participants exhibited diverse attitudinal paths, they all improved neonatal resuscitation performance after using the digital simulator and successfully transferred their knowledge to a new medium. Conclusions: Digital simulation improved HCPs' neonatal resuscitation performance. Medical education may benefit by incorporating technology during simulation training. © Copyright © 2020 Lu, Ghoman, Cutumisu and Schmölzer.","digital simulator; education; medical education; resuscitation; serious games; simulation; table-top simulator; training","Article; attitude to health; construct validity; female; follow up; health care personnel; hierarchical clustering; human; male; medical education; newborn; nurse; nurse practitioner; perception; physician; psychometry; respiratory therapist; resuscitation; simulation training; technology; unsupervised machine learning",Article,"Final","",Scopus,2-s2.0-85091597216
"Tychkov A.Y., Grachev A.V., Alimuradov A.K.","36731632400;56982633900;56595170300;","Virtual Reality in Information Transfer",2020,"Proceedings - 2020 International Russian Automation Conference, RusAutoCon 2020",,, 9208101,"826","830",,,"10.1109/RusAutoCon49822.2020.9208101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093936152&doi=10.1109%2fRusAutoCon49822.2020.9208101&partnerID=40&md5=b588ffda22ded34bf60a388b377b648f","Penza State University, Research Institute for Basic and Applied Studies, Penza, Russian Federation; Penza State University, Department of Radioengineering and Radioelectronic Systems, Penza, Russian Federation; Penza State University, Student Research and Production Business Incubator, Penza, Russian Federation","Tychkov, A.Y., Penza State University, Research Institute for Basic and Applied Studies, Penza, Russian Federation; Grachev, A.V., Penza State University, Department of Radioengineering and Radioelectronic Systems, Penza, Russian Federation; Alimuradov, A.K., Penza State University, Student Research and Production Business Incubator, Penza, Russian Federation","The article is devoted to technologies and virtual reality systems as comprehensive solutions for immersion of the user into virtual reality using specialized devices and interfaces. Technologies for wire/wireless transfer of audio-visual and parametric information in virtual reality systems are discussed. The aim of the article is to analyze and summarize advantages and disadvantages of modern facilities for multimedia and parametric information transfer used in virtual reality systems. A search method for research materials in Russian and international journals included in scientific citation databases was used. The paper analyzes the features (advantages and disadvantages) of using virtual reality in conditions of optical information transfer, wireless protocols (WiFi, Bluetooth, Wireless USB, LIDAR, ZigBee), and wire interfaces (Display Port, HDMI, USB) that provide user communication with virtual reality system. Virtual reality forms a new artificial real world transferred to the user via various wire/wireless (WiGig (802.11ad), WiFi 6 (802.11ax), WiHD (802.15.3c), and Display Port 2.0) interfaces, taking into account physiological, physical and psychometric indicators. Modern technical solutions should give impetus to the creation of adaptive virtual reality with a total immersion effect, when the user cannot distinguish a virtual world from real events. © 2020 IEEE.","adaptive virtual reality; tactile interfaces; wireless/wire information transfer","Audio systems; IEEE Standards; Optical communication; Optical radar; System buses; Virtual reality; Wi-Fi; Wire; Wireless local area networks (WLAN); Information transfers; International journals; Optical information; Parametric information; Technical solutions; User communication; Virtual reality system; Wireless protocol; Data communication systems",Conference Paper,"Final","",Scopus,2-s2.0-85093936152
"Catal C., Akbulut A., Tunali B., Ulug E., Ozturk E.","22633325800;25960607500;57211942369;57211949025;57211945039;","Evaluation of augmented reality technology for the design of an evacuation training game",2020,"Virtual Reality","24","3",,"359","368",,2,"10.1007/s10055-019-00410-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075481004&doi=10.1007%2fs10055-019-00410-z&partnerID=40&md5=1f2c3d6562a0c05b9c1b4538db9f7821","Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey","Catal, C., Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; Akbulut, A., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Tunali, B., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Ulug, E., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey; Ozturk, E., Department of Computer Engineering, Istanbul Kultur University, Istanbul, Turkey","Building evacuation training systems and training employees in an organization have a vital role in emergency cases in which people need to know what to do exactly. In every building, procedures, rules, and actions are attractively shown on the walls, but most of the people living in that building are not aware of these procedures and do not have any experience what to do in these dangerous situations. In order to be able to apply these procedures properly in an emergency situation, community members should be trained with the state-of-the-art equipment and technologies, but to do so, up-front investment and development of such a system are necessary. In this study, augmented reality (AR) technology was applied to realize a game-based evacuation training system that implements gamification practices. The architectural plans of a university were used to model the floors and the relevant environment. Employees are trained to learn how to reach the nearest exit location in the event of a fire or earthquake, and also, the system provides the shortest path for the evacuation. In addition to these features, our training game has educational animations about the fire, chemical attack, and earthquake events. A mobile application was implemented to train employees working in the building and inform them to know how to escape in an emergency situation. The technology acceptance model and the related questionnaire form were applied, and the response of 36 participants was analyzed. It was demonstrated that AR and relevant tools provide a flexible environment to develop evacuation systems in a university, our mobile application enabled participants to be trained in a realistic environment, and trainees were highly satisfied with the system. Educational animations were also another benefit for the trainees. © 2019, The Author(s).","Animation; ARKit framework; Augmented reality; Evacuation training system; Game engine; Software; Training; Unity3D","Animation; Augmented reality; Chemical attack; Computer software; Earthquakes; Investments; Mobile computing; Technology transfer; ARKit framework; Augmented reality technology; Game Engine; Realistic environments; State-of-the-art equipments; Technology acceptance model; Training Systems; Unity3d; Personnel training",Article,"Final","",Scopus,2-s2.0-85075481004
"Chen J., Liu C., Chang R., Gui P., Na S.","23090323300;57220998631;57219899921;57212382631;57200312577;","From traditional to vr-based online education platforms: A model of the mechanism influencing user migration",2020,"Information (Switzerland)","11","9", 423,"1","19",,,"10.3390/info11090423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097974976&doi=10.3390%2finfo11090423&partnerID=40&md5=2014b0cd71a30650376d848607f8a5be","School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea","Chen, J., School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea; Liu, C., School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea; Chang, R., School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea; Gui, P., School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea; Na, S., School of Business Administration, Wonkwang University, 460 Iksandae-ro, Iksan, Jeonbuk  54538, South Korea","VR technology can help create optimal virtual learning spaces. Such spaces offer new visual experiences that break through the limitations of time and space and greatly stimulate people’s imagination and creativity in learning. Currently, the bandwidth required for such spaces limits the large-scale application of virtual reality (VR) technology for this purpose. With the large-scale deployment and application of high-speed networks, however, online education platforms based on VR technology will be better able to meet the diversified and personalized learning needs of learners. To promote the development and popularization of new online education platforms based on VR, the factors influencing the migration of learners from traditional online education platforms to new platforms need to be understood more clearly. A model based on the theory of negative, positive, and anchoring effects can explain learners’ migration behavior in this connection. To this end, a structural equation model based on the PLS variance algorithm was used to analyze data obtained through offline and online questionnaires. It was found that in terms of “negative effects”, the afunction and loyalty associated with traditional online education platforms reduced learners’ willingness to migrate to new platforms based on VR technology. In terms of “positive effects”, the novel interactivity and personalization brought by the new platform increased the willingness of users of traditional platforms to migrate to new platforms. In terms of “anchoring effects”, the system quality and relationship quality of learners’ use of traditional online education platforms, as well as the transfer costs associated with the new platform, generated learners’ risk perception about platform migration. In addition, risk perception not only negatively affects learners’ migration to the new platforms, but also strengthens their cognition of the system quality and relationship quality of the traditional platforms, while reducing their interactive awareness of those platforms. Therefore, by adjusting the psychological component of virtual learning, the online education platforms based on VR technology can create high-quality platforms migrating from traditional platforms. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Migration mechanism; Online education; VR technology","Behavioral research; Electronic assessment; HIgh speed networks; Learning systems; Risk perception; Surveys; Anchoring effects; Large-scale applications; Large-scale deployment; Online questionnaire; Personalized learning; Relationship qualities; Structural equation modeling; Visual experiences; E-learning",Article,"Final","",Scopus,2-s2.0-85097974976
"González-García S., Rodríguez-Arce J., Loreto-Gómez G., Montaño-Serrano V.M.","57203662096;57215433451;57203661947;57203658489;","Teaching forward kinematics in a robotics course using simulations: transfer to a real-world context using LEGO mindstorms™",2020,"International Journal on Interactive Design and Manufacturing","14","3",,"773","787",,2,"10.1007/s12008-020-00670-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088816903&doi=10.1007%2fs12008-020-00670-z&partnerID=40&md5=0c09205d21ce654782362261f2399b2e","Tecnologico de Monterrey, Ave. Eugenio Garza Sada 2501, Monterrey, NL  64849, Mexico; Universidad Autonoma del Estado de México, Toluca, Edo De Mexico, Mexico; Instituto Tecnologico Superior de Uruapan, Michoacan, Mexico","González-García, S., Tecnologico de Monterrey, Ave. Eugenio Garza Sada 2501, Monterrey, NL  64849, Mexico; Rodríguez-Arce, J., Universidad Autonoma del Estado de México, Toluca, Edo De Mexico, Mexico; Loreto-Gómez, G., Instituto Tecnologico Superior de Uruapan, Michoacan, Mexico; Montaño-Serrano, V.M., Universidad Autonoma del Estado de México, Toluca, Edo De Mexico, Mexico","Previous studies show that traditional teaching methods such as oral explanations and PowerPoint presentations can be complemented with the use of computer simulations in problem-solving sessions to teach robotics. Nevertheless, these previous works rest upon the assumption that the knowledge that is learned in virtual laboratories transfers to its equivalent real task. The main contribution of this work is to validate that the knowledge obtained by students in an undergraduate robotics course using computer simulations of 3D model robots can be applied to its real-world context. The experimental platform used to run the computer simulations was based on the Simscape Multibody library of the MATLAB™ software. Results show a satisfactory knowledge transfer because more than 75% of the students finished a forward kinematics problem using a physical manipulator built with LEGO Mindstorms™. In addition, the observation analysis of the professor reveals that students reinforced their knowledge previously learned, and their problem-solving and critical thinking skills also improved. In this way, this study demonstrates that the proposed methodology not only helps the transfer of knowledge to the real-world context but also helps to develop student competencies. Future work is needed to replicate the findings in other robotics topics, such as the generation of trajectories, the analysis of the dynamics of manipulators, and the design of various controllers for the actuators of the robot. © 2020, Springer-Verlag France SAS, part of Springer Nature.","Computer simulations; Educational innovation; Educational platforms; Teaching strategy","3D modeling; Educational robots; Kinematics; Knowledge management; Machine design; MATLAB; Robotics; Robots; Students; Teaching; Critical thinking skills; Experimental platform; Forward kinematics problem; Power Point presentations; Problem-solving sessions; Student competencies; Transfer of knowledge; Virtual laboratories; Manipulators",Article,"Final","",Scopus,2-s2.0-85088816903
"Rodríguez-Martín M., Vergara D., Rodríguez-Gonzálvez P.","56400678600;57211856936;25650877600;","Simulation of a real call for research projects as activity to acquire research skills: Perception analysis of teacher candidates",2020,"Sustainability (Switzerland)","12","18", 7431,"","",,1,"10.3390/SU12187431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091657691&doi=10.3390%2fSU12187431&partnerID=40&md5=d9ed92dca50b750f20b185f57cb8b211","Technological Department, Catholic University of Avila (UCAV), Canteros, Ávila, 05005, Spain; Department of Mechanical Engineering, University of Salamanca, Av. Fernando Ballesteros, 0, Béjar, Salamanca, 37700, Spain; Department of Cartographic and Land Engineering, University of Salamanca, University of León, Avda, Astorga, Ponferrada, 24401, Spain","Rodríguez-Martín, M., Technological Department, Catholic University of Avila (UCAV), Canteros, Ávila, 05005, Spain, Department of Mechanical Engineering, University of Salamanca, Av. Fernando Ballesteros, 0, Béjar, Salamanca, 37700, Spain; Vergara, D., Technological Department, Catholic University of Avila (UCAV), Canteros, Ávila, 05005, Spain; Rodríguez-Gonzálvez, P., Department of Cartographic and Land Engineering, University of Salamanca, University of León, Avda, Astorga, Ponferrada, 24401, Spain","In this research, a novel methodology based on the simulation of a call for research projects was applied for the training of STEM secondary school teachers, with results raised and analyzed to determine the response of the students to this new methodology. The activity was applied in the same course during two academic years with student groups from very different teaching specialties such as mathematics, physics and chemistry, biology and geology, technology and health processes who were studying the Master's Degree in Secondary Education, specifically, the 3 European Credit Transfer and Accumulation System (ECTS) course of Initiation to Educational Research (IER), this Master's course being mandatory for working as a secondary professor. The Master's students are asked to write their own research project proposals for a fictitious call on a topic freely chosen by them, which might have been related to the research line of the final Master's thesis. In it, they had to propose all the contents studied in the course (such as writing a brief state of the art, establishing a research team, setting objectives, a description of the methodology for educational research, instruments, a plan for the dissemination of the results, the needed resources, etc.). The students' perceptions of the usefulness and reality of what they had learned for their professional development and for writing their final theses were assessed. The results based on the perceptions of the students demonstrate that the activity had been useful for assimilating concepts related to educational research in the context of secondary education (research skills), which will be useful for improving the critical sense of the students (teacher candidates) and for their professional future in the context of applied research in day-to-day secondary teacher activities. Furthermore, the results show the activity was useful for the development of the final Master's thesis. The difficult aspects that the activity presented for them were analyzed. The results were statistically compared for the students of the different specialties, deducing, in all cases, a homogeneous good acceptance with slight differences between them. © 2020 by the authors.","Curriculum development; Educational innovation; Quality education; Research project; Science learning; Secondary school; Teacher training","curriculum; education; educational attainment; perception; secondary education; student; teacher training; teaching",Article,"Final","",Scopus,2-s2.0-85091657691
"Allevato A.D., Schaertl Short E., Pryor M., Thomaz A.L.","57194037735;35957168900;7004159251;13007283900;","Iterative residual tuning for system identification and sim-to-real robot learning",2020,"Autonomous Robots","44","7",,"1167","1182",,,"10.1007/s10514-020-09925-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087301119&doi=10.1007%2fs10514-020-09925-w&partnerID=40&md5=9fa42e973cbb6ae699f4fe2b38f8e8bb","The University of Texas at Austin, Austin, TX  78712, United States; Tufts University, Medford, MA  02155, United States","Allevato, A.D., The University of Texas at Austin, Austin, TX  78712, United States; Schaertl Short, E., The University of Texas at Austin, Austin, TX  78712, United States, Tufts University, Medford, MA  02155, United States; Pryor, M., The University of Texas at Austin, Austin, TX  78712, United States; Thomaz, A.L., The University of Texas at Austin, Austin, TX  78712, United States","Robots are increasingly learning complex skills in simulation, increasing the need for realistic simulation environments. Existing techniques for approximating real-world physics with a simulation require extensive observation data and/or thousands of simulation samples. This paper presents iterative residual tuning (IRT), a deep learning system identification technique that modifies a simulator’s parameters to better match reality using minimal real-world observations. IRT learns to estimate the parameter difference between two parameterized models, allowing repeated iterations to converge on the true parameters similarly to gradient descent. In this paper, we develop and analyze IRT in depth, including its similarities and differences with gradient descent. Our IRT implementation, TuneNet, is pre-trained via supervised learning over an auto-generated simulated dataset. We show that TuneNet can perform rapid, efficient system identification even when the true parameter values lie well outside those in the network’s training data, and can also learn real-world parameter values from visual data. We apply TuneNet to a sim-to-real task transfer experiment, allowing a robot to perform a dynamic manipulation task with a new object after a single observation. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Physics prediction; Sim-to-real transfer; Simulation; System identification","Agricultural robots; Deep learning; Educational robots; Gradient methods; Parameter estimation; Religious buildings; Robots; Scattering parameters; Gradient descent; Manipulation task; Observation data; Parameterized model; Realistic simulation; Task transfer; Training data; Visual data; Learning systems",Article,"Final","",Scopus,2-s2.0-85087301119
"Leijte E., De Blaauw I., Rosman C., Botden S.M.B.I.","57202080193;6701566895;6602590209;16067890000;","Assessment of validity evidence for the RobotiX robot assisted surgery simulator on advanced suturing tasks",2020,"BMC Surgery","20","1", 183,"","",,,"10.1186/s12893-020-00839-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089610556&doi=10.1186%2fs12893-020-00839-z&partnerID=40&md5=97b28548e1768260646ddc9c19844714","Department of Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands; Department of Pediatric Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands","Leijte, E., Department of Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands, Department of Pediatric Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands; De Blaauw, I., Department of Pediatric Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands; Rosman, C., Department of Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands; Botden, S.M.B.I., Department of Pediatric Surgery, Radboud University Medical Center, Geert grooteplein 10 route 618, Nijmegen, 6500HB, Netherlands","Background: Robot assisted surgery has expanded considerably in the past years. Compared to conventional open or laparoscopic surgery, virtual reality (VR) training is an essential component in learning robot assisted surgery. However, for tasks to be implemented in a curriculum, the levels of validity should be studied for proficiency-based training. Therefore, this study was aimed to assess the validity evidence of advanced suturing tasks on a robot assisted VR simulator. Method: Participants were voluntary recruited and divided in the robotic experienced, laparoscopic experienced or novice group, based on self-reported surgical experience. Subsequently, a questionnaire on a five-point Likert scale was completed to assess the content validity. Three component tasks of complex suturing were performed on the RobotiX simulator (Task1: tilted plane needle transfer, Task: 2 intracorporal suturing, Task 3: anastomosis needle transfer). Accordingly, the outcome of the parameters was used to assess construct validity between robotic experienced and novice participants. Composite scores (0-100) were calculated from the construct parameters and corresponding pass/fail scores with false positive (FP) and false negative (FN) percentages. Results: Fifteen robotic experienced, 26 laparoscopic experienced and 29 novices were recruited. Overall content validity outcomes were scored positively on the realism (mean 3.7), didactic value (mean 4.0) and usability (mean 4.2). Robotic experienced participants significantly outperformed novices and laparoscopic experienced participants on multiple parameters on all three tasks of complex suturing. Parameters showing construct validity mainly consisted of movement parameters, needle precision and task completion time. Calculated composite pass/fail scores between robotic experienced and novice participants resulted for Task 1 in 73/100 (FP 21%, FN 5%), Task 2 in 85/100 (FP 28%, FN 4%) and Task 3 in 64/100 (FP 49%, FN 22%). Conclusion: This study assessed the validity evidence on multiple levels of the three studied tasks. The participants score the RobotiX good on the content validity level. The composite pass/fail scores of Tasks 1 and 2 allow for proficiency-based training and could be implemented in a robot assisted surgery training curriculum. © 2020 The Author(s).","Proficiency based training; Robotic surgery; Validation; Virtual reality simulation","adult; clinical competence; computer simulation; curriculum; devices; female; human; laparoscopy; male; middle aged; procedures; questionnaire; robot assisted surgery; suture technique; young adult; Adult; Clinical Competence; Computer Simulation; Curriculum; Female; Humans; Laparoscopy; Male; Middle Aged; Robotic Surgical Procedures; Surveys and Questionnaires; Suture Techniques; Young Adult",Article,"Final","",Scopus,2-s2.0-85089610556
"Lamb R., Etopio E.A.","36634407300;55377820300;","Virtual Reality: a Tool for Preservice Science Teachers to Put Theory into Practice",2020,"Journal of Science Education and Technology","29","4",,"573","585",,1,"10.1007/s10956-020-09837-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086093371&doi=10.1007%2fs10956-020-09837-5&partnerID=40&md5=cd13439157dd46a931e0208a267b9983","Neurocognition Science Laboratory, East Carolina University, Greenville, NC  27858, United States; University at Buffalo, Buffalo, NY, United States","Lamb, R., Neurocognition Science Laboratory, East Carolina University, Greenville, NC  27858, United States; Etopio, E.A., University at Buffalo, Buffalo, NY, United States","The purpose of the present study was to investigate, compare, and characterize interactive VR-based preservice science teacher clinical teaching environments with those of real-life teaching environments. Fifty-four college-aged students were assigned randomly to either real-life conditions or VR conditions. The main effect of the VR condition versus real-life was not statistically significant in terms of the retrospective engagement survey, psychological measures, and composite neuroimaging. This finding suggests that use of VR, in terms of the realism of the environment for the preservice science teachers allowed them to learn from modeled real-life situations for transfer of skills from VR to classroom use. © 2020, Springer Nature B.V.","Methods courses; Professional learning; Science education; Teacher preparation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85086093371
"Leijte E., de Blaauw I., Van Workum F., Rosman C., Botden S.","57202080193;6701566895;56782945600;6602590209;16067890000;","Robot assisted versus laparoscopic suturing learning curve in a simulated setting",2020,"Surgical Endoscopy","34","8",,"3679","3689",,7,"10.1007/s00464-019-07263-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075295921&doi=10.1007%2fs00464-019-07263-2&partnerID=40&md5=0ae3b75ad54a7193c55d7c73f4b9ddd8","Department of Pediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Department of Surgery, Radboud University Medical Centre, Nijmegen, Netherlands","Leijte, E., Department of Pediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; de Blaauw, I., Department of Pediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Van Workum, F., Department of Surgery, Radboud University Medical Centre, Nijmegen, Netherlands; Rosman, C., Department of Surgery, Radboud University Medical Centre, Nijmegen, Netherlands; Botden, S., Department of Pediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands","Background: Compared to conventional laparoscopy, robot assisted surgery is expected to have most potential in difficult areas and demanding technical skills like minimally invasive suturing. This study was performed to identify the differences in the learning curves of laparoscopic versus robot assisted suturing. Method: Novice participants performed three suturing tasks on the EoSim laparoscopic augmented reality simulator or the RobotiX robot assisted virtual reality simulator. Each participant performed an intracorporeal suturing task, a tilted plane needle transfer task and an anastomosis needle transfer task. To complete the learning curve, all tasks were repeated up to twenty repetitions or until a time plateau was reached. Clinically relevant and comparable parameters regarding time, movements and safety were recorded. Intracorporeal suturing time and cumulative sum analysis was used to compare the learning curves and phases. Results: Seventeen participants completed the learning curve laparoscopically and 30 robot assisted. Median first knot suturing time was 611 s (s) for laparoscopic versus 251 s for robot assisted (p < 0.001), and this was 324 s versus 165 (sixth knot, p < 0.001) and 257 s and 149 s (eleventh knot, p < 0.001) respectively on base of the found learning phases. The percentage of ‘adequate surgical knots’ was higher in the laparoscopic than in the robot assisted group. First knot: 71% versus 60%, sixth knot: 100% versus 83%, and eleventh knot: 100% versus 73%. When assessing the ‘instrument out of view’ parameter, the robot assisted group scored a median of 0% after repetition four. In the laparoscopic group, the instrument out of view increased from 3.1 to 3.9% (left) and from 3.0 to 4.1% (right) between the first and eleventh knot (p > 0.05). Conclusion: The learning curve of minimally invasive suturing shows a shorter task time curve using robotic assistance compared to the laparoscopic curve. However, laparoscopic outcomes show good end results with rapid outcome improvement. © 2019, The Author(s).","Laparoscopy training; Learning curve; Robotics training; Simulation","adult; anastomosis; Article; augmented reality; clinical article; clinical protocol; comparative effectiveness; female; human; laparoscopic surgery; learning curve; male; middle aged; operation duration; priority journal; robot assisted surgery; simulation training; suture technique; virtual reality",Article,"Final","",Scopus,2-s2.0-85075295921
"Colombo M., Dolhasz A., Harvey C.","57220106721;57193614682;48761392600;","A Computer Vision Inspired Automatic Acoustic Material Tagging System for Virtual Environments",2020,"IEEE Conference on Computatonal Intelligence and Games, CIG","2020-August",, 9231689,"736","739",,,"10.1109/CoG47356.2020.9231689","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096928895&doi=10.1109%2fCoG47356.2020.9231689&partnerID=40&md5=5ddcf3873d31b91b857e59df2851414c","Birmingham City University, Dmt Lab, United Kingdom","Colombo, M., Birmingham City University, Dmt Lab, United Kingdom; Dolhasz, A., Birmingham City University, Dmt Lab, United Kingdom; Harvey, C., Birmingham City University, Dmt Lab, United Kingdom","This paper presents the ongoing work on an approach to material information retrieval in virtual environments (VEs). Our approach uses convolutional neural networks to classify materials by performing semantic segmentation on images captured in the VE. Class maps obtained are then re-projected onto the environment. We use transfer learning and fine-tune a pretrained segmentation model on images captured in our VEs. The geometry and semantic information can then be used to create mappings between objects in the VE and acoustic absorption coefficients. This can then be input for physically-based audio renderers, allowing a significant reduction in manual material tagging. © 2020 IEEE.","acoustic applications; games; machine vision; rendering (computer graphics); semantic networks","Computer vision; Convolutional neural networks; Image segmentation; Semantics; Transfer learning; Acoustic absorption coefficients; Acoustic materials; Material information; Physically based; Segmentation models; Semantic information; Semantic segmentation; Tagging systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85096928895
"Lognoul M., Nasello J., Triffaux J.-M.","57192315139;57202678573;6602998181;","Virtual reality exposure therapy for post-traumatic stress disorders, obsessive-compulsive disorders and anxiety disorders: Indications, added value and limitations [La thérapie par exposition en réalité virtuelle pour les états de stress post-traumatiques, les troubles obsessionnels compulsifs et les troubles anxieux: indications, plus-value et limites]",2020,"Encephale","46","4",,"293","300",,1,"10.1016/j.encep.2020.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081206627&doi=10.1016%2fj.encep.2020.01.005&partnerID=40&md5=3a2b5f6da7ce4045868302f24c897585","Psychologie médicale et Psychosomatique, hôpital de jour universitaire “La Clé”, Liège, Belgium; Faculté de médecine, département de psychologie médicale, université de Liège, Liège, Belgium; Faculté de psychologie, département de psychologie clinique, université de Liège, Liège, Belgium","Lognoul, M., Psychologie médicale et Psychosomatique, hôpital de jour universitaire “La Clé”, Liège, Belgium, Faculté de médecine, département de psychologie médicale, université de Liège, Liège, Belgium; Nasello, J., Psychologie médicale et Psychosomatique, hôpital de jour universitaire “La Clé”, Liège, Belgium, Faculté de psychologie, département de psychologie clinique, université de Liège, Liège, Belgium; Triffaux, J.-M., Psychologie médicale et Psychosomatique, hôpital de jour universitaire “La Clé”, Liège, Belgium, Faculté de médecine, département de psychologie médicale, université de Liège, Liège, Belgium","The exposure in cognitive behavioral therapy (CBT) is a well-known intervention, widely investigated in scientific research. Several studies have shown the benefits of this intervention in the treatment of anxiety disorders, obsessive-compulsive disorders (OCD) and post-traumatic stress disorders (PTSD). The different exposure techniques are mainly based on the emotional processing of fear theory and use an emotional stimulation of fear, following by its habituation. However, new approaches have emerged and are based on the inhibitory learning theory. The virtual reality technology allows emotional involvement from patients and represents a complementary approach to the classical modalities of exposure therapy (e.g., mental or in vivo expositions). This modern approach presents specific features that need to be taken into account by the therapist. Firstly, the presence feeling, which is defined as the “be there” feeling. This feeling is dependent on immersive technical features and personality factors. Secondly, virtual reality sickness, similar to motion sickness, represents a limitation that might prejudice a virtual therapy. The main scientific investigations of Virtual Reality Exposure Therapy (VRET) for treating social phobia, specific phobia, PTSD, and panic disorders are encouraging and demonstrate a similar effectiveness between both in vivo and in virtuo exposures. The scarce investigations on generalized anxiety disorders and OCD also suggeste a similar effectiveness between these exposures. However, further scientific investigations are needed to support these preliminary findings. The attrition rates and deteriorating states are similar to classical CBT approaches. Nevertheless, scientific literature presents several limits: 1) much of the research on this topic has interest conflicts (e.g., developers are also authors of a large number of studies); 2) there is a high heterogeneity of materials and virtual environments used; 3) important measures are not always taken into account in scientific research (e.g., the presence feeling); and 4) a massive use of waiting lists as a control measure. Despite these limitations, the VRET have strong silver linings: 1) the easy access to exposure (less limited than standard exposure techniques) and a cost reduction; 2) highly guaranteed security; 3) the anonymization of exposures (i.e., the patients do not risk meeting someone they know during the exposure therapy); 4) the therapist has a greater control of exposures; 5) a standardization of the exposures; 6) a greater involvement in therapy for technophile patients. Virtual exposure also seems to be generally more accepted by patients. © 2020 L'Encéphale, Paris","Anxiety disorders; Cognitive behavioral therapy; Obsessive-compulsive disorder; Post-traumatic stress disorder; Virtual reality","anonymization; anxiety disorder; Article; computer security; cost control; cybersickness; emotion; fear; generalized anxiety disorder; habituation; human; learning theory; obsessive compulsive disorder; panic; personality; phobia; posttraumatic stress disorder; psychotherapist; social phobia; standardization; treatment indication; virtual reality; virtual reality exposure therapy",Article,"Final","",Scopus,2-s2.0-85081206627
"Talwar D., Guruswamy S., Ravipati N., Eirinaki M.","57219338177;57219338846;57219336031;8392359500;","Evaluating Validity of Synthetic Data in Perception Tasks for Autonomous Vehicles",2020,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence Testing, AITest 2020",,, 9176787,"73","80",,,"10.1109/AITEST49225.2020.00018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092285621&doi=10.1109%2fAITEST49225.2020.00018&partnerID=40&md5=e78e032bcbac9b35c4cdda41059f910b","San José State University, Department of Computer Engineering, San José, CA, United States","Talwar, D., San José State University, Department of Computer Engineering, San José, CA, United States; Guruswamy, S., San José State University, Department of Computer Engineering, San José, CA, United States; Ravipati, N., San José State University, Department of Computer Engineering, San José, CA, United States; Eirinaki, M., San José State University, Department of Computer Engineering, San José, CA, United States","Autonomous vehicles have the potential to completely upend the way we transport today, however deploying them safely at scale is not an easy task. Any autonomous driving system relies on multiple layers of software to function safely. Among these layers, the Perception layer is the most data intensive and also the most complex layer to get right. Companies need to collect and annotate lots of data to properly train deep learning perception models. Simulation systems have come up as an alternative to the expensive task of data collection and annotation. However, whether simulated data can be used as a proxy for real-world data is an ongoing debate. In this work, we attempt to address the question of whether models trained on simulated data can generalize well to the real-world. We collect datasets based on two different simulators with varying levels of graphics fidelity and use the KITTI dataset as an example of real-world data. We train three separate deep learning based object detection models on each of these datasets, and compare their performance on test sets collected from the same sources. We also add the recently released Waymo Open Dataset as a challenging test set. Performance is evaluated based on the mean average precision (mAP) metric for object detection. We find that training on simulation in general does not translate to generalizability on real-world data and that diversity in the training set is much more important than visual graphics' fidelity. © 2020 IEEE.","Autonomous Driving; Deep learning; Perception; Self-driving cars; YOLOv3","Deep learning; Learning systems; Object detection; Object recognition; Statistical tests; Autonomous driving; Data collection; Data intensive; Multiple layers; Perception model; Simulation systems; Synthetic data; Visual graphics; Autonomous vehicles",Conference Paper,"Final","",Scopus,2-s2.0-85092285621
"Hensen B., Klamma R.","57203715350;6603333022;","Comparing authoring workflows and learning paths in web mixed reality environments",2020,"Proceedings - IEEE 20th International Conference on Advanced Learning Technologies, ICALT 2020",,, 9155647,"319","321",,,"10.1109/ICALT49669.2020.00102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091165216&doi=10.1109%2fICALT49669.2020.00102&partnerID=40&md5=40e1befd400cbb1c1d7357c1b5345d55","Rwth Aachen University, Informatik 5, Aachen, Germany","Hensen, B., Rwth Aachen University, Informatik 5, Aachen, Germany; Klamma, R., Rwth Aachen University, Informatik 5, Aachen, Germany","With recent developments in information technology, we can realize learning experiences on the Web and in mixed reality. Many disciplines have profited from this, e.g. anatomy with the possibility to learn with 3D models in both environments. However, designing authoring workflows and learning paths can be different in these environments as the transfer of design ideas can affect the quality of the learning experiences. In this paper, we juxtapose a Web application and a mixed reality application of the same anatomy course. We highlight where differences need to be considered and which modules can be used in both environments, e.g. authentication and gamification. With the practical knowledge presented here, the design of learning experiences on the Web and in mixed reality can be enhanced. To support this, all code is available as open-source software on GitHub. © 2020 IEEE.","Augmented Reality; Gamification; Mixed Reality; Virtual Reality; Web-based Learning; WebXR","Open source software; Open systems; Design ideas; Learning experiences; Learning paths; Mixed-reality environment; WEB application; Work-flows; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85091165216
"Rout S.P.","57220038960;","6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",2020,"2020 11th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2020",,, 9225680,"","",,1,"10.1109/ICCCNT49239.2020.9225680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096544829&doi=10.1109%2fICCCNT49239.2020.9225680&partnerID=40&md5=58c77867b3e94be4b5a2d9b5edf945cb","Electronics and Communication Engineering, TempleCity Institute of Technology Engineering, Bhubaneswar, India","Rout, S.P., Electronics and Communication Engineering, TempleCity Institute of Technology Engineering, Bhubaneswar, India","The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented. © 2020 IEEE.","AR; BCI; BY5G; E-IoT; eMBB; Io-BNT; IoE; M2M; mMTC; MR; OAM; QoE; SM-MIMO; UAV; URLLC; VR","Augmented reality; Data transfer; Deep learning; Intelligent computing; Internet of things; MIMO systems; Optical data processing; Queueing networks; Security of data; Telecommunication equipment; Three dimensional computer graphics; Virtual corporation; Virtual reality; Cellular network; Mobile Technology; Network connectivity; Propagation models; Real-time transfer; Reliable data transmission; Triple Play services; Wireless communications; 5G mobile communication systems",Conference Paper,"Final","",Scopus,2-s2.0-85096544829
"Salvi A., Gavenski N., Pooch E., Tasoniero F., Barros R.","57219554005;57219412641;57201689919;57219549482;35247802600;","Attention-based 3D Object Reconstruction from a Single Image",2020,"Proceedings of the International Joint Conference on Neural Networks",,, 9206776,"","",,,"10.1109/IJCNN48605.2020.9206776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093857440&doi=10.1109%2fIJCNN48605.2020.9206776&partnerID=40&md5=8a5e219afb5c8f78a20a8b774d274cd6","Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil","Salvi, A., Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil; Gavenski, N., Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil; Pooch, E., Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil; Tasoniero, F., Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil; Barros, R., Pontifícia Universidade Católica Do Rio Grande Do sul, School of Technology, Porto Alegre, RS, 90619-900, Brazil","Recently, learning-based approaches for 3D reconstruction from 2D images have gained popularity due to its modern applications, e.g., 3D printers, autonomous robots, self-driving cars, virtual reality, and augmented reality. The computer vision community has applied a great effort in developing functions to reconstruct the full 3D geometry of objects and scenes. However, to extract image features, they rely on convolutional neural networks, which are ineffective in capturing long-range dependencies. In this paper, we propose to substantially improve Occupancy Networks, a state-of-the-art method for 3D object reconstruction. For such we apply the concept of self-attention within the network's encoder in order to leverage complementary input features rather than those based on local regions, helping the encoder to extract global information. With our approach, we were capable of improving the original work in 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10× the Chamfer-L1 distance. We also perform a qualitative study that shows that our approach was able to generate much more consistent meshes, confirming its increased generalization power over the current state-of-the-art. © 2020 IEEE.","3D Reconstruction; Computer Vision; Self-Attention","3D printers; Augmented reality; Convolutional neural networks; Signal encoding; Virtual reality; 3-D object reconstruction; 3D reconstruction; Global informations; Learning-based approach; Long-range dependencies; Modern applications; State-of-the-art methods; Vision communities; Image reconstruction",Conference Paper,"Final","",Scopus,2-s2.0-85093857440
"Eiris R., Jain A., Gheisari M., Wehle A.","57196006104;57216183570;36459705300;55344669000;","Safety immersive storytelling using narrated 360-degree panoramas: A fall hazard training within the electrical trade context",2020,"Safety Science","127",, 104703,"","",,3,"10.1016/j.ssci.2020.104703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082764482&doi=10.1016%2fj.ssci.2020.104703&partnerID=40&md5=414ed06acb963eaf989a6379e49e6ace","208 Rinker Hall, Rinker School of Construction Management, University of Florida, PO Box 115703, Gainesville, FL  32611-5703, United States; Plaza Construction, 120 NE 27th Street, Miami, FL  33137, United States; 322 Rinker Hall, Rinker School of Construction Management, University of Florida, PO Box 115703, Gainesville, FL  32611-5703, United States","Eiris, R., 208 Rinker Hall, Rinker School of Construction Management, University of Florida, PO Box 115703, Gainesville, FL  32611-5703, United States; Jain, A., Plaza Construction, 120 NE 27th Street, Miami, FL  33137, United States; Gheisari, M., 322 Rinker Hall, Rinker School of Construction Management, University of Florida, PO Box 115703, Gainesville, FL  32611-5703, United States; Wehle, A., 322 Rinker Hall, Rinker School of Construction Management, University of Florida, PO Box 115703, Gainesville, FL  32611-5703, United States","Safety training in the United States construction industry commonly employs classroom-based lecture and storytelling techniques to transfer knowledge to workers and professionals. However, low levels of engagement and low-fidelity representations of the construction jobsites have posed limitations for learners to easily visualize and understand hazard conditions. One emerging technology that has the potential to increase engagement and provide high-fidelity visualizations of construction jobsites is 360-degree panorama virtual environments. This study concentrates on using immersive storytelling within digital 360-degree panoramas to improve hazard recognition and risk perception. A proof-of-concept platform was developed to assess the produced virtual training environment in terms of hazard identification, risk perception, and sense of presence. The platform was conceptualized within the visual and narrative context of electrical trade fall hazards, as this trade often perform complex work at elevated surfaces making them especially susceptible to fall related injuries and fatalities. A between-subject pilot study was conducted with forty construction management student subjects, comparatively evaluating safety immersive storytelling and Occupational Safety and Health Administration (OSHA) trained participants (e.g., OSHA 10-hr, OSHA 30-hr). It was found that no statistical differences are present in the average Hazard Identification Index (HII) scores for both approaches, suggesting that the outcomes of the training techniques are equivalent for the narrow scope of fall hazards evaluated in this study (scissor/aerial lifts, scaffolds, roofs/unprotected edges). Nevertheless, time savings in hazard identification training were found; safety immersive storytelling required 15 min of training in contrast 10 or 30 h of OSHA training. Furthermore, it was detected that subjects assigned more or equal risk to ladders, scaffolds, and scissor/aerial lifts hazards for safety immersive storytelling compared to the OSHA condition. Although the subject risk perception scores demonstrate these trends, a statistical analysis performed showed no significant differences between the two experimental groups. All participants perceived that the immersive experience provided a high sense of presence. Based on the experimental results, it was concluded that safety immersive storytelling provides an analogous outcome to OSHA interventions for the studied fall hazards while reducing the required training time. © 2020 Elsevier Ltd","360-degree panorama; Immersive storytelling; Safety training; Virtual reality","Accident prevention; Commerce; Construction industry; E-learning; Hazardous materials; Hazards; Project management; Risk assessment; Risk perception; Tools; Virtual reality; 360-degree panorama; Construction management; Emerging technologies; Immersive; Occupational safety and health administrations; Safety training; Statistical differences; Virtual training environments; Occupational risks; 360 degree panorama; adult; Article; building industry; construction worker; controlled study; electrical trade; employee attitude; fall hazard training; falling; fatality; female; hazard identification; Hazard Identification Index; human; human experiment; male; methodology; motivation; occupational accident; occupational safety; outcome assessment; photography; pilot study; priority journal; proof of concept; risk assessment; risk perception; scoring system; statistical analysis; story telling; student; training; videorecording; virtual reality; vulnerable population; young adult",Article,"Final","",Scopus,2-s2.0-85082764482
"Zou X.-L., Ren Y., Feng D.-Y., He X.-Q., Guo Y.-F., Yang H.-L., Li X., Fang J., Li Q., Ye J.-J., Han L.-Q., Zhang T.-T.","56091603800;57214465775;57189508955;55606086000;56137646600;57205077743;57218300372;57218299409;57218298822;57218301361;57214443902;7404374444;","A promising approach for screening pulmonary hypertension based on frontal chest radiographs using deep learning: A retrospective study",2020,"PLoS ONE","15","7 July", e0236378,"","",,,"10.1371/journal.pone.0236378","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088680501&doi=10.1371%2fjournal.pone.0236378&partnerID=40&md5=986bec69ef39ad40eb99ae7abdd13eea","Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China; Center for Artificial Intelligence in Medicine, Research Institute of Tsinghua, Pearl River Delta, Guangzhou, China; Department of Medical Ultrasound, Third Affiliated Hospital of Sun Yat-sen University, Guangzhou, China; Department of Radiology, Third Affiliated Hospital of Sun Yat-sen University, Guangzhou, China; Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Yuedong Hospital, Meizhou, China; Department of Pumonary Diseases, Dongguan Tangxia Hospital, Dongguan, China","Zou, X.-L., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China; Ren, Y., Center for Artificial Intelligence in Medicine, Research Institute of Tsinghua, Pearl River Delta, Guangzhou, China; Feng, D.-Y., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China; He, X.-Q., Department of Medical Ultrasound, Third Affiliated Hospital of Sun Yat-sen University, Guangzhou, China; Guo, Y.-F., Department of Radiology, Third Affiliated Hospital of Sun Yat-sen University, Guangzhou, China; Yang, H.-L., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China; Li, X., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Yuedong Hospital, Meizhou, China; Fang, J., Department of Pumonary Diseases, Dongguan Tangxia Hospital, Dongguan, China; Li, Q., Center for Artificial Intelligence in Medicine, Research Institute of Tsinghua, Pearl River Delta, Guangzhou, China; Ye, J.-J., Center for Artificial Intelligence in Medicine, Research Institute of Tsinghua, Pearl River Delta, Guangzhou, China; Han, L.-Q., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China; Zhang, T.-T., Department of Pulmonary and Critical Care Medicine, Third Affiliated Hospital of Sun Yat-sen University, Institute of Respiratory Diseases, Sun Yat-sen University, Guangzhou, China","Background: To date, the missed diagnosis rate of pulmonary hypertension (PH) was high, and there has been limited development of a rapid, simple, and effective way to screen the disease. The purpose of this study is to develop a deep learning approach to achieve rapid detection of possible abnormalities in chest radiographs suggesting PH for screening patients suspected of PH. Methods: We retrospectively collected frontal chest radiographs and the pulmonary artery systolic pressure (PASP) value measured by Doppler transthoracic echocardiography from 762 patients (357 healthy controls and 405 with PH) from three institutes in China from January 2013 to May 2019. The wohle sample comprised 762 images (641 for training, 80 for internal test, and 41 for external test). We firstly performed a 8-fold cross-validation on the 641 images selected for training (561 for pre-training, 80 for validation), then decided to tune learning rate to 0.0008 according to the best score on validation data. Finally, we used all the pre-training and validation data (561+80 = 641) to train our models (Resnet50, Xception, and Inception V3), evaluated them on internal and external test dataset to classify the images as having manifestations of PH or healthy according to the area under the receiver operating characteristic curve (AUC/ROC). After that, the three deep learning models were further used for prediction of PASP using regression algorithm. Moreover, we invited an experienced chest radiologist to classify the images in the test dataset as having PH or not, and compared the prediction accuracy performed by deep learing models with that of manual classification. Results: The AUC performed by the best model (Inception V3) achieved 0.970 in the internal test, and slightly declined in the external test (0.967) when using deep learning algorithms to classify PH from normal based on chest X-rays. The mean absolute error (MAE) of the best model for prediction of PASP value was smaller in the internal test (7.45) compared to 9.95 in the external test. Manual classification of PH based on chest X-rays showed much lower AUCs compared to that performed by deep learning models both in the internal and external test. Conclusions: The present study used deep learning algorithms to classify abnormalities suggesting PH in chest radiographs with high accuracy and good generalizability. Once tested prospectively in clinical settings, the technology could provide a non-invasive and easy-to-use method to screen patients suspected of having PH. Copyright © 2020 Zou et al.",,"aged; analytical error; Article; China; controlled study; correlation coefficient; cross validation; data analysis; deep learning; diagnostic accuracy; diagnostic test accuracy study; disease classification; Doppler echocardiography; female; hospital; human; image analysis; intermethod comparison; lung artery pressure; major clinical study; male; mathematical model; mean absolute error; prediction; predictive value; pulmonary hypertension; radiologist; receiver operating characteristic; regression analysis; retrospective study; scoring system; screening; systolic blood pressure; thorax radiography; transthoracic echocardiography; adult; computer assisted diagnosis; diagnostic imaging; mass screening; middle aged; pathology; procedures; pulmonary hypertension; thorax; thorax radiography; very elderly; Adult; Aged; Aged, 80 and over; China; Deep Learning; Female; Humans; Hypertension, Pulmonary; Male; Mass Chest X-Ray; Mass Screening; Middle Aged; Radiographic Image Interpretation, Computer-Assisted; Retrospective Studies; Thorax",Article,"Final","",Scopus,2-s2.0-85088680501
"Lopez C.E., Ashour O., Cunningham J.D., Tucker C., Lynch P.C.","57193163382;36080456300;57194829435;15833577900;56300591400;","The CLICK approach and its impact on learning introductory probability concepts in an industrial engineering course",2020,"ASEE Annual Conference and Exposition, Conference Proceedings","2020-June",, 1339,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095792991&partnerID=40&md5=2b11cbd2f228cb0f4a897f4e2b19ce26","Lafayette College, United States; Penn State Erie, Behrend College, United States; Carnegie Mellon University, United States; Pennsylvania State University, Behrend College, United States","Lopez, C.E., Lafayette College, United States; Ashour, O., Penn State Erie, Behrend College, United States; Cunningham, J.D., Carnegie Mellon University, United States; Tucker, C., Carnegie Mellon University, United States; Lynch, P.C., Pennsylvania State University, Behrend College, United States","The objective of this work is to present an initial investigation of the impact the Connected Learning and Integrated Course Knowledge (CLICK) approach has had on students' motivation, engineering identity, and learning outcomes. CLICK is an approach that leverages Virtual Reality (VR) technology to provide an integrative learning experience in the Industrial Engineering (IE) curriculum. To achieve this integration, the approach aims to leverage VR learning modules to simulate a variety of systems. The VR learning modules offer an immersive experience and provide the context for real-life applications. The virtual simulated system represents a theme to transfer the system concepts and knowledge across multiple IE courses as well as connect the experience with real-world applications. The CLICK approach has the combined effect of immersion and learning-by-doing benefits. In this work, VR learning modules are developed for a simulated manufacturing system. The modules teach the concepts of measures of location and dispersion, which are used in an introductory probability course within the IE curriculum. This work presents the initial results of comparing the motivation, engineering identity, and knowledge gain between a control and an intervention group (i.e., traditional vs. CLICK teaching groups). The CLICK approach group showed greater motivation compared to a traditional teaching group. However, there were no effects on engineering identity and knowledge gain. Nevertheless, it is hypothesized that the VR learning modules will have a positive impact on the students' motivation, engineering identity, and knowledge gain over the long run and when used across the curriculum. Moreover, IE instructors interested in providing an immersive and integrative learning experience to their students could leverage the VR learning modules developed for this project. © American Society for Engineering Education 2020.",,"Curricula; Learning systems; Manufacture; Motivation; Students; Teaching; Virtual reality; Industrial engineering course; Integrated course; Integrative learning; Learning by doing; Learning modules; Learning outcome; Probability concepts; Real-life applications; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-85095792991
"Harrington M.C.R.","36836717500;","Virtual dioramas transform natural history museum exhibit halls & gardens to life with immersive AR",2020,"Extended Abstracts - Proceedings of the 2020 ACM Interaction Design and Children Conference, IDC 2020",,,,"276","279",,,"10.1145/3397617.3402036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091986221&doi=10.1145%2f3397617.3402036&partnerID=40&md5=99b27fde8a41e48fbd6d9bc34cabd113","University of Central Florida, United States","Harrington, M.C.R., University of Central Florida, United States","The original motivation and design process of the AR Perpetual Garden App is discussed in detail. Available on Apple iTunes and Google Play Stores, children and parents, teachers and students, may download and learn with it now, thus amplifying the learning impact of immersive experiences even at home. Inspired by the dioramas of the past, both the creation process and their use in museums as interactive, multimodal, knowledge artifacts are discussed and carefully analyzed. This paper may be of interest to researchers and practitioners alike. First, as a way to understand and generalize the critical design factors used and to extend findings into their design research, and second, as an iterative design and development process model, learner-user experience (LUX) design, extensible to other domains. © 2020 Owner/Author.","augmented reality; bioacoustics; data visualization; immersive; informal learning; information fidelity; interactive; multimodal; museums","Exhibitions; User experience; User interfaces; Creation process; Critical design; Design process; Design research; Iterative design; Knowledge artifacts; Museum exhibits; Natural history; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85091986221
"Hartless J.F., Ayer S.K., London J.S., Wu W.","57208328848;55358381000;38561546100;55707471100;","Comparison of Building Design Assessment Behaviors of Novices in Augmented-and Virtual-Reality Environments",2020,"Journal of Architectural Engineering","26","2", 04020002,"","",,2,"10.1061/(ASCE)AE.1943-5568.0000396","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082121907&doi=10.1061%2f%28ASCE%29AE.1943-5568.0000396&partnerID=40&md5=56136f424798e28dea0e25c612fa4a0b","Emerging Technologies BIM Research Group, School of Sustainable Engineering and the Built Environment, Arizona State Univ., 660 S. College Ave., Tempe, AZ  85281, United States; Dept. of Engineering Education, Virginia Tech, 363 Goodwin Hall, 635 Prices Fork Rd., Blacksburg, VA  24061, United States; Lyles College of Engineering, Californian State Univ. Fresno, 2320 E San Ramon Ave., Fresno, CA  93740, United States","Hartless, J.F., Emerging Technologies BIM Research Group, School of Sustainable Engineering and the Built Environment, Arizona State Univ., 660 S. College Ave., Tempe, AZ  85281, United States; Ayer, S.K., Emerging Technologies BIM Research Group, School of Sustainable Engineering and the Built Environment, Arizona State Univ., 660 S. College Ave., Tempe, AZ  85281, United States; London, J.S., Dept. of Engineering Education, Virginia Tech, 363 Goodwin Hall, 635 Prices Fork Rd., Blacksburg, VA  24061, United States; Wu, W., Lyles College of Engineering, Californian State Univ. Fresno, 2320 E San Ramon Ave., Fresno, CA  93740, United States","Design and construction professionals must make well-informed decisions for every project that meets both industry standards and building codes and also the specific needs of building users and clients. In order to make effective decisions, research suggests that explicit knowledge, defined as easily codified and communicated information, and tacit knowledge, considered to be the know-how of completing a task, must be effectively applied. While there is recognition of the need for both forms of knowledge, architecture engineering and construction (AEC) education has historically focused on covering content-related explicit knowledge in the classroom. As a result, students generally develop tacit knowledge over their careers. Due to an aging AEC workforce, there is a need to support tacit knowledge development in the classroom to enable students entering the industry to supplement the collective tacit knowledge that will exit the industry as the current generation of practitioners retires. Therefore, the authors of this paper explore the use of augmented reality (AR) and virtual reality (VR) to provide immersive virtual experiences aimed at replicating the types of scenarios that students might experience in their careers that would require them to apply tacit knowledge. The authors tasked students in construction-related disciplines with assessing a building design and making judgments about how the design should be modified to support an occupant in a wheelchair in both VR and AR. Using two similar models and a counterbalanced research methodology, the authors coded the statements and behaviors of the student participants during this design assessment exercise. The results of this work indicate that both technologies elicited statements that were indicative of explicit knowledge related to the needs of a wheelchair-bound occupant. When AR and VR were found to directly encourage physical exploration in the experience, both led to behaviors that simulated the completion of tasks that might be performed by a wheelchair-bound occupant. These behaviors were frequently followed by comments that were indicative of tacit knowledge. While this type of behavior was observed in both AR and VR, AR seemed to more directly encourage this type of interaction among participants. The contribution of this work is in providing observational evidence to demonstrate how the physical exploration affordances of AR and VR may be able to support experiences that foster the use and development of tacit knowledge related to AEC-related decision-making. © 2020 American Society of Civil Engineers.",,"Architectural design; Augmented reality; Building codes; Construction; Decision making; Education computing; Students; Technology transfer; Virtual reality; Wheelchairs; Architecture engineering; Augmented and virtual realities; Current generation; Design and construction; Design assessments; Explicit knowledge; Industry standards; Research methodologies; Structural design",Article,"Final","",Scopus,2-s2.0-85082121907
"Harrington M.C.R.","36836717500;","Connecting User Experience to Learning in an Evaluation of an Immersive, Interactive, Multimodal Augmented Reality Virtual Diorama in a Natural History Museum & the Importance of Story",2020,"Proceedings of 6th International Conference of the Immersive Learning Research Network, iLRN 2020",,, 9155202,"70","78",,1,"10.23919/iLRN47897.2020.9155202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091628294&doi=10.23919%2fiLRN47897.2020.9155202&partnerID=40&md5=f7fa4d4c17bd27da4a85962abc7a79ac","University of Central Florida, Games and Interactive Media, Orlando, FL, United States","Harrington, M.C.R., University of Central Florida, Games and Interactive Media, Orlando, FL, United States","Reported are the findings of user experience and learning outcomes from a July 2019 study of an immersive, interactive, multimodal augmented reality (AR) application, used in the context of a museum. The AR Perpetual Garden App is unique in creating an immersive multisensory experience of data. It allowed scientifically naïve visitors to walk into a virtual diorama constructed as a data visualization of a springtime woodland understory and interact with multimodal information directly through their senses. The user interface comprised of two different AR data visualization scenarios reinforced with data based ambient bioacoustics, an audio story of the curator's narrative, and interactive access to plant facts. While actual learning and dwell times were the same between the AR app and the control condition, the AR experience received higher ratings on perceived learning. The AR interface design features of ""Story""and ""Plant Info""showed significant correlations with actual learning outcomes, while ""Ease of Use""and ""3D Plants""showed significant correlations with perceived learning. As such, designers and developers of AR apps can generalize these findings to inform future designs. © 2020 Immersive Learning Research Network.","augmented reality; bioacoustics; data visualization; immersive; informal learning; information fidelity; interactive; multimodal; museums; narrative; photorealistic; place illusion; presence; virtual dioramas; virtual reality","Augmented reality; Data visualization; Museums; User experience; User interfaces; Virtual reality; Visualization; Ease-of-use; Future designs; Interface design features; Learning outcome; Multi-modal information; Multisensory; Natural history; Perceived learning; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85091628294
"Montoya Y.Y., Pillajo C.G., Ortiz J.S.","57218385133;55832576100;56425030300;","Training Assistant for LACT Process Through Augmented Reality",2020,"Iberian Conference on Information Systems and Technologies, CISTI","2020-June",, 9141081,"","",,,"10.23919/CISTI49556.2020.9141081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089026636&doi=10.23919%2fCISTI49556.2020.9141081&partnerID=40&md5=b0b81b5d80320a73617779f8c2f674ed","Universidad Politécnica Salesiana, Quito, Ecuador; Universidad de Las Fuerzas Armadas Espe, Sangolquí, Ecuador","Montoya, Y.Y., Universidad Politécnica Salesiana, Quito, Ecuador; Pillajo, C.G., Universidad Politécnica Salesiana, Quito, Ecuador; Ortiz, J.S., Universidad de Las Fuerzas Armadas Espe, Sangolquí, Ecuador","This article proposes the development of a 3D augmented reality app for mobile devices focused on staff training (training of operators) in the oil industry on the operation of Lease Automatic Custody Transfer Units (LACT Units). First, existing information about LACT Units was collected to define the basic and specific parameters for the virtual creation of a 3D model of the LACT Units (CAD files) which allows the visualization of the equipment and instruments commonly used in the LACT Units, also through the Unity 3D graphic engine a specific sequence of operation is defined attached to a real system. The application focuses on the recognition of PID through a smartphone allowing users to make changes in the variables involved in the process, e.g., pressure, temperature and flow, such that, if the user makes any changes to the variables, the system responds according to these changes based on mathematical models of the plant, achieving a more realistic experience between the user and the process. © 2020 AISTI.","Augmented Reality; LACT Unit; P ID; Unity 3D","3D modeling; Augmented reality; Computer aided design; Information systems; Information use; Petroleum industry; Three dimensional computer graphics; User experience; 3D graphics; CAD files; Custody transfer; Oil industries; Real systems; Specific sequences; Staff training; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-85089026636
"Yang C., Simon G., See J., Berger M.-O., Wang W.","55804990200;57197293840;56227831000;35609544900;7501758663;","WatchPose: A view-aware approach for camera pose data collection in industrial environments",2020,"Sensors (Switzerland)","20","11", 3045,"","",,2,"10.3390/s20113045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085714205&doi=10.3390%2fs20113045&partnerID=40&md5=730acb0a6b56ee4f74061288506160c7","MAGRIT Team, INRIA/LORIA, Nancy, 54600, France; Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Selangor  63100, Malaysia; School of Information Science and Technology, Northeast Normal University, Changchun, Jilin  130000, China","Yang, C., MAGRIT Team, INRIA/LORIA, Nancy, 54600, France; Simon, G., MAGRIT Team, INRIA/LORIA, Nancy, 54600, France; See, J., Faculty of Computing and Informatics, Multimedia University, Cyberjaya, Selangor  63100, Malaysia; Berger, M.-O., MAGRIT Team, INRIA/LORIA, Nancy, 54600, France; Wang, W., School of Information Science and Technology, Northeast Normal University, Changchun, Jilin  130000, China","Collecting correlated scene images and camera poses is an essential step towards learning absolute camera pose regression models. While the acquisition of such data in living environments is relatively easy by following regular roads and paths, it is still a challenging task in constricted industrial environments. This is because industrial objects have varied sizes and inspections are usually carried out with non-constant motions. As a result, regression models are more sensitive to scene images with respect to viewpoints and distances. Motivated by this, we present a simple but efficient camera pose data collection method, WatchPose, to improve the generalization and robustness of camera pose regression models. Specifically, WatchPose tracks nested markers and visualizes viewpoints in an Augmented Reality- (AR) based manner to properly guide users to collect training data from broader camera-object distances and more diverse views around the objects. Experiments show that WatchPose can effectively improve the accuracy of existing camera pose regression models compared to the traditional data acquisition method. We also introduce a new dataset, Industrial10, to encourage the community to adapt camera pose regression methods for more complex environments. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Augmented reality; Data acquisition; Deep learning; Industrial environments; Pose estimation","Augmented reality; Cameras; Regression analysis; Complex environments; Data collection; Data collection method; Industrial environments; Living environment; Object distance; Regression method; Regression model; Data acquisition; article; augmented reality; deep learning",Article,"Final","",Scopus,2-s2.0-85085714205
"Johnson D., Damian D., Tzanetakis G.","57214167710;57192297254;6602262192;","Evaluating the effectiveness of mixed reality music instrument learning with the theremin",2020,"Virtual Reality","24","2",,"303","317",,5,"10.1007/s10055-019-00388-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068827721&doi=10.1007%2fs10055-019-00388-8&partnerID=40&md5=7e681b8efa8420e7e54ec8a41f3173ea","University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada","Johnson, D., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada; Damian, D., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada; Tzanetakis, G., University of Victoria, 3800 Finnerty Rd, Victoria, BC  V8P 5C2, Canada","Learning music is a challenging process that requires years of practice to master, either with lessons from a professional teacher or through self-teaching. While practicing, students are expected to self-evaluate their performance which may be difficult without timely feedback from a professional. Research into computer-assisted music instrument tutoring (CAMIT) attempts to address this through the use of emerging technologies. In this paper, we study CAMIT for mixed reality (MR) by developing MR:emin, an immersive MR music learning environment for the theremin, an electronic music instrument that is controlled without physical contact. MR:emin integrates a physical theremin with the immersive learning environment. To better understand the effectiveness of such environments, we perform a user study with MR:emin comparing traditional music learning with two virtual learning environments, an immersive one and a non-immersive one. In a between-groups study, 30 participants were trained to play a sequence of notes on the theremin using one of the three training environments. Results of our statistical analysis show that performance error during training is significantly smaller in the immersive MR environment. This does not necessarily lead to improved performance after training; analysis of post-training improvement indicates that immersive training results in the smallest amount of improvement. Participants, however, indicate that the MR:emin environment is more engaging and increases confidence during practice. We discuss potential factors leading to the decrease in learning and provide some environment guidelines to aid in the design of engaging immersive music learning environments. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Immersive learning environments; Learning transfer; Mixed reality; Music pedagogy; Training","Computer aided instruction; Computer music; Mixed reality; Personnel training; Emerging technologies; Environment guidelines; Immersive learning; Learning environments; Learning Transfer; Music pedagogy; Performance error; Virtual learning environments; E-learning",Article,"Final","",Scopus,2-s2.0-85068827721
"Glaser N.J., Schmidt M.","57194409345;55267409100;","Usage Considerations of 3D Collaborative Virtual Learning Environments to Promote Development and Transfer of Knowledge and Skills for Individuals with Autism",2020,"Technology, Knowledge and Learning","25","2",,"315","322",,1,"10.1007/s10758-018-9369-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049893405&doi=10.1007%2fs10758-018-9369-9&partnerID=40&md5=ec331a7b776d07c1656ee62853c19f21","School of Education, College of Education, Criminal Justice, and Human Services, University of Cincinnati, PO Box 210002, Teachers/Dyer Hall, Cincinnati, OH  45221, United States","Glaser, N.J., School of Education, College of Education, Criminal Justice, and Human Services, University of Cincinnati, PO Box 210002, Teachers/Dyer Hall, Cincinnati, OH  45221, United States; Schmidt, M., School of Education, College of Education, Criminal Justice, and Human Services, University of Cincinnati, PO Box 210002, Teachers/Dyer Hall, Cincinnati, OH  45221, United States","This emerging technology report explores three-dimensional collaborative virtual learning environments (3D CVLEs) as an intervention modality with potential to foster development of knowledge and skills for people with autism spectrum disorder (ASD). Affordances and unique characteristics of 3D CVLEs are detailed and considered from the perspectives of learning, instruction, and assessment. Research suggests that 3D CVLEs can promote acquisition of social and communicative competencies for individuals with ASD in a safe and controllable manner. However, substantial challenges still exist related to overexposure and cybersickness. This report provides an analysis of current trends in the field, along with considerations of relevance and integration challenges with this unique learner population. Implications for further research are discussed. © 2018, Springer Nature B.V.","3D collaborative virtual learning environments; 3D CVLE; Autism interventions; Autism spectrum; Disorder; Technological interventions; Virtual reality","Computer aided instruction; Diseases; Knowledge management; Virtual reality; 3D-CVLE; Autism intervention; Autism spectrum; Collaborative virtual learning environments; Disorder; Technological interventions; E-learning",Article,"Final","",Scopus,2-s2.0-85049893405
"Bonatti R., Wang W., Ho C., Ahuja A., Gschwindt M., Camci E., Kayacan E., Choudhury S., Scherer S.","57215559144;57215073713;57214119276;57210809218;57214102446;57192545211;16646213800;55842232500;15846506300;","Autonomous aerial cinematography in unstructured environments with learned artistic decision-making",2020,"Journal of Field Robotics","37","4",,"606","641",,6,"10.1002/rob.21931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078254727&doi=10.1002%2frob.21931&partnerID=40&md5=711a70998e8ee798ccce6c2bedc436ce","The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Department of Computer Science, Technische Universität München, Munich, Germany; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; Department of Engineering, Aarhus University, Aarhus, Denmark; School of Computer Science, University of Washington, Seattle, WA, United States","Bonatti, R., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Wang, W., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Ho, C., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Ahuja, A., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Gschwindt, M., Department of Computer Science, Technische Universität München, Munich, Germany; Camci, E., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; Kayacan, E., Department of Engineering, Aarhus University, Aarhus, Denmark; Choudhury, S., School of Computer Science, University of Washington, Seattle, WA, United States; Scherer, S., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States","Aerial cinematography is revolutionizing industries that require live and dynamic camera viewpoints such as entertainment, sports, and security. However, safely piloting a drone while filming a moving target in the presence of obstacles is immensely taxing, often requiring multiple expert human operators. Hence, there is a demand for an autonomous cinematographer that can reason about both geometry and scene context in real-time. Existing approaches do not address all aspects of this problem; they either require high-precision motion-capture systems or global positioning system tags to localize targets, rely on prior maps of the environment, plan for short time horizons, or only follow fixed artistic guidelines specified before the flight. In this study, we address the problem in its entirety and propose a complete system for real-time aerial cinematography that for the first time combines: (a) vision-based target estimation; (b) 3D signed-distance mapping for occlusion estimation; (c) efficient trajectory optimization for long time-horizon camera motion; and (d) learning-based artistic shot selection. We extensively evaluate our system both in simulation and in field experiments by filming dynamic targets moving through unstructured environments. Our results indicate that our system can operate reliably in the real world without restrictive assumptions. We also provide in-depth analysis and discussions for each module, with the hope that our design tradeoffs can generalize to other related applications. Videos of the complete system can be found at https://youtu.be/ookhHnqmlaU. © 2020 Wiley Periodicals, Inc.","aerial robotics; cinematography; computer vision; learning; mapping; motion planning","Antennas; Cameras; Computer vision; Decision making; Mapping; Motion planning; Robot programming; Target drones; Aerial robotics; cinematography; High-precision motion; In-depth analysis; learning; Target estimations; Trajectory optimization; Unstructured environments; Time and motion study",Article,"Final","",Scopus,2-s2.0-85078254727
"Tcha-Tokey K., Schmidt C.T., Geslin E., Richir S.","57188737906;14036157300;36168103100;55917685500;","Improving humans enhancing the complex sociological being with the virtual",2020,"ACM International Conference Proceeding Series",,,,"","",,,"10.1145/3396339.3396401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086142743&doi=10.1145%2f3396339.3396401&partnerID=40&md5=21e5057c9f2f7522f43367755992b898","Polytech Nantes University, Arts et Metiers Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; Le Mans University and Arts et Metiers, Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; UCO Laval and Arts et Metiers, Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; Arts et Metiers Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France","Tcha-Tokey, K., Polytech Nantes University, Arts et Metiers Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; Schmidt, C.T., Le Mans University and Arts et Metiers, Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; Geslin, E., UCO Laval and Arts et Metiers, Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France; Richir, S., Arts et Metiers Institute of Technology, LAMPA, HESAM Université, Change, F-53810, France","In this paper, we argue in favour of using an immersive Virtual Environment (VE) in order to improve human capabilities. We develop this idea in order to advance the potential of VEs in enhancing humans. Training with VEs has proven in some cases to be more efficient than training in real world situations in terms of the reduction of time consumption, risk reduction, easily presenting specific simulation realism, all of which improve learning capabilities. The VE would be an environment for extending human capabilities, the goal being to increase experiences. This last affirmation could renew the scope of action for AH research: acquiring new needed capabilities from the virtual world that would be usable in both worlds, real and virtual. © 2020 Association for Computing Machinery.","Augmented human; Complex system; Knowledge transfer; Novel therapeutic discovery; Virtual environment","Human capability; Immersive virtual environments; Learning capabilities; Real world situations; Risk reductions; Time consumption; Virtual worlds; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85086142743
"Nataraj R., Hollinger D., Liu M., Shah A.","36671846700;57216889466;57216706155;57216709318;","Disproportionate positive feedback facilitates sense of agency and performance for a reaching movement task with a virtual hand",2020,"PLoS ONE","15","5", e0233175,"","",,2,"10.1371/journal.pone.0233175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085155142&doi=10.1371%2fjournal.pone.0233175&partnerID=40&md5=d0f10aa1f53c2d8bf07393fd564768a2","Movement Control Rehabilitation (MOCORE) Laboratory, Stevens Institute of Technology, Hoboken, NJ, United States; Department of Biomedical Engineering, Stevens Institute of Technology, Hoboken, NJ, United States","Nataraj, R., Movement Control Rehabilitation (MOCORE) Laboratory, Stevens Institute of Technology, Hoboken, NJ, United States, Department of Biomedical Engineering, Stevens Institute of Technology, Hoboken, NJ, United States; Hollinger, D., Movement Control Rehabilitation (MOCORE) Laboratory, Stevens Institute of Technology, Hoboken, NJ, United States, Department of Biomedical Engineering, Stevens Institute of Technology, Hoboken, NJ, United States; Liu, M., Movement Control Rehabilitation (MOCORE) Laboratory, Stevens Institute of Technology, Hoboken, NJ, United States, Department of Biomedical Engineering, Stevens Institute of Technology, Hoboken, NJ, United States; Shah, A., Movement Control Rehabilitation (MOCORE) Laboratory, Stevens Institute of Technology, Hoboken, NJ, United States, Department of Biomedical Engineering, Stevens Institute of Technology, Hoboken, NJ, United States","This study investigated the generalized effects of positive feedback (PF) versus negative feedback (NF) during training on performance and sense of agency for a reach-to-touch task with a virtual hand. Virtual reality (VR) is increasingly employed for rehabilitation after neuromuscular traumas such as stroke and spinal cord injury. However, VR methods still need to be optimized for greater effectiveness and engagement to increase rates of clinical retention. In this study, we observed that training with disproportionate PF subsequently produced greater reaching performance (minimizing path length) and greater agency (perception of control) than with disproportionate NF. During PF training, there was also progressive increase in agency, but conversely a decrease in performance. Thus, the increase in performance after training may not be due to positively bolstered learning, but rather priming higher confidence reflected in greater agency. Agency was positively measured as compression in perceived time-intervals between the action of touch to a sound consequence, as standard with intentional binding paradigms. Positive feedback desirably increased agency (∼180 msec) and reduced path length (1.8 cm) compared to negative feedback, which itself showed insignificant, or neutral, effects. Future investigations into optimizing virtual reality paradigms for neuromotor rehabilitation should consider agency as a driving factor for performance. These studies may serve to optimize how feedback is better presented with performance results for complex motor learning. Investigators should also ponder how personal characteristics, both cognitive and physical, may further affect sensitivity to feedback and the rate of neuromotor rehabilitation. © 2020 Nataraj et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; cohort analysis; controlled study; female; hand movement; human; human experiment; male; motor learning; motor performance; negative feedback; pilot study; positive feedback; task performance; virtual reality; young adult; clinical trial; hand; movement (physiology); pathophysiology; prospective study; touch; Adult; Female; Hand; Humans; Male; Movement; Prospective Studies; Touch; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85085155142
"Heravi N., Yuan W., Okamura A.M., Bohg J.","57219438768;57216124044;7103344370;35071071500;","Learning an Action-Conditional Model for Haptic Texture Generation",2020,"Proceedings - IEEE International Conference on Robotics and Automation",,, 9197447,"11088","11095",,,"10.1109/ICRA40945.2020.9197447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709202&doi=10.1109%2fICRA40945.2020.9197447&partnerID=40&md5=bab2a2b0b30322aa9c870f170ab86acd","Stanford University, Department of Mechanical Engineering, United States","Heravi, N., Stanford University, Department of Mechanical Engineering, United States; Yuan, W., Stanford University, Department of Mechanical Engineering, United States; Okamura, A.M., Stanford University, Department of Mechanical Engineering, United States; Bohg, J., Stanford University, Department of Mechanical Engineering, United States","Rich haptic sensory feedback in response to user interactions is desirable for an effective, immersive virtual reality or teleoperation system. However, this feedback depends on material properties and user interactions in a complex, non-linear manner. Therefore, it is challenging to model the mapping from material and user interactions to haptic feedback in a way that generalizes over many variations of the user's input. Current methodologies are typically conditioned on user interactions, but require a separate model for each material. In this paper, we present a learned action-conditional model that uses data from a vision-based tactile sensor (GelSight) and user's action as input. This model predicts an induced acceleration that could be used to provide haptic vibration feedback to a user. We trained our proposed model on a publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with GelSight measurements of the different materials. We show that a unified model over all materials outperforms previous methods and generalizes to new actions and new instances of the material categories in the dataset. © 2020 IEEE.",,"Agricultural robots; Robotics; Sensory feedback; Textures; Virtual reality; Conditional models; Haptic feedbacks; Immersive virtual reality; Induced accelerations; Teleoperation systems; Unified Modeling; User interaction; Vibration feedback; Feedback",Conference Paper,"Final","",Scopus,2-s2.0-85092709202
"Tewari A., Fried O., Thies J., Sitzmann V., Lombardi S., Sunkavalli K., Martin-Brualla R., Simon T., Saragih J., Nießner M., Pandey R., Fanello S., Wetzstein G., Zhu J.-Y., Theobalt C., Agrawala M., Shechtman E., Goldman D.B., Zollhöfer M.","57200618272;56695318400;56312656700;57195995941;57204687314;23052896200;36028239400;36444916900;16178291100;35772871600;57204394554;36170215300;24462821700;56316642900;6507027272;57204250599;55924548800;13007964200;36245738500;","State of the Art on Neural Rendering",2020,"Computer Graphics Forum","39","2",,"701","727",,11,"10.1111/cgf.14022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090397976&doi=10.1111%2fcgf.14022&partnerID=40&md5=5441f08963176bece23beed58e80ccfb","MPI Informatics, Germany; Stanford University, United States; Technical University of Munich, Germany; Facebook Reality Labs, United States; Adobe Research, United States; Google Inc, United States","Tewari, A., MPI Informatics, Germany; Fried, O., Stanford University, United States; Thies, J., Technical University of Munich, Germany; Sitzmann, V., Stanford University, United States; Lombardi, S., Facebook Reality Labs, United States; Sunkavalli, K., Adobe Research, United States; Martin-Brualla, R., Google Inc, United States; Simon, T., Facebook Reality Labs, United States; Saragih, J., Facebook Reality Labs, United States; Nießner, M., Technical University of Munich, Germany; Pandey, R., Google Inc, United States; Fanello, S., Google Inc, United States; Wetzstein, G., Stanford University, United States; Zhu, J.-Y., Adobe Research, United States; Theobalt, C., MPI Informatics, Germany; Agrawala, M., Stanford University, United States; Shechtman, E., Adobe Research, United States; Goldman, D.B., Google Inc, United States; Zollhöfer, M., Facebook Reality Labs, United States","Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems. © 2020 The Author(s) Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",,"Augmented reality; Computer vision; Machine learning; Semantics; Stochastic systems; Virtual reality; Visual communication; Automatic Generation; Free-viewpoint video; Machine learning techniques; Novel view synthesis; Photorealistic images; Scene representation; Social implication; Virtual and augmented reality; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85090397976
"James H.K., Pattison G.T.R., Griffin D.R., Fisher J.D.","57205098715;6603035884;57220617873;57213440606;","How Does Cadaveric Simulation Influence Learning in Orthopedic Residents?",2020,"Journal of Surgical Education","77","3",,"671","682",,2,"10.1016/j.jsurg.2019.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081244480&doi=10.1016%2fj.jsurg.2019.12.006&partnerID=40&md5=046a68770a71723a1443fd3a3089e4af","Clinical Trials Unit, Warwick Medical School, Coventry, United Kingdom; Department of Trauma and Orthopaedic Surgery, University Hospitals Coventry and Warwickshire, Coventry, United Kingdom","James, H.K., Clinical Trials Unit, Warwick Medical School, Coventry, United Kingdom, Department of Trauma and Orthopaedic Surgery, University Hospitals Coventry and Warwickshire, Coventry, United Kingdom; Pattison, G.T.R., Department of Trauma and Orthopaedic Surgery, University Hospitals Coventry and Warwickshire, Coventry, United Kingdom; Griffin, D.R., Clinical Trials Unit, Warwick Medical School, Coventry, United Kingdom, Department of Trauma and Orthopaedic Surgery, University Hospitals Coventry and Warwickshire, Coventry, United Kingdom; Fisher, J.D., Clinical Trials Unit, Warwick Medical School, Coventry, United Kingdom","Objective: The objectives of this study were to understand how cadaveric simulation impacts learning in orthopedic residents, why it is a useful training tool, and how skills learnt in the simulated environment translate into the workplace. Design: This is a qualitative research study using in-depth, semistructured interviews with orthopedic residents who underwent an intensive cadaveric simulation training course. Setting: The study was conducted at the University Hospital Coventry & Warwickshire, a tertiary care center with integrated cadaveric training laboratory in England, United Kingdom. Participants: Orthopedic surgery residents in the intervention group of a randomized controlled trial comparing intensive cadaveric simulation training with standard “on the job” training were invited to participate. Eleven of 14 eligible residents were interviewed (PGY 3-6, 8 male and 3 female). Results: Learning from cadaveric simulation can be broadly categorized into intrinsic, surgeon-driven factors, and extrinsic environmental factors. Intrinsic factors include participant ability to “buy-in” to the simulation exercise, willingness to push one's own learning boundaries in a “safe space” and take out on resident experience and self-reported confidence, with the greatest learning gains seen at around the PGY4 stage in individuals who reported low preintervention operative confidence. Extrinsic factors included; the opportunity to perform operations in their entirety without external pressures or attending “take-over,” leading to subjective improvement in participant operative fluency and confidence. The intensive supervision of subspecialist attending surgeons giving real-time performance feedback, tips and tricks, and the opportunity to practice unusual approaches was highly valued by participants, as was paired learning with alternating roles as primary surgeon/assistant and multidisciplinary involvement of scrub-staff and radiographers. Cadaveric simulation added educational value beyond that obtained in low-fidelity simulation training by “stirring into practice” and “becoming through doing.” In providing ultrarealistic representation of the space, ritualism, and costuming of the operating theater, cadaveric simulation training also enabled the development of a range of nontechnical skills and sociocultural “nontechnical” lessons of surgery. Conclusions: Cadaveric simulation enhances learning in both technical and nontechnical skills in junior orthopedic residents within a single training package. Direct transfer of skills learnt in the simulation training to the real-world operating theater, with consequent patient benefit, was reported. Cadaveric simulation in the UK training system of orthopedics may be of greatest utility at around the PGY 4 stage, at which point operative fluency, independence, and confidence can be rapidly improved in the cadaveric laboratory, to enable the attainment of competence in index trauma operations. © 2019 The Authors","cadaveric simulation training; high-fidelity simulation; orthopedic residency; preparing for practice; surgical education","intrinsic factor; Article; cadaver; clinical competence; environmental factor; female; human; learning; male; orthopedic surgeon; priority journal; qualitative research; randomized controlled trial (topic); resident; semi structured interview; simulation training",Article,"Final","",Scopus,2-s2.0-85081244480
"Brun D., George S., Gouin-Vallerand C.","57193572581;56911617500;24464753700;","Keycube: Text entry evaluation with a cubic device",2020,"Conference on Human Factors in Computing Systems - Proceedings",,, 3382837,"","",,,"10.1145/3334480.3382837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090240646&doi=10.1145%2f3334480.3382837&partnerID=40&md5=9f979c43b59c70ab8ef51b249597d339","Université TÉLUQ, Montreal, PQ, Canada; Le Mans University, Le Mans, France; Université de Sherbrooke, Sherbrooke, Canada","Brun, D., Université TÉLUQ, Montreal, PQ, Canada; George, S., Le Mans University, Le Mans, France; Gouin-Vallerand, C., Université de Sherbrooke, Sherbrooke, Canada","The keycube is a tangible cubic device including a text entry interface for different apparatuses such as augmented, mixed or virtual reality headsets, as well as smart TVs, desktop computers, laptops, tablets. The keycube comprises 80 keys equally disposed on 5 faces. In this paper we investigate keycube text entry performances and the potential typing skill transfer from traditional keyboard. Using prototype implementations, we conducted a user study comparing different cubic layouts and included a baseline from traditional keyboards. Experiments show that users are able to attain about 19 words per minute within one hundred minutes of practice with a QWERTY-based cubic layout, more than twice the speed of an unknown-based cubic layout with similar error rate, and about 30% of their speed with a traditional keyboard. © 2020 Owner/Author.","Cube; Device; Evaluation; Input speed; Keyboard; Text entry","Human engineering; Personal computers; Error rate; Prototype implementations; Skill transfer; Text entry; User study; Virtual-reality headsets; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85090240646
"Wentzel J., D'Eon G., Vogel D.","57188763196;57219111871;8435582600;","Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification",2020,"Conference on Human Factors in Computing Systems - Proceedings",,, 3376687,"","",,4,"10.1145/3313831.3376687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091311186&doi=10.1145%2f3313831.3376687&partnerID=40&md5=a2a6f22213bc5a2fc765a1f66b3d95ef","University of Waterloo, Waterloo, ON, Canada; Universiy of British Columbia, Vancouver, BC, Canada","Wentzel, J., University of Waterloo, Waterloo, ON, Canada; D'Eon, G., Universiy of British Columbia, Vancouver, BC, Canada; Vogel, D., University of Waterloo, Waterloo, ON, Canada","Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions. © 2020 ACM.","ergonomics; input re-mapping; interaction techniques","Ergonomics; Virtual reality; Confined space; Design and implementations; Hermite curves; Neutral points; Non linear; State of the art; Task performance; Transfer functions",Conference Paper,"Final","",Scopus,2-s2.0-85091311186
"Pallavicini F., Pepe A.","6701879031;55744410200;","Virtual reality games and the role of body involvement in enhancing positive emotions and decreasing anxiety: Within-subjects pilot study",2020,"JMIR Serious Games","8","2", e15635,"","",,1,"10.2196/15635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097499071&doi=10.2196%2f15635&partnerID=40&md5=c7fbb876de190177fb7d8f4c6dfacc7b","Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy","Pallavicini, F., Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy; Pepe, A., Department of Human Sciences for Education, University of Milano-Bicocca, Milan, Italy","Background: In the last few years, the introduction of immersive technologies, especially virtual reality, into the gaming market has dramatically altered the traditional concept of video games. Given the unique features of virtual reality in terms of interaction and its ability to completely immerse the individual into the game, this technology should increase the propensity for video games to effectively elicit positive emotions and decrease negative emotions and anxiety in the players. However, to date, few studies have investigated the ability of virtual reality games to induce positive emotions, and the possible effect of this new type of video game in diminishing negative emotions and anxiety has not yet been tested. Furthermore, given the critical role of body movement in individuals’ well-being and in emotional responses to video games, it seems critical to investigate how body involvement can be exploited to modulate the psychological benefits of virtual reality games in terms of enhancing players’ positive emotions and decreasing negative emotions and anxiety. Objective: This within-subjects study aimed to explore the ability of commercial virtual reality games to induce positive emotions and diminish negative emotions and state anxiety of the players, investigating the effects of the level of body involvement requested by the game (ie, high vs low). Methods: A total of 36 young adults played a low body-involvement (ie, Fruit Ninja VR) and a high body-involvement (ie, Audioshield) video game in virtual reality. The Visual Analogue Scale (VAS) and the State-Trait Anxiety Inventory, Form-Y1 (STAI-Y1) were used to assess positive and negative emotions and state anxiety. Results: Results of the generalized linear model (GLM) for repeated-measures multivariate analysis of variance (MANOVA) revealed a statistically significant increase in the intensity of happiness (P<.001) and surprise (P=.003) and, in parallel, a significant decrease in fear (P=.01) and sadness (P<.001) reported by the users. Regarding the ability to improve anxiety in the players, the results showed a significant decrease in perceived state anxiety after game play, assessed with both the STAI-Y1 (P=.003) and the VAS-anxiety (P=.002). Finally, the results of the GLM MANOVA showed a greater efficacy of the high body-involvement game (ie, Audioshield) compared to the low body-involvement game (ie, Fruit Ninja VR), both for eliciting positive emotions (happiness, P<.001; and surprise, P=.01) and in reducing negative emotions (fear, P=.05; and sadness, P=.05) and state anxiety, as measured by the STAI-Y1 (P=.05). Conclusions: The two main principal findings of this study are as follows: (1) virtual reality video games appear to be effective tools to elicit positive emotions and to decrease negative emotions and state anxiety in individuals and (2) the level of body involvement of the virtual video game has an important effect in determining the ability of the game to improve positive emotions and decrease negative emotions and state anxiety of the players. © 2020 JMIR Publications. All Rights Reserved.","Anxiety; Emotions; Positive emotions; State anxiety; Video games; Virtual reality; Virtual reality gaming",,Article,"Final","",Scopus,2-s2.0-85097499071
"Amini A., Gilitschenski I., Phillips J., Moseyko J., Banerjee R., Karaman S., Rus D.","56421607600;55415577300;57215003541;57205386562;57214122696;24923242500;57218886083;","Learning Robust Control Policies for End-to-End Autonomous Driving from Data-Driven Simulation",2020,"IEEE Robotics and Automation Letters","5","2", 8957584,"1143","1150",,10,"10.1109/LRA.2020.2966414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079674113&doi=10.1109%2fLRA.2020.2966414&partnerID=40&md5=ef20fab582230dd93d089e18f228858d","Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States","Amini, A., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Gilitschenski, I., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Phillips, J., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Moseyko, J., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Banerjee, R., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Karaman, S., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States; Rus, D., Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, United States","In this work, we present a data-driven simulation and training engine capable of learning end-to-end autonomous vehicle control policies using only sparse rewards. By leveraging real, human-collected trajectories through an environment, we render novel training data that allows virtual agents to drive along a continuum of new local trajectories consistent with the road appearance and semantics, each with a different view of the scene. We demonstrate the ability of policies learned within our simulator to generalize to and navigate in previously unseen real-world roads, without access to any human control labels during training. Our results validate the learned policy onboard a full-scale autonomous vehicle, including in previously un-encountered scenarios, such as new roads and novel, complex, near-crash situations. Our methods are scalable, leverage reinforcement learning, and apply broadly to situations requiring effective perception and robust operation in the physical world. © 2016 IEEE.","autonomous agents; data-driven simulation; Deep learning in robotics and automation; real world reinforcement learning","Accidents; Autonomous agents; Control system synthesis; Deep learning; Digital storage; Reinforcement learning; Road vehicles; Robust control; Semantics; Virtual reality; Autonomous driving; Autonomous vehicle control; Control policy; Crash situations; Data-driven simulation; Physical world; Real-world; Robust operation; Autonomous vehicles",Article,"Final","",Scopus,2-s2.0-85079674113
"Lohre R., Bois A.J., Athwal G.S., Goel D.P.","57215864647;55259753500;6603148899;34881653400;","Improved Complex Skill Acquisition by Immersive Virtual Reality Training: A Randomized Controlled Trial",2020,"Journal of Bone and Joint Surgery - American Volume","102","6",,"","",,10,"10.2106/JBJS.19.00982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082147105&doi=10.2106%2fJBJS.19.00982&partnerID=40&md5=4e329e9a54ded568ebd592911773531d","Department of Orthopaedics, University of British Columbia, Vancouver, BC, Canada; Section of Orthopaedic Surgery, Department of Surgery, University of Calgary, Calgary, AB, Canada; Roth McFarlane Hand and Upper Limb Center, Schulich School of Medicine and Dentistry, Western University, London, ON, Canada","Lohre, R., Department of Orthopaedics, University of British Columbia, Vancouver, BC, Canada; Bois, A.J., Section of Orthopaedic Surgery, Department of Surgery, University of Calgary, Calgary, AB, Canada; Athwal, G.S., Roth McFarlane Hand and Upper Limb Center, Schulich School of Medicine and Dentistry, Western University, London, ON, Canada; Goel, D.P., Department of Orthopaedics, University of British Columbia, Vancouver, BC, Canada","Background:There has been limited literature on immersive virtual reality (VR) simulation in orthopaedic education. The purpose of this multicenter, blinded, randomized controlled trial was to determine the validity and efficacy of immersive VR training in orthopaedic resident education.Methods:Nineteen senior orthopaedic residents (resident group) and 7 consultant shoulder arthroplasty surgeons (expert group) participated in the trial comparing immersive VR with traditional learning using a technical journal article as a control. The examined task focused on achieving optimal glenoid exposure. Participants completed demographic questionnaires, knowledge tests, and a glenoid exposure on fresh-frozen cadavers while being examined by blinded shoulder arthroplasty surgeons. Training superiority was determined by the outcome measures of the Objective Structured Assessment of Technical Skills (OSATS) score, a developed laboratory metric, verbal answers, and time to task completion.Results:Immersive VR had greater realism and was superior in teaching glenoid exposure than the control (p = 0.01). The expert group outperformed the resident group on knowledge testing (p = 0.04). The immersive VR group completed the learning activity and knowledge tests significantly faster (p < 0.001) at a mean time (and standard deviation) of 11 ± 3 minutes than the control group at 20 ± 4 minutes, performing 3 to 5 VR repeats for a reduction in learning time of 570%. The immersive VR group completed the glenoid exposure significantly faster (p = 0.04) at a mean time of 14 ± 7 minutes than the control group at 21 ± 6 minutes, with superior OSATS instrument handling scores (p = 0.03). The immersive VR group scored equivalently in surprise verbal scores (p = 0.85) and written knowledge scores (p = 1.0).Conclusions:Immersive VR demonstrated substantially improved translational technical and nontechnical skills acquisition over traditional learning in senior orthopaedic residents. Additionally, the results demonstrate the face, content, construct, and transfer validity for immersive VR.Clinical Relevance:This adequately powered, randomized controlled trial demonstrated how an immersive VR system can efficiently (570%) teach a complex surgical procedure and also demonstrate improved translational skill and knowledge acquisition when compared with a traditional learning method. © 2020 Lippincott Williams and Wilkins. All rights reserved.",,"Article; cadaver; cohort analysis; comparative study; controlled study; demography; glenoid cavity; human; medical expert; multicenter study; Objective Structured Assessment of Technical Skills score; orthopedic specialist; orthopedic surgeon; orthopedics; priority journal; professional knowledge; questionnaire; randomized controlled trial; residency education; scientific literature; scoring system; shoulder arthroplasty; virtual reality; work experience; Canada; clinical competence; clinical trial; education; medical education; procedures; reproducibility; shoulder replacement; simulation training; single blind procedure; Arthroplasty, Replacement, Shoulder; Canada; Clinical Competence; Humans; Internship and Residency; Orthopedics; Reproducibility of Results; Simulation Training; Single-Blind Method; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85082147105
"Logishetty K., Gofton W.T., Rudran B., Beaulé P.E., Cobb J.P.","35574197100;6508101071;57205023949;7003898709;7202728836;","Fully Immersive Virtual Reality for Total Hip Arthroplasty: Objective Measurement of Skills and Transfer of Visuospatial Performance after a Competency-Based Simulation Curriculum",2020,"Journal of Bone and Joint Surgery - American Volume","102","6", e27,"","",,4,"10.2106/JBJS.19.00629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082148057&doi=10.2106%2fJBJS.19.00629&partnerID=40&md5=ce3c4083d57c9a883f9025c6bef05f48","MSk Lab, Department of Surgery and Cancer, Imperial College, London, United Kingdom; Division of Orthopaedic Surgery, Ottawa Hospital, Ottawa, ON, Canada","Logishetty, K., MSk Lab, Department of Surgery and Cancer, Imperial College, London, United Kingdom; Gofton, W.T., Division of Orthopaedic Surgery, Ottawa Hospital, Ottawa, ON, Canada; Rudran, B., MSk Lab, Department of Surgery and Cancer, Imperial College, London, United Kingdom; Beaulé, P.E., Division of Orthopaedic Surgery, Ottawa Hospital, Ottawa, ON, Canada; Cobb, J.P., MSk Lab, Department of Surgery and Cancer, Imperial College, London, United Kingdom","Background:Fully immersive virtual reality (VR) uses headsets to situate a surgeon in a virtual operating room to perform open surgical procedures. The aims of this study were to determine (1) if a VR curriculum for training residents to perform anterior approach total hip replacement (AA-THR) was feasible, (2) if VR enabled residents' performance to be measured objectively, and (3) if cognitive and motor skills that were learned with use of VR were transferred to the physical world.Methods:The performance of 32 orthopaedic residents (surgical postgraduate years [PGY]-1 through 4) with no prior experience with AA-THR was measured during 5 consecutive VR training and assessment sessions. Outcome measures were related to procedural sequence, efficiency of movement, duration of surgery, and visuospatial precision in acetabular component positioning and femoral neck osteotomy, and were compared with the performance of 4 expert hip surgeons to establish competency-based criteria. Pretraining and post-training assessments on dry bone models were used to assess the transfer of visuospatial skills from VR to the physical world.Results:Residents progressively developed surgical skills in VR on a learning curve through repeated practice, plateauing, on average, after 4 sessions (4.1 ± 0.6 hours); they reached expert VR levels for 9 of 10 metrics (except femoral osteotomy angle). Procedural errors were reduced by 79%, assistive prompts were reduced by 70%, and procedural duration was reduced by 28%. Dominant and nondominant hand movements were reduced by 35% and 36%, respectively, and head movement was reduced by 44%. Femoral osteotomy was performed more accurately, and acetabular implant orientation improved in VR assessments. In the physical world assessments, experts were more accurate than residents prior to simulation, but were matched by residents after simulation for all of the metrics except femoral osteotomy angle. The residents who performed best in VR were the most accurate in the physical world, while 2 residents were unable to achieve competence despite sustained practice.Conclusions:For novice surgeons learning AA-THR skills, fully immersive VR technology can objectively measure progress in the acquisition of surgical skills as measured by procedural sequence, efficiency of movement, and visuospatial accuracy. Skills learned in this environment are transferred to the physical environment. © 2020 Lippincott Williams and Wilkins. All rights reserved.",,"adult; Article; clinical practice; cognition; depth perception; education program; female; femur osteotomy; hand movement; head movement; human; learning curve; male; motor performance; normal human; operation duration; orthopedic surgeon; outcome assessment; postgraduate education; priority journal; professional competence; residency education; resident; simulation; surgical approach; technology; total hip replacement; virtual reality; anatomic model; Canada; clinical competence; curriculum; economics; education; hip replacement; medical education; orthopedics; procedures; psychomotor performance; simulation training; single blind procedure; Adult; Arthroplasty, Replacement, Hip; Canada; Clinical Competence; Competency-Based Education; Female; Humans; Internship and Residency; Learning Curve; Male; Models, Anatomic; Orthopedics; Psychomotor Performance; Simulation Training; Single-Blind Method; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85082148057
"Philipsen M.P., Moeslund T.B.","56442556900;6507267791;","Cutting pose prediction from point clouds",2020,"Sensors (Switzerland)","20","6", 1563,"","",,1,"10.3390/s20061563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081957898&doi=10.3390%2fs20061563&partnerID=40&md5=e718b792ed47cb35058c2056d615969e","Media Technology, Aalborg University, Aalborg, 9000, Denmark; Danish Technological Institute, Gregersensvej 9, Taastrup, 2630, Denmark","Philipsen, M.P., Media Technology, Aalborg University, Aalborg, 9000, Denmark, Danish Technological Institute, Gregersensvej 9, Taastrup, 2630, Denmark; Moeslund, T.B., Media Technology, Aalborg University, Aalborg, 9000, Denmark","The challenge of getting machines to understand and interact with natural objects is encountered in important areas such as medicine, agriculture, and, in our case, slaughterhouse automation. Recent breakthroughs have enabled the application of Deep Neural Networks (DNN) directly to point clouds, an efficient and natural representation of 3D objects. The potential of these methods has mostly been demonstrated for classification and segmentation tasks involving rigid man-made objects. We present a method, based on the successful PointNet architecture, for learning to regress correct tool placement from human demonstrations, using virtual reality. Our method is applied to a challenging slaughterhouse cutting task, which requires an understanding of the local geometry including the shape, size, and orientation. We propose an intermediate five-Degree of Freedom (DoF) cutting plane representation, a point and a normal vector, which eases the demonstration and learning process. A live experiment is conducted in order to unveil issues and begin to understand the required accuracy. Eleven cuts are rated by an expert, with 8/11 being rated as acceptable. The error on the test set is subsequently reduced through the addition of more training data and improvements to the DNN. The result is a reduction in the average translation from 1.5 cm to 0.8 cm and the orientation error from 4.59◦ to 4.48◦. The method’s generalization capacity is assessed on a similar task from the slaughterhouse and on the very different public LINEMOD dataset for object pose estimation across view points. In both cases, the method shows promising results. Code, datasets, and Supplementary Materials are available at https://github.com/markpp/PoseFromPointClouds. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Automation; Meat production; Point cloud; PointNet; Pose prediction","Automation; Closed loop control systems; Deep learning; Deep neural networks; Degrees of freedom (mechanics); Virtual reality; Degree of freedom (dof); Generalization capacity; Human demonstrations; Meat production; Natural representation; Point cloud; PointNet; Pose predictions; Learning systems; article; automation; case report; clinical article; deep neural network; geometry; human; human experiment; learning; prediction; slaughterhouse; virtual reality",Article,"Final","",Scopus,2-s2.0-85081957898
"Jakl A., Lienhart A.-M., Baumann C., Jalaeefar A., Schlager A., Schoffer L., Bruckner F.","35242591600;57216955058;57216949185;57216948882;57216950067;57205459948;56562723100;","Enlightening Patients with Augmented Reality",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,, 9089476,"195","203",,1,"10.1109/VR46266.2020.1581532258804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526448&doi=10.1109%2fVR46266.2020.1581532258804&partnerID=40&md5=4538f04908530fbf0229b5588000429e","University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria","Jakl, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Lienhart, A.-M., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Baumann, C., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Jalaeefar, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Schlager, A., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Schoffer, L., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria; Bruckner, F., University of Applied Sciences, Institute of Creative\Media/Technologies, St. Pölten, Austria","Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures. © 2020 IEEE.","concepts and paradigms Human-centered computing; Human-centered computing; Interaction design theory; Interface design prototyping Human-centered computing; Mixed / augmented reality Human-centered computing; Usability testing","Augmented reality; User interfaces; Complementary tools; Information sheets; Information transfers; Interactive storytelling; Medical procedures; Patient education; System usability; Target group; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085526448
"Tsuchiya K., Koizumi N.","57205476444;36117906000;","An Optical Design for Avatar-User Co-axial Viewpoint Telepresence",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,, 9089599,"108","116",,1,"10.1109/VR46266.2020.1581039456803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085502301&doi=10.1109%2fVR46266.2020.1581039456803&partnerID=40&md5=c03cd4ec5a2d9c8038db72edce4b185e","University of Electro-Communications, Japan; University of Electro-Communications, JST PRESTO, Japan","Tsuchiya, K., University of Electro-Communications, Japan; Koizumi, N., University of Electro-Communications, JST PRESTO, Japan","We propose a mid-air image system for telepresence. Virtual reality (VR) social networks enable users to interact with each other through CG avatars and choose their appearances freely. However, this is only possible in VR space. We propose a system that takes the avatar from VR space to real space with the help of mid-air imaging technology. In this system, the micro-mirror array plates (MMAPs) display the mid-air image and optically transfer the camera viewpoint to capture users from the mid-air image position. Luminance measurement and modulation transfer function (MTF) measurement were performed to evaluate the image capturing performance of this system. As a result, we found that the MMAPs ccause a decrease in brightness and an increase in blur. In addition, the stray light generated by the MMAPs was in the captured video. We also confirmed that face detection works correctly on the captured video by adjusting the ISO sensitivity of the camera. Furthermore, we designed an application for telepresence called Levitar, which uses a dual camera to output the captured video to the HMD and controls the camera gaze direction. © 2020 IEEE.","Displays and imagers; Human computer interaction (HCI); Human-centered computing; Interaction devices","Cameras; Face recognition; Imaging techniques; Luminance; Optical design; Stray light; User interfaces; Visual communication; Gaze direction; Image capturing; Image position; Image systems; Imaging technology; Luminance measurements; Micromirror array; Modulation transfer function measurements; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085502301
"McAfee R., Haxton C., Harrison M., Gess J.","57211430363;57219341623;57203269478;56377503700;","Thermal Characterization of a Virtual Reality Headset during Transient and Resting Operation",2020,"36th Annual Semiconductor Thermal Measurement, Modeling and Management Symposium, SEMI-THERM 2020 - Proceedings",,, 9142850,"131","136",,,"10.23919/SEMI-THERM50369.2020.9142850","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092285747&doi=10.23919%2fSEMI-THERM50369.2020.9142850&partnerID=40&md5=22b521e31bbda9480fddc59b5f1cfd82","Oregon State University, 204 Rogers Hall, Corvallis, OR  97331, United States","McAfee, R., Oregon State University, 204 Rogers Hall, Corvallis, OR  97331, United States; Haxton, C., Oregon State University, 204 Rogers Hall, Corvallis, OR  97331, United States; Harrison, M., Oregon State University, 204 Rogers Hall, Corvallis, OR  97331, United States; Gess, J., Oregon State University, 204 Rogers Hall, Corvallis, OR  97331, United States","Virtual Reality (VR) is a powerful tool for maintenance process development, engineering design, pedagogy, and combat training. The evolution of the gaming industry has driven the demand for comfortable and reliable VR performance. By using the headset's user datasheet defined MicroController Unit's (MCU) operational temperature, the thermal resistance of the headset used in this study was found to have an external resistance, Rja, of 29.1 K/W, but 28.6% of this heat load is transmitted to the user. Relying on the user's body, specifically the forehead, as a heat sink results in uncomfortable perspiration during usage. Collected data show that after 2 hours of operation, the temperature increases 2.8°C on average when the headset is removed from the user and placed at rest while still operational. The maximum temperature increase is 5.6°C at the top of the VR headset, the surface nearest the internal MCU. This temperature spike proves that the headset needs more effective convective surface area in order to maintain a steady and comfortable operational temperature during 'headset on' and 'headset off' usage as the user's body was not available as a heat sink in the latter mode. A copper sheet has been added inside the headset to thermally connect all of the external surfaces on the device, effectively increasing the optimal convective heat transfer by 61%. The temperature nearest the MCU dropped 6.5°C with the improved thermal management solution and users reported a 25% reduction in perspiration during prolonged use. © 2020 STEF.","comfort measurements; consumer products; free convection; heat transfer; thermal management; Virtual Reality; Wearable","Heat convection; Heat resistance; Heat sinks; Microcontrollers; Thermal variables measurement; Convective heat transfer; Convective surfaces; Maximum temperature; Microcontroller unit; Operational temperature; Temperature increase; Thermal characterization; Virtual-reality headsets; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85092285747
"Gainer S., Eadara S., Haskins J., Huse W., Zhu B., Boyd B., Laird C., Farantatos J.J., Jerald J.","57210917259;57216937631;57210912110;57210916941;57216935372;57210914790;57210912932;57216935098;6507201978;","A Customized Input Device for Simulating the Detection of Hazardous Materials",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,, 9090446,"7","12",,1,"10.1109/VRW50115.2020.0-267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085388847&doi=10.1109%2fVRW50115.2020.0-267&partnerID=40&md5=39fb1b83ac3d65296b85cb03779968a6","NextGen Interactions","Gainer, S., NextGen Interactions; Eadara, S., NextGen Interactions; Haskins, J., NextGen Interactions; Huse, W., NextGen Interactions; Zhu, B., NextGen Interactions; Boyd, B., NextGen Interactions; Laird, C., NextGen Interactions; Farantatos, J.J., NextGen Interactions; Jerald, J., NextGen Interactions","Although consumer VR controllers work well for gaming and general VR usage, they are not necessarily appropriate for specific professional needs. Instead of attempting to build generalized hand controllers with the goal of satisfying a broad range of users, we built a novel input device for the specific needs of a specific target audience - firefighters. We accomplished this by talking with over 30 firefighters. Their input was in the form of one-on-one discussions, focus groups, questionnaires, and feedback on our work in progress. Based on their input, we 1) identified a specific need - the simulation of detecting hazardous materials, 2) built a physical air monitoring device, that more closely matches firefighters actual air monitoring devices than standard VR controllers, and 3) implemented a hazardous materials scenario that consist of both physical and virtual elements. © 2020 IEEE.","Hardware - Emerging technologies - Emerging interfaces; Human-centered computing - Human computer interaction (HCI) - Interaction devices - Haptic devices; Human-centered computing - Human computer interaction (HCI) - Interaction paradigms - Virtual reality","Air pollution; Controllers; Fire extinguishers; Hazardous materials; Hazards; Knobs; Surveys; User interfaces; Air monitoring; Detection of hazardous materials; Focus groups; Input devices; Target audience; Virtual elements; Work in progress; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085388847
"Charlton C.T., Kellems R.O., Black B., Bussey H.C., Ferguson R., Goncalves B., Jensen M., Vallejo S.","35268100800;36170797200;57212386847;57212373554;57212391738;57212388966;57212377648;57212374967;","Effectiveness of avatar-delivered instruction on social initiations by children with Autism Spectrum Disorder",2020,"Research in Autism Spectrum Disorders","71",, 101494,"","",,3,"10.1016/j.rasd.2019.101494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076530317&doi=10.1016%2fj.rasd.2019.101494&partnerID=40&md5=f0dae624469a2ad642c0f90abc0197f9","Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States","Charlton, C.T., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Kellems, R.O., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Black, B., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Bussey, H.C., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Ferguson, R., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Goncalves, B., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Jensen, M., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States; Vallejo, S., Department of Counseling Psychology & Special Education, 340-N MCKB, Brigham Young University, Provo, UT  84602, United States","Background: Individuals with autism often struggle to establish and maintain positive relationships with peers and adults due to communication and social difficulties. Reviews of the research literature on social skills curricula suggest these approaches may be less than effective and often fail to promote generalization of acquired skills. Advances in the availability and adaptability of live animation and digital avatars could provide interventionists a tool to maximize engagement during instruction and program for generalization. The purpose of this study was to examine the effects of an avatar-based social skills intervention on participants’ social initiations in a clinical setting and with same-age peers. Method: The research team used a nonconcurrent multiple baseline design to evaluate the effects of instruction from an avatar on students' social skills. Systematic direct observation and the Social Skills Improvement System were used to evaluate the effects of an avatar delivered social skills lesson. Social validity was measured from the perspective of both participants and their parents. Results: Following instruction from an avatar, participants’ percentage of independent steps completed in the skill “starting a conversation” increased to consistently above 80 %, or above typical mastery levels. Participants’ conversation skills generalized to interactions with same-age peers. Finally, parents reported small positive gains in social skills, and participants indicated the intervention was socially valid. Conclusions: The findings from this study support the use of technology-aided interventions and instruction, specifically the use of live animation avatars. Future studies could extend use of this technology beyond the relatively simple, yet effective, application described here. © 2019","Autism; Avatar; Conversations; Generalization; Live-animation; Social skills","Article; autism; avatar; child; clinical article; clinical effectiveness; controlled study; conversation; female; human; interpersonal communication; male; priority journal; problem behavior; self control; social competence; social interaction; social validity; virtual reality",Article,"Final","",Scopus,2-s2.0-85076530317
"Rodrigues Armijo P., Huang C.-K., Carlson T., Oleynikov D., Siu K.-C.","55945306500;55208770400;57212145600;35474063300;57192938181;","Ergonomics Analysis for Subjective and Objective Fatigue Between Laparoscopic and Robotic Surgical Skills Practice Among Surgeons",2020,"Surgical Innovation","27","1",,"81","87",,2,"10.1177/1553350619887861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076003369&doi=10.1177%2f1553350619887861&partnerID=40&md5=b28e3591514c60df1910744cad0bb011","University of Nebraska Medical Center, Omaha, NE, United States; University of Kansas Medical Center, Kansas City, KS, United States","Rodrigues Armijo, P., University of Nebraska Medical Center, Omaha, NE, United States; Huang, C.-K., University of Nebraska Medical Center, Omaha, NE, United States, University of Kansas Medical Center, Kansas City, KS, United States; Carlson, T., University of Nebraska Medical Center, Omaha, NE, United States; Oleynikov, D., University of Nebraska Medical Center, Omaha, NE, United States; Siu, K.-C., University of Nebraska Medical Center, Omaha, NE, United States","Introduction. Our aim was to determine how self-reported and objectively measured fatigue of upper limb differ between laparoscopic and robotic surgical training environments. Methods. Surgeons at the 2016 SAGES Conference Learning Center and at our institution were enrolled. Two standardized surgical tasks (peg transfer [PT] and needle passing [NP]) were performed twice in each surgical skills practical environments: (1) laparoscopic training-box environment (Fundamentals of Laparoscopic Surgery [FLS]) and (2) Mimic dV-trainer (MIMIC). Muscle activation of upper trapezius (UT), anterior deltoid (AD), flexor carpi radialis, and extensor digitorum were recorded using surface electromyography (EMG; Trigno, Delsys, Inc, Natick, MA). Subjective fatigue was self-reported using Piper Fatigue Scale-12. Analysis was done using SPSS v25.0, α =.05. Results. Demographics were similar between FLS (N = 14) and MIMIC (N = 12). For PT, MIMIC had a significant increase in EMGRMS of UT (P &lt;.001) and AD (P &lt;.001). Conversely, FLS led to significant decreased muscle fatigue in UT (P =.015). For NP, MIMIC had a significant increase in EMGRMS for UT (P =.034) and AD (P =.031), but FLS induced more muscle fatigue for AD (P =.004). There was significant decrease in self-reported fatigue after performing FLS tasks (P =.030) but not after MIMIC (P =.663). Conclusion. Our results showed that practice with MIMIC resulted in greater activation of shoulder muscles, while FLS caused more significant muscle fatigue in the same muscles. This could be due to ergonomic disadvantages and nonoptimal ergonomic settings. Further studies are needed to understand the optimal ergonomics and its impact on fatigue and muscle activation during use of both the FLS and MIMIC training systems. © The Author(s) 2019.","ergonomics; human factors study; simulation; surgical education","adult; Article; disease assessment; electromyography; ergonomics; extensor digitorum longus muscle; female; flexor carpi radialis muscle; Functional Assessment of Chronic Illness Therapy Fatigue Scale; health survey; human; laparoscopic surgery; male; minimally invasive surgery; muscle fatigue; needle passing; outcome assessment; peg transfer; piper fatigue scale 12; procedures; questionnaire; robotic surgical procedure; self report; skill; surgeon; surgical technique; training; trapezius muscle; upper limb; clinical competence; education; ergonomics; laparoscopy; muscle fatigue; physiology; procedures; robot assisted surgery; skeletal muscle; surgeon; Adult; Clinical Competence; Electromyography; Ergonomics; Female; Humans; Laparoscopy; Male; Muscle Fatigue; Muscle, Skeletal; Robotic Surgical Procedures; Surgeons; Upper Extremity",Article,"Final","",Scopus,2-s2.0-85076003369
"Kaplan A.D., Cruit J., Endsley M., Beers S.M., Sawyer B.D., Hancock P.A.","57203577853;55957076800;7003719103;57215654078;36025972800;7102723856;","The Effects of Virtual Reality, Augmented Reality, and Mixed Reality as Training Enhancement Methods: A Meta-Analysis",2020,"Human Factors",,,,"","",,12,"10.1177/0018720820904229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081603757&doi=10.1177%2f0018720820904229&partnerID=40&md5=23181c622324b7e6e21a3cb74deec76b","University of Central Florida, Orlando, United States; SA Technologies, Gold Canyon, AZ, United States; MITRE Corporation, Colorado Springs, CO, United States","Kaplan, A.D., University of Central Florida, Orlando, United States; Cruit, J., University of Central Florida, Orlando, United States; Endsley, M., SA Technologies, Gold Canyon, AZ, United States; Beers, S.M., MITRE Corporation, Colorado Springs, CO, United States; Sawyer, B.D., University of Central Florida, Orlando, United States; Hancock, P.A., University of Central Florida, Orlando, United States","Objective: The objective of this meta-analysis is to explore the presently available, empirical findings on transfer of training from virtual (VR), augmented (AR), and mixed reality (MR) and determine whether such extended reality (XR)-based training is as effective as traditional training methods. Background: MR, VR, and AR have already been used as training tools in a variety of domains. However, the question of whether or not these manipulations are effective for training has not been quantitatively and conclusively answered. Evidence shows that, while extended realities can often be time-saving and cost-saving training mechanisms, their efficacy as training tools has been debated. Method: The current body of literature was examined and all qualifying articles pertaining to transfer of training from MR, VR, and AR were included in the meta-analysis. Effect sizes were calculated to determine the effects that XR-based factors, trainee-based factors, and task-based factors had on performance measures after XR-based training. Results: Results showed that training in XR does not express a different outcome than training in a nonsimulated, control environment. It is equally effective at enhancing performance. Conclusion: Across numerous studies in multiple fields, extended realities are as effective of a training mechanism as the commonly accepted methods. The value of XR then lies in providing training in circumstances, which exclude traditional methods, such as situations when danger or cost may make traditional training impossible. © Copyright 2020, Commonwealth of Australia, as represented by the Department of Defence, Science, and Technology.","immersive environments; meta-analysis; transfer of training; virtual environments","Augmented reality; Mixed reality; Virtual reality; Control environment; Empirical findings; Immersive environment; Meta analysis; Performance measure; Training enhancement; Training methods; Transfer of trainings; E-learning; article; augmented reality; effect size; human; meta analysis; transfer of learning; virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85081603757
"Carreon A., Smith S.J., Mosher M., Rao K., Rowland A.","57200799882;8063955600;57219305897;57221021182;56821362700;","A Review of Virtual Reality Intervention Research for Students With Disabilities in K–12 Settings",2020,"Journal of Special Education Technology",,,,"","",,,"10.1177/0162643420962011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092143583&doi=10.1177%2f0162643420962011&partnerID=40&md5=24117c6927194ddf8e5de88897d87620","The University of Kansas, Lawrence, KS, United States; University of Hawaii, Honolulu, HI, United States","Carreon, A., The University of Kansas, Lawrence, KS, United States; Smith, S.J., The University of Kansas, Lawrence, KS, United States; Mosher, M., The University of Kansas, Lawrence, KS, United States; Rao, K., University of Hawaii, Honolulu, HI, United States; Rowland, A., The University of Kansas, Lawrence, KS, United States","Virtual reality (VR) technology has improved in access and availability in the area of K–12 instruction, increasingly being cited for its promise to meet the varied learning needs of individuals with disabilities. This descriptive review of 25 research studies conducted in K–12 settings examined the defining characteristics of immersion levels associated with VR, the purpose and application of the augmented reality intervention, the outcomes associated with the current use of VR, and the possibility of generalization beyond VR. The results of the review reveal that a majority of studies are utilizing nonimmersive screen-based simulations. While still considered under the VR domain, these technologies do not take advantage of the features of semi- and fully immersive VR which make it an appealing intervention for students with disabilities. Based on the results of this review, we provide recommendations to establish a strong research base on emerging VR technology and its use for students with disabilities in the K–12 classroom. © The Author(s) 2020.","assistive technology; instructional technology; literature review; methodologies; mixed reality; mobile devices; technology perspectives; virtual reality",,Article,"Article in Press","",Scopus,2-s2.0-85092143583
"Cristea D.S., Rusu C.C., Mistodie L.R., Ivanov M., Leontin A.","39860898100;18435404300;8531354900;57220005717;57220008357;","Immersive data analytics for enhancing organisational knowledge transfer processes through a custom developed virtual reality framework",2020,"eLearning and Software for Education Conference",,,,"92","100",,,"10.12753/2066-026X-20-097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096497931&doi=10.12753%2f2066-026X-20-097&partnerID=40&md5=0f6fe1ccb533535691d1258b52874aba","University ‘Dunărea de Jos’, Str. Domnească 47, Galati, Romania; S.C. ALTFACTOR SRL, Str. Domnească 47, Galati, Romania","Cristea, D.S., University ‘Dunărea de Jos’, Str. Domnească 47, Galati, Romania; Rusu, C.C., University ‘Dunărea de Jos’, Str. Domnească 47, Galati, Romania; Mistodie, L.R., University ‘Dunărea de Jos’, Str. Domnească 47, Galati, Romania; Ivanov, M., S.C. ALTFACTOR SRL, Str. Domnească 47, Galati, Romania; Leontin, A., S.C. ALTFACTOR SRL, Str. Domnească 47, Galati, Romania","During the past years, virtual reality (VR) and augmented reality (AR) improved a lot from both accessibility and hardware capabilities perspectives. Currently, this type of software applications is encouraged to be part of various domains. This paper presents how virtual reality technologies can provide a novel, immersive, approach of one of the most used data science tasks, respectively data analytics, that can be used to enhance the effectiveness of the organizational knowledge transfer. Knowledge transfer, a process used in various industries including education, business, social and technology, plays an important role in the overall learning process, introducing new ways of sharing resources and people experience. As a data scientist, it is usually said that about 80% of the time will be used doing EDA (exploratory data analytics scenarios). Our research targeted the enhancement of organizational knowledge transfer processes by using immersive EDA. Our paper emphasizes the fact that VR offers extended possibilities for sustaining data analytics strategies, improving them with a new immersive perspective. Virtual learning environments can be used to build skills and one challenge for virtual reality, when applied in education or training, is to assess Virtual Environments effectiveness. Immersive data analytics can ease the effort of quantifying how effective a specific virtual scenario was for a group of users. The presented study also emphasises on how VR technologies can provide more detailed immersive analytics that can be used in optimizing knowledge transfer and implicitly learning processes, by potentially analysing every move of the learners, assessing the evolution of the user learning/performance, respectively understanding user reaction speed (response times) in various contexts similar to real world scenarios. Also, through immersive analytics, it is possible to determine learners emotional state and their level of attention, being possible to provide a more personalised interactive experience for enhancing learning efficiency. © 2020, National Defence University - Carol I Printing House. All rights reserved.","Data science; E-learning; Immersive data analytics; Knowledge transfer; Virtual reality",,Conference Paper,"Final","",Scopus,2-s2.0-85096497931
"Balzerkiewitz H.-P., Stechert C.","57219207718;24448259400;","Use of Virtual Reality in Product Development by Distributed Teams",2020,"Procedia CIRP","91",,,"577","582",,,"10.1016/j.procir.2020.02.216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091709372&doi=10.1016%2fj.procir.2020.02.216&partnerID=40&md5=b9c2cf61ceefb8d3f8b8d602c11ebcad","Ostfalia University of Applied Science, Wolfenbüttel, Germany","Balzerkiewitz, H.-P., Ostfalia University of Applied Science, Wolfenbüttel, Germany; Stechert, C., Ostfalia University of Applied Science, Wolfenbüttel, Germany","This paper shows how future VR software improvements can support work in distributed teams. The comprehensive summary to the topics ""VR-technology"" and ""working in distributed teams"" lead to the following recommendations. In order to improve distributed teamwork in product development in the future the VR-technology can be a suitable tool. Therefor the data transfer between CAD and VR must be smooth. Moreover, existing or new VR-Software must be adapted in a way that allows the creation of 3D objects directly in the virtual environment. © 2017 The Authors. Published by Elsevier B.V.","Distributed Teams; Product Development; Virtual Reality","Computer aided design; Data transfer; Product development; 3D object; Distributed teams; VR technology; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85091709372
"Matsangidou M., Otkhmezuri B., Ang C.S., Avraamides M., Riva G., Gaggioli A., Iosif D., Karekla M.","57196007400;57196009850;15831174100;6506947139;56962750600;6603138127;57219454910;7801543407;","“Now i can see me” designing a multi-user virtual reality remote psychotherapy for body weight and shape concerns",2020,"Human-Computer Interaction",,,,"","",,3,"10.1080/07370024.2020.1788945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092795269&doi=10.1080%2f07370024.2020.1788945&partnerID=40&md5=67a8f0e1d96c2ded63dded4a70672b50","School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus; School of Psychology, University of Cyprus, Nicosia, Cyprus; School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy; Applied Technology for Neuro-Psychology Lab, Milan, Italy","Matsangidou, M., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom, Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus; Otkhmezuri, B., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Ang, C.S., School of Engineering and Digital Arts, University of Kent, Kent, United Kingdom; Avraamides, M., Research Center on Interactive Media, Smart systems and Emerging technologies ltd, Nicosia, Cyprus, School of Psychology, University of Cyprus, Nicosia, Cyprus; Riva, G., School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab, Milan, Italy; Gaggioli, A., School of Psychology, Università Cattolica Del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab, Milan, Italy; Iosif, D., School of Psychology, University of Cyprus, Nicosia, Cyprus; Karekla, M., School of Psychology, University of Cyprus, Nicosia, Cyprus","Recent years have seen a growing research interest towards designing computer-assisted health interventions aiming to improve mental health services. Digital technologies are becoming common methods for diagnosis, therapy, and training. With the advent of lower-cost VR head-mounted-displays (HMDs) and high internet data transfer capacity, there is a new opportunity for applying immersive VR tools to augment existing interventions. This study is among the first to explore the use of a Multi-User Virtual Reality (MUVR) system as a therapeutic medium for participants at high-risk for developing Eating Disorders. This paper demonstrates the positive effect of using MUVR remote psychotherapy to enhance traditional therapeutic practices. The study capitalises on the opportunities which are offered by a MUVR remote psychotherapeutic session to enhance the outcome of Acceptance and Commitment Therapy, Play Therapy and Exposure Therapy for sufferers with body shape and weight concerns. Moreover, the study presents the design opportunities and challenges of such technology, while strengths on the feasibility, and the positive user acceptability of introducing MUVR to facilitate remote psychotherapy. Finally, the appeal of using VR for remote psychotherapy and its observed positive impact on both therapists and participants is discussed. © 2020 The Author(s). Published with license by Taylor & Francis Group, LLC.","Acceptance and Commitment Therapy (ACT); high-risk for eating disorders; Multi-User virtual reality; Play Therapy; Exposure Therapy; remote psychotherapy","Data transfer; Helmet mounted displays; Computer assisted; Digital technologies; Head mounted displays; Health interventions; Mental health services; Research interests; Therapeutic practices; Transfer capacities; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85092795269
"Ke Y., Liu P., An X., Song X., Ming D.","55760984800;57213603004;57214860474;55814078100;9745824400;","An online SSVEP-BCI system in an optical see-through augmented reality environment",2020,"Journal of Neural Engineering","17","1", 016066,"","",,5,"10.1088/1741-2552/ab4dc6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080070514&doi=10.1088%2f1741-2552%2fab4dc6&partnerID=40&md5=bd56e2fce8c2131522bac0a349d1c469","Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Department of Biomedical Engineering, College of Precision Instrument and Optoelectronics Engineering, Tianjin University, Tianjin, 300072, China","Ke, Y., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Liu, P., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; An, X., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Song, X., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China; Ming, D., Academy of Medical Engineering and Translational Medicine, Tianjin International Joint Research Centre for Neural Engineering, Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin, 300072, China, Department of Biomedical Engineering, College of Precision Instrument and Optoelectronics Engineering, Tianjin University, Tianjin, 300072, China","Objective. This study aimed to design and evaluate a high-speed online steady-state visually evoked potential (SSVEP)-based brain-computer interface (BCI) in an optical see-through (OST) augmented reality (AR) environment. Approach. An eight-class BCI was designed in an OST-AR headset which is wearable and allows users to see the user interface of the BCI and the device to be controlled in the same view field via the OST head-mounted display. The accuracies, information transfer rates (ITRs), and SSVEP signal characteristics of the AR-BCI were evaluated and compared with a computer screen-based BCI implemented with a laptop in offline and online cue-guided tasks. Then, the performance of the AR-BCI was evaluated in an online robotic arm control task. Main results. The offline results obtained during the cue-guided task performed with the AR-BCI showed maximum averaged ITRs of 65.50 ± 9.86 bits min-1 according to the extended canonical correlation analysis-based target identification method. The online cue-guided task achieved averaged ITRs of 65.03 ± 11.40 bits min-1. The online robotic arm control task achieved averaged ITRs of 45.57 ± 7.40 bits min-1. Compared with the screen-based BCI, some limitations of the AR environment impaired BCI performance and the quality of SSVEP signals. Significance. The results showed the potential for providing a high-performance brain-control interaction method by combining AR and BCI. This study could provide methodological guidelines for developing more wearable BCIs in OST-AR environments and will also encourage more interesting applications involving BCIs and AR techniques. © 2020 IOP Publishing Ltd.","augmented reality; brain-computer interface; electroencephalogram; optical see-through; steady-state visual evoked potential","Augmented reality; Electroencephalography; Helmet mounted displays; Interface states; Robotic arms; Robotics; User interfaces; Wearable computers; Canonical correlation analysis; Head mounted displays; Information transfer rate; Methodological guidelines; Optical see-through; Steady state visual evoked potentials; Steady state visually evoked potentials; Target identification; Brain computer interface; adult; augmented reality; Conference Paper; controlled study; correlation analysis; female; human; human experiment; male; normal human; online system; priority journal; robotics; signal transduction; task performance; visual evoked potential",Conference Paper,"Final","",Scopus,2-s2.0-85080070514
"Li L., Qiao X., Lu Q., Ren P., Lin R.","57216895263;7101964811;57216900710;57203302963;57216892225;","Rendering Optimization for Mobile Web 3D Based on Animation Data Separation and On-Demand Loading",2020,"IEEE Access","8",, 9090999,"88474","88486",,,"10.1109/ACCESS.2020.2993613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085213056&doi=10.1109%2fACCESS.2020.2993613&partnerID=40&md5=9b9586c3fcbcc5772777743625e47b13","School of Electronics and Information, Communication University of Zhejiang, Hangzhou, 310037, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Li, L., School of Electronics and Information, Communication University of Zhejiang, Hangzhou, 310037, China; Qiao, X., State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Lu, Q., School of Electronics and Information, Communication University of Zhejiang, Hangzhou, 310037, China; Ren, P., State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Lin, R., School of Electronics and Information, Communication University of Zhejiang, Hangzhou, 310037, China","Based on advances in image processing technology and Web-enabling technologies for mobile devices, mobile Augmented Reality (AR) and Virtual Reality (VR) has developed rapidly. The rendering and interaction of 3D models is an important part of AR and VR applications and is closely related to user experience. However, since the existing WebGL 3D JavaScript libraries for Web-based mobile 3D (represented by three.js and babylon.js) load the entire model file at once, large-size 3D models with complex interactions cannot be rendered smoothly due to limited data transmission, the weak computation capabilities of mobile Web browsers, and the latency of 3D model rendering. In this paper, we first propose model-animation data separation and an on-demand loading mechanism to improve the data request and loading process of Web 3D models. The main mechanisms are the following: (1) The model data are segmented into topological data and animation data sequences, and only the necessary data of the model are loaded when the Web-based mobile 3D model is first rendered. (2) The 3D model animation data sequence is semantically decomposed, and a multigranular model animation data service is established to provide continuous animation data support. (3) An asynchronous request-response mechanism is used to optimize the loading method of the model data. The model rendering mechanism uses an on-demand request and rendering method to transform the centralized loading process of the 3D model into a decentralized process. According to the testing and verification results, this optimization method can reduce the latency of mobile Web 3D in model data transmission and rendering by 24.72% for the experiment models. The interaction experience of Web-based mobile AR and VR is substantially improved relative to existing Web 3D rendering engines and rendering mechanisms, especially in complex interactive service scenarios. © 2013 IEEE.","augmented reality; interfacing data services; Mobile Web 3D; on-demand loading; rendering interactive computing; virtual reality","3D modeling; Animation; Augmented reality; Data transfer; Image processing; Rendering (computer graphics); Topology; Transmissions; User experience; Virtual reality; Web browsers; Websites; Image processing technology; Interaction experiences; Interactive services; Mobile augmented reality; Optimization method; Rendering optimizations; Response mechanisms; Verification results; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85085213056
"Bellalouna F.","18933453600;","New approach for industrial training using virtual reality technology",2020,"Procedia CIRP","93",,,"262","267",,,"10.1016/j.procir.2020.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092432354&doi=10.1016%2fj.procir.2020.03.008&partnerID=40&md5=05e8e2f253a68a1c8bc857d201728f9f","University of Applied Sciences Karlsruhe, Moltkestraße 30, Karlsruhe, 76133, Germany","Bellalouna, F., University of Applied Sciences Karlsruhe, Moltkestraße 30, Karlsruhe, 76133, Germany","This paper presents two case studies achieved within industrial cooperation projects between the University of Applied Sciences Karlsruhe and German manufacturers for special appliances. The aim of the case studies is development and implementation of training applications for the use and the handling of special vehicle using the virtual reality technology. Based on the experiences gathered during these cooperation projects the challenges that face the VR introduction in the industrial area is outlined in this paper. Furthermore, a best practice approach on how to transfer CAD to VR data to implement industrial VR application is presented in this contribution. © 2020 The Authors.","CAD data; Cognitive approach; Intuitive approach; Virtual reality; VR data","Automobile manufacture; E-learning; Industrial area; Industrial cooperation; Industrial training; IS development; Training applications; University of applied science; Virtual reality technology; VR applications; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85092432354
"Eiris R., Gheisari M., Esmaeili B.","57196006104;36459705300;35388139800;","Desktop-based safety training using 360-degree panorama and static virtual reality techniques: A comparative experimental study",2020,"Automation in Construction","109",, 102969,"","",,6,"10.1016/j.autcon.2019.102969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073757844&doi=10.1016%2fj.autcon.2019.102969&partnerID=40&md5=910f18b71dc4dced51aa43348f942f02","Rinker School of Construction Management, University of Florida, United States; Sid and Reva Dewberry Department of Civil, Environmental, and Infrastructure Engineering, George Mason University, United States","Eiris, R., Rinker School of Construction Management, University of Florida, United States; Gheisari, M., Sid and Reva Dewberry Department of Civil, Environmental, and Infrastructure Engineering, George Mason University, United States; Esmaeili, B., Sid and Reva Dewberry Department of Civil, Environmental, and Infrastructure Engineering, George Mason University, United States","Virtual reality (VR)-based approaches have been used to facilitate safety knowledge transfer and increase hazard awareness by providing safe and controlled experiences of unsafe scenarios in construction safety training applications. However, the long development times and high computational costs associated with existing VR methods have posed significant challenges to using such VR-based safety training platforms. Unlike VR settings that deliver computer-generated reproductions of the environment, 360-degree panorama can create true-to-reality simulations of construction jobsites. This research developed and compared two hazard-identification training platforms based on VR and 360-degree panorama. Construction students and professionals participated in an experiment to determine their perception of realism and evaluate their hazard-identification skills. It was found that students perceived the 360-degree panorama conditions as more realistic than the VR conditions, but professionals perceived no difference between them. Moreover, differences were found in the average hazard identification index (HII) scores for all participants, with higher scores for the VR conditions than for the 360-degree panorama conditions. Finally, it was found that there was an inverse correlation between the presence scores and the average HII scores for the participants in the study. © 2019 Elsevier B.V.","360-degree panorama; Construction safety training; Hazard recognition; Virtual reality","E-learning; Hazardous materials; Knowledge management; Students; Virtual reality; 360-degree panorama; Computational costs; Computer generated; Construction safety; Hazard identification; Inverse correlation; Training platform; Virtual reality techniques; Hazards",Article,"Final","",Scopus,2-s2.0-85073757844
"Ojelade A., Paige F.","57203984321;57202021599;","Virtual reality postural training for construction",2020,"Construction Research Congress 2020: Safety, Workforce, and Education - Selected Papers from the Construction Research Congress 2020",,,,"565","573",,,"10.1061/9780784482872.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096943100&doi=10.1061%2f9780784482872.061&partnerID=40&md5=3f57eb8f400f3d339c7ab720e408cc14","Occupational Ergonomics and Safety, Dept. of Industrial and Systems Engineering, Virginia Tech, Blacksburg, VA, United States; Construction and Engineering Management, Dept. of Civil Engineering, Virginia Tech, Blacksburg, VA, United States","Ojelade, A., Occupational Ergonomics and Safety, Dept. of Industrial and Systems Engineering, Virginia Tech, Blacksburg, VA, United States; Paige, F., Construction and Engineering Management, Dept. of Civil Engineering, Virginia Tech, Blacksburg, VA, United States","Currently, the U.S. construction workforce is aging faster than the next generation is being trained. While an insufficient workforce will cause major issues with the advancement of infrastructure, it also causes less discussed worker health safety hazards. Traditional experiential learning is a major limitation of the construction industry's growth rate due to the non-availability of real environments, time, and training supervisors. Virtual reality (VR) allows for a virtual environment to efficiently mimic dangerous, expensive, and difficult to set up training scenarios. This paper presents an exploration of a virtual approach to training future construction workers. VR environments and a RGB camera based pose estimation pedagogical tool are evaluated for their ability to improve the training process of future construction workers. Study participants will be assessed on their ability to learn, identify, and assess health safety risks, specifically proper lifting and postural techniques associated with construction tasks. Findings of this work in progress will provide quantified learning gains by measuring training time, changes in behavior, and risk assessment ability; and a qualitative understanding of the experience in the virtual environment through participant interviews. Findings from this study transfer to a much-needed digitization of pedagogical approaches for blue collar industries. © 2020 American Society of Civil Engineers.",,"Construction industry; Health hazards; Health risks; Occupational risks; Personnel training; Risk assessment; Virtual reality; Construction workers; Construction workforces; Experiential learning; Pedagogical approach; Pedagogical tools; Postural training; Real environments; Training scenario; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85096943100
"Vyas R.M., Sayadi L.R., Bendit D., Hamdan U.S.","18434994300;57202441723;57219769674;57190420794;","Using virtual augmented reality to remotely proctor overseas surgical outreach: Building long-term international capacity and sustainability",2020,"Plastic and Reconstructive Surgery",,,,"622E","629E",,1,"10.1097/PRS.0000000000007293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090839470&doi=10.1097%2fPRS.0000000000007293&partnerID=40&md5=1068ba089ebd59cc4aedcfa01a718958","Department of Plastic Surgery, University of California, Irvine, United States; Global Smile Foundation","Vyas, R.M., Department of Plastic Surgery, University of California, Irvine, United States, Global Smile Foundation; Sayadi, L.R., Department of Plastic Surgery, University of California, Irvine, United States, Global Smile Foundation; Bendit, D., Department of Plastic Surgery, University of California, Irvine, United States, Global Smile Foundation; Hamdan, U.S., Department of Plastic Surgery, University of California, Irvine, United States, Global Smile Foundation","Background: Cleft lip affects one in 700 children globally, and the prevalence far surpasses capacity to deliver cleft care in underresourced and endemic regions. A hands-on educational presence is needed to promote overseas surgical autonomy, build overseas capacity, and ensure a sustained clinical and educational infrastructure. The goal of this study was to build and assess an augmented reality educational platform that allows a remote yet virtual interactive presence to transfer cleft surgery knowledge/skills to overseas colleagues. Methods: A prospective study assessing a 13-month overseas augmented reality-based cleft surgery curriculum was conducted. Three semiannual site visits engaged two Peruvian surgeons in evidence-based didactics, on-site cleft surgery, and familiarization with the augmented reality platform. During 10 remote augmented reality visits, a surgeon stationed in United States guided the same Peruvian surgeons through cleft surgery. Quarterly assessments of the Peruvian surgeons were performed using visual analogue scale questionnaires. Results: Visual analogue scale scores by both the remote and overseas surgeons demonstrated significant, progressive improvement in all facets of cleft lip repair throughout the curriculum. Site visits preferentially built capacity for cleft diagnosis and preoperative counseling (p < 0.001), principles of repair (p < 0.001), repair technique (p < 0.02) and intraoperative decision-making (p < 0.001). Remote sessions preferentially developed understanding of cleft operative design/anthropometry (p < 0.04), cleft anatomy (p < 0.01), and operative efficiency (p < 0.001). At 30-month follow-up, no children with cleft lip required transfer to tertiary care centers. Conclusion: A curriculum that combines on-site training and augmented reality-based hands-on remote teaching can build sustained capacity of comprehensive cleft care in underresourced areas. © 2020 World Scientific Publishing Co. Pte Ltd. All rights reserved.",,"cleft lip; cleft palate; clinical competence; curriculum; education; global disease burden; high fidelity simulation training; human; infant; international cooperation; intraoperative period; orthognathic surgery; pilot study; procedures; proof of concept; prospective study; reproducibility; sustainable development; virtual reality; Augmented Reality; Cleft Lip; Cleft Palate; Clinical Competence; Curriculum; Global Burden of Disease; High Fidelity Simulation Training; Humans; Infant; International Cooperation; Intraoperative Period; Orthognathic Surgical Procedures; Pilot Projects; Proof of Concept Study; Prospective Studies; Reproducibility of Results; Sustainable Development; Virtual Reality",Article,"Article in Press","",Scopus,2-s2.0-85090839470
"Harris D.J., Buckingham G., Wilson M.R., Brookes J., Mushtaq F., Mon-Williams M., Vine S.J.","57192429891;14069958400;55574207642;57197801653;56999078200;7006287402;36811509000;","Exploring sensorimotor performance and user experience within a virtual reality golf putting simulator",2020,"Virtual Reality",,,,"","",,,"10.1007/s10055-020-00480-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092909275&doi=10.1007%2fs10055-020-00480-4&partnerID=40&md5=c0dd2e10981714f6c085f31365d25818","School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Bradford Teaching Hospitals NHS Foundation Trust, Bradford, West Yorkshire, United Kingdom; National Centre for Optics, Vision and Eye Care, University of South-Eastern Norway, Hasbergs vei 36, Kongsberg, 3616, Norway","Harris, D.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Buckingham, G., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Wilson, M.R., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom; Brookes, J., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Mushtaq, F., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom; Mon-Williams, M., School of Psychology, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Centre for Immersive Technologies, University of Leeds, Leeds, LS2 9JZ, United Kingdom, Bradford Teaching Hospitals NHS Foundation Trust, Bradford, West Yorkshire, United Kingdom, National Centre for Optics, Vision and Eye Care, University of South-Eastern Norway, Hasbergs vei 36, Kongsberg, 3616, Norway; Vine, S.J., School of Sport and Health Sciences, University of Exeter, St Luke’s Campus, Exeter, EX1 2LU, United Kingdom","In light of recent advances in technology, there has been growing interest in virtual reality (VR) simulations for training purposes in a range of high-performance environments, from sport to nuclear decommissioning. For a VR simulation to elicit effective transfer of training to the real-world, it must provide a sufficient level of validity, that is, it must be representative of the real-world skill. In order to develop the most effective simulations, assessments of validity should be carried out prior to implementing simulations in training. The aim of this work was to test elements of the physical fidelity, psychological fidelity and construct validity of a VR golf putting simulation. Self-report measures of task load and presence in the simulation were taken following real and simulated golf putting to assess psychological and physical fidelity. The performance of novice and expert golfers in the simulation was also compared as an initial test of construct validity. Participants reported a high degree of presence in the simulation, and there was little difference between real and virtual putting in terms of task demands. Experts performed significantly better in the simulation than novices (p =.001, d = 1.23), and there was a significant relationship between performance on the real and virtual tasks (r =.46, p =.004). The results indicated that the simulation exhibited an acceptable degree of construct validity and psychological fidelity. However, some differences between the real and virtual tasks emerged, suggesting further validation work is required. © 2020, The Author(s).","Construct validity; Simulation; Sport; Training; VR","Golf; User experience; Construct validity; Nuclear decommissioning; Real-world; Task demand; Test elements; Training purpose; Transfer of trainings; VR simulation; Virtual reality",Article,"Article in Press","",Scopus,2-s2.0-85092909275
"Hoppe A.H., Marek F., De Camp F.V., Stiefelhagen R.","57195069885;57210910641;57194787505;6602180348;","Extending movable surfaces with touch interaction using the virtualtablet: An extended view",2020,"Advances in Science, Technology and Engineering Systems","5","2",,"328","337",,,"10.25046/AJ050243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087525968&doi=10.25046%2fAJ050243&partnerID=40&md5=df0c63da1836cd541fada857bd9f6d7e","Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany; Fraunhofer IOSB, Interactive Analysis and Diagnosis (IAD), Karlsruhe, 76131, Germany; Fraunhoferstr. 1, Karlsruhe, 76131, Germany","Hoppe, A.H., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany, Fraunhoferstr. 1, Karlsruhe, 76131, Germany; Marek, F., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany; De Camp, F.V., Fraunhofer IOSB, Interactive Analysis and Diagnosis (IAD), Karlsruhe, 76131, Germany; Stiefelhagen, R., Karlsruhe Institute of Technology (KIT), Institute for Anthropomatics and Robotics (IAR), cv:hci Lab, Karlsruhe, 76131, Germany","Immersive output and natural input are two core aspects of a virtual reality experience. Current systems are frequently operated by a controller or gesture-based approach. However, these techniques are either very accurate but require an effort to learn, or very natural but miss haptic feedback for optimal precision. We transfer ubiquitous touch interaction with haptic feedback into a virtual environment. To validate the performance of our implementation, we performed a user study with 28 participants. As the results show, the movable and cheap real world object supplies an accurate touch detection that is equal to a laserpointer-based interaction with a controller. Moreover, the virtual tablet can extend the functionality of a real world tablet. Additional information can be displayed in mid-air around the touchable area and the tablet can be turned over to interact with both sides. Therefore, touch interaction in virtual environments allows easy to learn and precise system interaction and can even augment the established touch metaphor with new paradigms. © 2020 ASTES Publishers. All rights reserved.","Haptic feedback; Touch interaction; Virtual environment; Virtual reality; VirtualTablet",,Article,"Final","",Scopus,2-s2.0-85087525968
"Zhao X., Liu C., Xu Z., Zhang L., Zhang R.","17436661300;57214105967;57214126904;57201266084;56898266500;","SSVEP Stimulus Layout Effect on Accuracy of Brain-Computer Interfaces in Augmented Reality Glasses",2020,"IEEE Access","8",, 8947980,"5990","5998",,,"10.1109/ACCESS.2019.2963442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078237308&doi=10.1109%2fACCESS.2019.2963442&partnerID=40&md5=149148c8923287a093bfa864726e505a","School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China","Zhao, X., School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Liu, C., School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; Xu, Z., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China; Zhang, L., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China; Zhang, R., Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, School of Electrical Engineering, Zhengzhou University, Zhengzhou, 450001, China","Steady-state visual evoked potentials-based brain-computer interfaces (SSVEP-BCI) has the advantage of high information transfer rate (ITR) and little user training, and it has a high application value in the field of disability assistance and human-computer interaction. Generally SSVEP-BCI requires a personal computer screen (PC) to display several repetitive visual stimuli for inducing the SSVEP response, which reduces its portability and flexibility. Using augmented reality (AR) glasses worn on the head to display the repetitive visual stimuli could solve the above drawbacks, but whether it could achieve the same accuracy as PC screen in the case of reduced brightness and increased interference is unknown. In current study, we firstly designed 4 stimulus layouts and displayed them with Microsoft HoloLens (AR-SSVEP) glasses, comparison analysis showed that the classification accuracies are influenced by the stimulus layout when the stimulus duration is less than 3s. When the stimulus duration exceeds 3s, there is no significant accuracy difference between the 4 layouts. Then we designed a similar experimental paradigm on PC screen (PC-SSVEP) based on the best layout of AR. Classification results showed that AR-SSVEP achieved similar accuracy with PC-SSVEP when the stimulus duration is more than 3s, but when the stimulus duration is less than 2s, the accuracy of AR-SSVEP is lower than PC-SSVEP. Brain topological analysis indicated that the spatial distribution of SSVEP responses is similar, both of which are strongest in the occipital region. Current study indicated that stimulus layout is a key factor when building SSVEP-BCI with AR glasses, especially when the stimulation time is short. © 2013 IEEE.","augmented reality (AR); brain-computer interfaces (BCI); human-computer interaction; optical see-through (OST); Steady-state visual evoked potentials (SSVEP)","Augmented reality; Electrophysiology; Glass; Human computer interaction; Interface states; Personal computers; Topology; Classification accuracy; Classification results; Comparison analysis; High application value; Information transfer rate; Optical see-through; Steady state visual evoked potentials; Topological analysis; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85078237308
"Panchenko L.F., Muzyka I.O.","57210124439;56768036800;","Analytical review of augmented reality MOOCs",2020,"CEUR Workshop Proceedings","2547",,,"168","180",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079571403&partnerID=40&md5=a1a96532d1e6dec292a88b4cf89f9478","National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, 37, Peremohy Ave., Kyiv, 03056, Ukraine; Kryvyi Rih National University, 11, Vitaliy Matusevych Str., Kryvyi Rih, 50027, Ukraine","Panchenko, L.F., National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, 37, Peremohy Ave., Kyiv, 03056, Ukraine; Muzyka, I.O., Kryvyi Rih National University, 11, Vitaliy Matusevych Str., Kryvyi Rih, 50027, Ukraine","The aim of the article is to provide an analytical review of the content of massive open online courses about augmented reality and its use in education with the further intent to create a special course for the professional development system for the research and teaching personnel in postgraduate education. The object of research is massive open online courses. The subject of the study is the structure and content of augmented reality MOOCs which are offered by acclaimed providers of the world. The methods of research are: the analysis of publications on the problem; the analysis of MOOCs’ content, including observation; systematization and generalization of research information in order to design a special course about augmented reality for the system of professional training and retraining for educators in postgraduate education. The results of the research are the following: the content and program of specialized course “Augmented Reality as a Storytelling Tool” for the professional development of teachers. The purpose of the specialized course is to consider and discuss the possibilities of augmented reality as a new direction in the development of educational resources, to identify its benefits and constraints, as well as its components and the most appropriate tools for educators, to discuss the problems of teacher and student co-creation on the basis of the use of augmented reality, and to provide students with personal experience in designing their own stories and methodical tools in the form of augmented books and supplementary training aids with the help of modern digital services. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Augmented books; Augmented reality; Massive open online courses; Professional training; Retraining","Augmented reality; E-learning; Information services; Personnel training; Professional aspects; Students; Teaching; Augmented book; Massive open online course; Methods of researches; Postgraduate education; Professional development; Professional development of teachers; Professional training; Retraining; Curricula",Conference Paper,"Final","",Scopus,2-s2.0-85079571403
"Riemann T., Kreß A., Roth L., Klipfel S., Metternich J., Grell P.","57216947870;57194567019;57216952566;57216945850;6507096724;57216945501;","Agile implementation of virtual reality in learning factories",2020,"Procedia Manufacturing","45",,,"1","6",,5,"10.1016/j.promfg.2020.04.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085522794&doi=10.1016%2fj.promfg.2020.04.029&partnerID=40&md5=7eb2225c75269422795ea229476aa652","Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany","Riemann, T., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany; Kreß, A., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany; Roth, L., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany; Klipfel, S., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany; Metternich, J., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany; Grell, P., Technische Universität Darmstadt, Otto-Berndt-Str. 2, Darmstadt, 64287, Germany","The concept of learning factories fulfills current learning theoretical requirements in terms of the situation, process orientation, as well as authenticity. Nevertheless, due to the high complexity of the industrial production environment, it is challenging to transfer learned skills into the operational application situation. With Virtual Reality, training participants have the ability to learn with transfer-oriented action tasks in virtual space directly after the training in physical learning environments. The learning process can be personalized and adapted in the virtual learning environment. Each participant in the training can individually determine elements of the learning situation. For example, the entire learning environment can be adapted to the individual real production environment of the training participant. Through Virtual Reality, new forms of reflection are possible, e.g. recording the learning process. Technical, didactic and organizational requirements were identified by a systematic literature analysis. The research project is based on training courses in the process learning factory “Center for industrial Productivity” (CiP) located at TU Darmstadt. In order to assess and prioritize the requirements, expert surveys were conducted. The surveys are based on the Kano model in order to classify requirements. Must-be quality requirements are implemented in a minimum viable product (MVP). The MVP allows fast learning by testing and experimenting. Based on the agile manifesto, further requirements can be implemented agilely in the virtual environment. © 2020 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the 10th Conference on Learning Factories 2020.","Agile Project Management; Kano Model; Learning Factory; Learning Scenario; Virtual Reality",,Conference Paper,"Final","",Scopus,2-s2.0-85085522794
"Schwarze A., Kampling H., Niehaves B.","57217049797;57189869875;12139031400;","Advantages and propositions of learning emotion recognition in virtual reality for people with austism",2020,"27th European Conference on Information Systems - Information Systems for a Sharing Society, ECIS 2019",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087108839&partnerID=40&md5=3f886071823c7f5052fd3f89580e1d9f","University of Siegen, Siegen, Germany","Schwarze, A., University of Siegen, Siegen, Germany; Kampling, H., University of Siegen, Siegen, Germany; Niehaves, B., University of Siegen, Siegen, Germany","People with an autism spectrum disorder face the daily challenge of social interactions - particularly in non-verbal communication. These difficulties make adequate interpersonal interactions “in real-time” a challenging obstacle to overcome in many cases and can lead to excessive demands, frustration and isolation (low level of Theory of Mind). Emotion cards are usually used in autism therapy to learn basic skills for recognizing emotions. Learning with autism is characterized by spontaneous - sometimes-extraordinary - mastery of complex contents. People with autism learn facts, details and routines well but have difficulties to transfer the learned contents to another context (Weak Central Coherence) or to react flexible to unpredicted events (low Executive Function). In addition, research has shown that autistics learn social competences while using a computer and performing practical exercises. Such systems provide the possibility to use an accepted computer simulated (virtual) environment in which autistic children can be taught social competences as emotion recognition. Consequently, we assume that learning emotion recognition in virtual learning environments can remove barriers and obstacles for autistics as they are more successful in solving social problems. Therefore, we are discussing in the paper at hand the potentials of how emotion recognition can be learned in virtual reality. © 27th European Conference on Information Systems - Information Systems for a Sharing Society, ECIS 2019. All rights reserved.","Autism; Case Study Research; Emotion Recognition Learning; Virtual Reality","Diseases; E-learning; Information systems; Information use; Speech recognition; Virtual reality; Autism spectrum disorders; Emotion recognition; Non-verbal communications; Recognizing emotions; Social competences; Social interactions; Virtual learning environments; Weak central coherences; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-85087108839
"Prumes M., da Silva T.D., de Oliveira Alberissi C.A., Capelini C.M., Del Ciello de Menezes L., da Rocha J.B.F., Favero F.M., de Mello Monteiro C.B.","55358258900;55546962700;57222091652;57195469943;57222098295;57222101223;36113108600;36952872700;","Motor learning through a non-immersive virtual task in people with limb-girdle muscular dystrophies",2020,"Journal of Human Growth and Development","30","3",,"461","471",,1,"10.7322/jhgd.v30.11115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101350817&doi=10.7322%2fjhgd.v30.11115&partnerID=40&md5=feff79ca53a954d44af79380bd99c3c1","Laboratório de Delineamento de Estudos e Escrita Científica. Centro Universitário Saúde ABC (FMABC), Santo André, SP, Brazil; Departamento de Cardiologia, Escola Paulista de Medicina, Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil; Faculdade de Medicina, Universidade Cidade de São Paulo (UNICID), São Paulo, SP, Brazil; Programa de Pós-Graduação em Ciências da Reabilitação, Faculdade de Medicina da Universidade de São Paulo, São Paulo, SP, Brazil; Escola de Artes, Ciências e Humanidades (EACH), Universidade de São Paulo (USP), São Paulo, SP, Brazil; Setor de Investigação nas Doenças Neuromusculares, Departamento de Neurologia e Neurocirurgia, Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil","Prumes, M., Laboratório de Delineamento de Estudos e Escrita Científica. Centro Universitário Saúde ABC (FMABC), Santo André, SP, Brazil; da Silva, T.D., Departamento de Cardiologia, Escola Paulista de Medicina, Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil, Faculdade de Medicina, Universidade Cidade de São Paulo (UNICID), São Paulo, SP, Brazil, Programa de Pós-Graduação em Ciências da Reabilitação, Faculdade de Medicina da Universidade de São Paulo, São Paulo, SP, Brazil; de Oliveira Alberissi, C.A., Escola de Artes, Ciências e Humanidades (EACH), Universidade de São Paulo (USP), São Paulo, SP, Brazil; Capelini, C.M., Programa de Pós-Graduação em Ciências da Reabilitação, Faculdade de Medicina da Universidade de São Paulo, São Paulo, SP, Brazil; Del Ciello de Menezes, L., Departamento de Cardiologia, Escola Paulista de Medicina, Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil; da Rocha, J.B.F., Laboratório de Delineamento de Estudos e Escrita Científica. Centro Universitário Saúde ABC (FMABC), Santo André, SP, Brazil; Favero, F.M., Setor de Investigação nas Doenças Neuromusculares, Departamento de Neurologia e Neurocirurgia, Universidade Federal de São Paulo (UNIFESP), São Paulo, SP, Brazil; de Mello Monteiro, C.B., Laboratório de Delineamento de Estudos e Escrita Científica. Centro Universitário Saúde ABC (FMABC), Santo André, SP, Brazil, Programa de Pós-Graduação em Ciências da Reabilitação, Faculdade de Medicina da Universidade de São Paulo, São Paulo, SP, Brazil, Escola de Artes, Ciências e Humanidades (EACH), Universidade de São Paulo (USP), São Paulo, SP, Brazil","Introduction: Limb-girdle muscular dystrophies (LGMDs) are neuromuscular and genetic disorders that progress with weakness and damage of the proximal muscles, developing with loss of functionality. Virtual reality environments are suggested as an effective alternative for performance of daily life activities. However, there is no evidence in the literature on the use of virtual reality in this population. Objective: Assess motor performance through a motor learning protocol in a coincident timing task. Methods: 10 participants with LGMD and 10 healthy individuals were selected and included in the study to perform a non-immersive virtual reality task divided into three phases: acquisition (20 attempts), retention (5 attempts), and transfer (5 attempts, with speed increase). Results: It is observed that the accuracy of movement improves from the beginning to the end of the acquisition (p = 0.01); however, there is a marginal difference between the groups in block A1 (p = 0.089). Regarding the variability of touches, observed by the variable error, both groups improved performance in all phases. Conclusion: Even with lower performance than the control group at the beginning of the practice, individuals with LGMD showed the potential to optimize motor function during the practice of a non-immersive virtual reality activity and were able to match their performance with the control group after a few attempts. © The authors (2020). this article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.","Limb-Girdle Muscular Dystrophies; Motor Learning; Muscular Dystrophies; Virtual Reality",,Article,"Final","",Scopus,2-s2.0-85101350817
"Menn J.P., Severengiz M., Lorenz A.K., Wassermann J., Ulbrich C., Krüger J., Seliger G.","57189306069;56790112300;57216839260;57195326425;57189299534;36190849500;56789736100;","Augmented learning for industrial education",2020,"International Journal of Sustainable Manufacturing","4","2-4",,"396","412",,,"10.1504/IJSM.2020.107139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084932104&doi=10.1504%2fIJSM.2020.107139&partnerID=40&md5=353d1674a1560ca510572f55a1d87a57","Department of Handling and Manufacturing Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Department of Industrial Automation Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Design, ECC Integrally Geared Compressor, MAN Energy Solutions SE Berlin, Egellsstraße 21, Berlin, 13507, Germany","Menn, J.P., Department of Handling and Manufacturing Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Severengiz, M., Department of Industrial Automation Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Lorenz, A.K., Department of Handling and Manufacturing Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Wassermann, J., Department of Industrial Automation Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Ulbrich, C., Design, ECC Integrally Geared Compressor, MAN Energy Solutions SE Berlin, Egellsstraße 21, Berlin, 13507, Germany; Krüger, J., Department of Industrial Automation Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany; Seliger, G., Department of Handling and Manufacturing Technology, Technische Universität Berlin, Pascalstraße 8-9, Berlin, 10587, Germany","An efficient learning environment is required to cope with today's increasing innovation speed. Companies need methods and tools to transfer knowledge to employees in a fast way. Learners' cognitive focus should be shifted towards learning at the learning object, instead of transferring information from teaching material to the real world. Current learning environments are mostly incapable to merge physical learning tools with digital content at its point of use; therefore, the learner has to do it. Augmented reality offers the opportunity to show learning content directly on physical objects and to interact with it. Within this paper, two approaches on how to use augmented reality for teaching purposes are shown. One is for special machinery assembly of turbomachinery and the other for cocoa liquor production. © 2020 Inderscience Enterprises Ltd.","Augmented reality; Cocoa liquor production; Learning; Learnstruments; Special machinery","Augmented reality; Machinery; Efficient learning; Industrial education; Innovation speed; Learning contents; Learning environments; Learning objects; Physical objects; Teaching materials; Computer aided instruction",Article,"Final","",Scopus,2-s2.0-85084932104
"Coelho A., Cardoso P., Van Zeller M., Santos L., Raimundo J., Vaz R.","23089899600;57192416173;57217996598;57217605046;57216526950;57220402899;","Gamifying the museological experience",2020,"CEUR Workshop Proceedings","2618",,,"5","8",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087832942&partnerID=40&md5=e6609bfec2f83285b55a85e428879493","FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; FBAUP, Faculty of Fine Arts, University of Porto, Avenida Rodrigues de Freitas, 265, Porto, 4049-021, Portugal; INESC TEC, Institute for Systems and Computer Engineering, Technology and Science, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal","Coelho, A., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal, INESC TEC, Institute for Systems and Computer Engineering, Technology and Science, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; Cardoso, P., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal, FBAUP, Faculty of Fine Arts, University of Porto, Avenida Rodrigues de Freitas, 265, Porto, 4049-021, Portugal, INESC TEC, Institute for Systems and Computer Engineering, Technology and Science, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; Van Zeller, M., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; Santos, L., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; Raimundo, J., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal, INESC TEC, Institute for Systems and Computer Engineering, Technology and Science, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; Vaz, R., FEUP, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal","Museums continue to exert fascination in their visitors. However, the new generation of visitors expects museological experiences that promote their active participation. It is in this context that games and the gamification of such experiences capitalize on experiential learning by experimenting and enacting with in-game embedded artefact surrogates and know-how. In this article, we present four distinct projects that aim to enhance the visitors' experience in museums and green spaces, and also their effectiveness in informal learning. In the first project, gamification is used in combination with Augmented Reality to provide a more engaging experience in a boat museum. The drive of this experience is the metaphor of the stickers album collection to unleash the relevant information of the key-artefacts of the museum collection. The second and third projects focus on the use of pervasive games, more specifically location-based games, to enhance the visitors' experience and informal learning in a natural park and a botanical garden, respectively. The second project presents the concept of a mobile app for outdoor nature experiences. The drive for the experience in the third project is the narrative that intertwines specific locations in the botanic garden and a story inspired by the same place. Finally, in the fourth project, we focus on the potential of technology to provide accessibility in museums for people with special needs or disability, focusing more specifically on blind visitors. Copyright © 2020 for this paper by its authors.","Accessibility; Augmented Reality; Gamification; Location-based games; Pervasive games","Augmented reality; Gamification; Human computer interaction; Technology transfer; Botanical gardens; Experiential learning; Informal learning; Location based games; Museum collections; Pervasive game; Special needs; Specific location; Museums",Conference Paper,"Final","",Scopus,2-s2.0-85087832942
"Yuan F.-G., Zargar S.A., Chen Q., Wang S.","7201350037;57211514292;57217009972;57218333667;","Machine learning for structural health monitoring: Challenges and opportunities",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11379",, 1137903,"","",,7,"10.1117/12.2561610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085696005&doi=10.1117%2f12.2561610&partnerID=40&md5=6d85ba5f0d5cbe49677d0882bd7ce991","Department of Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, NC  27606, United States","Yuan, F.-G., Department of Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, NC  27606, United States; Zargar, S.A., Department of Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, NC  27606, United States; Chen, Q., Department of Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, NC  27606, United States; Wang, S., Department of Mechanical and Aerospace Engineering, North Carolina State University, Raleigh, NC  27606, United States","A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented. © 2020 SPIE.","Artificial neural networks; Augmented reality; Damage diagnosis; Impact diagnosis; Machine learning; Physics-informed learning; Structural health monitoring; Visual inspection","Damage detection; Dynamic models; Inverse problems; Learning systems; Neural networks; Nondestructive examination; Supervised learning; Wireless sensor networks; Controlled environment; Data-driven approach; Different damages; Full-field measurement; Sensing technology; Simple structures; Structural health monitoring (SHM); Supervised machine learning; Structural health monitoring",Conference Paper,"Final","",Scopus,2-s2.0-85085696005
"Nikouline A., Jimenez M.C., Okrainec A.","47461465000;57198326439;6508182372;","Feasibility of remote administration of the fundamentals of laparoscopic surgery (FLS) skills test using Google wearable device",2020,"Surgical Endoscopy","34","1",,"443","449",,,"10.1007/s00464-019-06788-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065202277&doi=10.1007%2fs00464-019-06788-w&partnerID=40&md5=f4d93a9dae247ee244cc935b13172ff4","Division of Emergency Medicine, Department of Medicine, University of Toronto, Toronto, Canada; Division of General Surgery, University Health Network (UHN), Toronto, Canada; Department of Surgery, University of Toronto, Toronto, Canada; Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada; University Health Network (UHN), 399 Bathurst Street, Toronto, ON  M5T 2S8, Canada","Nikouline, A., Division of Emergency Medicine, Department of Medicine, University of Toronto, Toronto, Canada; Jimenez, M.C., Division of General Surgery, University Health Network (UHN), Toronto, Canada, Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada; Okrainec, A., Division of General Surgery, University Health Network (UHN), Toronto, Canada, Department of Surgery, University of Toronto, Toronto, Canada, Temerty/Chang Telesimulation Centre, University Health Network (UHN), Toronto, Canada, University Health Network (UHN), 399 Bathurst Street, Toronto, ON  M5T 2S8, Canada","Background: The fundamentals of laparoscopic surgery (FLS) program is a simulation-based training program designed to teach and assess the basic skills necessary for laparoscopic surgery. Preliminary work has demonstrated the feasibility of using Skype™ as a telesimulation modality in reliably scoring the exam for remote centers. Google Glass (GG) (Mountain View, California) is referred to as a wearable computer containing a heads-up display and front-facing camera allowing point-of-view video transmission. The objective of this study was to evaluate the feasibility of GG in scoring the technical skills component of the FLS exam. Methods: Twenty-eight participants were asked to complete the peg transfer and intracorporeal knot tasks of FLS using both GG and Skype™ setups. GG employed a third-party HIPAA-compliant video software (Pristine; Austin, TX) for video transmission. Participants were alternated between setups and evaluated by onsite and remote proctors. Times and errors were recorded by both proctors. Interrater reliability of their FLS scores was compared using Intraclass Correlation Coefficients (ICCs). GG experience was evaluated based on participant survey responses using a 5-point Likert scale. Results: Interrater reliability for GG demonstrated a statistically significant correlation between onsite (OP) and remote (RP) proctors with ICCs of 0.985 (95% Confidence Interval [CI], 0.969–0.993) and 0.997 (95% CI 0.993–0.998), respectively, for peg and suture tasks. Skype™ demonstrated ICCs of 1.0 (95% CI 1.0–1.0). Average Likert scale responses found GG to be distracting (2.71), obstructive of the view (2.79), and a limitation to task execution (2.75). Overall, there was no statistical difference in scores between GG and Skype™ setups for either the peg (t = 1.446, p = 0.154) or suture task (t = − 0.710, p = 0.480), only 1 participant found the use of GG superior to Skype™. Conclusions: Our findings suggest that although GG are feasible in remote assessment of FLS with strong interrater reliability (ICC > 0.95), Skype™ was the preferred modality. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Fundamentals of laparoscopic surgery (FLS); Google Glass; Telesimulation","adult; Article; computer simulation; error; female; human; interrater reliability; laparoscopic surgery; male; operating room personnel; pilot study; priority journal; surgical training; telehealth; time; videoconferencing; Canada; clinical competence; education; electronic device; feasibility study; laparoscopy; procedures; reproducibility; simulation training; Adult; Canada; Clinical Competence; Education, Distance; Feasibility Studies; Female; Humans; Laparoscopy; Male; Pilot Projects; Reproducibility of Results; Simulation Training; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85065202277
"Hu Z., Gan Z., Li W., Wen J.Z., Zhou D., Wang X.","57222472066;54419884000;57196304553;57222473067;57222463161;57209803457;","Two-stage model-agnostic meta-learning with noise mechanism for one-shot imitation",2020,"IEEE Access","8",,,"182720","182730",,,"10.1109/ACCESS.2020.3029220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102776246&doi=10.1109%2fACCESS.2020.3029220&partnerID=40&md5=9069a4f84b7efd4a3f4c6ed872c30525","Academy for Engineer and Technology, Fudan University, Shanghai, 200433, China; Jihua Laboratory, Foshan, 528000, China","Hu, Z., Academy for Engineer and Technology, Fudan University, Shanghai, 200433, China; Gan, Z., Academy for Engineer and Technology, Fudan University, Shanghai, 200433, China, Jihua Laboratory, Foshan, 528000, China; Li, W., Academy for Engineer and Technology, Fudan University, Shanghai, 200433, China, Jihua Laboratory, Foshan, 528000, China; Wen, J.Z., Jihua Laboratory, Foshan, 528000, China; Zhou, D., Jihua Laboratory, Foshan, 528000, China; Wang, X., Academy for Engineer and Technology, Fudan University, Shanghai, 200433, China","Given that humans and animals can learn new behaviors in a short time by observing others, the question we need to consider is how to make robots behave like humans or animals, that is, through effective demonstration, robots can quickly understand and learn a new ability. One possible solution is imitation based meta-learning, but most of the related approaches are limited in a particular network structure or a specific task. Particularly, meta-learning methods based on gradient-update are prone to overfit. In this article, we propose a generic meta-learning algorithm that divides the learning process into two independent stages (skill cloning and skill transfer) with a noise mechanism which is compatible with any model. The skill cloning stage enables a good understanding of the demonstration, which helps the skill transfer stage when the robot applies the learned experience into new tasks. The experimental results show that our algorithm can alleviate the phenomenon of overfitting by introducing a noise mechanism. Our method not only performs well on the regression task but is significantly better than the existing state-of-the-art one-shot imitation learning methods in the same simulation environments (i.e., simulated pushing and simulated reaching). © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Learning to learn; Meta-learning; One-shot imitation learning; Robot learning","Animals; Clone cells; Cloning; Learning systems; Robots; Imitation learning; Learning process; Network structures; Noise mechanisms; Simulation environment; Skill transfer; State of the art; Two stage model; Learning algorithms",Article,"Final","",Scopus,2-s2.0-85102776246
"Patel E.A., Aydin A., Cearns M., Dasgupta P., Ahmed K.","57205083589;55976433600;57212668191;7201405986;15764669400;","A Systematic Review of Simulation-Based Training in Neurosurgery, Part 1: Cranial Neurosurgery",2020,"World Neurosurgery","133",,,"e850","e873",,7,"10.1016/j.wneu.2019.08.262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073812751&doi=10.1016%2fj.wneu.2019.08.262&partnerID=40&md5=7c1b07e843f0159be1844632b7633443","Barts and the London School of Medicine and Dentistry, Queen Mary University of London, London, United Kingdom; MRC Centre for Transplantation, King's College London, London, United Kingdom; Department of Neurosurgery, Institute of Neurological Sciences, Glasgow, United Kingdom","Patel, E.A., Barts and the London School of Medicine and Dentistry, Queen Mary University of London, London, United Kingdom; Aydin, A., MRC Centre for Transplantation, King's College London, London, United Kingdom; Cearns, M., Department of Neurosurgery, Institute of Neurological Sciences, Glasgow, United Kingdom; Dasgupta, P., MRC Centre for Transplantation, King's College London, London, United Kingdom; Ahmed, K., MRC Centre for Transplantation, King's College London, London, United Kingdom","Objective: The recent emphasis on simulation-based training in neurosurgery has led to the development of many simulation models and training courses. We aim to identify the currently available simulators and training courses for neurosurgery, assess their validity, and determine their effectiveness. Methods: Both MEDLINE and Embase were searched for English language articles which validate simulation models for neurosurgery. Each study was screened according to the Messick validity framework and rated in each domain. The McGaghie model of translational outcomes was then used to determine a level of effectiveness (LoE) for each simulator or training course. Results: On screening of 6006 articles, 114 were identified to either validate or determine an LoE for 108 simulation-based training models or courses. Achieving the highest rating for each validity domain were 6 models and training courses for content validity, 12 for response processes, 4 for internal structure, 14 for relations to other variables, and none for consequences. For translational outcomes, 6 simulators or training achieved an LoE >2 and thus showed skills transfer beyond the simulation setting. Conclusions: With the advent of increasing neurosurgery simulators and training tools, there is a need for more validity studies. Further attempts to investigate translational outcomes to the operating theater when using these simulators is particularly warranted. More training tools incorporating full-immersion simulation and nontechnical skills training are recommended. © 2019 Elsevier Inc.","Education; Neurosurgery; Simulation; Training","Article; cancer surgery; clinical outcome; endoscopic endonasal surgery; human; neurosurgery; simulation training; surgical training; systematic review; transsphenoidal surgery; ventriculostomy; brain; clinical competence; education; neurosurgery; procedures; simulation training; skull; surgery; Brain; Clinical Competence; Humans; Neurosurgical Procedures; Simulation Training; Skull",Article,"Final","",Scopus,2-s2.0-85073812751
"Marez D., Borden S., Nans L.","57197782269;57211069473;57218159417;","UAV detection with a dataset augmented by domain randomization",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11398",, 1139807,"","",,,"10.1117/12.2558864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088093023&doi=10.1117%2f12.2558864&partnerID=40&md5=2d25abb51fbcf4958dfbb901b2e45771","Naval Information Warfare Center Pacific, Point Loma, CA, United States","Marez, D., Naval Information Warfare Center Pacific, Point Loma, CA, United States; Borden, S., Naval Information Warfare Center Pacific, Point Loma, CA, United States; Nans, L., Naval Information Warfare Center Pacific, Point Loma, CA, United States","Object detection for computer vision systems continues to be a complicated problem in real-world situations. For instance, autonomous vehicles need to operate with very small margins of error as they encounter safety-critical scenarios such as pedestrian and vehicle detection. The increased use of unmanned aerial vehicles (UAVs) by both government and private citizens has created a need for systems which can reliably detect UAVs in a large variety of conditions and environments. In order to achieve small margins of error, object detection systems, especially those reliant on deep learning methods, require large amounts of annotated data. The use of synthetic datasets provides a way to alleviate the need to collect annotated data. Unfortunately, the nature of synthetic dataset generation introduces a reality and simulation gap that hinders an object detector's ability to generalize on real world data. Domain randomization is a technique that generates a variety of different scenarios in a randomized fashion both to close the reality and simulation gap and to augment a hand-crafted dataset. In this paper, we combine the AirSim simulation environment with domain randomization to train a robust object detector. As a final step, we fine-tune our object detector on real-world data and compare it with object detectors trained solely on real-world data. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep Learning; Domain Randomization; Object Detection; Synthetic Data; UAVs","Advanced driver assistance systems; Aircraft detection; Antennas; Computer vision; Deep learning; Error detection; Learning systems; Object recognition; Pedestrian safety; Random processes; Unmanned aerial vehicles (UAV); Computer vision system; Learning methods; Object detection systems; Object detectors; Real world situations; Simulation environment; Synthetic datasets; Vehicle detection; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85088093023
"Patel E.A., Aydin A., Cearns M., Dasgupta P., Ahmed K.","57205083589;55976433600;57212668191;7201405986;15764669400;","A Systematic Review of Simulation-Based Training in Neurosurgery, Part 2: Spinal and Pediatric Surgery, Neurointerventional Radiology, and Nontechnical Skills",2020,"World Neurosurgery","133",,,"e874","e892",,4,"10.1016/j.wneu.2019.08.263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073761722&doi=10.1016%2fj.wneu.2019.08.263&partnerID=40&md5=a0b1c88d1697564e9dd2c9ddb7f78c45","Barts and the London School of Medicine and Dentistry, Queen Mary University of London, London, United Kingdom; MRC Centre for Transplantation, King's College London, London, United Kingdom; Department of Neurosurgery, Institute of Neurological Sciences, Glasgow, United Kingdom","Patel, E.A., Barts and the London School of Medicine and Dentistry, Queen Mary University of London, London, United Kingdom; Aydin, A., MRC Centre for Transplantation, King's College London, London, United Kingdom; Cearns, M., Department of Neurosurgery, Institute of Neurological Sciences, Glasgow, United Kingdom; Dasgupta, P., MRC Centre for Transplantation, King's College London, London, United Kingdom; Ahmed, K., MRC Centre for Transplantation, King's College London, London, United Kingdom","Objective: The increasing challenges facing the training of future neurosurgeons have led to continued development of simulation-based training, particularly for neurosurgical subspecialties. The simulators must be scientifically validated to fully assess their benefit and determine their educational effects. In this second part, we aim to identify the available simulators for spine, pediatric neurosurgery, interventional neuroradiology, and nontechnical skills, assess their validity, and determine their effectiveness. Methods: Both Medline and Embase were searched for English language articles that validate simulation models for neurosurgery. Each study was screened according to the Messick validity framework, and rated in each domain. The McGaghie model of translational outcomes was then used to determine a level of effectiveness for each simulator or training course. Results: Overall, 114 articles for 108 simulation-based training models or courses were identified. These articles included 24 for spine simulators, 3 for nontechnical skills, 10 for 9 pediatric neurosurgery simulators, and 12 for 11 interventional neuroradiology simulators. Achieving the highest rating for each validity domain were 3 models for content validity; 16 for response processes; 1 for internal structure; 2 for relations to other variables; and only 1 for consequences. For translational outcomes, 2 training courses achieved a level of effectiveness of >2, showing skills transfer beyond the simulator environment. Conclusions: With increasing simulators, there is a need for more validity studies and attempts to investigate translational outcomes to the operating theater when using these simulators. Nontechnical skills training is notably lacking, despite demand within the field. © 2019 Elsevier Inc.","Education; Neurosurgery; Simulation; Training","Article; cadaver; human; neuroradiology; neurosurgeon; neurosurgery; pediatric surgery; skill; spine surgery; surgical training; systematic review; child; clinical competence; education; procedures; radiology; simulation training; spinal cord; spine; surgery; Child; Clinical Competence; Humans; Neurosurgical Procedures; Radiology; Simulation Training; Spinal Cord; Spine",Article,"Final","",Scopus,2-s2.0-85073761722
"Gjoreski M., Gams M.Ž., Luštrek M., Genc P., Garbas J.-U., Hassan T.","56470741800;35617835400;12796966600;57209510645;8367980700;57193091146;","Machine Learning and End-to-End Deep Learning for Monitoring Driver Distractions from Physiological and Visual Signals",2020,"IEEE Access","8",, 9062481,"70590","70603",,4,"10.1109/ACCESS.2020.2986810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083889984&doi=10.1109%2fACCESS.2020.2986810&partnerID=40&md5=53469c45f932b96cb1ddd88970e9d6af","Jožef Stefan Institute, Ljubljana, 1000, Slovenia; Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany","Gjoreski, M., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Gams, M.Ž., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Luštrek, M., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Genc, P., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany; Garbas, J.-U., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany; Hassan, T., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany","It is only a matter of time until autonomous vehicles become ubiquitous; however, human driving supervision will remain a necessity for decades. To assess the driver's ability to take control over the vehicle in critical scenarios, driver distractions can be monitored using wearable sensors or sensors that are embedded in the vehicle, such as video cameras. The types of driving distractions that can be sensed with various sensors is an open research question that this study attempts to answer. This study compared data from physiological sensors (palm electrodermal activity (pEDA), heart rate and breathing rate) and visual sensors (eye tracking, pupil diameter, nasal EDA (nEDA), emotional activation and facial action units (AUs)) for the detection of four types of distractions. The dataset was collected in a previous driving simulation study. The statistical tests showed that the most informative feature/modality for detecting driver distraction depends on the type of distraction, with emotional activation and AUs being the most promising. The experimental comparison of seven classical machine learning (ML) and seven end-to-end deep learning (DL) methods, which were evaluated on a separate test set of 10 subjects, showed that when classifying windows into distracted or not distracted, the highest F1-score of 79% was realized by the extreme gradient boosting (XGB) classifier using 60-second windows of AUs as input. When classifying complete driving sessions, XGB's F1-score was 94%. The best-performing DL model was a spectro-temporal ResNet, which realized an F1-score of 75% when classifying segments and an F1-score of 87% when classifying complete driving sessions. Finally, this study identified and discussed problems, such as label jitter, scenario overfitting and unsatisfactory generalization performance, that may adversely affect related ML approaches. © 2013 IEEE.","deep learning; driver distraction; facial expressions; Machine learning; sensors","Adaptive boosting; Chemical activation; Classification (of information); Eye tracking; Learning systems; Physiology; Vehicles; Video cameras; Wearable sensors; Driver distractions; Driving distractions; Driving simulation; Electrodermal activity; Experimental comparison; Generalization performance; Physiological sensors; Research questions; Deep learning",Article,"Final","",Scopus,2-s2.0-85083889984
"Pianini D., Casadei R., Viroli M., Natali A.","36997527800;55358634800;55665532100;7007127383;","Partitioned integration and coordination via the self-organising coordination regions pattern",2020,"Future Generation Computer Systems","114",,,"44","68",,4,"10.1016/j.future.2020.07.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089073530&doi=10.1016%2fj.future.2020.07.032&partnerID=40&md5=bdd6d50484c7a2518425582575614f8b","Department of Computer Science and Engineering, Alma Mater Studiorum—Università di Bologna, Cesena, Italy","Pianini, D., Department of Computer Science and Engineering, Alma Mater Studiorum—Università di Bologna, Cesena, Italy; Casadei, R., Department of Computer Science and Engineering, Alma Mater Studiorum—Università di Bologna, Cesena, Italy; Viroli, M., Department of Computer Science and Engineering, Alma Mater Studiorum—Università di Bologna, Cesena, Italy; Natali, A., Department of Computer Science and Engineering, Alma Mater Studiorum—Università di Bologna, Cesena, Italy","In software engineering, knowledge about recurrent problems, along with blueprints of associated solutions for diverse design contexts, are often captured in so-called design patterns. Identifying design patterns is particularly valuable for novel and still largely unexplored application contexts such as the Internet of Things, Cyber–Physical Systems, and Edge Computing, as it would help keeping a balanced trade-off between generality and applicability, guiding the mainstream development of language mechanisms, algorithms, architectures, and supporting platforms. Based on recurrence of related solutions found in the literature, in this work we present a design pattern for self-adaptive systems, named Self-organising Coordination Regions (SCR): its goal is to organise a process of interconnecting devices into teams, to solve local tasks in cooperation. Specifically, it is a decentralised coordination pattern for partitioned integration and coordination of devices, which relies on continuous adaptivity to context change to provide resilient distributed decision-making in large-scale situated systems. It leverages the divide-and-conquer principle, partitioning (in a self-organising fashion) the network of devices into regions, where internal coordination activities are regulated via feedback/control flows among leaders and subordinate nodes. We present the pattern, provide a template implementation in the Aggregate Computing framework, and evaluate it through simulation of two case studies in edge computing and hierarchical, heterogeneous networks. © 2020 Elsevier B.V.","Aggregate programming; Coordination; Design patterns; Distributed systems; Edge computing; Self-improving integration; Self-organisation","Adaptive systems; Computer architecture; Decision making; Economic and social effects; Edge computing; Heterogeneous networks; Large scale systems; Recurrent neural networks; Software engineering; Application contexts; Coordination patterns; Distributed decision making; Divide-and-conquer principle; Interconnecting device; Internal coordination; Partitioned integration; Self-adaptive system; Human resource management",Article,"Final","",Scopus,2-s2.0-85089073530
"Dhiman H., Buttner S., Rocker C., Reisch R.","57202890221;56954752800;55919922900;57214365751;","Handling work complexity with ar/deep learning",2019,"ACM International Conference Proceeding Series",,,,"518","522",,1,"10.1145/3369457.3370919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078696444&doi=10.1145%2f3369457.3370919&partnerID=40&md5=197e2b1fe56134c8ada58b36cf5d8ccf","Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Fraunhofer IOSB-INA, Lemgo, Germany; Resolto Informatik GmbH, Herford, Germany","Dhiman, H., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany; Buttner, S., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Human-Centered Information Systems, Clausthal University of Technology, Clausthal-Zellerfeld, Germany; Rocker, C., Institute Industrial IT, Ostwestfalen-Lippe University of Applied Sciences and Arts, Lemgo, Germany, Fraunhofer IOSB-INA, Lemgo, Germany; Reisch, R., Resolto Informatik GmbH, Herford, Germany","Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair. © 2019 Association for Computing Machinery.","Augmented Reality; Deep Learning","Augmented reality; Human computer interaction; Object recognition; Product design; Repair; Assistance system; Digital design techniques; Hardware/software; Learning approach; Product assembly; Work complexity; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85078696444
"Papanastasiou G., Drigas A., Skianis C., Lytras M., Papanastasiou E.","56205162600;8662839500;15124619200;55830169000;57195954324;","Virtual and augmented reality effects on K-12, higher and tertiary education students’ twenty-first century skills",2019,"Virtual Reality","23","4",,"425","436",,21,"10.1007/s10055-018-0363-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053216727&doi=10.1007%2fs10055-018-0363-2&partnerID=40&md5=8c4e5953263f908e07bb10c44dfd8d19","NCSR Demokritos, Patr. Gregoriou E’ & 27, Neapoleos str., Agia Paraskevi, 15341, Greece; University of Aegean, Karlovassi, Samos, 83200, Greece; The American College of Greece, 6 Gravias str., Agia Paraskevi, 15342, Greece; King Abdulaziz University, Jeddah, Saudi Arabia; National Technical University of Athens, Zografou Campus 9, Iroon Polytechniou str., Athens, 15780, Greece","Papanastasiou, G., NCSR Demokritos, Patr. Gregoriou E’ & 27, Neapoleos str., Agia Paraskevi, 15341, Greece, University of Aegean, Karlovassi, Samos, 83200, Greece; Drigas, A., NCSR Demokritos, Patr. Gregoriou E’ & 27, Neapoleos str., Agia Paraskevi, 15341, Greece; Skianis, C., University of Aegean, Karlovassi, Samos, 83200, Greece; Lytras, M., The American College of Greece, 6 Gravias str., Agia Paraskevi, 15342, Greece, King Abdulaziz University, Jeddah, Saudi Arabia; Papanastasiou, E., National Technical University of Athens, Zografou Campus 9, Iroon Polytechniou str., Athens, 15780, Greece","The purpose of this review article is to present state-of-the-art approaches and examples of virtual reality/augmented reality (VR/AR) systems, applications and experiences which improve student learning and the generalization of skills to the real world. Thus, we provide a brief, representative and non-exhaustive review of the current research studies, in order to examine the effects, as well as the impact of VR/AR technologies on K-12, higher and tertiary education students’ twenty-first century skills and their overall learning. According to the literature, there are promising results indicating that VR/AR environments improve learning outcomes and present numerous advantages of investing time and financial resources in K-12, higher and tertiary educational settings. Technological tools such as VR/AR improve digital-age literacy, creative thinking, communication, collaboration and problem solving ability, which constitute the so-called twenty-first century skills, necessary to transform information rather than just receive it. VR/AR enhances traditional curricula in order to enable diverse learning needs of students. Research and development relative to VR/AR technology is focused on a whole ecosystem around smart phones, including applications and educational content, games and social networks, creating immersive three-dimensional spatial experiences addressing new ways of human–computer interaction. Raising the level of engagement, promoting self-learning, enabling multi-sensory learning, enhancing spatial ability, confidence and enjoyment, promoting student-centered technology, combination of virtual and real objects in a real setting and decreasing cognitive load are some of the pedagogical advantages discussed. Additionally, implications of a growing VR/AR industry investment in educational sector are provided. It can be concluded that despite the fact that there are various barriers and challenges in front of the adoption of virtual reality on educational practices, VR/AR applications provide an effective tool to enhance learning and memory, as they provide immersed multimodal environments enriched by multiple sensory features. © 2018, Springer-Verlag London Ltd., part of Springer Nature.","Higher education; K-12; Tertiary education; Twenty-first century skills; Virtual and augmented reality","Augmented reality; Computer games; E-learning; Human computer interaction; Problem solving; Smartphones; Social sciences computing; Virtual reality; Higher education; Multi-sensory learning; Problem-solving abilities; Research and development; State-of-the-art approach; Tertiary education; Twenty-first century skills; Virtual and augmented reality; Students",Article,"Final","",Scopus,2-s2.0-85053216727
"Abidi M.H., Al-Ahmari A., Ahmad A., Ameen W., Alkhalefah H.","55582207400;6603483674;8244331200;56789986100;57203434201;","Assessment of virtual reality-based manufacturing assembly training system",2019,"International Journal of Advanced Manufacturing Technology","105","9",,"3743","3759",,12,"10.1007/s00170-019-03801-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066028866&doi=10.1007%2fs00170-019-03801-3&partnerID=40&md5=eb3341de426ddac10e671c4a1662aee4","Raytheon Chair for Systems Engineering (RCSE Chair), Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia; Industrial Engineering Department, College of Engineering, King Saud University, Riyadh, 11421, Saudi Arabia; Louisiana Community and Technical College System-Manufacturing Extension Partnership, Baton Rouge, LA, United States; Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia","Abidi, M.H., Raytheon Chair for Systems Engineering (RCSE Chair), Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia, Industrial Engineering Department, College of Engineering, King Saud University, Riyadh, 11421, Saudi Arabia; Al-Ahmari, A., Raytheon Chair for Systems Engineering (RCSE Chair), Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia, Industrial Engineering Department, College of Engineering, King Saud University, Riyadh, 11421, Saudi Arabia; Ahmad, A., Louisiana Community and Technical College System-Manufacturing Extension Partnership, Baton Rouge, LA, United States; Ameen, W., Raytheon Chair for Systems Engineering (RCSE Chair), Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia; Alkhalefah, H., Advanced Manufacturing Institute, King Saud University, Riyadh, 11421, Saudi Arabia","Digital manufacturing concept is gaining a lot of attention and popularity due to its enormous benefits. It is considered as one of the pillars or component of Industry 4.0. With the advancements in technology, digital manufacturing is becoming a reality rather than a concept only. It is applied to various stages of the manufacturing process such as design, prototyping, and assembly training. Virtual reality (VR) is a cog in a wheel of digital manufacturing. It can be used in various phases of manufacturing. Planning and conducting assembly operations account for the majority of the cost of a product. It is difficult to design and train assembly operations during the early stages of product design. Assembly is a vital step in manufacturing, so firms provide training to their employees and it costs them time and money. Therefore, this research work extends VR applications in manufacturing by integrating concepts and studies from training simulations to the evaluation of assembly training effectiveness and transfer of training. VR provides a platform for “learning by doing” instead of learning by seeing, listening, or observing. A series of user-based evaluation studies are conducted to ensure that the virtual manufacturing assembly simulation provides an effective and efficient means for evaluating assembly operations and for training assembly personnel. Different feedback cues of VR are implemented to evaluate the system. Moreover, several case studies are used to assess the effectiveness of VR-based training. The results of the study reveal that participants trained by VR committed fewer errors and took lesser time in actual product assembly when compared against the participant from traditional or baseline training group. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Assembly training; Industry 4.0; Manufacturing assembly; Virtual Reality","Agile manufacturing systems; E-learning; Industrial research; Industry 4.0; Personnel training; Product design; Virtual reality; Assembly operations; Assembly trainings; Digital manufacturing; Manufacturing assembly; Manufacturing process; Transfer of trainings; User-based evaluations; Virtual manufacturing; Assembly",Article,"Final","",Scopus,2-s2.0-85066028866
"Guest W., Wild F., Mitri D.D., Klemke R., Karjalainen J., Helin K.","57195718220;55876635700;57203509639;16230731700;53979892900;23396633500;","Architecture and Design Patterns for Distributed, Scalable Augmented Reality and Wearable Technology Systems",2019,"TALE 2019 - 2019 IEEE International Conference on Engineering, Technology and Education",,, 9225855,"","",,2,"10.1109/TALE48000.2019.9225855","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096513730&doi=10.1109%2fTALE48000.2019.9225855&partnerID=40&md5=558bd1a56655ad452b46c286a3c476f4","Performance Augmentation Lab, Oxford Brookes University, United Kingdom; Open University of the Netherlands, Netherlands; Vtt Technical Research Centre of Finland Ltd, Tampere, Finland","Guest, W., Performance Augmentation Lab, Oxford Brookes University, United Kingdom; Wild, F., Performance Augmentation Lab, Oxford Brookes University, United Kingdom; Mitri, D.D., Open University of the Netherlands, Netherlands; Klemke, R., Open University of the Netherlands, Netherlands; Karjalainen, J., Vtt Technical Research Centre of Finland Ltd, Tampere, Finland; Helin, K., Vtt Technical Research Centre of Finland Ltd, Tampere, Finland","This paper presents a novel reference software architecture and supporting pattern language for an augmented reality authoring and training system. Industry-based augmented reality training is considered an essential element of the next techno-industrial revolution. These next generation learning environments allow a trainee to offload complexity and giving them live (or on-demand) feedback on their progress and performance through workplace augmentation is already being taken up by industry forerunners. This reference architecture - for wearable experience for knowledge intensive training - incorporates head-mounted augmented vision, an array of wearable sensors that monitor movement and physiological signals, a data-layer managing sensor data and a cloud-based repository for storing information about the activity and workplace. Moreover, this architecture has been tested in a range of knowledge intensive workplaces, in the aeronautic, medical and space industries. Two iterations of the architecture were developed and validated, together with over 500 participants, producing datasets on activity performance, physiological state and assessment of the platform. The components and links of the architecture are presented here as generalizable design patterns to support wider development. We then propose a pattern language for augmented reality training applications. © 2019 IEEE.","Augmented reality; design pattern; pattern language; software architecture; wearable technology","Aerospace industry; Augmented reality; Computer aided instruction; Computer architecture; Engineering education; Physiology; Wearable computers; Augmented reality authoring; Essential elements; Industrial revolutions; Learning environments; Physiological signals; Physiological state; Reference architecture; Training applications; Wearable sensors",Conference Paper,"Final","",Scopus,2-s2.0-85096513730
"Pellas N., Fotaris P., Kazanidis I., Wells D.","55930215100;8681925900;23990530500;57198347883;","Augmenting the learning experience in primary and secondary school education: a systematic review of recent trends in augmented reality game-based learning",2019,"Virtual Reality","23","4",,"329","346",,33,"10.1007/s10055-018-0347-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047192636&doi=10.1007%2fs10055-018-0347-2&partnerID=40&md5=5990c796b07ebdf43407524e15735234","Department of Product and Systems Design Engineering, University of the Aegean, Ermoúpolis, Greece; School of Computing, Engineering and Mathematics, University of Brighton, Brighton, United Kingdom; Information Technology Department of Eastern Macedonia and Thrace Institute of Technology, Advanced Educational Technologies and Mobile Applications Lab, Kavala, Greece; Cass School of Education and Communities, University of East London, London, United Kingdom","Pellas, N., Department of Product and Systems Design Engineering, University of the Aegean, Ermoúpolis, Greece; Fotaris, P., School of Computing, Engineering and Mathematics, University of Brighton, Brighton, United Kingdom; Kazanidis, I., Information Technology Department of Eastern Macedonia and Thrace Institute of Technology, Advanced Educational Technologies and Mobile Applications Lab, Kavala, Greece; Wells, D., Cass School of Education and Communities, University of East London, London, United Kingdom","There is a significant body of research relating to augmented reality (AR) uses for learning in the primary and the secondary education sectors across the globe. However, there is not such a substantial amount of work exploring the combination of AR with game-based learning (ARGBL). Although ARGBL has the potential to enable new forms of teaching and transform the learning experience, it remains unclear how ARGBL applications can impact students’ motivation, achievements, and learning performance. This study reports a systematic review of the literature on ARGBL approaches in compulsory education considering the advantages, disadvantages, instructional affordances, and/or effectiveness of ARGBL across various primary and secondary education subjects. In total, 21 studies published between 2012 and 2017 in 11 indexed journals were analysed, with 14 studies focusing on primary education and 7 on secondary. The main findings from this review provide the current state of the art research in ARGBL in compulsory education. Trends and the vision towards the future are also discussed, as ARGBL can potentially influence the students’ attendance, knowledge transfer, skill acquisition, hands-on digital experience, and positive attitude towards their learning. This review aims to lay the groundwork for educators, technology developers, and other stakeholders involved in the development of literacy programmes for young children by offering new insights with effective advice and suggestions on how to increase student motivation and improve learning outcomes and the learning experience by incorporating ARGBL into their teaching. © 2018, Springer-Verlag London Ltd., part of Springer Nature.","Augmented reality; Game-based learning; Primary education; Secondary education; Systematic review","Augmented reality; Knowledge management; Motivation; Game-based Learning; Learning experiences; Learning performance; Primary and secondary education; Primary and secondary schools; Primary education; Student motivation; Systematic Review; Students",Article,"Final","",Scopus,2-s2.0-85047192636
"Shi Y., Du J.","57189999728;57219889677;","Simulation of Spatial Memory for Human Navigation Based on Visual Attention in Floorplan Review",2019,"Proceedings - Winter Simulation Conference","2019-December",, 9004783,"3031","3040",,1,"10.1109/WSC40007.2019.9004783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081124854&doi=10.1109%2fWSC40007.2019.9004783&partnerID=40&md5=558fedad8dd7b812c1d9056972537659","University of Florida, Department of Civil and Coastal Engineering, Gainesville, FL  32611, United States","Shi, Y., University of Florida, Department of Civil and Coastal Engineering, Gainesville, FL  32611, United States; Du, J., University of Florida, Department of Civil and Coastal Engineering, Gainesville, FL  32611, United States","Human navigation simulation is critical to many civil engineering tasks and is of central interest to the simulation community. Most human navigation simulation approaches focus on the classic psychology evidence, or assumptions that still need further proofs. The overly simplified and generalized assumption of navigation behaviors does not highlight the need of capturing individual differences in spatial cognition and navigation decision-making, or the impacts of diverse ways of spatial information display. This study proposes the visual attention patterns in floorplan review to be a stronger predictor of human navigation behaviors. To set the theoretical foundation, a Virtual Reality (VR) experiment was performed to test if visual attention patterns during spatial information review can predict the quality of spatial memory, and how the relationship is affected by the diverse ways of information display, including 2D, 3D and VR. The results set a basis for future prediction model developments. © 2019 IEEE.",,"Decision making; Navigation; Virtual reality; Future predictions; Human navigation; Individual Differences; Information display; Navigation behavior; Spatial cognition; Spatial informations; Theoretical foundations; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85081124854
"Zhao M., Dai S.","57211961053;7203008798;","Intent inference of human hand motion for haptic feedback systems",2019,"Proceedings - 2019 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2019",,, 8942346,"218","223",,,"10.1109/AIVR46125.2019.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078023267&doi=10.1109%2fAIVR46125.2019.00046&partnerID=40&md5=40ed22bb33662574bee4ab71325471c7","State Key Laboratory of Virtual Reality, Technology and System, Beihang University, Beijing, China","Zhao, M., State Key Laboratory of Virtual Reality, Technology and System, Beihang University, Beijing, China; Dai, S., State Key Laboratory of Virtual Reality, Technology and System, Beihang University, Beijing, China","The haptic feedback system (HFS) in the virtual cockpit system (VCS) can definitely enhance the sense of immersion. Most HFSs in prior works sacrificed the native advantages of VCSs to achieve haptic interaction. This paper addresses the problem by proposing a novel framework for the HFS, which can predict the most likely interacting target of the human hand in advance. We introduce a HFS with a non-contact visual tracking sensor and a probability inference method based on Bayesian statistics, the features extracted by this HFS could be low-cost, high generality and flexibility. Simulations show that human intent inference can be computed in real-Time and the results can meet the requirements of the HFM, which provides an important basis for haptic interactions in VCSs. © 2019 IEEE.","Haptic Feedback; Human Computer Interaction; Intent Inference; Kalman Filter; Virtual Cockpit System","Artificial intelligence; Feedback control; Human computer interaction; Kalman filters; Bayesian statistics; Cockpit systems; Haptic feedback systems; Haptic feedbacks; Haptic interactions; Human hand motions; Intent inference; Probability inference; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85078023267
"Hasson C.J., Jalili P.F.","6603078042;57211040967;","Visual dynamics cues in learning complex physical interactions",2019,"Scientific Reports","9","1", 13496,"","",,,"10.1038/s41598-019-49637-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072383848&doi=10.1038%2fs41598-019-49637-5&partnerID=40&md5=f004362cbc8329e284477b1fed36515f","Neuromotor Systems Laboratory, Northeastern University, Boston, MA, United States; Departments of Physical Therapy, Movement and Rehabilitation Sciences, Bioengineering, and Biology, Boston, United States","Hasson, C.J., Neuromotor Systems Laboratory, Northeastern University, Boston, MA, United States, Departments of Physical Therapy, Movement and Rehabilitation Sciences, Bioengineering, and Biology, Boston, United States; Jalili, P.F., Neuromotor Systems Laboratory, Northeastern University, Boston, MA, United States, Departments of Physical Therapy, Movement and Rehabilitation Sciences, Bioengineering, and Biology, Boston, United States","This study investigated the role of visual dynamics cues (VDCs) in learning to interact with a complex physical system. Manual gait training was used as an exemplary case, as it requires therapists to control the non-trivial locomotor dynamics of patients. A virtual analog was developed that allowed naïve subjects to manipulate the leg of a virtual stroke survivor (a virtual patient; VP) walking on a treadmill using a small robotic manipulandum. The task was to make the VP’s leg pass through early, mid, and late swing gait targets. One group of subjects (n = 17) started practice seeing the VP’s affected thigh and shank (i.e., VDCs); a second control group (n = 16) only saw the point-of-contact (VP ankle). It was hypothesized that, if seeing the VP’s leg provides beneficial dynamics information, the VDC group would have better task performance and generalization than controls. Results were not supportive. Both groups had similar task performance, and for the late swing gait target, a decrement in manipulative accuracy was observed when VDCs were removed in a generalization task. This suggests that when learning to manipulate complex dynamics, VDCs can create a dependency that negatively affects generalization if the visual context is changed. © 2019, The Author(s).",,"adult; cerebrovascular accident; controlled study; female; gait; human; learning; male; pathophysiology; randomized controlled trial; virtual reality exposure therapy; vision; Adult; Female; Gait; Humans; Learning; Male; Stroke; Virtual Reality Exposure Therapy; Vision, Ocular",Article,"Final","",Scopus,2-s2.0-85072383848
"Meena Y.K., Cecotti H., Wong-Lin K.F., Prasad G.","57212351705;8838569700;26532134800;55812562500;","Design and evaluation of a time adaptive multimodal virtual keyboard",2019,"Journal on Multimodal User Interfaces","13","4",,"343","361",,3,"10.1007/s12193-019-00293-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061282696&doi=10.1007%2fs12193-019-00293-z&partnerID=40&md5=4b3d90c23ab7b84d8cb09f3fbd862063","Future Interaction Technology Lab, Swansea University, Swansea, United Kingdom; Department of Computer Science, College of Science and Mathematics, Fresno State University, Fresno, CA, United States; School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom","Meena, Y.K., Future Interaction Technology Lab, Swansea University, Swansea, United Kingdom; Cecotti, H., Department of Computer Science, College of Science and Mathematics, Fresno State University, Fresno, CA, United States; Wong-Lin, K.F., School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom; Prasad, G., School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry, United Kingdom","The usability of virtual keyboard based eye-typing systems is currently limited due to the lack of adaptive and user-centered approaches leading to low text entry rate and the need for frequent recalibration. In this work, we propose a set of methods for the dwell time adaptation in asynchronous mode and trial period in synchronous mode for gaze based virtual keyboards. The rules take into account commands that allow corrections in the application, and it has been tested on a newly developed virtual keyboard for a structurally complex language by using a two-stage tree-based character selection arrangement. We propose several dwell-based and dwell-free mechanisms with the multimodal access facility wherein the search of a target item is achieved through gaze detection and the selection can happen via the use of a dwell time, soft-switch, or gesture detection using surface electromyography in asynchronous mode; while in the synchronous mode, both the search and selection may be performed with just the eye-tracker. The system performance is evaluated in terms of text entry rate and information transfer rate with 20 different experimental conditions. The proposed strategy for adapting the parameters over time has shown a significant improvement (more than 40%) over non-adaptive approaches for new users. The multimodal dwell-free mechanism using a combination of eye-tracking and soft-switch provides better performance than adaptive methods with eye-tracking only. The overall system receives an excellent grade on adjective rating scale using the system usability scale and a low weighted rating on the NASA task load index, demonstrating the user-centered focus of the system. © 2019, The Author(s).","Adaptive control; Eye-typing; Gaze-based access control; Graphical user interface; Human-computer interaction; Multimodal dwell-free control; Virtual keyboard","Access control; Computer keyboards; Graphical user interfaces; Human computer interaction; NASA; Scales (weighing instruments); Virtual reality; Adaptive Control; Design and evaluations; Experimental conditions; Eye-typing; Free control; Information transfer rate; Surface electromyography; Virtual Keyboards; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061282696
"Dangi S., Linte C.A., Yaniv Z.","56895915700;57202997616;8555308800;","A distance map regularized CNN for cardiac cine MR image segmentation",2019,"Medical Physics","46","12",,"5637","5651",,12,"10.1002/mp.13853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074595228&doi=10.1002%2fmp.13853&partnerID=40&md5=e49d5b643280b5d657eba92082261710","Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Biomedical Engineering, Rochester Institute of Technology, Rochester, NY  14623, United States; MSC LLC., Rockville, MD  20852, United States; National Institute of Allergy and Infectious Diseases, NIH, Bethesda, MD  20814, United States","Dangi, S., Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Linte, C.A., Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States, Biomedical Engineering, Rochester Institute of Technology, Rochester, NY  14623, United States; Yaniv, Z., MSC LLC., Rockville, MD  20852, United States, National Institute of Allergy and Infectious Diseases, NIH, Bethesda, MD  20814, United States","Purpose: Cardiac image segmentation is a critical process for generating personalized models of the heart and for quantifying cardiac performance parameters. Fully automatic segmentation of the left ventricle (LV), the right ventricle (RV), and the myocardium from cardiac cine MR images is challenging due to variability of the normal and abnormal anatomy, as well as the imaging protocols. This study proposes a multi-task learning (MTL)-based regularization of a convolutional neural network (CNN) to obtain accurate segmenation of the cardiac structures from cine MR images. Methods: We train a CNN network to perform the main task of semantic segmentation, along with the simultaneous, auxiliary task of pixel-wise distance map regression. The network also predicts uncertainties associated with both tasks, such that their losses are weighted by the inverse of their corresponding uncertainties. As a result, during training, the task featuring a higher uncertainty is weighted less and vice versa. The proposed distance map regularizer is a decoder network added to the bottleneck layer of an existing CNN architecture, facilitating the network to learn robust global features. The regularizer block is removed after training, so that the original number of network parameters does not change. The trained network outputs per-pixel segmentation when a new patient cine MR image is provided as an input. Results: We show that the proposed regularization method improves both binary and multi-class segmentation performance over the corresponding state-of-the-art CNN architectures. The evaluation was conducted on two publicly available cardiac cine MRI datasets, yielding average Dice coefficients of 0.84 ± 0.03 and 0.91 ± 0.04. We also demonstrate improved generalization performance of the distance map regularized network on cross-dataset segmentation, showing as much as 42% improvement in myocardium Dice coefficient from 0.56 ± 0.28 to 0.80 ± 0.14. Conclusions: We have presented a method for accurate segmentation of cardiac structures from cine MR images. Our experiments verify that the proposed method exceeds the segmentation performance of three existing state-of-the-art methods. Furthermore, several cardiac indices that often serve as diagnostic biomarkers, specifically blood pool volume, myocardial mass, and ejection fraction, computed using our method are better correlated with the indices computed from the reference, ground truth segmentation. Hence, the proposed method has the potential to become a non-invasive screening and diagnostic tool for the clinical assessment of various cardiac conditions, as well as a reliable aid for generating patient specific models of the cardiac anatomy for therapy planning, simulation, and guidance. © 2019 American Association of Physicists in Medicine","cardiac segmentation; convolutional neural network; magnetic resonance imaging; multi-task learning; regularization; task uncertainty weighting","Convolution; Convolutional neural networks; Heart; Inverse problems; Learning systems; Magnetic resonance imaging; Multi-task learning; Network architecture; Patient treatment; Pixels; Semantics; Cardiac image segmentation; Cardiac segmentation; Generalization performance; Multi-class segmentations; regularization; Segmentation performance; State-of-the-art methods; task uncertainty weighting; Image segmentation; Article; cardiac muscle; convolutional neural network; heart; image segmentation; machine learning; multi task learning; nuclear magnetic resonance; cine magnetic resonance imaging; diagnostic imaging; heart; image processing; procedures; Heart; Image Processing, Computer-Assisted; Magnetic Resonance Imaging, Cine; Neural Networks, Computer",Article,"Final","",Scopus,2-s2.0-85074595228
"Pietroszek K.","18037978700;","IRIS: Inter-reality intera",2019,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,, 3364731,"","",,1,"10.1145/3359996.3364731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076146923&doi=10.1145%2f3359996.3364731&partnerID=40&md5=80b95c0fc86c12ee4478f0edc589963f","IDEAS Lab, American University, Washington, DC, United States","Pietroszek, K., IDEAS Lab, American University, Washington, DC, United States","While many metaphors were developed for interactions from a specific point at the reality-virtuality continuum, much less attention has been paid to designing metaphors that allow the users to cross the boundaries between the virtual, the augmented, and the real. We propose a use of an Inter-Reality Interactive Surface (IRIS) that enables users to collaborate across the reality-virtuality continuum within the same application. While we examine IRIS in the context of an immersive educational platform, UniVResity, the metaphor can be generalized to many other application domains. © 2019 Copyright held by the owner/author(s).","Augmented reality; Collaboration; Immersive learning; Interaction metaphor; Reality-virtuality continuum; Virtual reality","Augmented reality; Collaboration; Educational platforms; Immersive; Immersive learning; Interaction metaphors; Interactive surfaces; Virtuality continuum; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85076146923
"Piccione J., Collett J., De Foe A.","57211538030;57206464821;55555885000;","Virtual skills training: the role of presence and agency",2019,"Heliyon","5","11", e02583,"","",,6,"10.1016/j.heliyon.2019.e02583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074370179&doi=10.1016%2fj.heliyon.2019.e02583&partnerID=40&md5=41c139affc0f49a955d0d2674602e6aa","RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Piccione, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; Collett, J., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia; De Foe, A., RMIT University, School of Health and Biomedical Sciences, Discipline of Psychology, Australia","Virtual reality (VR) simulations provide increased feelings of presence and agency that could allow increased skill improvement during VR training. Direct relationships between active agency in VR and skill improvement have previously not been investigated. This study examined the relationship between (a) presence and agency, and (b) presence and skills improvement, via active and passive VR simulations and through measuring real-world golf-putting skill. Participants (n = 23) completed baseline putting skill assessment before using an Oculus Rift VR head-mounted display to complete active (putting with a virtual golf club) and passive (watching a game of golf) VR simulations. Measures of presence and agency were administered after each simulation, followed by a final putting skill assessment. The active simulation induced higher feelings of general presence and agency. However, no relationship was identified between presence and either agency or skill improvement. No skill improvement was evident in either the active or passive simulations, potentially due to the short training period applied, as well as a lack of realism in the VR simulations inhibiting a transfer of skills to a real environment. These findings reinforce previous literature that shows active VR to increase feelings of presence and agency. This study generates a number of fruitful research questions about the relationship between presence and skills training. © 2019 The AuthorsPsychology; Virtual reality; Presence; Human factors; Sport psychology © 2019 The Authors","Human factors; Presence; Psychology; Sport psychology; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85074370179
"Kostrub D., Ostradicky P.","57202500405;57216272626;","A qualitative methodology framework of investigation of learning and teaching based on the use of augmented reality",2019,"ICETA 2019 - 17th IEEE International Conference on Emerging eLearning Technologies and Applications, Proceedings",,, 9040150,"425","440",,,"10.1109/ICETA48886.2019.9040150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082986140&doi=10.1109%2fICETA48886.2019.9040150&partnerID=40&md5=3101b70287f513d716450c35116747b2","Faculty of Education of Comenius University, Department of Pedagogy and Social Pedagogy, Bratislava, Slovakia","Kostrub, D., Faculty of Education of Comenius University, Department of Pedagogy and Social Pedagogy, Bratislava, Slovakia; Ostradicky, P., Faculty of Education of Comenius University, Department of Pedagogy and Social Pedagogy, Bratislava, Slovakia","The methodological framework deals with the methodological inspiration of the specific project augmented reality in the teaching and learning process. It is a reflection of the thinking, conduct and research effort of investigation the views (social representations in private interpretations) of subjects of the teaching and learning process in which the concept of augmented reality is applied. Augmented reality alters the conditions of the learning process, and it is desirable for teachers to know how the learning conditions change and to understand how these changed conditions affect the learning and teaching subjects. The methodological framework emphasizes the need to investigate these changed conditions in order to re-evaluate the didactic framework under the influence of their action in the context of didactical relations and norms. © 2019 IEEE.","Augmented reality; Didactic interaction; Qualitative methodology; Qualitative research; Students; Teachers","Augmented reality; E-learning; Students; Teaching; Technology transfer; Didactic interaction; Learning and teachings; Methodological frameworks; Qualitative methodologies; Qualitative research; Social representations; Teachers; Teaching and learning; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85082986140
"Arts E.E.A., Leijte E., Witteman B.P.L., Jakimowicz J.J., Verhoeven B., Botden S.M.B.I.","36552010600;57202080193;35085647600;35391591600;57207587593;16067890000;","Face, content, and construct validity of the take-home eosim augmented reality laparoscopy simulator for basic laparoscopic tasks",2019,"Journal of Laparoendoscopic and Advanced Surgical Techniques","29","11",,"1419","1426",,3,"10.1089/lap.2019.0070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074738574&doi=10.1089%2flap.2019.0070&partnerID=40&md5=bcddba19643432be02a5602fb608cf72","Department of Surgery, Radboud University Medical Center, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Department of Surgery, Rijnstate Hospital, Arnhem, Netherlands; Department of Industrial Design, Technical University Delft, Delft, Netherlands; Department of Surgery, Catharina Hospital, Eindhoven, Netherlands","Arts, E.E.A., Department of Surgery, Radboud University Medical Center, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Leijte, E., Department of Surgery, Radboud University Medical Center, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Witteman, B.P.L., Department of Surgery, Rijnstate Hospital, Arnhem, Netherlands; Jakimowicz, J.J., Department of Industrial Design, Technical University Delft, Delft, Netherlands, Department of Surgery, Catharina Hospital, Eindhoven, Netherlands; Verhoeven, B., Department of Surgery, Radboud University Medical Center, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands; Botden, S.M.B.I., Department of Surgery, Radboud University Medical Center, Geert Grooteplein 10 Route 618, Nijmegen, 6500HB, Netherlands","Background: The eoSim® laparoscopic augmented reality (AR) simulator has instrument tracking capabilities that may be suitable for implementation in laparoscopic training. The objective is to assess face, content, and construct validity of this simulator for basic laparoscopic skills training. Methods: Participants were divided into three groups: novices (no training), intermediates (&lt;50 laparoscopic procedures), and experts (&gt;50 laparoscopic procedures). Three basic tasks were completed on the simulator: thread transfer (1), cyst dissection (2), and tube ligation (3). A questionnaire was completed on realism, didactic value, and usability of the simulator. Measured outcome parameters were as follows: time, distance, time off screen, average speed, acceleration, and smoothness. Results: Mean ± standard deviation scores on realism were positive (Task 1 or T1; 3.9 ± 0.7, P = .13, T2; 3.7 ± 0.7, P = .07, T3; 3.7 ± 0.07), as well as didactic value (T1; 3.9 ± 0.8, P = .71, T2; 3.9 ± 0.8, P = .31, T3; 4.0 ± 0.8, P = .40). Usability was valued the highest, with mean scores between 3.9 and 4.3 (T1; P = .71, T2; P = .80, T3; P = .85). Scores did not differ significantly between groups. Experts were significantly faster (Task 1; P &lt; .001, Task 2; P = .042, Task 3: P &lt; .001) with higher handling speed for tasks 2 and 3 (Task 1; P = .20, task 2; P = .034, task 3; P = .049). Results for other outcome parameters also indicated experts had better instrument control and efficiency than novices, although these differences did not reach statistical significance. Conclusions: The eoSim laparoscopic AR simulator is regarded as a realistic, accessible, and useful tool for the training of basic laparoscopic skills, with good face validity. Construct validity of the eoSim AR simulator was demonstrated on several core variables, but not all. © Copyright 2019, Mary Ann Liebert, Inc., publishers 2019.","laparoscopy training; simulation; validity","acceleration; adult; Article; comparative study; construct validity; content validity; controlled study; female; health care cost; human; laparoscopic surgery; male; outcome assessment; priority journal; questionnaire; simulation training; surgical training; task performance; treatment outcome; validation study; clinical competence; cyst; dissection; education; laparoscopy; middle aged; procedures; reproducibility; software; uterine tube sterilization; young adult; Adult; Augmented Reality; Clinical Competence; Cysts; Dissection; Female; Humans; Laparoscopy; Male; Middle Aged; Reproducibility of Results; Simulation Training; Software; Sterilization, Tubal; Surveys and Questionnaires; Young Adult",Article,"Final","",Scopus,2-s2.0-85074738574
"Leijte E., Arts E., Witteman B., Jakimowicz J., De Blaauw I., Botden S.","57202080193;36552010600;35085647600;35391591600;6701566895;16067890000;","Construct, content and face validity of the eoSim laparoscopic simulator on advanced suturing tasks",2019,"Surgical Endoscopy","33","11",,"3635","3643",,12,"10.1007/s00464-018-06652-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060510159&doi=10.1007%2fs00464-018-06652-3&partnerID=40&md5=d17d4cb4173ab23bcb95552be84e4b2d","Department of Paediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Nijmegen, Netherlands; Department of Surgery, Rijnstate Hospital, Arnhem, Netherlands; Department of Industrial Design, Technical University Delft, Delft, Netherlands; Department of Surgery, Catharina Hospital, Eindhoven, Netherlands","Leijte, E., Department of Paediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Nijmegen, Netherlands; Arts, E., Department of Paediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Nijmegen, Netherlands; Witteman, B., Department of Surgery, Rijnstate Hospital, Arnhem, Netherlands; Jakimowicz, J., Department of Industrial Design, Technical University Delft, Delft, Netherlands, Department of Surgery, Catharina Hospital, Eindhoven, Netherlands; De Blaauw, I., Department of Paediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Nijmegen, Netherlands; Botden, S., Department of Paediatric Surgery, Radboud University Medical Centre - Amalia Children’s Hospital, Nijmegen, Netherlands","Background: The purpose of this study was to validate the eoSim, an affordable and mobile inanimate laparoscopic simulator with instrument tracking capabilities, regarding face, content and construct validity on complex suturing tasks. Methods: Participants recruited for this study were novices (no laparoscopic experience), target group for this training (surgical/gynaecologic/urologic residents, > 10 basic and < 20 advanced laparoscopic procedures) and experts (> 20 advanced laparoscopic procedures). Each participant performed the intracorporeal suturing exercise (Task 1), an upside down needle transfer (Task 2, developed for this study) and an anastomosis needle transfer (Task 3). Following, the participants completed a questionnaire regarding their demographics and opinion on the eoSim in terms of realism, didactic value and usability. Measured outcome parameters were time, distance, percentage of instrument tip off-screen, working area, speed, acceleration and smoothness. Results: In total, 104 participants completed the study, of which 60 novices, 31 residents and 13 experts. Face and content validity results showed a mean positive opinion on realism (3.9 Task 1, 3.6 Task 2 and 3.7 Task 3), didactic value (4.0, 3.4 and 3.7, respectively) and usability (4.2. 3.7 and 4.0, respectively). There were no significant differences in these outcomes between the specified expertise groups. Construct validity results showed significant differences between experts, target group or novices for Task 1 in terms of time (means 339, 607 and 1224 s, respectively, p < 0.001) and distance (means 8.1, 15.6 and 21.7 m, respectively, p < 0.001). Task 2 showed significant differences between groups regarding time (p < 0.001), distance (p 0.003), off-screen (p < 0.001) and working area (p < 0.001). Task 3 showed significant differences between groups, after subanalyses, on total number of stitches (p < 0.001), time per stitch (p < 0.001) and distance per stitch (p < 0.001). Conclusions: The results of this study indicate that the eoSim is a potential meaningful and valuable simulator in the training of suturing tasks. © 2019, The Author(s).","Augmented reality simulator; Construct validity; Content validity; Face validity; Laparoscopy training; Simulation","abdominal wall; acceleration; adult; Article; construct validity; content validity; end to end anastomosis; face validity; fatigue; human; priority journal; questionnaire; resident; surgical training; time; velocity; clinical competence; clinical trial; computer simulation; education; female; gynecologic surgery; laparoscopy; medical education; multicenter study; Netherlands; procedures; reproducibility; suture; urologic surgery; Adult; Clinical Competence; Computer Simulation; Education, Medical, Graduate; Female; Gynecologic Surgical Procedures; Humans; Laparoscopy; Netherlands; Reproducibility of Results; Surveys and Questionnaires; Sutures; Urologic Surgical Procedures",Article,"Final","",Scopus,2-s2.0-85060510159
"Liang Y., Ge J., Zhang S., Wu J., Tang Z., Luo B.","57192991761;13404777700;55713694700;55837752300;57212767296;57202866205;","A Utility-Based Optimization Framework for Edge Service Entity Caching",2019,"IEEE Transactions on Parallel and Distributed Systems","30","11", 8708961,"2384","2395",,3,"10.1109/TPDS.2019.2915218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077398612&doi=10.1109%2fTPDS.2019.2915218&partnerID=40&md5=ac17a3d9b59f765e25fe0988fe47aa68","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China; Software Institute, Nanjing University, Nanjing, 210008, China; Department of Compute Science and Technology, Nanjing University, Nanjing, 210008, China; Center for Networked Computing, Temple University, Philadelphia, PA  19122, United States","Liang, Y., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China, Software Institute, Nanjing University, Nanjing, 210008, China; Ge, J., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China, Software Institute, Nanjing University, Nanjing, 210008, China; Zhang, S., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China, Department of Compute Science and Technology, Nanjing University, Nanjing, 210008, China; Wu, J., Center for Networked Computing, Temple University, Philadelphia, PA  19122, United States; Tang, Z., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China, Software Institute, Nanjing University, Nanjing, 210008, China; Luo, B., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210008, China, Software Institute, Nanjing University, Nanjing, 210008, China","Edge computing is one of the emerging technologies aiming to enable timely computation at the network edge. With virtualization technologies, the role of the traditional edge providers is separated into two: edge infrastructure providers (EIPs), who manage the physical edge infrastructure, and edge service providers (ESPs), who purchase slices of physical resources (e.g., CPU, bandwidth, memory space, disk storage) from EIPs and then cache service entities to offer their own value-added services to end users. When an ESP caches a service entity in an edge server, the ESP has to pay some fees (i.e, the cache cost) to the EIP that owns the edge server. One of the fundamental problems in edge virtualization is the so-called service entity caching problem, i.e., where to place service entities for an ESP to minimize the cache cost. In this paper, we study the service entity caching problem from the utility perspective. We use 'utility' to denote the positive impact on a client from caching a service entity in an edge server, and the exact meaning of utility can vary depending on specific scenarios. We formulate the Utility-based Service Entity Caching (UtilitySEC) problem, which can be generalized to many existing problems by modifying the 'utility'. We prove that the UtilitySEC problem is NP-complete and design an approximation algorithm for it. Extensive simulations are conducted to evaluate the performance of the proposed framework. © 1990-2012 IEEE.","Edge computing; service entity caching; set cover; utility","Cache memory; Edge computing; Virtual reality; Virtualization; Emerging technologies; Extensive simulations; Infrastructure providers; service entity caching; Set cover; utility; Utility based optimization; Virtualization technologies; Approximation algorithms",Article,"Final","",Scopus,2-s2.0-85077398612
"Fuchs K., Grundmann T., Fleisch E.","57191988486;57211689639;6602498499;","Towards identification of packaged products via computer vision: Convolutional neural networks for object detection and image classification in retail environments",2019,"ACM International Conference Proceeding Series",,, a26,"","",,1,"10.1145/3365871.3365899","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076132529&doi=10.1145%2f3365871.3365899&partnerID=40&md5=6edcb6696917a493da691ad9f4121a6a","D-MTEC, Auto-ID Labs ETH, HSG, ETH Zurich, Zurich, ZH, Switzerland","Fuchs, K., D-MTEC, Auto-ID Labs ETH, HSG, ETH Zurich, Zurich, ZH, Switzerland; Grundmann, T., D-MTEC, Auto-ID Labs ETH, HSG, ETH Zurich, Zurich, ZH, Switzerland; Fleisch, E., D-MTEC, Auto-ID Labs ETH, HSG, ETH Zurich, Zurich, ZH, Switzerland","Identification of packaged products in retail environments still relies on barcodes, requiring active user input and limited to one product at a time. Computer vision (CV) has already enabled many applications, but has so far been under-discussed in the retail domain, albeit allowing for faster, hands-free, more natural human-object interaction (e.g. via mixed reality headsets). To assess the potential of current convolutional neural network (CNN) architectures to reliably identify packaged products within a retail environment, we created and open-source a dataset of 300 images of vending machines with 15k labeled instances of 90 products. We assessed observed accuracies from transfer learning for image-based product classification (IC) and multi-product object detection (OD) on multiple CNN architectures, and the number of images instances required per product to achieve meaningful predictions. Results show that as little as six images are enough for 90% IC accuracy, but around 30 images are needed for 95% IC accuracy. For simultaneous OD, 42 instances per product are necessary and far more than 100 instances to produce robust results. Thus, this study demonstrates that even in realistic, fast-paced retail environments, image-based product identification provides an alternative to barcodes, especially for use-cases that do not require perfect 100% accuracy. © 2019 Copyright is held by the owner/author(s).","CNN; Computer vision; Product identification","Bar codes; Convolution; Human computer interaction; Image classification; Integrated circuits; Internet of things; Mixed reality; Network architecture; Neural networks; Object detection; Object recognition; Sales; Convolutional neural network; Human-object interaction; Multi-products; Open sources; Packaged products; Product classification; Product identification; Transfer learning; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85076132529
"Niu B., Liu C., Liu J., Deng Y., Wan Q., Ma N.","57212564916;57206721598;57212564775;57215348135;57213381353;57201777764;","Impacts of different types of scaffolding on academic performance, cognitive load and satisfaction in scientific inquiry activities based on augmented reality",2019,"Proceedings - 2019 8th International Conference of Educational Innovation through Technology, EITT 2019",,, 8924045,"239","244",,2,"10.1109/EITT.2019.00053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077047450&doi=10.1109%2fEITT.2019.00053&partnerID=40&md5=7e8addeb1b1db45891a576dbc0acdf8f","School of Educational Technology, Beijing Normal University, Beijing, China","Niu, B., School of Educational Technology, Beijing Normal University, Beijing, China; Liu, C., School of Educational Technology, Beijing Normal University, Beijing, China; Liu, J., School of Educational Technology, Beijing Normal University, Beijing, China; Deng, Y., School of Educational Technology, Beijing Normal University, Beijing, China; Wan, Q., School of Educational Technology, Beijing Normal University, Beijing, China; Ma, N., School of Educational Technology, Beijing Normal University, Beijing, China","The emergence of augmented reality technology has injected new vitality into teaching, especially when the visualization and interactivity have increased students' interest in scientific inquiry. Although augmented reality technology has brought freshness to students, it is necessary to design suitable teaching support to promote student learning. Therefore, knowing how to design a teaching support system has become an important research problem. The researchers have designed open scaffolding and convergent scaffold based on augmented reality technology as a teaching support. Twenty-one elementary school students were randomly assigned to use either the convergent scaffolding or the open scaffolding for a learning task. Using a mixed research method, this study examined the effects of different scaffoldings on knowledge retention, knowledge transfer, learning satisfaction, and cognitive load. The results revealed that open scaffolding could improve students' knowledge retention, knowledge transfer, and reduce cognitive load. © 2019 IEEE.","Academic performance; Augmented reality; Cognitive load; Scaffolding; Scientific inquiry","Augmented reality; Engineering research; Knowledge management; Scaffolds; Academic performance; Augmented reality technology; Cognitive loads; Knowledge retention; Learning satisfactions; Scaffolding; Scientific inquiry; Students' interests; Students",Conference Paper,"Final","",Scopus,2-s2.0-85077047450
"Fernandes F., Werner C.","57202446568;7201754422;","Towards immersive learning in object-oriented paradigm: A preliminary study",2019,"Proceedings - 2019 21st Symposium on Virtual and Augmented Reality, SVR 2019",,, 8921030,"59","68",,,"10.1109/SVR.2019.00026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077235257&doi=10.1109%2fSVR.2019.00026&partnerID=40&md5=18c5e0ff98980061903b8ebf8218153d","Computer Science Department, IF Sudeste MG, Federal Institute Southeast of Minas Gerais, Manhuaçu, MG, Brazil; System Engineering and Computer Science Department, COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil","Fernandes, F., Computer Science Department, IF Sudeste MG, Federal Institute Southeast of Minas Gerais, Manhuaçu, MG, Brazil; Werner, C., System Engineering and Computer Science Department, COPPE/UFRJ, Federal University of Rio de Janeiro, Rio de Janeiro, RJ, Brazil","Object-Oriented Paradigm Teaching is mandatory in the curriculum of the courses of the Computing area. Students are taught fundamental concepts about this paradigm, such as class, object, encapsulation, polymorphism, generalization and composition. One of the major challenges of Software Engineering is to teach complex and abstract concepts in a short time, with examples or simple projects done in academic environments. Virtual Reality has demonstrated advantages applied to Education, providing immersive experiences and new ways of visualization and interaction. However, this technology has not been extensively explored in Software Engineering. In this sense, this paper aims to present a disruptive method of teaching and learning support on fundamentals in object orientation paradigm based on Immersive Learning, called OO Game VR. In addition, an initial heuristic evaluation was performed with 6 subjects in order to identify usability problems. Despite problems found related to support learning, navigation and orientation, the natural expression of action and clear entry and exit points, the subjects were able to perform all the tasks, showing indications that the application has the potential to support teaching of OOP teaching through Immersive Learning. In future works, the usability problems will be fixed and specific methods will be applied for measuring influence immersion on learning outcomes. © 2019 IEEE.","Immersive learning; Object-oriented paradigm; Software engineering education; Virtual reality","Augmented reality; Engineering education; Learning systems; Software engineering; Teaching; Virtual reality; Academic environment; Fundamental concepts; Heuristic evaluation; Immersive learning; Object orientation; Object oriented paradigm; Teaching and learning; Usability problems; Object oriented programming",Conference Paper,"Final","",Scopus,2-s2.0-85077235257
"Tarng S., Wang D., Hu Y.","57194037189;23480778300;57201840458;","Estimating cognitive processes related to haptic interaction within virtual environments",2019,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2019-October",, 8913853,"2823","2828",,,"10.1109/SMC.2019.8913853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076757499&doi=10.1109%2fSMC.2019.8913853&partnerID=40&md5=844a96a2408eae30665e9308bfeda79e","University of Calgary, Department of Electrical and Computer Engineering, Calgary, AB, Canada","Tarng, S., University of Calgary, Department of Electrical and Computer Engineering, Calgary, AB, Canada; Wang, D., University of Calgary, Department of Electrical and Computer Engineering, Calgary, AB, Canada; Hu, Y., University of Calgary, Department of Electrical and Computer Engineering, Calgary, AB, Canada","Efforts exist to combine a brain-machine interface (BMI) into a 3D virtual environment (VE) for visual tasks. User interaction via haptic stimuli within the VE is still unexplored for developing the BMI however, due to little understanding of cognitive processes related to such haptic interaction. Hence, we investigated a feasibility of estimating cognitive processes related to haptic interaction. Involved in the investigation, human participants undertook a task via different haptic stimuli (e.g., force and vibration) within a 3D VE. Their brain activities evoked by the stimuli were acquired as electroencephalography signals. Patterns of event-related potential and power spectral density were extracted from the signals, indicating activation in certain brain areas. The estimation of connectivity among these areas used directed transfer function, emphasizing on the middle of the beta band (14 30) © 2019 IEEE.","Brain-machine interface (BMI); Electroencephalography (EEG); Haptic interaction; Virtual environment (VE)","Brain; Cognitive systems; Electroencephalography; Electrophysiology; Haptic interfaces; Spectral density; Virtual reality; Vision; 3-D virtual environment; Brain activity; Brain machine interface; Cognitive process; Directed transfer functions; Event related potentials; Haptic interactions; User interaction; Brain computer interface",Conference Paper,"Final","",Scopus,2-s2.0-85076757499
"Vasiliu V., Soros G.","57215279941;34771961400;","Coherent rendering of virtual smile previews with fast neural style transfer",2019,"Proceedings - 2019 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2019",,, 8943754,"66","73",,1,"10.1109/ISMAR.2019.00-25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078307773&doi=10.1109%2fISMAR.2019.00-25&partnerID=40&md5=e86fb7bcc47caa28949d9061a84c5293","Kapanu AG, EPFL, Switzerland; Kapanu AG, Nokia Bell Labs, Hungary","Vasiliu, V., Kapanu AG, EPFL, Switzerland; Soros, G., Kapanu AG, Nokia Bell Labs, Hungary","Coherent rendering in augmented reality deals with synthesizing virtual content that seamlessly blends in with the real content. Unfortunately, capturing or modeling every real aspect in the virtual rendering process is often unfeasible or too expensive. We present a post-processing method that improves the look of rendered overlays in a dental virtual try-on application. We combine the original frame and the default rendered frame in an autoencoder neural network in order to obtain a more natural output, inspired by artistic style transfer research. Specifically, we apply the original frame as style on the rendered frame as content, repeating the process with each new pair of frames. Our method requires only a single forward pass, our shallow architecture ensures fast execution, and our internal feedback loop inherently enforces temporal consistency. © 2019 IEEE.","Augmented reality; Autoencoder; Coherent rendering; Convolutional neural network; Style transfer","Learning systems; Neural networks; Artistic style transfer; Auto encoders; Coherent rendering; Convolutional neural network; Internal feedback loop; Postprocessing methods; Style transfer; Temporal consistency; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-85078307773
"Gupta A., Cecil J., Tapia O., Sweet-Darter M.","57200825298;7005940561;57207105216;56719728800;","Design of cyber-human frameworks for immersive learning",2019,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2019-October",, 8914205,"1563","1568",,,"10.1109/SMC.2019.8914205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076785558&doi=10.1109%2fSMC.2019.8914205&partnerID=40&md5=24f60e31ee08e1c61b6ac33241f99727","Oklahoma State University, Center for Cyber-Physical Systems, Stillwater, United States; Anselm Center, Edmond, OK, United States","Gupta, A., Oklahoma State University, Center for Cyber-Physical Systems, Stillwater, United States; Cecil, J., Oklahoma State University, Center for Cyber-Physical Systems, Stillwater, United States; Tapia, O., Oklahoma State University, Center for Cyber-Physical Systems, Stillwater, United States; Sweet-Darter, M., Anselm Center, Edmond, OK, United States","This paper focuses on the creation of information centric Cyber-Human Learning Frameworks involving Virtual Reality based mediums. A generalized framework is proposed, which is adapted for two educational domains: One to support education and training of residents in orthopedic surgery and the other focusing on science learning for children with autism. Users, experts and technology based mediums play a key role in the design of such a Cyber-Human framework. Virtual Reality based immersive and haptic mediums were two of the technologies explored in the implementation of the framework for these learning domains. The proposed framework emphasizes the importance of Information-Centric Systems Engineering (ICSE) principles which emphasizes a user centric approach along with formalizing understanding of target subjects or processes for which the learning environments are being created. © 2019 IEEE.","Human-Computer Interaction; Virtual Learning; Virtual Reality","Computer aided instruction; Human computer interaction; Virtual reality; Children with autisms; Education and training; Immersive learning; Information-centric; Learning environments; Orthopedic surgery; Technology-based; Virtual learning; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85076785558
"Venkat A., Patel C., Agrawal Y., Sharma A.","57215330597;57215968709;57215971082;57214355748;","HumanMeshNet: Polygonal mesh recovery of humans",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,, 9022021,"2178","2187",,3,"10.1109/ICCVW.2019.00273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082475988&doi=10.1109%2fICCVW.2019.00273&partnerID=40&md5=85ecb92cdc82027adef796ff3800085f","International Institute of Information Technology, Hyderabad, India","Venkat, A., International Institute of Information Technology, Hyderabad, India; Patel, C., International Institute of Information Technology, Hyderabad, India; Agrawal, Y., International Institute of Information Technology, Hyderabad, India; Sharma, A., International Institute of Information Technology, Hyderabad, India","3D Human Body Reconstruction from a monocular image is an important problem in computer vision with applications in virtual and augmented reality platforms, animation industry, en-commerce domain, etc. While several of the existing works formulate it as a volumetric or parametric learning with complex and indirect reliance on re-projections of the mesh, we would like to focus on implicitly learning the mesh representation. To that end, we propose a novel model, HumanMeshNet, that regresses a template mesh's vertices, as well as receives a regularization by the 3D skeletal locations in a multi-branch, multi-task setup. The image to mesh vertex regression is further regularized by the neighborhood constraint imposed by mesh topology ensuring smooth surface reconstruction. The proposed paradigm can theoretically learn local surface deformations induced by body shape variations and can therefore learn high-resolution meshes going ahead. We show comparable performance with SoA (in terms of surface and joint error) with far lesser computational complexity, modeling cost and therefore real-time reconstructions on three publicly available datasets. We also show the generalizability of the proposed paradigm for a similar task of predicting hand mesh models. Given these initial results, we would like to exploit the mesh topology in an explicit manner going ahead. © 2019 IEEE.","3D human reconstruction; 3D monocular reconstruction; Mesh recovery; SMPL","Augmented reality; Computer vision; Mesh generation; Object recognition; Topology; Virtual reality; Mesh representation; Mesh topologies; Monocular reconstruction; Parametric learning; Polygonal meshes; Real-time reconstruction; SMPL; Virtual and augmented reality; Image reconstruction",Conference Paper,"Final","",Scopus,2-s2.0-85082475988
"Pezent E., Fani S., Clark J., Bianchi M., O'malley M.K.","57197801010;56416323000;57197725603;57202798446;35917120900;","Spatially Separating Haptic Guidance from Task Dynamics through Wearable Devices",2019,"IEEE Transactions on Haptics","12","4", 8723084,"581","593",,8,"10.1109/TOH.2019.2919281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077078793&doi=10.1109%2fTOH.2019.2919281&partnerID=40&md5=9df40e15c8bc8dd7102361324603b397","Mechatronics and Haptic Interfaces Laboratory, Department of Mechanical Engineering, Rice University, Houston, TX  77005, United States; Department of Information Engineering, Research Center Enrico Piaggio, University of Pisa, Pisa, Italy; Soft Robotics for Human Cooperation and Rehabilitation Research Line, Istituto Italiano di Tecnologia, Genova, 16163, Italy","Pezent, E., Mechatronics and Haptic Interfaces Laboratory, Department of Mechanical Engineering, Rice University, Houston, TX  77005, United States; Fani, S., Department of Information Engineering, Research Center Enrico Piaggio, University of Pisa, Pisa, Italy, Soft Robotics for Human Cooperation and Rehabilitation Research Line, Istituto Italiano di Tecnologia, Genova, 16163, Italy; Clark, J., Mechatronics and Haptic Interfaces Laboratory, Department of Mechanical Engineering, Rice University, Houston, TX  77005, United States; Bianchi, M., Department of Information Engineering, Research Center Enrico Piaggio, University of Pisa, Pisa, Italy; O'malley, M.K., Mechatronics and Haptic Interfaces Laboratory, Department of Mechanical Engineering, Rice University, Houston, TX  77005, United States","Haptic devices have a high potential for delivering tailored training to novices. These devices can simulate forces associated with real-world tasks, or provide guidance forces that convey task completion and learning strategies. It has been shown, however, that providing both task forces and guidance forces simultaneously through the same haptic interface can lead to novices depending on guidance, being unable to demonstrate skill transfer, or learning the wrong task altogether. This paper presents a novel solution whereby task forces are relayed via a kinesthetic haptic interface, while guidance forces are spatially separated through a cutaneous skin stretch modality. We explore different methods of delivering cutaneous based guidance to subjects in a dynamic trajectory following task. We next compare cutaneous guidance to kinesthetic guidance, as is traditional to spatially separated assistance. We further investigate the role of placing cutaneous guidance ipsilateral versus contralateral to the task force device. The efficacies of each guidance condition are compared by examining subject error and movement smoothness. Results show that cutaneous guidance can be as effective as kinesthetic guidance, making it a practical and cost-effective alternative for spatially separated assistance. © 2008-2011 IEEE.","cutaneous feedback; Haptic guidance; skin stretch; spatially separated assistance; training; trajectory following","Cost effectiveness; Mice (computer peripherals); Personnel training; Separation; Wearable technology; Cutaneous feedbacks; Haptic guidance; Skin stretch; spatially separated assistance; Trajectory following; Haptic interfaces; electronic device; equipment design; human; learning; motor performance; physiology; sensory feedback; touch; Equipment Design; Feedback, Sensory; Humans; Learning; Motor Skills; Touch; Touch Perception; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85077078793
"Ip H.H.S., Li C., Leoni S., Chen Y., Ma K.-F., Wong C.H.-T., Li Q.","7005395690;56180363500;57192005943;57187275900;57192007385;57204512944;56190105400;","Design and Evaluate Immersive Learning Experience for Massive Open Online Courses (MOOCs)",2019,"IEEE Transactions on Learning Technologies","12","4", 8515107,"503","515",,3,"10.1109/TLT.2018.2878700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055879387&doi=10.1109%2fTLT.2018.2878700&partnerID=40&md5=262dab079e4e3ad4edea8953a8886e91","Department of Computer Science, AIMtech Centre, City University of Hong Kong, Hong Kong; AIMtech Centre, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Chinese and History, City University of Hong Kong, Kowloon Tong, Hong Kong","Ip, H.H.S., Department of Computer Science, AIMtech Centre, City University of Hong Kong, Hong Kong; Li, C., Department of Computer Science, AIMtech Centre, City University of Hong Kong, Hong Kong; Leoni, S., AIMtech Centre, City University of Hong Kong, Hong Kong; Chen, Y., Department of Computer Science, City University of Hong Kong, Hong Kong; Ma, K.-F., Department of Chinese and History, City University of Hong Kong, Kowloon Tong, Hong Kong; Wong, C.H.-T., Department of Chinese and History, City University of Hong Kong, Kowloon Tong, Hong Kong; Li, Q., Department of Computer Science, City University of Hong Kong, Hong Kong","Massive open online courses (MOOCs), a unique form of online education enabled by web-based learning technologies, allow learners from anywhere in the world with any level of educational background to enjoy online education experience provided by many top universities all around the world. Traditionally, MOOC learning contents are always delivered as text-based or video-based materials. Although introducing immersive learning experience for MOOCs may sound exciting and potentially significative, there are a number of challenges given this unique setting. In this paper, we present the design and evaluation methodologies for delivering immersive learning experience to MOOC learners via multiple media. Specifically, we have applied the techniques in the production of a MOOC entitled Virtual Hong Kong: New World, Old Traditions, led by AIMtech Centre, City University of Hong Kong, which is the first MOOC (as our knowledge) that delivers immersive learning content for distant learners to appreciate and experience how the traditional culture and folklore of Hong Kong impact upon the lives of its inhabitants in the 21st Century. The methodologies applied here can be further generalized as the fundamental framework of delivering immersive learning for future MOOCs. © 2008-2011 IEEE.","e-learning.; immersive learning; Massive open online course; virtual reality","Computer aided instruction; Curricula; Design; Education; Learning systems; Virtual reality; Google; Immersive learning; Information and Communication Technologies; Massive open online course; Solid model; Urban areas; E-learning",Article,"Final","",Scopus,2-s2.0-85055879387
"Braly A.M., Nuernberger B., Kim S.Y.","57195557852;56155189000;57205624514;","Augmented Reality Improves Procedural Work on an International Space Station Science Instrument",2019,"Human Factors","61","6",,"866","878",,4,"10.1177/0018720818824464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060878478&doi=10.1177%2f0018720818824464&partnerID=40&md5=d2c29980abe0ea26b6a5af91df1ed688","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States","Braly, A.M., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Nuernberger, B., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Kim, S.Y., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States","Objective: The purpose of the current study was to determine whether an augmented reality instruction method would result in faster task completion times, lower mental workload, and fewer errors for simple tasks in an operational setting. Background: Prior research on procedural work that directly compared augmented reality instructions to traditional instruction methods (e.g., paper) showed that augmented reality instructions can enhance procedural work, but this was not true for simple tasks in an operational setting. Method: Participants completed simple procedural tasks on spaceflight hardware using an augmented reality instruction method and a paper instruction method. Results: Our results showed that the augmented reality instruction method resulted in faster task completion times and lower levels of mental and temporal demand compared with paper instructions. When participants used the augmented reality instruction method before the paper instruction method, there was a transfer of training that improved a subsequent procedure using the paper instruction method. Conclusion: An off-the-shelf augmented reality head-mounted display (HoloLens) can enhance procedural work for simple tasks in an operational setting. Application: The ability of augmented reality to enhance procedural work for simple tasks in an operational setting can help in reducing costs and mitigating risks that could ultimately lead to accidents and critical failures. © 2019, Human Factors and Ergonomics Society.","augmented reality; procedural work; spaceflight","Helmet mounted displays; Space flight; Space stations; Critical failures; Head mounted displays; Instruction methods; International Space stations; procedural work; Space-flight hardware; Traditional instruction; Transfer of trainings; Augmented reality; adult; devices; female; human; male; middle aged; space flight; task performance; workload; young adult; Adult; Augmented Reality; Female; Humans; Male; Middle Aged; Spacecraft; Task Performance and Analysis; Workload; Young Adult",Article,"Final","",Scopus,2-s2.0-85060878478
"Palmas F., Labode D., Plecher D.A., Klinker G.","57208681848;57211522317;55923283100;6603530980;","Comparison of a gamified and non-gamified virtual reality training assembly task",2019,"2019 11th International Conference on Virtual Worlds and Games for Serious Applications, VS-Games 2019 - Proceedings",,, 8864583,"1DUUMY","",,5,"10.1109/VS-Games.2019.8864583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074257749&doi=10.1109%2fVS-Games.2019.8864583&partnerID=40&md5=c74c3636133e6da9ac1e76ca8a13f78c","Research Group Augmented Reality, Technical University of Munich, Munich, Germany","Palmas, F., Research Group Augmented Reality, Technical University of Munich, Munich, Germany; Labode, D., Research Group Augmented Reality, Technical University of Munich, Munich, Germany; Plecher, D.A., Research Group Augmented Reality, Technical University of Munich, Munich, Germany; Klinker, G., Research Group Augmented Reality, Technical University of Munich, Munich, Germany","By using simulations in virtual reality (VR), people have the chance to train without supervision in a safe and controlled environment. VR simulation training allows users to gain new skills and apply them to real-life situations. However, the learning curve of this technology from a novice level could influence the expected learning results of a training session. A training approach based on the combination of VR and gamification could speed up this overall learning process and not just for a novice. In this paper we evaluate how gamification in a VR training session can improve the efficiency of the training and the accuracy of the task execution in a real-world practical test. In the training scenario of this study, 50 randomly assigned participants were divided into two groups. The groups were assigned to a gamified and a non-gamified version of the same VR training and were then guided through a step-by-step tutorial outlining how to solve an assembly task. Performance differences were evaluated based on time taken and specific errors made during the training session. The results of this study show, in general, that beneficial effects can be attributed to the use of gamification in the conducted VR training simulation, particularly for the VR novice participants. © 2019 IEEE.","Assembly task; Gamification; Learning transfer; Training; Virtual reality; Virtual training","Personnel training; Virtual reality; Assembly tasks; Beneficial effects; Controlled environment; Gamification; Learning Transfer; Training simulation; Virtual reality training; Virtual training; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85074257749
"Alfalah S.F.M., Falah J.F.M., Alfalah T., Elfalah M., Muhaidat N., Falah O.","55583479900;55350011200;56410895700;57203174172;57203165392;53879792700;","A comparative study between a virtual reality heart anatomy system and traditional medical teaching modalities",2019,"Virtual Reality","23","3",,"229","234",,7,"10.1007/s10055-018-0359-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050790522&doi=10.1007%2fs10055-018-0359-y&partnerID=40&md5=09f0d586cab75a4b324d47e1476bdefb","Department of Computer Information Systems, The University of Jordan, Amman, Jordan; Department of Computer Science, Al-Balqa` Applied University, Amman, Jordan; Department of Business Administration, Applied Science University, Amman, Jordan; Department of Ophthalmology, Faculty of Medicine, The University of Jordan, Amman, Jordan; Department of Obstetrics and Gynaecology, Faculty of Medicine, The University of Jordan, Amman, Jordan; Department of Vascular Surgery, The University of Edinburgh, Edinburgh, United Kingdom","Alfalah, S.F.M., Department of Computer Information Systems, The University of Jordan, Amman, Jordan; Falah, J.F.M., Department of Computer Science, Al-Balqa` Applied University, Amman, Jordan; Alfalah, T., Department of Business Administration, Applied Science University, Amman, Jordan; Elfalah, M., Department of Ophthalmology, Faculty of Medicine, The University of Jordan, Amman, Jordan; Muhaidat, N., Department of Obstetrics and Gynaecology, Faculty of Medicine, The University of Jordan, Amman, Jordan; Falah, O., Department of Vascular Surgery, The University of Edinburgh, Edinburgh, United Kingdom","The aim of using virtual reality (VR) as a medical training tool is to offer additional means to teach students and to improve the quality of medical skills. A novel system was developed to fulfil the requirements of modern medical education and overcome the challenges faced by both students and lecturers in the process of knowledge transfer. A heart three-dimensional model presented in a virtual reality (VR) environment has been implemented in order to facilitate a new educational modality. This paper reports the outcome of a comparative study between traditional medical teaching modalities and virtual reality technology. This study was conducted in the Faculty of Medicine in the University of Jordan. The participants were asked to perform system trials and experiment with the system by navigating through the system interfaces, as well as being exposed to the traditional physical model of the human heart that is currently used in the faculty during practical anatomy sessions. Afterwards, they were asked to provide feedback via a comparative questionnaire. The participants’ replies to the questions regarding the Physical Heart Model and VR heart anatomy system were assessed for reliability using Cronbach’s alpha. The first group’s (Physical Heart Model questions) α value was 0.689. The second group’s (VR heart anatomy system questions) α value was 0.791. Comparing students’ experience results between the traditional method (Physical Heart Model) and the VR heart anatomy system, the mean scores showed a distinct increase in the values. This indicates that the developed system enhanced their experience in anatomy learning and the provided tools improved their understanding of heart anatomy. Results demonstrated the usefulness of the system by showing a higher satisfaction rate for the provided tools regarding structure and visualisation. © 2018, Springer-Verlag London Ltd., part of Springer Nature.","Anatomy; Comparative study; Heart; Medical education; Virtual reality","Education computing; Heart; Knowledge management; Medical education; Students; Teaching; Virtual reality; Anatomy; Comparative studies; Knowledge transfer; Medical training; Satisfaction rates; System interfaces; Three-dimensional model; Virtual reality technology; E-learning",Article,"Final","",Scopus,2-s2.0-85050790522
"Michalski S.C., Szpak A., Saredakis D., Ross T.J., Billinghurst M., Loetscher T.","57210961158;56544337100;57208775758;57208580953;7006142663;14422378000;","Getting your game on: Using virtual reality to improve real table tennis skills",2019,"PLoS ONE","14","9", e0222351,"","",,9,"10.1371/journal.pone.0222351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072012518&doi=10.1371%2fjournal.pone.0222351&partnerID=40&md5=ff27cc648e975d990d1ddedb0226ded6","Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia; Empathic Computing Lab, School of Information Technology and Mathematical Sciences, University of South Australia, Adelaide, SA, Australia","Michalski, S.C., Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia; Szpak, A., Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia; Saredakis, D., Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia; Ross, T.J., Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia; Billinghurst, M., Empathic Computing Lab, School of Information Technology and Mathematical Sciences, University of South Australia, Adelaide, SA, Australia; Loetscher, T., Cognitive Ageing and Impairment Neurosciences Laboratory, School of Psychology, University of South Australia, Adelaide, SA, Australia","Objective The present study investigates skill transfer from Virtual Reality (VR) sports training to the real world, using the fast-paced sport of table tennis. Background A key assumption of VR training is that the learned skills and experiences transfer to the real world. Yet, in certain application areas, such as VR sports training, the research testing this assumption is sparse. Design Real-world table tennis performance was assessed using a mixed-model analysis of variance. The analysis comprised a between-subjects (VR training group vs control group) and a within-subjects (pre- and post-training) factor. Method Fifty-seven participants (23 females) were either assigned to a VR training group (n = 29) or no-training control group (n = 28). During VR training, participants were immersed in competitive table tennis matches against an artificial intelligence opponent. An expert table tennis coach evaluated participants on real-world table tennis playing before and after the training phase. Blinded regarding participant's group assignment, the expert assessed participants' backhand, forehand and serving on quantitative aspects (e.g. count of rallies without errors) and quality of skill aspects (e.g. technique and consistency). Results VR training significantly improved participants' real-world table tennis performance compared to a no-training control group in both quantitative (p < .001, Cohen's d = 1.08) and quality of skill assessments (p < .001, Cohen's d = 1.10). Conclusions This study adds to a sparse yet expanding literature, demonstrating real-world skill transfer from Virtual Reality in an athletic task. © 2019 Public Library of Science. All rights reserved.",,"adult; analysis of variance; article; artificial intelligence; controlled study; female; human; human experiment; major clinical study; male; quantitative analysis; skill; tennis; virtual reality; adolescent; athletic performance; clinical trial; young adult; Adolescent; Adult; Athletic Performance; Female; Humans; Male; Tennis; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85072012518
"Smith M., Walford N.S., Jimenez-Bescos C.","57202867830;7003391757;36608195700;","Using 3D modelling and game engine technologies for interactive exploration of cultural heritage: An evaluation of four game engines in relation to roman archaeological heritage",2019,"Digital Applications in Archaeology and Cultural Heritage","14",, e00113,"","",,4,"10.1016/j.daach.2019.e00113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071373812&doi=10.1016%2fj.daach.2019.e00113&partnerID=40&md5=16efc06926cd483d946d1c5dec411b78","Geography Subject Area, School of Humanities, Religion and Philosophy, York St John University, Lord Mayor's Walk, York, YO31 7EX, United Kingdom; Department of Geography, Geology and the Environment, Kingston University, Penrhyn Road, Kingston upon ThamesKT1 2EE, United Kingdom; Department of Architecture and Built Environment, University of Nottingham, University Park Campus, Nottingham, NG7 2Rd, United Kingdom","Smith, M., Geography Subject Area, School of Humanities, Religion and Philosophy, York St John University, Lord Mayor's Walk, York, YO31 7EX, United Kingdom; Walford, N.S., Department of Geography, Geology and the Environment, Kingston University, Penrhyn Road, Kingston upon ThamesKT1 2EE, United Kingdom; Jimenez-Bescos, C., Department of Architecture and Built Environment, University of Nottingham, University Park Campus, Nottingham, NG7 2Rd, United Kingdom","Developments in information technology have challenged the traditional model of museums, libraries and similar venues acting as relatively passive ‘learning spaces’ for the public to access ‘knowledge’ as an exchange between tutor and learner, or in this context curator and visitor enabling them to offer more immersive and interactive modes of transfer. This article examines the development of a 3D model built from plans of a Roman edifice and its transfer into four game engines as vehicles for independent navigation around the ‘virtual building’. The game engines are evaluated in respect of their ability to enhance visitors’ experience by using an on-site facility when visiting a museum constructed over the physical remains. Cost and licensing override technical factors such as audiovisual and functional fidelity or composability and installing the system on a PC is preferable to more specialist game control devices if a broad user base is targeted. © 2019 Elsevier Ltd","3D modelling; Roman archaeology; Serious gaming; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85071373812
"Unberath M., Zaech J.-N., Gao C., Bier B., Goldmann F., Lee S.C., Fotouhi J., Taylor R., Armand M., Navab N.","56893868600;57203974543;57209547031;55750550000;57208408895;57185365000;56352068600;7405756438;35236429300;7003458998;","Enabling machine learning in X-ray-based procedures via realistic simulation of image formation",2019,"International Journal of Computer Assisted Radiology and Surgery","14","9",,"1517","1528",,11,"10.1007/s11548-019-02011-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067389055&doi=10.1007%2fs11548-019-02011-2&partnerID=40&md5=dc02f361a385386ae3375e08d4cdde74","Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States; Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States; Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Johns Hopkins University Applied Physics Laboratory, Laurel, MD, United States","Unberath, M., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Zaech, J.-N., Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Gao, C., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States; Bier, B., Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Goldmann, F., Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Lee, S.C., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Fotouhi, J., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States; Taylor, R., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States; Armand, M., Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Johns Hopkins University Applied Physics Laboratory, Laurel, MD, United States; Navab, N., Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computational Sensing + Robotics, Johns Hopkins University, Baltimore, MD, United States, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD, United States","Purpose: Machine learning-based approaches now outperform competing methods in most disciplines relevant to diagnostic radiology. Image-guided procedures, however, have not yet benefited substantially from the advent of deep learning, in particular because images for procedural guidance are not archived and thus unavailable for learning, and even if they were available, annotations would be a severe challenge due to the vast amounts of data. In silico simulation of X-ray images from 3D CT is an interesting alternative to using true clinical radiographs since labeling is comparably easy and potentially readily available. Methods: We extend our framework for fast and realistic simulation of fluoroscopy from high-resolution CT, called DeepDRR, with tool modeling capabilities. The framework is publicly available, open source, and tightly integrated with the software platforms native to deep learning, i.e., Python, PyTorch, and PyCuda. DeepDRR relies on machine learning for material decomposition and scatter estimation in 3D and 2D, respectively, but uses analytic forward projection and noise injection to ensure acceptable computation times. On two X-ray image analysis tasks, namely (1) anatomical landmark detection and (2) segmentation and localization of robot end-effectors, we demonstrate that convolutional neural networks (ConvNets) trained on DeepDRRs generalize well to real data without re-training or domain adaptation. To this end, we use the exact same training protocol to train ConvNets on naïve and DeepDRRs and compare their performance on data of cadaveric specimens acquired using a clinical C-arm X-ray system. Results: Our findings are consistent across both considered tasks. All ConvNets performed similarly well when evaluated on the respective synthetic testing set. However, when applied to real radiographs of cadaveric anatomy, ConvNets trained on DeepDRRs significantly outperformed ConvNets trained on naïve DRRs (p< 0.01). Conclusion: Our findings for both tasks are positive and promising. Combined with complementary approaches, such as image style transfer, the proposed framework for fast and realistic simulation of fluoroscopy from CT contributes to promoting the implementation of machine learning in X-ray-guided procedures. This paradigm shift has the potential to revolutionize intra-operative image analysis to simplify surgical workflows. © 2019, CARS.","Artificial intelligence; Computer assisted surgery; Image guidance; Monte Carlo simulation; Robotic surgery; Segmentation","anatomic landmark; Article; cadaver; computer assisted surgery; computer simulation; controlled study; convolutional neural network; deep learning; fluoroscopy; image analysis; image display; machine learning; priority journal; robot assisted surgery; x-ray computed tomography; algorithm; anatomic model; fluoroscopy; human; image processing; procedures; radiation scattering; three dimensional imaging; X ray; x-ray computed tomography; Algorithms; Cadaver; Computer Simulation; Fluoroscopy; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Machine Learning; Models, Anatomic; Neural Networks, Computer; Scattering, Radiation; Tomography, X-Ray Computed; X-Rays",Article,"Final","",Scopus,2-s2.0-85067389055
"Warriar V.R., Woodward J.R., Tokarchuk L.","57211241347;35753840700;6602852642;","Modelling player preferences in AR mobile games",2019,"IEEE Conference on Computatonal Intelligence and Games, CIG","2019-August",, 8848082,"","",,2,"10.1109/CIG.2019.8848082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073121266&doi=10.1109%2fCIG.2019.8848082&partnerID=40&md5=c61e54edda46ff3542ac1c92d8161dc1","School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom","Warriar, V.R., School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom; Woodward, J.R., School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom; Tokarchuk, L., School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom","In this paper, we use preference learning techniques to model players' emotional preferences in an AR mobile game. This exploratory study uses player behaviour to make these preference predictions. The described techniques successfully predict players' frustration and challenge levels with high accuracy while all other preferences tested (boredom, excitement and fun) perform better than random chance. This paper describes the AR treasure hunt game we developed, the user study conducted to collect player preference data, analysis performed, and preference learning techniques applied to model this data. This work is motivated to personalize players' experiences by using these computational models to optimize content creation and game balancing systems in these environments. The generality of our technique, limitations, and usability as a tool for personalization of AR mobile games is discussed. © 2019 IEEE.","Augmented reality; Content creation; Linear classifiers; Mobile games; Player experience; Player preference modelling; Preference learning","Augmented reality; Learning algorithms; Content creation; Linear classifiers; Mobile games; Player experience; Preference learning; Preference modelling; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85073121266
"Luo Z., Wu C., Li Z., Zhou W.","57193724300;15836048100;55666266700;56393476500;","Scaling Geo-Distributed Network Function Chains: A Prediction and Learning Framework",2019,"IEEE Journal on Selected Areas in Communications","37","8", 8756207,"1838","1850",,5,"10.1109/JSAC.2019.2927068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068566932&doi=10.1109%2fJSAC.2019.2927068&partnerID=40&md5=1d0c2f24e1621bb4827aa9dc9b2c45a4","Department of Computer Science, University of Hong Kong, Hong Kong; School of Computer Science, Wuhan University, Wuhan, China; Wireless Network Research Department, Huawei Technologies, Shanghai, China","Luo, Z., Department of Computer Science, University of Hong Kong, Hong Kong; Wu, C., Department of Computer Science, University of Hong Kong, Hong Kong; Li, Z., School of Computer Science, Wuhan University, Wuhan, China; Zhou, W., Wireless Network Research Department, Huawei Technologies, Shanghai, China","Geo-distributed virtual network function (VNF) chaining has been useful, such as in network slicing in 5G networks and for network traffic processing in the WAN. Agile scaling of the VNF chains according to real-time traffic rates is the key in network function virtualization. Designing efficient scaling algorithms is challenging, especially for geo-distributed chains, where bandwidth costs and latencies incurred by the WAN traffic are important but difficult to handle in making scaling decisions. Existing studies have largely resorted to optimization algorithms in scaling design. Aiming at better decisions empowered by in-depth learning from experiences, this paper proposes a deep learning-based framework for scaling of the geo-distributed VNF chains, exploring inherent pattern of traffic variation and good deployment strategies over time. We novelly combine a recurrent neural network as the traffic model for predicting upcoming flow rates and a deep reinforcement learning (DRL) agent for making chain placement decisions. We adopt the experience replay technique based on the actor-critic DRL algorithm to optimize the learning results. Trace-driven simulation shows that with limited offline training, our learning framework adapts quickly to traffic dynamics online and achieves lower system costs, compared to the existing representative algorithms. © 2019 IEEE.","deep learning; Network function virtualization; reinforcement learning; service function chain","5G mobile communication systems; Deep learning; Learning algorithms; Machine learning; Recurrent neural networks; Reinforcement learning; Transfer functions; Virtual reality; Wide area networks; Deployment strategy; Distributed networks; Learning frameworks; Learning from experiences; Optimization algorithms; Real time traffics; Service functions; Trace driven simulation; Network function virtualization",Article,"Final","",Scopus,2-s2.0-85068566932
"Steffen J.H., Gaskin J.E., Meservy T.O., Jenkins J.L., Wolman I.","57200579874;36006215900;8965919200;36713489900;57210317891;","Framework of Affordances for Virtual Reality and Augmented Reality",2019,"Journal of Management Information Systems","36","3",,"683","729",,10,"10.1080/07421222.2019.1628877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070323987&doi=10.1080%2f07421222.2019.1628877&partnerID=40&md5=4cc7e578379bb372fe5452b489db6237","Terry College of Business, University of Georgia, United States; Management Information Systems, Brigham Young University, United States; Information Systems, Brigham Young University, United States; Information Systems, Brigham Young University, United States; Master of Information Systems Management program, Brigham Young University, United States","Steffen, J.H., Terry College of Business, University of Georgia, United States; Gaskin, J.E., Management Information Systems, Brigham Young University, United States; Meservy, T.O., Information Systems, Brigham Young University, United States; Jenkins, J.L., Information Systems, Brigham Young University, United States; Wolman, I., Master of Information Systems Management program, Brigham Young University, United States","Virtual reality (VR) and augmented reality (AR) technologies continue to grow and present possibilities to change the ways we learn, accomplish tasks, and interact with the world. However, widespread adoption has continually languished below purported potential. We suggest that a more complete understanding of the underlying motives driving users to take advantage of VR and AR would aid researchers by consolidating fragmented knowledge across domains and by identifying paths for additional inquiry. Additionally, practitioners could identify areas of unmet motives for using VR and AR. To examine the motives for virtualization, we draw upon Gibson’s seminal work on affordances to create a framework of generalized affordances for virtually assisted activities relative to the affordances of physical reality. This framework facilitates comparison of virtualized activities to non-virtualized activities, comparison of similar activities across VR and AR, and delineates areas of inquiry for future research. The validity of the framework was explored through two quantitative studies and one qualitative study of a wide variety of professionals. We found that participants perceive a significant difference between physical reality and both VR and AR for all proposed affordances, and that for many affordances, users perceive a difference in the ability of AR and VR to enact them. The qualitative study confirmed the general structure of the framework, while also revealing additional sub-affordances to explore. Theoretically, this suggests that examining the affordances that differentiate these technologies from physical reality may be a valid approach to understanding why users adopt these technologies. Practitioners may find success by focusing development on the specific affordances that VR or AR is best equipped to enact. ©, Copyright © Taylor & Francis Group, LLC.","adoption motivations; augmented reality; technology adoption; technology affordances; theoretical framework; virtual reality; virtually assisted activities","Augmented reality; Virtual reality; Adoption motivations; Technology adoption; Technology affordances; Theoretical framework; virtually assisted activities; Engineering education",Article,"Final","",Scopus,2-s2.0-85070323987
"Archambault P.S., Bigras C.","55328034600;57205472507;","Improving wheelchair driving performance in a virtual reality simulator",2019,"International Conference on Virtual Rehabilitation, ICVR","2019-July",, 8994644,"","",,1,"10.1109/ICVR46560.2019.8994644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080110171&doi=10.1109%2fICVR46560.2019.8994644&partnerID=40&md5=67fd7ae697f3ac312afd3e5d3240ef81","McGill University, School of Physical and Occupational Therapy, Montreal, Canada","Archambault, P.S., McGill University, School of Physical and Occupational Therapy, Montreal, Canada; Bigras, C., McGill University, School of Physical and Occupational Therapy, Montreal, Canada","In this study, we measured if practice of a wheelchair activity in a virtual reality simulator (entering an elevator) improved wheelchair positioning skills in naïve, healthy adults. Performance was assessed immediately after practice, two days later (retention) and in a real-world equivalent task (transfer). The influence of augmented feedback on retention and transfer was also assessed. Forty participants were randomized to either an augmented feedback group (who received information on collisions and on task completion time) and a no-feedback group. Following training, both groups improved their wheelchair positioning abilities. Learning was maintained at retention and skills transferred to the real-world wheelchair. Augmented feedback did not procure any additional effects. Practice in a virtual reality simulator significantly improved wheelchair positioning skills. Higher performance gains could be achieved by providing task-specific feedback. © 2019 IEEE.","learning; performance; power wheelchair; simulator; transfer","Automobile drivers; Simulators; Virtual reality; Wheelchairs; Augmented feedback; Driving performance; learning; performance; Power wheelchair; Task completion time; transfer; Virtual reality simulator; Transfer learning",Conference Paper,"Final","",Scopus,2-s2.0-85080110171
"Levac D.E., Glegg S., Pradhan S., Fox E.J., Espy D., Chicklis E., Deutsch J.E.","25937323900;36871721200;35146628200;7202093916;34876570500;57209688091;7201985389;","A comparison of virtual reality and active video game usage, attitudes and learning needs among therapists in Canada and the US",2019,"International Conference on Virtual Rehabilitation, ICVR","2019-July",, 8994624,"","",,,"10.1109/ICVR46560.2019.8994624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080126548&doi=10.1109%2fICVR46560.2019.8994624&partnerID=40&md5=76dbcb4ce339178f44905371c7592a26","Northeastern University, Dept. of Physical Therapy, Boston, United States; Sunny Hill Health Centre for Children, Therapy Department, Vancouver, Canada; University of Washington, Div. of Physical Therapy, Seattle, United States; University of Florida, Dept. of Physical Therapy, Gainesville, FL, United States; Cleveland State University, Dept. of Health Sciences, Cleveland, United States; Northeastern University, Rehabilitation Games and Virtual Reality Lab, Boston, MA, United States; Rutgers University, Dept. of Rehab and Movement Sciences, Newark, United States","Levac, D.E., Northeastern University, Dept. of Physical Therapy, Boston, United States; Glegg, S., Sunny Hill Health Centre for Children, Therapy Department, Vancouver, Canada; Pradhan, S., University of Washington, Div. of Physical Therapy, Seattle, United States; Fox, E.J., University of Florida, Dept. of Physical Therapy, Gainesville, FL, United States; Espy, D., Cleveland State University, Dept. of Health Sciences, Cleveland, United States; Chicklis, E., Northeastern University, Rehabilitation Games and Virtual Reality Lab, Boston, MA, United States; Deutsch, J.E., Rutgers University, Dept. of Rehab and Movement Sciences, Newark, United States","Differences in health care funding and policies between the United States and Canada may influence uptake of and attitudes towards virtual reality (VR) and active video gaming (AVG) systems by physical (PTs) and occupational therapists (OTs) in each country. The purpose of this study was to undertake a cross-country comparison of VR/AVG uptake to inform the content of educational interventions designed to promote implementation of these technologies into practice. A cross-sectional online survey that included the Assessing Determinants of Prospective Take-up of Virtual Reality (version 2; ADOPT-VR2) Instrument was conducted in 2014-2015 (Canada) and replicated in 2017-2018 (US). Recruitment took place via convenience and snowball sampling, using email, social media and newsletter postings. Therapists in the US reported greater past experience with, current use of, and intention to use VR/AVGs than did those in Canada. They also rated facilitators more positively and barriers less negatively. Use of customized VR systems was low, with specific system prevalence differing between countries. The most frequently used AVG systems, populations and settings of use, functional goals, predictors of use, learning needs and preferred forms of support were similar between countries. These similarities support the generalizability of educational interventions for both countries. Materials to be developed will focus on non-customized AVG systems. Subsequent work will examine how uptake relates to country-specific health care funding and policies, probe differences in learning needs between therapists with experience using customized versus non-customized VR/AVG systems, and extend the survey to other countries where VR/AVG use is prevalent. © 2019 IEEE.","active video games; knowledge translation; rehabilitation; survey; virtual reality","E-learning; Health care; Interactive computer graphics; Patient rehabilitation; Surveying; Surveys; User experience; Virtual reality; Canada and the US; Country comparisons; Educational intervention; Intention to use; Knowledge translation; Online surveys; Video game; Video gaming; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85080126548
"Odette K., Fu Q.","57194717753;36162233700;","A Physics-based Virtual Reality Environment to Quantify Functional Performance of Upper-limb Prostheses",2019,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,, 8857850,"3807","3810",,,"10.1109/EMBC.2019.8857850","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077877471&doi=10.1109%2fEMBC.2019.8857850&partnerID=40&md5=9e2c624bfcf1441762dbfd8351d606c2","Modeling and Simulation Program, University of Central Florida, Orlando, FL  32827, United States; NeuroMechanical Systems Laboratory, Mechanical and Aerospace Engineering, Biionix (Bionic Materials, Implants Interfaces) Cluster, University of Central Florida, Orlando, FL  32827, United States","Odette, K., Modeling and Simulation Program, University of Central Florida, Orlando, FL  32827, United States; Fu, Q., NeuroMechanical Systems Laboratory, Mechanical and Aerospace Engineering, Biionix (Bionic Materials, Implants Interfaces) Cluster, University of Central Florida, Orlando, FL  32827, United States","Usability of upper-limb prostheses remains to be a challenge due to the complexity of hand-object interactions in activities of daily living. Functional evaluation is critical for the optimization of prosthesis performance during device design and parameter tuning phase. Therefore, we implemented a low-cost physics-based virtual reality environment (VRE) capable of simulating wide range of object grasping and manipulation tasks to enable human-in-the-loop optimization. Importantly, our novel VRE can assess user performance quantitatively using movement kinematics and interaction forces. We present a preliminary experiment to validate our VRE. Four able-bodied subjects performed object transfer tasks with a simulated myoelectric one DoF soft-synergy prosthetic hand, while wearing braces to restrain different levels of wrist motion. We found that the task completion time was similar across conditions, however limited wrist pronation led to more shoulder compensatory motion whereas challenging object orientation caused more torso compensatory motion. © 2019 IEEE.",,"Virtual reality; Activities of Daily Living; Functional evaluation; Functional performance; Movement kinematics; Object interactions; Task completion time; Upper limb prosthesis; Virtual-reality environment; Artificial limbs",Conference Paper,"Final","",Scopus,2-s2.0-85077877471
"Wang H., Wang Q., Hu F.","57214057788;57145088700;57205211655;","Are you afraid of heights and suitable for working at height?",2019,"Biomedical Signal Processing and Control","52",,,"23","31",,4,"10.1016/j.bspc.2019.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063350700&doi=10.1016%2fj.bspc.2019.03.011&partnerID=40&md5=aa4056a7eff76371071e3dc5157f14a2","NO. 3-11, Wenhua Road, Heping District, Shenyang, 110819, China","Wang, H., NO. 3-11, Wenhua Road, Heping District, Shenyang, 110819, China; Wang, Q., NO. 3-11, Wenhua Road, Heping District, Shenyang, 110819, China; Hu, F., NO. 3-11, Wenhua Road, Heping District, Shenyang, 110819, China","Fear of highs is one of the most common phobias all around world. It could affect people's life, work and health. Standing on high-altitude can lead to fear, anxiety or even panic to some people. In this paper, EEG method is creatively combined with VR technology to assess the severity of fear of heights. By doing time-frequency analysis, we found that alpha band (8–13 Hz) and high beta (20–30 Hz) are sensitive to fear of heights and frontal and parietotemporal areas are the regions of interests for fear of heights. Then using cross mutual information we built up a functional brain networks of every subject. And we extracted EEG features from the brain networks. Statistical analysis was performed to select the features based on significance of difference. Finally, we implemented classification. The performance of classifiers (the average accuracy could reach 94.44%) based on the proposed method was compared to the performance of classifiers based on the traditional physiological features. As a result, the proposed method was verified to be reliable and superior on estimating the severity of fear of heights. In addition, the system was tested on elderly people and came out with good performance. It turns out that the proposed system has good generalization capability and adaptability. © 2019","EEG; Fear of heights; Functional brain network; VR","Biomedical engineering; Control engineering; Electroencephalography; Brain networks; Fear of heights; Generalization capability; Mutual informations; Performance of classifier; Physiological features; Regions of interest; Time frequency analysis; Signal processing; acrophobia; adult; agitation; anxiety; Article; brain function; controlled study; electroencephalogram; electrooculogram; entropy; fear; female; human; male; muscle tone; nerve cell network; normal human; priority journal; supervised machine learning; virtual reality; work environment",Article,"Final","",Scopus,2-s2.0-85063350700
"Katrychuk D., Griffith H.K., Komogortsev O.V.","57210105027;57189386560;6506328653;","Power-efficient and shift-robust eye-tracking sensor for portable VR headsets",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,, a19,"","",,7,"10.1145/3314111.3319821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069483095&doi=10.1145%2f3314111.3319821&partnerID=40&md5=457b404170995b342473d55c75de6d33","Department of Computer Science, Texas State University, San Marcos, TX, United States","Katrychuk, D., Department of Computer Science, Texas State University, San Marcos, TX, United States; Griffith, H.K., Department of Computer Science, Texas State University, San Marcos, TX, United States; Komogortsev, O.V., Department of Computer Science, Texas State University, San Marcos, TX, United States","Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models. © 2019 Association for Computing Machinery.","Eye-tracking; Machine learning; ML; Photo-sensor oculography; PSOG; Virtual reality; VR","Augmented reality; Convolution; Economic and social effects; Learning systems; Machine learning; Neural networks; Optical sensors; Virtual reality; Computational requirements; Convolutional model; Convolutional neural network; Eye-tracking sensors; Performance degradation; Photo-sensors; PSOG; Virtual and augmented reality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069483095
"Lopez C.E., Ashour O., Tucker C.","57193163382;36080456300;15833577900;","An introduction to the CLICK approach: Leveraging virtual reality to integrate the industrial engineering curriculum",2019,"ASEE Annual Conference and Exposition, Conference Proceedings",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076433701&partnerID=40&md5=0b9ee7dc3d5e083d23ad59a1d87115c6","Pennsylvania State University, University Park, United States; Penn State Erie, Behrend College, United States","Lopez, C.E., Pennsylvania State University, University Park, United States; Ashour, O., Penn State Erie, Behrend College, United States; Tucker, C., Pennsylvania State University, University Park, United States","This work introduces a new approach called Connected Learning and Integrated Course Knowledge (CLICK). CLICK is intended to provide an integrative learning experience by leveraging Virtual Reality (VR) technology to help provide a theme to connect and transfer the knowledge of engendering concepts. Integrative learning is described as the process of creating connections between concepts (i.e., skill and knowledge) from different resources and experiences, linking theory and practice, and using a variation of platforms to help students' understanding. In the CLICK approach, the integration is achieved by VR learning modules that serve as a platform for a common theme and include various challenges and exercises from multiple courses across the IE curriculum. Moreover, the modules will provide an immersive and realistic experience, which the authors hypothesize, will improve how the students relate what they learn in a classroom, to real-life experiences. The goals of the CLICK approach are to (i) provide the needed connection between courses and improve students' learning, and (ii) provide the needed linkage between theory and practice through a realistic representation of systems using VR. This work presents the results from an initial usability test performed on one of the VR modules. The results from the usability test indicate that participants liked the realism of the VR module. However, there are still some areas for improvement, and future work will focus on assessing the impact of the CLICK approach on students' learning, motivation, and preparation to be successful engineers, areas which could translate to a STEM pipeline for the future workforce. © American Society for Engineering Education, 2019",,"Curricula; Engineering education; Knowledge management; Virtual reality; Integrated course; Integrative learning; Learning modules; Life experiences; Multiple course; New approaches; Theory and practice; Usability tests; Students",Conference Paper,"Final","",Scopus,2-s2.0-85076433701
"Toth K.A., Moreland J., Zhou C.Q., Balachandran A., Zhang M.F., Roudebush J.C.","57208035174;35109608300;7403347472;57214444492;57214450285;57205038347;","Development of an educational wind turbine troubleshooting and safety simulator",2019,"ASEE Annual Conference and Exposition, Conference Proceedings",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078726658&partnerID=40&md5=d49e56d904b56e935334d3c19ae651f4","Purdue University Northwest, United States; CIVS, Purdue University Northwest, United States; Center for Innovation through Visualization and Simulation; Ivy Tech Community College","Toth, K.A., Purdue University Northwest, United States; Moreland, J., Purdue University Northwest, United States; Zhou, C.Q., CIVS, Purdue University Northwest, United States; Balachandran, A., Center for Innovation through Visualization and Simulation; Zhang, M.F., Purdue University Northwest, United States; Roudebush, J.C., Ivy Tech Community College","This project is developing a web-based, interactive 3D simulator for community college wind energy technician training programs. Directly contributing to the current project are two previous wind energy projects: “Wind Tech TV,” a 2010 NSF Advanced Technological Education (ATE) project that compiled a library of online training materials for wind turbine technician training, and “Mixed Reality Simulators for Wind Energy Education,” a U.S. Department of Education Fund for the Improvement of Postsecondary Education (FIPSE) project that produced a series of simulators for wind energy educators and students to provide hands-on experiences and promote critical thinking. The new simulator is designed to teach troubleshooting and safety strategies, promote critical-thinking and problem-solving skills, and enhance the transfer of knowledge from classrooms to real-world situations. Learner immersion in a simulated environment and progression through numerous troubleshooting scenarios is expected to provide a better-prepared and more skilled workforce for the wind energy industry. Multiple community colleges, a wind energy company, two NSF ATE Centers, and a university research center comprise the multidisciplinary team working on this project. Community colleges are leading the curriculum and educational module design and implementation, industry collaborators are advising on needed skills and recommended activities, and the university research center is developing the software. Data from implementation in one community college so far show a 10% score increase for students using the simulator compared to traditional instruction alone. © American Society for Engineering Education, 2019",,"Curricula; Engineering education; Knowledge management; Mixed reality; Simulators; Wind power; Wind turbines; Advanced Technological Education; Department of Education; Multi-disciplinary teams; Postsecondary education; Problem solving skills; Simulated environment; Traditional instruction; Transfer of knowledge; Students",Conference Paper,"Final","",Scopus,2-s2.0-85078726658
"Vertemati M., Cassin S., Rizzetto F., Vanzulli A., Elli M., Sampogna G., Gallieni M.","6602085318;57205576019;57205575796;16026541900;7003507564;57193081915;7004243711;","A Virtual Reality Environment to Visualize Three-Dimensional Patient-Specific Models by a Mobile Head-Mounted Display",2019,"Surgical Innovation","26","3",,"359","370",,7,"10.1177/1553350618822860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060629868&doi=10.1177%2f1553350618822860&partnerID=40&md5=e7b6cc98e1f592ddf43393f6e8f6d519","Università degli Studi di Milano, Milan, Italy","Vertemati, M., Università degli Studi di Milano, Milan, Italy; Cassin, S., Università degli Studi di Milano, Milan, Italy; Rizzetto, F., Università degli Studi di Milano, Milan, Italy; Vanzulli, A., Università degli Studi di Milano, Milan, Italy; Elli, M., Università degli Studi di Milano, Milan, Italy; Sampogna, G., Università degli Studi di Milano, Milan, Italy; Gallieni, M., Università degli Studi di Milano, Milan, Italy","Introduction. With the availability of low-cost head-mounted displays (HMDs), virtual reality environments (VREs) are increasingly being used in medicine for teaching and clinical purposes. Our aim was to develop an interactive, user-friendly VRE for tridimensional visualization of patient-specific organs, establishing a workflow to transfer 3-dimensional (3D) models from imaging datasets to our immersive VRE. Materials and Methods. This original VRE model was built using open-source software and a mobile HMD, Samsung Gear VR. For its validation, we enrolled 33 volunteers: morphologists (n = 11), trainee surgeons (n = 15), and expert surgeons (n = 7). They tried our VRE and then filled in an original 5-point Likert-type scale 6-item questionnaire, considering the following parameters: ease of use, anatomy comprehension compared with 2D radiological imaging, explanation of anatomical variations, explanation of surgical procedures, preoperative planning, and experience of gastrointestinal/neurological disorders. Results in the 3 groups were statistically compared using analysis of variance. Results. Using cross-sectional medical imaging, the developed VRE allowed to visualize a 3D patient-specific abdominal scene in 1 hour. Overall, the 6 items were evaluated positively by all groups; only anatomy comprehension was statistically significant different among the 3 groups. Conclusions. Our approach, based on open-source software and mobile hardware, proved to be a valid and well-appreciated system to visualize 3D patient-specific models, paving the way for a potential new tool for teaching and preoperative planning. © The Author(s) 2019.","anatomy; head-mounted display; mobile; training; virtual reality","anatomy; Article; clinical practice; computer assisted tomography; human; image segmentation; medical education; nuclear magnetic resonance imaging; partial nephrectomy; questionnaire; radiography; surgeon; surgical technique; surgical training; three-dimensional imaging; training; treatment planning; virtual reality; computer assisted surgery; computer interface; devices; equipment design; software; three dimensional imaging; x-ray computed tomography; Equipment Design; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Software; Surgery, Computer-Assisted; Surveys and Questionnaires; Tomography, X-Ray Computed; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85060629868
"Chen M., Saad W., Yin C., Debbah M.","57113807700;57203259001;7201995655;35588784300;","Data Correlation-Aware Resource Management in Wireless Virtual Reality (VR): An Echo State Transfer Learning Approach",2019,"IEEE Transactions on Communications","67","6", 8648419,"4267","4280",,16,"10.1109/TCOMM.2019.2900624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067570259&doi=10.1109%2fTCOMM.2019.2900624&partnerID=40&md5=a5394beb4157a6063a3abd74b6a89eb1","Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA  24061, United States; Mathematical and Algorithmic Sciences Lab, Huawei France RD, Paris, 92100, France","Chen, M., Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Saad, W., Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA  24061, United States; Yin, C., Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Debbah, M., Mathematical and Algorithmic Sciences Lab, Huawei France RD, Paris, 92100, France","Providing seamless connectivity for wireless virtual reality (VR) users has emerged as a key challenge for future cloud-enabled cellular networks. In this paper, the problem of wireless VR resource management is investigated for a wireless VR network in which VR contents are sent by a cloud to cellular small base stations (SBSs). The SBSs will collect tracking data from the VR users, over the uplink, in order to generate the VR content and transmit it to the end-users using downlink cellular links. For this model, the data requested or transmitted by the users can exhibit correlation, since the VR users may engage in the same immersive virtual environment with different locations and orientations. As such, the proposed resource management framework can factor in such spatial data correlation, so as to better manage uplink and downlink traffic. This potential spatial data correlation can be factored into the resource allocation problem to reduce the traffic load in both the uplink and downlink. In the downlink, the cloud can transmit 360° contents or specific visible contents (e.g., user field of view) that are extracted from the original 360° contents to the users according to the users' data correlation so as to reduce the backhaul traffic load. In the uplink, each SBS can associate with the users that have similar tracking information so as to reduce the tracking data size. This data correlation-aware resource management problem is formulated as an optimization problem whose goal is to maximize the users' successful transmission probability, defined as the probability that the content transmission delay of each user satisfies an instantaneous VR delay target. To solve this problem, a machine learning algorithm that uses echo state networks (ESNs) with transfer learning is introduced. By smartly transferring information on the SBS's utility, the proposed transfer-based ESN algorithm can quickly cope with changes in the wireless networking environment due to users' content requests and content request distributions. Simulation results demonstrate that the developed algorithm achieves up to 15.8% and 29.4% gains in terms of successful transmission probability compared to Q-learning with data correlation and Q-learning without data correlation, respectively. © 1972-2012 IEEE.","echo state networks; resource allocation; transfer learning; Virtual reality","E-learning; Information management; Learning algorithms; Machine learning; Natural resources management; Probability; Problem solving; Resource allocation; Virtual reality; Echo state networks; Immersive virtual environments; Resource allocation problem; Resource management framework; Resource management problems; Transfer learning; Transmission probabilities; Wireless networking environment; Data reduction",Conference Paper,"Final","",Scopus,2-s2.0-85067570259
"De Boer I.R., Lagerweij M.D., Wesselink P.R., Vervoorn J.M.","55323421600;6603541390;35845287700;6602342672;","The Effect of Variations in Force Feedback in a Virtual Reality Environment on the Performance and Satisfaction of Dental Students",2019,"Simulation in Healthcare","14","3",,"169","174",,5,"10.1097/SIH.0000000000000370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067368183&doi=10.1097%2fSIH.0000000000000370&partnerID=40&md5=00e72ce4bf34988aab1c46c69ab818c2","Institute of Education, Department of Cardiology Endodontology and Pedodontology, GustavMahlerlaan 3004, Amsterdam, 1081 LA, Netherlands; Academic Centre for Dentistry Amsterdam (ACTA), University of Amsterdam, Netherlands; Vrije Universiteit Amsterdam, Amsterdam, Netherlands","De Boer, I.R., Institute of Education, Department of Cardiology Endodontology and Pedodontology, GustavMahlerlaan 3004, Amsterdam, 1081 LA, Netherlands, Vrije Universiteit Amsterdam, Amsterdam, Netherlands; Lagerweij, M.D., Academic Centre for Dentistry Amsterdam (ACTA), University of Amsterdam, Netherlands; Wesselink, P.R., Academic Centre for Dentistry Amsterdam (ACTA), University of Amsterdam, Netherlands; Vervoorn, J.M., Institute of Education, Department of Cardiology Endodontology and Pedodontology, GustavMahlerlaan 3004, Amsterdam, 1081 LA, Netherlands","Introduction The aim of this study was to investigate the transfer of skills between various levels of force feedback (FFB) using the Simodont dental trainer (Moog, Nieuw-Vennep, the Netherlands). Students practiced a manual dexterity exercise in a virtual reality environment at a standard level of FFB and then were tested at the standard and an altered level of FFB. In addition, the students' satisfaction with the training exercise was evaluated. Methods One hundred twenty-six first-year dental students were randomly distributed into four groups and underwent a manual dexterity test in the virtual reality environment with automatic assessment after a 3-month period of practicing with standard FFB. The test consisted of drilling with the standard FFB and an altered level of FFB to evaluate the effect on performance. After the test, the participants completed a questionnaire. Results The results showed that 74% of the students who passed completed between one of three and three of three successful attempts at FFB levels at which they had not previously practiced. Conclusions The results of this study imply that if students practice a sufficient amount of time at one level of FFB, a skill is transferable from one level of FFB to another. © Lippincott Williams & Wilkins.","Dental education; force feedback; haptics; Simodont dental trainer; virtual reality; virtual reality environment","clinical competence; computer interface; constructive feedback; dental education; dental student; human; motor performance; Netherlands; procedures; psychology; simulation training; time factor; virtual reality; Clinical Competence; Education, Dental; Formative Feedback; Humans; Motor Skills; Netherlands; Simulation Training; Students, Dental; Time Factors; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85067368183
"Wang W., Ceylan D., Mech R., Neumann U.","57195424528;55575553100;24169244200;7103378063;","3DN: 3D deformation network",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",, 8954215,"1038","1046",,23,"10.1109/CVPR.2019.00113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078766740&doi=10.1109%2fCVPR.2019.00113&partnerID=40&md5=67054dbd3bc8cef75dc1d95292648cf8","University of Southern California, Los Angeles, CA, United States; Adobe, San Jose, CA, United States","Wang, W., University of Southern California, Los Angeles, CA, United States; Ceylan, D., Adobe, San Jose, CA, United States; Mech, R., Adobe, San Jose, CA, United States; Neumann, U., University of Southern California, Los Angeles, CA, United States","Applications in virtual and augmented reality create a demand for rapid creation and easy access to large sets of 3D models. An effective way to address this demand is to edit or deform existing 3D models based on a reference, e.g., a 2D image which is very easy to acquire. Given such a source 3D model and a target which can be a 2D image, 3D model, or a point cloud acquired as a depth scan, we introduce 3DN, an end-to-end network that deforms the source model to resemble the target. Our method infers per-vertex offset displacements while keeping the mesh connectivity of the source model fixed. We present a training strategy which uses a novel differentiable operation, mesh sampling operator, to generalize our method across source and target models with varying mesh densities. Mesh sampling operator can be seamlessly integrated into the network to handle meshes with different topologies. Qualitative and quantitative results show that our method generates higher quality results compared to the state-of-the art learning-based methods for 3D shape generation. © 2019 IEEE.","3D from Single Image; Vision + Graphics","3D modeling; Augmented reality; Computer vision; Mesh generation; Personnel training; End-to-end network; Learning-based methods; Mesh connectivity; Quantitative result; Single images; State of the art; Training strategy; Virtual and augmented reality; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85078766740
"Xu W., Li X., Gong L., Huang Y., Zheng Z., Zhao Z., Zhao L., Chen B., Yang H., Cao L., Liu C.","57205769664;57195481734;36637488200;54417393600;57210109321;57205766182;57203174083;57205769195;57203171802;55877295986;7409795747;","Natural teaching for humanoid robot via human-in-the-loop scene-motion cross-modal perception",2019,"Industrial Robot","46","3",,"404","414",,2,"10.1108/IR-06-2018-0118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069436316&doi=10.1108%2fIR-06-2018-0118&partnerID=40&md5=8da8823c8dbb783f78e2aed65cd3ba29","School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Institution of mechatronics, Shanghai, China; Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University Medical School Affiliated Ruijin Hospital, Shanghai, China","Xu, W., School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Li, X., Shanghai Jiao Tong University, Shanghai, China; Gong, L., Institution of mechatronics, Shanghai, China; Huang, Y., Shanghai Jiao Tong University, Shanghai, China; Zheng, Z., School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Zhao, Z., Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Zhao, L., Shanghai Jiao Tong University, Shanghai, China; Chen, B., Shanghai Jiao Tong University, Shanghai, China; Yang, H., Shanghai Jiao Tong University, Shanghai, China; Cao, L., Shanghai Jiao Tong University Medical School Affiliated Ruijin Hospital, Shanghai, China; Liu, C., Shanghai Jiao Tong University, Shanghai, China","Purpose: This paper aims to present a human-in-the-loop natural teaching paradigm based on scene-motion cross-modal perception, which facilitates the manipulation intelligence and robot teleoperation. Design/methodology/approach: The proposed natural teaching paradigm is used to telemanipulate a life-size humanoid robot in response to a complicated working scenario. First, a vision sensor is used to project mission scenes onto virtual reality glasses for human-in-the-loop reactions. Second, motion capture system is established to retarget eye-body synergic movements to a skeletal model. Third, real-time data transfer is realized through publish-subscribe messaging mechanism in robot operating system. Next, joint angles are computed through a fast mapping algorithm and sent to a slave controller through a serial port. Finally, visualization terminals render it convenient to make comparisons between two motion systems. Findings: Experimentation in various industrial mission scenes, such as approaching flanges, shows the numerous advantages brought by natural teaching, including being real-time, high accuracy, repeatability and dexterity. Originality/value: The proposed paradigm realizes the natural cross-modal combination of perception information and enhances the working capacity and flexibility of industrial robots, paving a new way for effective robot teaching and autonomous learning. © 2019, Wenbin Xu, Xudong Li, Liang Gong, Yixiang Huang, Zeyuan Zheng, Zelin Zhao, Lujie Zhao, Binhao Chen, Haozhe Yang, Li Cao and Chengliang Liu.","Cross-modal perception; Human-in-the-loop; Humanoid robot; Motion imitation; Natural teaching","Anthropomorphic robots; Artificial life; Conformal mapping; Data transfer; Eye movements; Industrial robots; Intelligent robots; Machine design; Virtual reality; Cross-modal; Design/methodology/approach; Human-in-the-loop; Humanoid robot; Messaging mechanisms; Motion capture system; Motion imitations; Robot operating system; Visual servoing",Article,"Final","",Scopus,2-s2.0-85069436316
"Rogers J.M., Duckworth J., Middleton S., Steenbergen B., Wilson P.H.","57205393446;36460871400;7102867349;6701413521;7404348396;","Elements virtual rehabilitation improves motor, cognitive, and functional outcomes in adult stroke: Evidence from a randomized controlled pilot study",2019,"Journal of NeuroEngineering and Rehabilitation","16","1", 56,"","",,11,"10.1186/s12984-019-0531-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066015714&doi=10.1186%2fs12984-019-0531-y&partnerID=40&md5=05d2f102a4b02f5b33162a9028b41868","University of Sydney, Faculty of Health Sciences, Sydney, NSW, Australia; School of Design, RMIT, Melbourne, VIC, Australia; Nursing Research Institute, St Vincent's Health Australia and Australian Catholic University, Sydney, NSW, Australia; Behavioural Science Institute, Radboud University, Nijmegen, Netherlands; Centre for Disability and Development Research (CeDDR) and School of Behavioural and Health Science, Australian Catholic University, Melbourne, VIC, Australia","Rogers, J.M., University of Sydney, Faculty of Health Sciences, Sydney, NSW, Australia; Duckworth, J., School of Design, RMIT, Melbourne, VIC, Australia; Middleton, S., Nursing Research Institute, St Vincent's Health Australia and Australian Catholic University, Sydney, NSW, Australia; Steenbergen, B., Behavioural Science Institute, Radboud University, Nijmegen, Netherlands; Wilson, P.H., Centre for Disability and Development Research (CeDDR) and School of Behavioural and Health Science, Australian Catholic University, Melbourne, VIC, Australia","Background: Virtual reality technologies show potential as effective rehabilitation tools following neuro-trauma. In particular, the Elements system, involving customized surface computing and tangible interfaces, produces strong treatment effects for upper-limb and cognitive function following traumatic brain injury. The present study evaluated the efficacy of Elements as a virtual rehabilitation approach for stroke survivors. Methods: Twenty-one adults (42-94 years old) with sub-acute stroke were randomized to four weeks of Elements virtual rehabilitation (three weekly 30-40 min sessions) combined with treatment as usual (conventional occupational and physiotherapy) or to treatment as usual alone. Upper-limb skill (Box and Blocks Test), cognition (Montreal Cognitive Assessment and selected CogState subtests), and everyday participation (Neurobehavioral Functioning Inventory) were examined before and after inpatient training, and one-month later. Results: Effect sizes for the experimental group (d = 1.05-2.51) were larger compared with controls (d = 0.11-0.86), with Elements training showing statistically greater improvements in motor function of the most affected hand (p = 0.008), and general intellectual status and executive function (p ≤ 0.001). Proportional recovery was two- to three-fold greater than control participants, with superior transfer to everyday motor, cognitive, and communication behaviors. All gains were maintained at follow-up. Conclusion: A course of Elements virtual rehabilitation using goal-directed and exploratory upper-limb movement tasks facilitates both motor and cognitive recovery after stroke. The magnitude of training effects, maintenance of gains at follow-up, and generalization to daily activities provide compelling preliminary evidence of the power of virtual rehabilitation when applied in a targeted and principled manner. Trial registration: this pilot study was not registered. © 2019 The Author(s).","Cognition; Motor activity; Rehabilitation; Stroke; Upper extremity; Virtual reality","adult; aged; Article; Box and Blocks Test; cerebrovascular accident; clinical article; clinical effectiveness; cognition; CogState subtest; controlled study; effect size; Elements virtual rehabilitation; executive function; female; follow up; human; male; Montreal cognitive assessment; motor performance; Neurobehavioral Functioning Inventory; neurologic examination; occupational therapy; outcome assessment; physiotherapy; pilot study; priority journal; randomized controlled trial (topic); telerehabilitation; very elderly; cerebrovascular accident; convalescence; devices; middle aged; motor activity; pathophysiology; physiology; procedures; randomized controlled trial; stroke rehabilitation; virtual reality; Adult; Aged; Aged, 80 and over; Cognition; Female; Humans; Male; Middle Aged; Motor Activity; Physical Therapy Modalities; Pilot Projects; Recovery of Function; Stroke; Stroke Rehabilitation; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85066015714
"Cao Y., Li F., Huo K., Xu Z., Zhong W., Ramani K.","57200501189;57209308650;56406175800;57200502760;57209310415;7006683312;","V.Ra: An in-situ visual authoring system for robot-Iot task planning with augmented reality",2019,"Conference on Human Factors in Computing Systems - Proceedings",,, 3312797,"","",,1,"10.1145/3290607.3312797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067284273&doi=10.1145%2f3290607.3312797&partnerID=40&md5=1521699046c22f779386df9257e301f2","Purdue University, West Lafayette, IN, United States; Tsinghua University, Beijing, China","Cao, Y., Purdue University, West Lafayette, IN, United States; Li, F., Tsinghua University, Beijing, China; Huo, K., Purdue University, West Lafayette, IN, United States; Xu, Z., Purdue University, West Lafayette, IN, United States; Zhong, W., Purdue University, West Lafayette, IN, United States; Ramani, K., Purdue University, West Lafayette, IN, United States","We present V.Ra, a visual and spatial programming system for robot-IoT task authoring. In V.Ra, programmable mobile robots serve as binding agents to link the stationary IoTs and perform collaborative tasks. We establish an ecosystem that coherently connects the three key elements of robot task planning (human-robot-IoT) with one single AR-SLAM device. Users can perform task authoring in an analogous manner with the Augmented Reality (AR) interface. Then placing the device onto the mobile robot directly transfers the task plan in a what-you-do-is-what-robot-does (WYDWRD) manner. The mobile device mediates the interactions between the user, robot and IoT oriented tasks, and guides the path planning execution with the SLAM capability. © 2019 Copyright held by the owner/author(s).","Augmented-Reality; Internet-of-Robotic-thing; Path planning; Robotic task authoring; SLAM","Augmented reality; Human computer interaction; Human engineering; Internet of things; Mobile agents; Mobile robots; Motion planning; Robotics; Authoring systems; Binding agent; Collaborative tasks; Human robots; Robotic tasks; SLAM; Spatial programming; Task planning; Robot programming",Conference Paper,"Final","",Scopus,2-s2.0-85067284273
"Lomov I., Makarov I.","57211431110;57203060623;","Generative Models for Fashion Industry using Deep Neural Networks",2019,"2nd International Conference on Computer Applications and Information Security, ICCAIS 2019",,, 8769486,"","",,,"10.1109/CAIS.2019.8769486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073897440&doi=10.1109%2fCAIS.2019.8769486&partnerID=40&md5=b6ad6ca1cb156ba94bb2e5db3dd2b4fc","School of Data Analysis and Artificial Intelligence, National Research University, Higher School of Economics, 3 Kochnovskiy Proezd, Moscow, 125319, Russian Federation","Lomov, I., School of Data Analysis and Artificial Intelligence, National Research University, Higher School of Economics, 3 Kochnovskiy Proezd, Moscow, 125319, Russian Federation; Makarov, I., School of Data Analysis and Artificial Intelligence, National Research University, Higher School of Economics, 3 Kochnovskiy Proezd, Moscow, 125319, Russian Federation","The progress of deep learning models in image and video processing leads to new artificial intelligence applications in Fashion industry. We consider the application of Generative Adversarial Networks and Neural Style Transfer for Digital Fashion presented as Virtual fashion for trying new clothes. Our model generate humans in clothes with respect to different fashion preferences, color layouts and fashion style. We propose that the virtual fashion industry will be highly impacted by accuracy of generating personalized human model taking into account different aspects of product and human preferences. We compare our model with state-of-art VITON model and show that using new perceptual loss in deep neural network architecture lead to better qualitative results in generating humans in clothes. © 2019 IEEE.","Artificial Intelligence for Fashion; Computational Design; Design Methodology; Generative Adversarial Networks; Human Model Generation; Image Generation; Neural Style Transfer","Artificial intelligence; Arts computing; Network architecture; Security of data; Video signal processing; Virtual reality; Adversarial networks; Computational design; Design Methodology; Human Model; Image generations; Neural Style Transfer; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85073897440
"Logishetty K., Western L., Morgan R., Iranpour F., Cobb J.P., Auvinet E.","35574197100;57208528368;57204792267;23970806500;7202728836;26321326600;","Can an Augmented Reality Headset Improve Accuracy of Acetabular Cup Orientation in Simulated THA? A Randomized Trial",2019,"Clinical Orthopaedics and Related Research","477","5",,"1190","1199",,15,"10.1097/CORR.0000000000000542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062878746&doi=10.1097%2fCORR.0000000000000542&partnerID=40&md5=fadc07e7897ed87f5c6706792f1f66f2","MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom","Logishetty, K., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom; Western, L., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom; Morgan, R., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom; Iranpour, F., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom; Cobb, J.P., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom; Auvinet, E., MSk Lab, Imperial College London, Fulham Palace Road, London, W6 8RF, United Kingdom","Background: Accurate implant orientation reduces wear and increases stability in arthroplasty but is a technically demanding skill. Augmented reality (AR) headsets overlay digital information on top of the real world. We have developed an enhanced AR headset capable of tracking bony anatomy in relation to an implant, but it has not yet been assessed for its suitability as a training tool for implant orientation. Questions/purposes: (1) In the setting of simulated THA performed by novices, does an AR headset improve the accuracy of acetabular component positioning compared with hands-on training by an expert surgeon? (2) What are trainees' perceptions of the AR headset in terms of realism of the task, acceptability of the technology, and its potential role for surgical training? Methods: Twenty-four study participants (medical students in their final year of school, who were applying to surgery residency programs, and who had no prior arthroplasty experience) participated in a randomized simulation trial using an AR headset and a simulated THA. Participants were randomized to two groups completing four once-weekly sessions of baseline assessment, training, and reassessment. One group trained using AR (with live holographic orientation feedback) and the other received one-on-one training from a hip arthroplasty surgeon. Demographics and baseline performance in orienting an acetabular implant to six patient-specific values on the phantom pelvis were collected before training and were comparable. The orientation error in degrees between the planned and achieved orientations was measured and was not different between groups with the numbers available (surgeon group mean error ± SD 16° ± 7° versus AR 14° ± 7°; p = 0.22). Participants trained by AR also completed a validated posttraining questionnaire evaluating their experiences. Results: During the four training sessions, participants using AR-guidance had smaller mean (± SD) errors in orientation than those receiving guidance from the surgeon: 1° ± 1° versus AR 6° ± 4°, p < 0.001. In the fourth session's assessment, participants in both groups had improved (surgeon group mean improvement 6°, 95% CI, 4-8°; p < 0.001 versus AR group 9°, 95% CI 7-10°; p < 0.001). There was no difference between participants in the surgeon-trained and AR-trained group: mean difference 1.2°, 95% CI, -1.8 to 4.2°; p = 0.281. In posttraining evaluation, 11 of 12 participants would use the AR platform as a training tool for developing visuospatial skills and 10 of 12 for procedure-specific rehearsals. Most participants (11 of 12) stated that a combination of an expert trainer for learning and AR for unsupervised training would be preferred. Conclusions: A novel head-mounted AR platform tracked an implant in relation to bony anatomy to a clinically relevant level of accuracy during simulated THA. Learners were equally accurate, whether trained by AR or a surgeon. The platform enabled the use of real instruments and gave live feedback; AR was thus considered a feasible and valuable training tool as an adjunct to expert guidance in the operating room. Although there were no differences in accuracy between the groups trained using AR and those trained by an expert surgeon, we believe the tool may be useful in education because it demonstrates that some motor skills for arthroplasty may be learned in an unsupervised setting. Future studies will evaluate AR-training for arthroplasty skills other than cup orientation and its transfer validity to real surgery. Level of Evidence: Level I, therapeutic study. Copyright © 2019 by the Association of Bone and Joint Surgeons.",,"accuracy; acetabulum; adult; anatomic landmark; Article; controlled study; decision making; experience; female; hip arthroplasty; human; male; medical student; motor performance; normal human; orientation; pelvic tilt; priority journal; simulation; surgical training; young adult",Article,"Final","",Scopus,2-s2.0-85062878746
"Friedman J., Raveh E., Weiss T., Itkin S., Niv D., Hani M., Portnoy S.","9337239400;56702458500;57209358221;57209358642;57209357594;57209357579;15519623300;","Applying incongruent visual-tactile stimuli during object transfer with vibro-tactile feedback",2019,"Journal of Visualized Experiments","2019","147", e59493,"","",,,"10.3791/59493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067541999&doi=10.3791%2f59493&partnerID=40&md5=b7416c4039ad420115731164ba84ef6a","Department of Physical Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel","Friedman, J., Department of Physical Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Raveh, E., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Weiss, T., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Itkin, S., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Niv, D., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Hani, M., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel; Portnoy, S., Department of Occupational Therapy, Sackler Faculty of Medicine, Tel Aviv University, Israel","The application of incongruent sensory signals that involves disrupted tactile feedback is rarely explored, specifically with the presence of vibrotactile feedback (VTF). This protocol aims to test the effect of VTF on the response to incongruent visual-tactile stimuli. The tactile feedback is acquired by grasping a block and moving it across a partition. The visual feedback is a real-time virtual presentation of the moving block, acquired using a motion capture system. The congruent feedback is the reliable presentation of the movement of the block, so that the subject feels that the block is grasped and see it move along with the path of the hand. The incongruent feedback appears as the movement of the block diverts from the actual movement path, so that it seems to drop from the hand when it is actually still held by the subject, thereby contradicting the tactile feedback. Twenty subjects (age 30.2 ± 16.3) repeated 16 block transfers, while their hand was hidden. These were repeated with VTF and without VTF (total of 32 block transfers). Incongruent stimuli were presented randomly twice within the 16 repetitions in each condition (with and without VTF). Each subject was asked to rate the difficulty level of performing the task with and without the VTF. There were no statistically significant differences in the length of the hand paths and durations between transfers recorded with congruent and incongruent visual-tactile signals - with and without the VTF. The perceived difficulty level of performing the task with the VTF significantly correlated with the normalized path length of the block with VTF (r = 0.675, p = 0.002). This setup is used to quantify the additive or reductive value of VTF during motor function that involves incongruent visual-tactile stimuli. Possible applications are prosthetics design, smart sport-wear, or any other garments that incorporate VTF. © 2019 Journal of Visualized Experiments.","Behavior; Feedback; Hand-eye coordination; Issue 147; Motion capture; Perception; Virtual manipulation; Virtual reality","adult; female; human; male; physiology; psychomotor performance; sensory feedback; touch; Adult; Feedback, Sensory; Female; Humans; Male; Psychomotor Performance; Touch",Article,"Final","",Scopus,2-s2.0-85067541999
"Emmanuel G., Hungilo G.G., Rahardjo Emanuel A.W.","57210203813;57210205674;7005767682;","A mobile application system for community health workers - A review",2019,"ACM International Conference Proceeding Series",,,,"106","110",,1,"10.1145/3330482.3330485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071072720&doi=10.1145%2f3330482.3330485&partnerID=40&md5=2f199d651423e3d66da1ef95f3ca0065","Magister Teknik Informatika, Universitas Atma Jaya Yogyakarta, Indonesia","Emmanuel, G., Magister Teknik Informatika, Universitas Atma Jaya Yogyakarta, Indonesia; Hungilo, G.G., Magister Teknik Informatika, Universitas Atma Jaya Yogyakarta, Indonesia; Rahardjo Emanuel, A.W., Magister Teknik Informatika, Universitas Atma Jaya Yogyakarta, Indonesia","Community Health workers (CHWs) are the foundation of public health services aimed to connect the gap between communities, health and social service system, and it is done by navigating the health and human services system and educating communities on disease prevention. Unfortunately, the way of sharing and accessing information for delivering the services is often very unreliable by using manual system for reporting which can cause error and falsification. Furthermore, the Staff which performs these duties often they do not have knowledge about disease and health system training or education. To address this need, a mobile application System for CHWs is needed, which enables community health workers to automatically send a report of monthly activities without using any manual input form. Making use of the digital device (the smartphone, PDAs, and The Augmented Reality Personal Digital Assistant .The mobile application will automatically allow submit a report, transfer knowledge, sharing information and receiving training by using the user interface which will have the features like social media. Also the electronic file for entering information will be filled automatically. The system will be recording and uploaded to a central server for use by CHWs supervisor and the health manager official. This article provides ICTs with a regard to Mobile Health System and the probable of field which are lacking. Its absence is root of challenges faced by CHWs, the solutions to challenges is to design technological (Mobile Health System) which create durable, imperishable answers for tending to the world's wellbeing need. © 2019 Association for Computing Machinery.","Community health workers; Mobile application system; Social","Artificial intelligence; Augmented reality; Digital devices; Information dissemination; mHealth; Mobile computing; Personal digital assistants; Personnel training; Public health; User interfaces; Community Health Workers; Disease prevention; Health and human services; Mobile applications; Mobile health systems; Public health services; Sharing information; Social; Electronic document exchange",Conference Paper,"Final","",Scopus,2-s2.0-85071072720
"Weng P.-L., Bouck E.C.","55561026500;21734344700;","Comparing the effectiveness of two app-based number lines to teach price comparison to students with autism spectrum disorders",2019,"Disability and Rehabilitation: Assistive Technology","14","3",,"281","291",,1,"10.1080/17483107.2018.1430869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041340636&doi=10.1080%2f17483107.2018.1430869&partnerID=40&md5=7ad148cb714cfbb35be132bd1760c1a0","Department of Special Education and Professional Counseling, William Paterson University of New Jersey, Wayne, NJ, United States; Department of Counseling, Educational Psychology and Special Education, Michigan State University, East Lansing, MI, United States","Weng, P.-L., Department of Special Education and Professional Counseling, William Paterson University of New Jersey, Wayne, NJ, United States; Bouck, E.C., Department of Counseling, Educational Psychology and Special Education, Michigan State University, East Lansing, MI, United States","Purpose: A number line consisting of Arabic numerals is a commonly used instructional tool for teaching price comparison. However, typical number lines lack concrete visual cues, which may benefit students with autism spectrum disorders (ASD) who have not yet mastered the representation of Arabic numerals. Method: This study investigated the effects of additional visual cues (i.e., dots) by comparing two types of app-based number line conditions: number lines with and without dots. A single-subject, alternating treatment design study was employed across five secondary students with ASD. Results: Both number line conditions were effective for four of the students in assisting them to select cheaper items and complete task analysis steps. The number line with dots was effective or slightly more effective in selecting smaller numbers for three of the students. Conclusions: The findings of this study support the literature on the use of number lines as an effective tool to assist students in price comparison. The benefits of adding concrete visual cues and other teaching strategies (e.g., the holistic and decomposition models) were discussed.Implications for Rehabilitation This study investigated the effectiveness of concrete visual cues, such as dots, on a number line app for teaching students with ASD who had not yet developed the association of quantities with the numerals. We found that incorporation of a hybrid number comparison model–first holistic (for whole numbers) and then decomposition (for numbers after the decimal point)–is effective when teaching students how to compare prices with an uneven number of digits. This study provides an alternative for special education teachers to schedule practice, such as the use of simulated settings to achieve mastery, then transitioning to community-based settings to test skill generalization. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","ASD; iPAD; Number comparison; number line app; price comparison","adolescent; autism; child; comparative study; female; human; male; mathematics; mobile application; psychology; student; teaching; Adolescent; Autism Spectrum Disorder; Child; Female; Humans; Male; Mathematics; Mobile Applications; Students; Teaching Materials",Article,"Final","",Scopus,2-s2.0-85041340636
"Makransky G., Terkildsen T.S., Mayer R.E.","50361371800;57205338475;7403065717;","Adding immersive virtual reality to a science lab simulation causes more presence but less learning",2019,"Learning and Instruction","60",,,"225","236",,159,"10.1016/j.learninstruc.2017.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039065813&doi=10.1016%2fj.learninstruc.2017.12.007&partnerID=40&md5=bec8f76c5a5c407cea29ee47829f155c","Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Psychological and Brain Sciences, University of California Santa BarbaraCA, United States","Makransky, G., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Terkildsen, T.S., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Mayer, R.E., Psychological and Brain Sciences, University of California Santa BarbaraCA, United States","Virtual reality (VR) is predicted to create a paradigm shift in education and training, but there is little empirical evidence of its educational value. The main objectives of this study were to determine the consequences of adding immersive VR to virtual learning simulations, and to investigate whether the principles of multimedia learning generalize to immersive VR. Furthermore, electroencephalogram (EEG) was used to obtain a direct measure of cognitive processing during learning. A sample of 52 university students participated in a 2 × 2 experimental cross-panel design wherein students learned from a science simulation via a desktop display (PC) or a head-mounted display (VR); and the simulations contained on-screen text or on-screen text with narration. Across both text versions, students reported being more present in the VR condition (d = 1.30); but they learned less (d = 0.80), and had significantly higher cognitive load based on the EEG measure (d = 0.59). In spite of its motivating properties (as reflected in presence ratings), learning science in VR may overload and distract the learner (as reflected in EEG measures of cognitive load), resulting in less opportunity to build learning outcomes (as reflected in poorer learning outcome test performance). © 2017 Elsevier Ltd","Cognitive load; EEG; Presence; Redundancy principle; Simulation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85039065813
"Farra S., Hodgson E., Miller E.T., Timm N., Brady W., Gneuhs M., Ying J., Hausfeld J., Cosgrove E., Simon A., Bottomley M.","54921506600;14053915200;7404491434;16311140300;57189643415;54882273500;57213118440;25947532900;57189640346;57194346781;57190347218;","Effects of Virtual Reality Simulation on Worker Emergency Evacuation of Neonates",2019,"Disaster Medicine and Public Health Preparedness","13","2",,"301","308",,5,"10.1017/dmp.2018.58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055001091&doi=10.1017%2fdmp.2018.58&partnerID=40&md5=9e0b02a8fa6048c76fb01b477e2eadde","Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH, United States; Miami University, Miami, FL, United States; University of Cincinnati, Cincinnati, OH, United States; Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States","Farra, S., Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH, United States; Hodgson, E., Miami University, Miami, FL, United States; Miller, E.T., University of Cincinnati, Cincinnati, OH, United States; Timm, N., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Brady, W., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Gneuhs, M., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Ying, J., University of Cincinnati, Cincinnati, OH, United States; Hausfeld, J., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Cosgrove, E., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Simon, A., Cincinnati Children's Hospital and Medical Center, Cincinnati, OH, United States; Bottomley, M., Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH, United States","Objective: This study examined differences in learning outcomes among newborn intensive care unit (NICU) workers who underwent virtual reality simulation (VRS) emergency evacuation training versus those who received web-based clinical updates (CU). Learning outcomes included a) knowledge gained, b) confidence with evacuation, and c) performance in a live evacuation exercise. Methods: A longitudinal, mixed-method, quasi-experimental design was implemented utilizing a sample of NICU workers randomly assigned to VRS training or CUs. Four VRS scenarios were created that augmented neonate evacuation training materials. Learning was measured using cognitive assessments, self-efficacy questionnaire (baseline, 0, 4, 8, 12 months), and performance in a live drill (baseline, 12 months). Data were collected following training and analyzed using mixed model analysis. Focus groups captured VRS participant experiences. Results: The VRS and CU groups did not statistically differ based upon the scores on the Cognitive Assessment or perceived self-efficacy. The virtual reality group performance in the live exercise was statistically (P<.0001) and clinically (effect size of 1.71) better than that of the CU group. Conclusions: Training using VRS is effective in promoting positive performance outcomes and should be included as a method for disaster training. VRS can allow an organization to train, test, and identify gaps in current emergency operation plans. In the unique case of disasters, which are low-volume and high-risk events, the participant can have access to an environment without endangering themselves or clients. © Copyright 2018 Society for Disaster Medicine and Public Health, Inc.","disaster; evacuation; virtual reality","article; controlled study; disaster; effect size; emergency surgery; exercise; female; human; human experiment; learning; male; newborn; questionnaire; self concept; simulation; virtual reality; worker; adult; computer simulation; disaster medicine; education; longitudinal study; neonatal intensive care unit; organization and management; patient transport; procedures; Adult; Computer Simulation; Disaster Medicine; Female; Humans; Infant, Newborn; Intensive Care Units, Neonatal; Longitudinal Studies; Male; Patient Transfer; Surveys and Questionnaires; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85055001091
"Stroe I.P., Ciupe A., Meza S.N., Orza B.","57209343958;57105700600;24829838400;24503681900;","FireEscape: A gamified coordinative aproach to multiplayer fire-safety training",2019,"IEEE Global Engineering Education Conference, EDUCON","April-2019",, 8725148,"1316","1323",,,"10.1109/EDUCON.2019.8725148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067504866&doi=10.1109%2fEDUCON.2019.8725148&partnerID=40&md5=5f665dbff98f0a0805983f4572211f3d","Multimedia Systems and Applications Laboratory, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","Stroe, I.P., Multimedia Systems and Applications Laboratory, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Ciupe, A., Multimedia Systems and Applications Laboratory, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Meza, S.N., Multimedia Systems and Applications Laboratory, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Orza, B., Multimedia Systems and Applications Laboratory, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","Emergency trainings simulated in alternative environments are fundamentally dependent on user engagement and may be addressed as crucial validation scenarios for proposing new approaches to learning designs. Developing an interactive engaging experience while building a know-how becomes a challenging demand to be addressed in current training setups. A fire-safety training has been recreated as a gamified learning experience, where coordinative multiplayer gameplay elements have been integrated in 3D fire emergency simulation. 3C implications (collaboration, cooperation, coordination) have been analyzed for modeling multiuser navigation and a coordinative approach has been selected for accomplishing the goal-oriented mission of successful evacuation in a fire-based emergency scenario. Game-based analytics have been collected and analyzed in an experimental pre-test, addressed to 1st year students enrolled in the Faculty of Electronics, Telecommunications and Information Technology of the Technical University of Cluj-Napoca, Romania, where out of a total number of 196 students, 22 participants actively ran the simulation as either belonging to the control or experimental group. © 2019 IEEE.","Coordinative interaction; Gameplay analytics; Gamified simulation; Learning design; Multiplayer setup","Air navigation; Engineering education; Human computer interaction; Students; Technology transfer; Coordinative interaction; Gameplay; Gamified simulation; Learning designs; Multiplayers; Fires",Conference Paper,"Final","",Scopus,2-s2.0-85067504866
"Sportillo D., Paljic A., Ojeda L.","56912310500;34873176200;57194572140;","On-Road Evaluation of Autonomous Driving Training",2019,"ACM/IEEE International Conference on Human-Robot Interaction","2019-March",, 8673277,"182","190",,9,"10.1109/HRI.2019.8673277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063979607&doi=10.1109%2fHRI.2019.8673277&partnerID=40&md5=bd03eba8222bcb01343450cc3fe89a9d","PSL Research University, Center for Robotics, MINES ParisTech Groupe PSA, Paris, France; Groupe PSA, Velizy-Villacoublay, France","Sportillo, D., PSL Research University, Center for Robotics, MINES ParisTech Groupe PSA, Paris, France; Paljic, A., PSL Research University, Center for Robotics, MINES ParisTech Groupe PSA, Paris, France; Ojeda, L., Groupe PSA, Velizy-Villacoublay, France","Driver interaction with increasingly automated vehicles requires prior knowledge of system capabilities, operational know-how to use novel car equipment and responsiveness to unpredictable situations. With the purpose of getting drivers ready for autonomous driving, in a between-subject study sixty inexperienced participants were trained with an on-board video tutorial, an Augmented Reality (AR) program and a Virtual Reality (VR) simulator. To evaluate the transfer of training to real driving scenarios, a test drive on public roads was conducted implementing, for the first time in these conditions, the Wizard of Oz (WoZ) protocol. Results suggest that VR and AR training can foster knowledge acquisition and improve reaction time performance in take-over requests. Moreover, participants' behavior during the test drive highlights the ecological validity of the experiment thanks to the effective implementation of the WoZ methodology. © 2019 IEEE.","Augmented Reality; Automated Vehicles; Human-Vehicle Interaction; TOR; Transfer of Training; Virtual Reality; Wizard of Oz","Augmented reality; Human reaction time; Man machine systems; Network security; Technology transfer; Vehicles; Virtual reality; Automated vehicles; Autonomous driving; Driver interaction; Ecological validity; Human vehicle interactions; System capabilities; Transfer of trainings; Wizard of Oz; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85063979607
"Levac D.E., Lu A.S.","25937323900;26036008500;","Does Narrative Feedback Enhance Children's Motor Learning in a Virtual Environment?",2019,"Journal of Motor Behavior","51","2",,"199","211",,2,"10.1080/00222895.2018.1454398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046081669&doi=10.1080%2f00222895.2018.1454398&partnerID=40&md5=22fda2e96473a32984bae8220d54a906","Department of Physical Therapy, Movement and Rehabilitation Sciences, Bouve College of Health Sciences, Northeastern University, Boston, MA, United States; Department of Communication Studies, College of Arts, Media and Design, Northeastern University, Boston, MA, United States; Department of Health Sciences, Bouvé College of Health Sciences, Northeastern University, Boston, MA, United States","Levac, D.E., Department of Physical Therapy, Movement and Rehabilitation Sciences, Bouve College of Health Sciences, Northeastern University, Boston, MA, United States; Lu, A.S., Department of Communication Studies, College of Arts, Media and Design, Northeastern University, Boston, MA, United States, Department of Health Sciences, Bouvé College of Health Sciences, Northeastern University, Boston, MA, United States","Augmented feedback has motivational and informational functions in motor learning, and is a key feature of practice in a virtual environment (VE). This study evaluated the impact of narrative (story-based) feedback as compared to standard feedback during practice of a novel task in a VE on typically developing children's motor learning, motivation and engagement. Thirty-eight children practiced navigating through a virtual path, receiving narrative or non-narrative feedback following each trial. All participants improved their performance on retention but not transfer, with no significant differences between groups. Self-reported engagement was associated with acquisition, retention and transfer for both groups. A narrative approach to feedback delivery did not offer an additive benefit; additional affective advantages of augmented feedback for motor learning in VEs should be explored. © 2018, Copyright © Taylor & Francis Group, LLC.","augmented feedback; children; engagement; motivation; motor learning; narratives","article; child; clinical article; controlled clinical trial; controlled study; female; human; male; motivation; motor learning; narrative; adolescent; attention; learning; motivation; motor performance; physiology; psychological feedback; virtual reality; Adolescent; Attention; Child; Feedback, Psychological; Female; Humans; Learning; Male; Motivation; Motor Skills; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85046081669
"Wu W., Tesei A., Ayer S., London J., Luo Y., Gunji V.","55707471100;57207983741;55358381000;38561546100;56139591000;57208004617;","Closing the Skills Gap: Construction and Engineering Education Using Mixed Reality - A Case Study",2019,"Proceedings - Frontiers in Education Conference, FIE","2018-October",, 8658992,"","",,3,"10.1109/FIE.2018.8658992","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063477812&doi=10.1109%2fFIE.2018.8658992&partnerID=40&md5=aea105a3521ecbc9134b176639868589","Dept. of Construction Mgmt, Cal. State University, Fresno Fresno, CA, United States; Del E. Webb School of Construction, Arizona State University, Tempe, AZ, United States; Ira A. Fulton Schools of Engineering, Polytechnic School, Mesa, AZ, United States","Wu, W., Dept. of Construction Mgmt, Cal. State University, Fresno Fresno, CA, United States; Tesei, A., Dept. of Construction Mgmt, Cal. State University, Fresno Fresno, CA, United States; Ayer, S., Del E. Webb School of Construction, Arizona State University, Tempe, AZ, United States; London, J., Ira A. Fulton Schools of Engineering, Polytechnic School, Mesa, AZ, United States; Luo, Y., Dept. of Construction Mgmt, Cal. State University, Fresno Fresno, CA, United States; Gunji, V., Dept. of Construction Mgmt, Cal. State University, Fresno Fresno, CA, United States","This research work-in-progress paper investigated the application of emerging mixed reality (MR) technology in construction and engineering education. The construction industry is facing a severe shortage of skilled workforce. As the baby boomers are retiring, the younger generation, especially college students, are often criticized for their lack of professional experience and career-specific competency. To close the skills gap and accelerate the transition of college students to competent workforce, this paper proposed a new genre of learning and professional training using MR. The main promise of the MR technology resides in its ability to augment virtual contents on top of the physical reality to facilitate tacit knowledge learning, and simulate learning activities that traditionally can only be obtained from actual professional experience. An undergraduate wood framing lab was designed as a case study to explore how students might perform in this new learning and training environment. Specifically, the case study investigated if MR would facilitate student design comprehension and transfer such understanding into the knowledge and skills needed to build the wood structure. A randomly selected student control group was given traditional paper-based construction drawings to perform the same tasks with other student groups with various visualization technology assistance. Project performance and behavior of student groups were compared to determine if there was a significant difference between the control group and the experiment groups. A pair of pre- and post-survey on MR-intervened learning experience was also conducted to explore student perceptions towards this new genre of learning and training. The research design proposed in this work-in-progress study and its preliminary results could be a good reference and foundation to future research in this arena. © 2018 IEEE.","Construction; Learning; Mixed reality; Skills; Workforce","Construction; Construction industry; Engineering education; Engineering research; Mixed reality; Professional aspects; Wooden buildings; Construction drawings; Learning; Learning and training; Mixed reality technologies; Professional experiences; Skills; Visualization technologies; Workforce; Students",Conference Paper,"Final","",Scopus,2-s2.0-85063477812
"Huynh B., Orlosky J., Hollerer T.","57192544046;55641218100;8358959700;","In-situ labeling for augmented reality language learning",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8798358,"1606","1611",,2,"10.1109/VR.2019.8798358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071876610&doi=10.1109%2fVR.2019.8798358&partnerID=40&md5=85211cef84f8fbe0b5cffddb75bffc83","University of California, Santa Barbara, United States; Osaka University, Japan","Huynh, B., University of California, Santa Barbara, United States; Orlosky, J., Osaka University, Japan; Hollerer, T., University of California, Santa Barbara, United States","Augmented Reality is a promising interaction paradigm for learning applications. It has the potential to improve learning outcomes by merging educational content with spatial cues and semantically relevant objects within a learner's everyday environment. The impact of such an interface could be comparable to the method of loci, a well known memory enhancement technique used by memory champions and polyglots. However, using Augmented Reality in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is a significant challenge, and interaction with arbitrary (unmodeled) physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a framework for in-situ object labeling and selection in Augmented Reality, with a particular focus on language learning applications. Our framework uses a generalized object recognition model to identify objects in the world in real time, integrates eye tracking to facilitate selection and interaction within the interface, and incorporates a personalized learning model that dynamically adapts to student's growth. We show our current progress in the development of this system, including preliminary tests and benchmarks. We explore challenges with using such a system in practice, and discuss our vision for the future of AR language learning applications. © 2019 IEEE.","Centered computing; Human; Mixed and augmented reality; Semi; Supervised learning; Theory and algorithms for application domains","Augmented reality; Computation theory; Eye tracking; Genetic algorithms; Object recognition; Supervised learning; User interfaces; Virtual reality; Centered computing; Educational contents; Human; Interaction paradigm; Memory enhancement; Mixed and augmented realities; Personalized learning; Semi; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85071876610
"Rojas-Munõz E., Andersen D., Cabrera M.E., Popescu V., Marley S., Zarzaur B., Mullis B., Wachs J.P.","57205650789;56661805900;56661822000;7103266698;56661934200;57207592471;55619834500;9241519000;","Augmented reality as a medium for improved telementoring",2019,"Military Medicine","184",,,"57","64",,1,"10.1093/milmed/usy300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063691710&doi=10.1093%2fmilmed%2fusy300&partnerID=40&md5=ac0d5869aef6bd34ae6bb2680b41ae7e","School of Industrial Engineering, Purdue University, 315N. Grant St., West Lafayette, IN, United States; Department of Computer Science, Purdue University, 305N. University St., West Lafayette, IN, United States; Indiana University School of Medicine, 340 West 10th St., Indianapolis, IN, United States","Rojas-Munõz, E., School of Industrial Engineering, Purdue University, 315N. Grant St., West Lafayette, IN, United States; Andersen, D., Department of Computer Science, Purdue University, 305N. University St., West Lafayette, IN, United States; Cabrera, M.E., School of Industrial Engineering, Purdue University, 315N. Grant St., West Lafayette, IN, United States; Popescu, V., Department of Computer Science, Purdue University, 305N. University St., West Lafayette, IN, United States; Marley, S., Indiana University School of Medicine, 340 West 10th St., Indianapolis, IN, United States; Zarzaur, B., Indiana University School of Medicine, 340 West 10th St., Indianapolis, IN, United States; Mullis, B., Indiana University School of Medicine, 340 West 10th St., Indianapolis, IN, United States; Wachs, J.P., School of Industrial Engineering, Purdue University, 315N. Grant St., West Lafayette, IN, United States","Combat trauma injuries require urgent and specialized care. When patient evacuation is infeasible, critical life-saving care must be given at the point of injury in real-time and under austere conditions associated to forward operating bases. Surgical telementoring allows local generalists to receive remote instruction from specialists thousands of miles away. However, current telementoring systems have limited annotation capabilities and lack of direct visualization of the future result of the surgical actions by the specialist. The System for Telementoring with Augmented Reality (STAR) is a surgical telementoring platform that improves the transfer of medical expertise by integrating a full-size interaction table for mentors to create graphical annotations, with augmented reality (AR) devices to display surgical annotations directly onto the generalist's field of view. Along with the explanation of the system's features, this paper provides results of user studies that validate STAR as a comprehensive AR surgical telementoring platform. In addition, potential future applications of STAR are discussed, which are desired features that state-of-the-art AR medical telementoring platforms should have when combat trauma scenarios are in the spotlight of such technologies. © 2019 Association of Military Surgeons of the United States. All rights reserved.","augmented reality; surgical telementoring; telemedicine","human; mentoring; procedures; standards; teaching; teleconsultation; trends; virtual reality; Humans; Mentoring; Remote Consultation; Teaching; Virtual Reality",Conference Paper,"Final","",Scopus,2-s2.0-85063691710
"Hurnmukainen O.S., Schlecht S.J., Robotham T., Plinge A., Habets E.A.P.","57210918036;55546914800;57190864032;47061909300;13608885900;","Perceptual study of near-field binaural audio rendering in six-degrees-of-freedom virtual reality",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8798177,"448","454",,,"10.1109/VR.2019.8798177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071839312&doi=10.1109%2fVR.2019.8798177&partnerID=40&md5=f77a479b511db471c45be8f0778503e7","International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany","Hurnmukainen, O.S., International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; Schlecht, S.J., International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; Robotham, T., International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; Plinge, A., International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany; Habets, E.A.P., International Audio Laboratories Erlangen, A Joint Institution of the Friedrich-Alexander-University Erlangen-Nürnberg (FAU), Fraunhofer Institute for Integrated Circuits (IIS), Germany","Auditory localization cues in the near-field ( 1.0\ \mathrm{m}) are significantly different than in the far-field. The near-field region is within an arm's length of the listener allowing to integrate proprioceptive cues to determine the location of an object in space. This perceptual study compares three non-individualized methods to apply head-related transfer functions (HRTFs) in six-degrees-of-freedom near-field audio rendering, namely, far-field measured HRTFs, multi-distance measured HRTFs, and spherical-model-based HRTFs with near-field extrapolation. To set our findings in context, we provide a real-world hand-held audio source for comparison along with a distance-invariant condition. Two modes of interaction are compared in an audio-visual virtual reality: One allowing the participant to move the audio object dynamically and the other with a stationary audio object but a freely moving listener. © 2019 IEEE.","Centered computing; Centered computing; Centered computing; Human; Human; Human; Human computer interaction (hci); Human computer interaction (hci); Human computer interaction (hci)hci design and evaluation methods; Interaction paradigms; Interaction paradigmsmixed / augmented reality; User studies; Virtual reality","Degrees of freedom (mechanics); Electric arc welding; Sound reproduction; Transfer functions; User interfaces; Virtual reality; Centered computing; HCI design; Human; Human computer interaction (HCI); Interaction paradigm; Interaction paradigmsmixed/augmented reality; User study; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85071839312
"Bialkova S., Dickhoff B.","36247103900;57210915943;","Encouraging rehabilitation trials: The potential of 360° immersive instruction videos",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8797805,"1443","1447",,,"10.1109/VR.2019.8797805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071832880&doi=10.1109%2fVR.2019.8797805&partnerID=40&md5=d7ce467e61a2a59195ec7197703c62de","Utrecht University, Netherlands; Saxion University of Applied Sciences, Netherlands","Bialkova, S., Utrecht University, Netherlands; Dickhoff, B., Saxion University of Applied Sciences, Netherlands","Despite the rapid growth of the VR/AR/ XR applications in the health-care sector, enhancing health and well-being with innovative technologies often is a challenge. Part of the challenge is the limited knowledge transfer between the healthcare, technology, the patients demands, and how these demands could be appropriately met. The current study addressed this challenge when exploring the potential of 360° immersive instruction videos in encouraging rehabilitation trials. A professional VR/ video maker studio created the video for the purpose of the current research. A rehabilitation therapist was recorded while performing rehabilitation exercise as it is done in the real life practice. Patients currently in various rehabilitation trials (motor vs. cardiovascular) were exposed to the 360° immersive instruction video. Their experience was compared with the control group, i.e. healthy people. The VR experience and the rehabilitation exercise experience were evaluated as measures of the Virtual Reality Rehabilitation (VRR) impact. Results showed that 360° immersive videos engaged patients well, irrespective of the rehabilitation trial they are currently in. Regression modelling further demonstrated that the more people liked the VR experience, the more they enjoyed the rehabilitation activity. The more the rehabilitation activity was enjoyed, the more people were satisfied with the VRR. Current outcomes are discussed in the framework of a model of VRR impact, which is a solid base for long-term exercise adherence. The model could be implemented to successfully develop immersive instructional videos as efficient tools in the course of physical rehabilitation trials, and thus, to enhance health and well-being. © 2019 IEEE.","360 videos; Patients engagement; Virtual reality rehabilitation","Health care; Knowledge management; User interfaces; Virtual reality; 360 videos; Innovative technology; Instructional videos; Patients engagement; Physical rehabilitation; Regression modelling; Rehabilitation activities; Rehabilitation exercise; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-85071832880
"Danieau F., Gubins I., Olivier N., Dumas O., Denis B., Lopez T., Mollet N., Frager B., Avril Q.","37057062300;57210915853;57210919806;57163193100;57210918555;7103065998;24481532200;57210914421;38662147000;","Automatic generation and stylization of 3d facial rigs",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8798208,"784","792",,1,"10.1109/VR.2019.8798208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071882939&doi=10.1109%2fVR.2019.8798208&partnerID=40&md5=f362bc829724f056f31ddb2b21e43cad","Technicolor, France; Utrecht University, Netherlands; ESIR, France; Technicolor Experience Center, United States","Danieau, F., Technicolor, France; Gubins, I., Utrecht University, Netherlands; Olivier, N., ESIR, France; Dumas, O., Technicolor, France; Denis, B., Technicolor, France; Lopez, T., Technicolor, France; Mollet, N., Technicolor, France; Frager, B., Technicolor Experience Center, United States; Avril, Q., Technicolor, France","In this paper, we present a fully automatic pipeline for generating and stylizing high geometric and textural quality facial rigs. They are automatically rigged with facial blendshapes for animation, and can be used across platforms for applications including virtual reality, augmented reality, remote collaboration, gaming and more. From a set of input facial photos, our approach is to be able to create a photorealistic, fully rigged character in less than seven minutes. The facial mesh reconstruction is based on state-of-The art photogrammetry approaches. Automatic landmarking coupled with ICP registration with regularization provide direct correspondence and registration from a given generic mesh to the acquired facial mesh. Then, using deformation transfer, existing blendshapes are transferred from the generic to the reconstructed facial mesh. The reconstructed face is then fit to the full body generic mesh. Extra geometry such as jaws, teeth and nostrils are retargeted and transferred to the character. An automatic iris color extraction algorithm is performed to colorize a separate eye texture, animated with dynamic UVs. Finally, an extra step applies a style to the photorealis-tic face to enable blending of personalized facial features into any other character. The user's face can then be adapted to any human or non-human generic mesh. A pilot user study was performed to evaluate the utility of our approach. Up to 65% of the participants were successfully able to discern the presence of one's unique facial features when the style was not too far from a humanoid shape. © 2019 IEEE.","Animation; Character; Pipeline; Virtual reality","Animation; Augmented reality; Blending; Mesh generation; Pipelines; Textures; Virtual reality; Automatic Generation; Automatic landmarking; Character; Color extraction; Deformation transfer; Mesh reconstruction; Remote collaboration; Textural quality; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85071882939
"Voong T.M., Oehler M.","57205547907;57195979767;","Auditory spatial perception using bone conduction headphones along with fitted head related transfer functions",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,, 8798218,"1211","1212",,4,"10.1109/VR.2019.8798218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071835642&doi=10.1109%2fVR.2019.8798218&partnerID=40&md5=eb449f8b26f4ed7c2691a7f6797b017e","Osnabrück University, Germany","Voong, T.M., Osnabrück University, Germany; Oehler, M., Osnabrück University, Germany","An approach is presented how to practically determine which head-related transfer function (HRTF) profiles fit best for individuals wearing a bone conduction headphone. Such headphones may be particularly useful for visually impaired people (e.g., for navigation applications) as they do not obstruct the outer ear. Hence, it is still possible to perceive environmental sounds without restraints while wearing such headphones. For a fast and user-friendly identification of fitting HRTF profiles, an adapted tournament system is proposed. It could be shown that the results of the tournament method, where participants had to rate overall preference, externalization and envelopment, correlated well with the results of the localization task. The correlation was higher for the conventional headphones condition than for the bone conduction headphones condition. Analyses of the transmission characteristics show an uneven frequency response of bone conduction headphones compared to conventional headphones or speakers. In future research it will be investigated whether these findings are relevant for the auditory spatial perception at all and to what extent best fitting HRTFs may compensate for these phenomena. © 2019 IEEE.","Bone; Conductionheadphone; Functions; Head; Related; Tournament methods; Transfer","Bone; Frequency response; Functions; Headphones; Loudspeakers; User interfaces; Virtual reality; Wear of materials; Conductionheadphone; Head; Related; Tournament method; Transfer; Transfer functions",Conference Paper,"Final","",Scopus,2-s2.0-85071835642
"Wälti M.J., Woolley D.G., Wenderoth N.","57208053401;11839714300;57206162402;","Reinstating verbal memories with virtual contexts: Myth or reality?",2019,"PLoS ONE","14","3", e0214540,"","",,4,"10.1371/journal.pone.0214540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063680448&doi=10.1371%2fjournal.pone.0214540&partnerID=40&md5=cc683e6cb6fa67ee6cb942a63665b850","Department of Health Sciences and Technology, ETH Zurich, Zurich, Switzerland; Neuroscience Center Zurich (ZNZ), University and ETH Zurich, Zurich, Switzerland","Wälti, M.J., Department of Health Sciences and Technology, ETH Zurich, Zurich, Switzerland, Neuroscience Center Zurich (ZNZ), University and ETH Zurich, Zurich, Switzerland; Woolley, D.G., Department of Health Sciences and Technology, ETH Zurich, Zurich, Switzerland; Wenderoth, N., Department of Health Sciences and Technology, ETH Zurich, Zurich, Switzerland, Neuroscience Center Zurich (ZNZ), University and ETH Zurich, Zurich, Switzerland","When learning new information, contextual information about the encoding situation is stored in addition to the focal memory content. Later, these strings of extra information can help retrieve the learned content as demonstrated by experiments where contextual cues from an encoding situation facilitate remembering and improve memory performance when reinstated during retrieval. This context-dependent memory effect has been investigated over the course of several decades and has been demonstrated with many different types of contexts. Based on this, the widely held belief is that context-dependent memory is a strong and robust effect, with transferable substance for everyday learning and potential clinical applications. Here we report the results of a multi-study design investigating the influence of reinstated visual contexts on memory performance. Data from 120 participants were included in three studies comprising a variety of visual cues. We show convincingly that even rich, salient and fully surrounding visual contexts provided by virtual reality are not sufficient to induce effects of context-dependency in a free recall memory task. We also investigated contextual modulation of oscillatory brain activity in order to test the effect of reinstated neural contexts, which failed to evoke a robust effect when re-tested in an internal conceptual replication study. Moreover, a Bayesian sequential statistical analysis revealed moderate to strong evidence against the hypothesis that reinstatement of visual contexts benefits free recall memory tasks indicating that effects are small and may not be suitable for transfer into everyday learning. © 2019 Wälti et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; article; context-dependent memory; female; human; human experiment; learning; major clinical study; male; modulation; replication study; statistical analysis; verbal memory; virtual reality; association; brain; physiology; recall; vision; young adult; Brain; Cues; Female; Humans; Male; Mental Recall; Visual Perception; Young Adult",Article,"Final","",Scopus,2-s2.0-85063680448
"Lee Moh I.S., Mustafa Zaidi S.F.","57210390684;57210390903;","Game Cinematography - Towards Understanding Relationship between Spatial Distortion and Game-play",2019,"ACM International Conference Proceeding Series",,,,"26","32",,1,"10.1145/3332305.3332313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070536532&doi=10.1145%2f3332305.3332313&partnerID=40&md5=e4d66e3f57da96f7c15678c165d3ad24","Media Design School, New Zealand","Lee Moh, I.S., Media Design School, New Zealand; Mustafa Zaidi, S.F., Media Design School, New Zealand","Although work is in progress to get a more realistic 3D simulation of a real environment, digital cinematography in games has so far delivered what it has promised in terms of high fidelity and realistic experience for the users. Therefore, with the available technology, we have investigated how video game cinematography could further be integrated into game design through the use of realistic camera lens distortion? To answer this question, our research develops a test bed using a camera model that simulates spatial distortion generalizing real-world lens behaviour. We implemented this camera in a test bed that simulates three use cases in games. Through this test bed, we tested our simulations with a survey sample of 21 people and made observations of how it impacted the game-play experience. Through these observations, we measured two metrics that comprise the game- play experience - agency and experience. From our observations and discussion, we demonstrate the use cases where realistic lens simulation and spatial distortion in game-cinematography could improve the game-play experience. Our research shows that further research should be made in exploring how game play experiences are shaped by the implementation of cinematographic techniques in a interactive game setting. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Game Camera Design; Lens Distortion; Player Experience; Video Game Cinematic; Virtual Cinematography","Augmented reality; Cameras; Equipment testing; Software design; Camera design; Lens distortion; Player experience; Video game; Virtual cinematography; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85070536532
"Scaffidi M.A., Khan R., Walsh C.M., Pearl M., Winger K., Kalaichandran R., Lin P., Grover S.C.","56652134800;55760572400;14032361500;57207039190;57207048731;57202132789;57207032326;37080920800;","Protocol for a randomised trial evaluating the effect of applying gamification to simulation-based endoscopy training",2019,"BMJ Open","9","2", e024134,"","",,1,"10.1136/bmjopen-2018-024134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062174277&doi=10.1136%2fbmjopen-2018-024134&partnerID=40&md5=f82582a69c0c87a9800bc19bbdaae671","Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Division of Gastroenterology, Hepatology, and Nutrition, Learning Institute, and Research Institute, Hospital for Sick Children, Toronto, ON, Canada; Wilson Centre, University of Toronto, Toronto, Canada; Li Ka Shing Knowledge Institute, St. Michael's Hospital, Toronto, ON, Canada","Scaffidi, M.A., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Khan, R., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Walsh, C.M., Division of Gastroenterology, Hepatology, and Nutrition, Learning Institute, and Research Institute, Hospital for Sick Children, Toronto, ON, Canada, Wilson Centre, University of Toronto, Toronto, Canada; Pearl, M., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Winger, K., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Kalaichandran, R., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Lin, P., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada; Grover, S.C., Division of Gastroenterology, St. Michael's Hospital, Toronto, Canada, Li Ka Shing Knowledge Institute, St. Michael's Hospital, Toronto, ON, Canada","Background Simulation-based training (SBT) provides a safe environment and effective means to enhance skills development. Simulation-based curricula have been developed for a number of procedures, including gastrointestinal endoscopy. Gamification, which is the application of game-design principles to non-game contexts, is an instructional strategy with potential to enhance learning. No studies have investigated the effects of a comprehensive gamification curriculum on the acquisition of endoscopic skills among novice endoscopists. Methods and analysis Thirty-six novice endoscopists will be randomised to one of two endoscopy SBT curricula: (1) the Conventional Curriculum Group, in which participants will receive 6 hours of one-on-one simulation training augmented with expert feedback and interlaced with 4 hours of small group teaching on the theory of colonoscopy or (2) the Gamified Curriculum Group, in which participants will receive the same curriculum with integration of the following game-design elements: A leaderboard summarising participants' performance, game narrative, achievement badges and rewards for top performance. In line with a progressive learning approach, simulation training for participants will progress from low to high complexity simulators, starting with a bench-top model and then moving to the EndoVR virtual reality simulator. Performance will be assessed at three points: Pretraining, immediately post-training and 4-6 weeks after training. Assessments will take place on the simulator at all three time points and transfer of skills will be assessed during two clinical colonoscopies 4-6 weeks post-training. Mixed factorial ANOVAs will be used to determine if there is a performance difference between the two groups during simulated and clinical assessments. Ethics and dissemination Ethical approval was obtained at St. Michael's Hospital. Results of this trial will be submitted for presentation at academic meetings and for publication in a peer-reviewed journal. Trial registration number NCT03176251. © Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC.","endoscopy; simulation","Article; clinical assessment; clinical effectiveness; clinical evaluation; clinical protocol; colonoscopy; computer aided design; controlled study; curriculum; endoscopist; endoscopy; human; machine learning; medical education; randomized controlled trial; simulation training; single blind procedure; tactile feedback; computer interface; education; gastrointestinal endoscopy; procedures; questionnaire; simulation training; video game; Curriculum; Educational Measurement; Endoscopy, Gastrointestinal; Humans; Simulation Training; Surveys and Questionnaires; User-Computer Interface; Video Games",Article,"Final","",Scopus,2-s2.0-85062174277
"Bigras C., Kairy D., Archambault P.S.","57205472507;6508266422;55328034600;","Augmented feedback for powered wheelchair training in a virtual environment",2019,"Journal of NeuroEngineering and Rehabilitation","16","1", 12,"","",,1,"10.1186/s12984-019-0482-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060140090&doi=10.1186%2fs12984-019-0482-3&partnerID=40&md5=10900dea4a41979a17573bc46e311771","Integrated Program in Neuroscience, McGill University, Montreal, Canada; Interdisciplinary Research Center in Rehabilitation (CRIR), Montreal, Canada; École de Réadaptation, Faculté de Médecine, Université de Montréal, Montreal, Canada; School of Physical and Occupational Therapy, McGill University, Montreal, Canada","Bigras, C., Integrated Program in Neuroscience, McGill University, Montreal, Canada, Interdisciplinary Research Center in Rehabilitation (CRIR), Montreal, Canada; Kairy, D., École de Réadaptation, Faculté de Médecine, Université de Montréal, Montreal, Canada; Archambault, P.S., Interdisciplinary Research Center in Rehabilitation (CRIR), Montreal, Canada, School of Physical and Occupational Therapy, McGill University, Montreal, Canada","Background: Powered wheelchair (PW) driving is a complex activity and requires the acquisition of several skills. Given the risks involved with PW use, safe and effective training methods are needed. Virtual reality training allows users to practice difficult tasks in a safe environment. An additional benefit is that augmented feedback can be provided to optimize learning. The purpose of this study was to investigate whether providing augmented feedback during powered wheelchair simulator training results in superior performance, and whether skills learned in a virtual environment transfer to real PW driving. Methods: Forty healthy young adults were randomly allocated to two groups: one received augmented feedback during simulator training while the control group received no augmented feedback. PW driving performance was assessed at baseline in both the real and virtual environment (RE and VE), after training in VE and two days later in VE and RE (retention and transfer tests). Results: Both groups showed significantly better task completion time and number of collisions in the VE after training and these results were maintained two days later. The transfer test indicated better performance in the RE compared to baseline for both groups. Because time and collisions interact, a post-hoc 2D Kolmogonov-Smirnov test was used to investigate the differences in the speed-accuracy distributions for each group; a significant difference was found for the group receiving augmented feedback, before and after training, whereas the difference was not significant for the control group. There were no differences at the retention test, suggesting that augmented feedback was most effective during and immediately after training. Conclusions: PW simulator training is effective in improving task completion time and number of collisions. A small effect of augmented feedback was seen when looking at differences in the speed-accuracy distributions, highlighting the importance of accounting for the speed-accuracy tradeoff for PW driving. © 2019 The Author(s).","Augmented feedback; Powered wheelchair; Training; Virtual reality","adult; Article; environmental factor; feedback system; human; human experiment; information technology; Kolmogorov Smirnov test; measurement accuracy; normal human; priority journal; retention time; skill; training; velocity; virtual reality; computer interface; computer simulation; controlled study; feedback system; female; male; physiology; procedures; psychomotor performance; randomized controlled trial; simulation training; wheelchair; young adult; Computer Simulation; Feedback; Female; Humans; Male; Psychomotor Performance; Simulation Training; User-Computer Interface; Virtual Reality; Wheelchairs; Young Adult",Article,"Final","",Scopus,2-s2.0-85060140090
"Geronazzo M., Sikstrom E., Kleimola J., Avanzini F., De Gotzen A., Serafin S.","36720522500;55354784700;24829233900;7005300654;24724148200;6603367536;","The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of Immersive Virtual Reality Scenarios",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613754,"90","97",,8,"10.1109/ISMAR.2018.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062179431&doi=10.1109%2fISMAR.2018.00034&partnerID=40&md5=5a0b7c6c5270986682096e1aad3ed439","Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Virsabi ApS, Denmark; Hefio Ltd, Denmark; Dept. of Computer Science, University of Milano, Italy","Geronazzo, M., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Sikstrom, E., Virsabi ApS, Denmark; Kleimola, J., Hefio Ltd, Denmark; Avanzini, F., Dept. of Computer Science, University of Milano, Italy; De Gotzen, A., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark; Serafin, S., Dept. of Architecture, Design and Media Technology, Aalborg University, Denmark","Achieving a full 3D auditory experience with head-related transfer functions (HRTFs) is still one of the main challenges of spatial audio rendering. HRTFs capture the listener's acoustic effects and personal perception, allowing immersion in virtual reality (VR) applications. This paper aims to investigate the connection between listener sensitivity in vertical localization cues and experienced presence, spatial audio quality, and attention. Two VR experiments with head-mounted display (HMD) and animated visual avatar are proposed: (i) a screening test aiming to evaluate the participants' localization performance with HRTFs for a non-visible spatialized audio source, and (ii) a 2 minute free exploration of a VR scene with five audiovisual sources in a both non-spatialized (2D stereo panning) and spatialized (free-field HRTF rendering) listening conditions. The screening test allows a distinction between good and bad localizers. The second one shows that no biases are introduced in the quality of the experience (QoE) due to different audio rendering methods; more interestingly, good localizers perceive a lower audio latency and they are less involved in the visual aspects. © 2018 IEEE.","Auditory feedback; Human-centered computing; Interaction devices; Interaction paradigms; Interaction techniques; Sound-based input / output Human-centered computing; Virtual reality; Human-centered computing","Augmented reality; Helmet mounted displays; Three dimensional computer graphics; Transfer functions; Virtual reality; Auditory feedback; Human-centered computing; Input/output; Interaction devices; Interaction paradigm; Interaction techniques; Sound reproduction",Conference Paper,"Final","",Scopus,2-s2.0-85062179431
"Werrlich S., Daniel A., Ginger A., Nguyen P.-A., Notni G.","57195069564;57201618829;57207031366;57209915720;7004204934;","Comparing HMD-Based and Paper-Based Training",2019,"Proceedings of the 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2018",,, 8613759,"134","142",,22,"10.1109/ISMAR.2018.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062171885&doi=10.1109%2fISMAR.2018.00046&partnerID=40&md5=c206a2dd977504ba7c080ff7a8072a51","BMW Group, Germany; TU Ilmenau, Germany","Werrlich, S., BMW Group, Germany; Daniel, A., BMW Group, Germany; Ginger, A., BMW Group, Germany; Nguyen, P.-A., BMW Group, Germany; Notni, G., TU Ilmenau, Germany","Collaborative Systems are in daily use by millions of people promising to improve everyone's life. Smartphones, smartwatches and tablets are everyday objects and life without these unimaginable. New assistive systems such as head-mounted displays (HMDs) are becoming increasingly important for various domains, especially for the industrial domain, because they claim to improve the efficiency and quality of procedural tasks. A range of scientific laboratory studies already demonstrated the potential of augmented reality (AR) technologies especially for training tasks. However, most researches are limited in terms of inadequate task complexity, measured variables and lacking comparisons. In this paper, we want to close this gap by introducing a novel multimodal HMD-based training application and compare it to paper-based learning for manual assembly tasks. We perform a user study with 30 participants measuring the training transfer of an engine assembly training task, the user satisfaction and perceived workload during the experiment. Established questionnaires such as the system usability scale (SUS), the user experience questionnaire (UEQ) and the Nasa Task Load Index (NASA-TLX) are used for the assessment. Results indicate significant differences between both learning approaches. Participants perform significantly faster and significantly worse using paper-based instructions. Furthermore, all trainees preferred HMD-based learning for future assembly trainings which was scientifically proven by the UEQ. © 2018 IEEE.","Augmented Reality; Evaluation; Head Mounted Displays; Training","Augmented reality; NASA; Personnel training; Surveys; Assembly trainings; Collaborative systems; Evaluation; Head mounted displays; Laboratory studies; Learning approach; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85062171885
"Boyce M.W., Rowan C.P., Shorter P.L., Moss J.D., Amburn C.R., Garneau C.J., Sottilare R.A.","57189320664;51564868300;57204817713;24332598600;55391888100;24341035700;6506196843;","The impact of surface projection on military tactics comprehension",2019,"Military Psychology","31","1",,"45","59",,4,"10.1080/08995605.2018.1529487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057323107&doi=10.1080%2f08995605.2018.1529487&partnerID=40&md5=feb5b6c8776f21becab5cc98fa94ee9d","Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Department of Behavioral Sciences and Leadership, United States Military Academy at West Point, West Point, NY, United States","Boyce, M.W., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Rowan, C.P., Department of Behavioral Sciences and Leadership, United States Military Academy at West Point, West Point, NY, United States; Shorter, P.L., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Moss, J.D., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Amburn, C.R., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Garneau, C.J., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States; Sottilare, R.A., Human Research and Engineering Directorate, US Army Research Laboratory, Orlando, FL, United States","This experiment assessed how displaying information onto different surfaces (flat vs. raised) influenced the performance, workload, and engagement of cadets answering questions on military tactics. Sixty-two cadets in a within-subjects design each answered 24 tactics-related questions across 2 conditions (12 on flat, 12 on raised) which were measured by accuracy and time on task. After each set of 12 questions, the cadets took postsurveys assessing engagement, measured by a modified User Engagement Scale and the System Usability Scale, and workload measured by the NASA-TLX. Findings indicated that raised terrain surface led to reduced workload and increased engagement and time on task as compared to the flat terrain surface. A practice effect drove performance metrics (time on task and accuracy), where the learner performed better on the second surface type displayed. This research contributes to expanding the literature base that supports alternative display methods to increase engagement and augment instruction of military tactics tasks. © 2019, © 2019 Society for Military Psychology Division 19 of the American Psychological Association.","Augmented reality sandtable (ARES); generalized intelligent framework for tutoring (GIFT); military tactics; terrain; United States military academy (USMA)","adult; army; article; comprehension; female; human; human experiment; major clinical study; male; United States; workload",Article,"Final","",Scopus,2-s2.0-85057323107
"Qin Z., Tai Y., Xia C., Peng J., Huang X., Chen Z., Li Q., Shi J.","57197755201;36984437500;57205556936;57214109873;18436902100;36633870500;55911980700;8640031700;","Towards Virtual VATS, Face, and Construct Evaluation for Peg Transfer Training of Box, VR, AR, and MR Trainer",2019,"Journal of Healthcare Engineering","2019",, 6813719,"","",,4,"10.1155/2019/6813719","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060519204&doi=10.1155%2f2019%2f6813719&partnerID=40&md5=3f4666c758e681e13ee471dd0a467665","Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Department of Thoracic Surgery, Yunnan First People's Hospital, Kunming, 650000, China","Qin, Z., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Tai, Y., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Xia, C., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Peng, J., Department of Thoracic Surgery, Yunnan First People's Hospital, Kunming, 650000, China; Huang, X., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Chen, Z., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Li, Q., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China; Shi, J., Yunnan Key Laboratory of Opto-Electronic Information Technology, Yunnan Normal University, Kunming, 650000, China","The aim of this study is to develop and assess the peg transfer training module face, content and construct validation use of the box, virtual reality (VR), cognitive virtual reality (CVR), augmented reality (AR), and mixed reality (MR) trainer, thereby to compare advantages and disadvantages of these simulators. Training system (VatsSim-XR) design includes customized haptic-enabled thoracoscopic instruments, virtual reality helmet set, endoscope kit with navigation, and the patient-specific corresponding training environment. A cohort of 32 trainees comprising 24 novices and 8 experts underwent the real and virtual simulators that were conducted in the department of thoracic surgery of Yunnan First People's Hospital. Both subjective and objective evaluations have been developed to explore the visual and haptic potential promotions in peg transfer education. Experiments and evaluation results conducted by both professional and novice thoracic surgeons show that the surgery skills from experts are better than novices overall, AR trainer is able to provide a more balanced training environments on visuohaptic fidelity and accuracy, box trainer and MR trainer demonstrated the best realism 3D perception and surgical immersive performance, respectively, and CVR trainer shows a better clinic effect that the traditional VR trainer. Combining these in a systematic approach, tuned with specific fidelity requirements, medical simulation systems would be able to provide a more immersive and effective training environment. © 2019 Zhibao Qin et al.",,"Augmented reality; E-learning; Mixed reality; Transplantation (surgical); Evaluation results; Medical simulations; Patient specific; Subjective and objective evaluations; Thoracic surgery; Training modules; Training Systems; Virtual simulators; Surgical equipment; adult; Article; augmented reality; construct validity; facial recognition; female; human; intermethod comparison; male; medical education; mixed reality; postgraduate student; simulation training; skill; thoracic surgeon; video assisted thoracoscopic surgery; virtual reality; clinical competence; comparative study; computer interface; computer simulation; education; lung tumor; middle aged; procedures; software; teaching; video assisted thoracoscopic surgery; virtual reality; young adult; Adult; Augmented Reality; Clinical Competence; Computer Simulation; Computer-Assisted Instruction; Female; Humans; Lung Neoplasms; Male; Middle Aged; Software; Thoracic Surgery, Video-Assisted; User-Computer Interface; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85060519204
"Kalarat K., Koomhin P.","16319061300;55619841400;","Real-time volume rendering interaction in Virtual Reality",2019,"International Journal of Technology","10","7",,"1307","1314",,,"10.14716/ijtech.v10i7.3259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076044621&doi=10.14716%2fijtech.v10i7.3259&partnerID=40&md5=005226cee497ef28d0d76d480aa41a00","School of Informatics, Walailak University, 222 Thaiburi, Thasala District, Nakhon Si Thammarat, 80161, Thailand; School of Medicine, Walailak University, 222 Thaiburi, Thasala District, Nakhon Si Thammarat, 80161, Thailand","Kalarat, K., School of Informatics, Walailak University, 222 Thaiburi, Thasala District, Nakhon Si Thammarat, 80161, Thailand; Koomhin, P., School of Medicine, Walailak University, 222 Thaiburi, Thasala District, Nakhon Si Thammarat, 80161, Thailand","Volume visualization using Direct Volume Rendering (DVR) techniques is used to view information inside 3D volumetric data. Data is classified using a transfer function to emphasize or filter some parts of volumetric information, such as that from Computed Tomography (CT) or Magnetic Resonance Imaging (MRI). In this paper, we introduced an application for real-time volume rendering interaction with 1D transfer functions using Virtual Reality (VR) technology based on the Oculus Rift headset and Oculus Touch controllers. Resulting images were visualized stereoscopically at 60 frames per second using a ray-casting shader, which works based on Graphics Processing Unit (GPU). To evaluate the system, 20 participants interacted with the application to complete three tasks, including a free viewpoint scan, clipping planes renderer, and an editable transfer function in the virtual environment. Then, a survey was carried out using a questionnaire to gather data. Findings showed that the average usability score for the application was 87.54, which suggested that it was highly usable. © IJTech 2019.","Direct volume rendering; Ray casting; Virtual environment; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85076044621
"Kramberger I., Kacic Z., Donaj G.","6602310875;7004382027;55874344700;","Binocular Phase-Coded Visual Stimuli for SSVEP-Based BCI",2019,"IEEE Access","7",, 8688386,"48912","48922",,2,"10.1109/ACCESS.2019.2910737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065069612&doi=10.1109%2fACCESS.2019.2910737&partnerID=40&md5=a0d4afa389cdff1a61934e32668abebb","Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia","Kramberger, I., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia; Kacic, Z., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia; Donaj, G., Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, 2000, Slovenia","This paper presents a method of binocular visual stimulation for brain-computer interfaces (BCIs) based on steady-state visual evoked potentials (SSVEPs) using phase-coded symbols. The proposed method's emphasis is on a binocular phase-coded visual stimulus, which is based on the phase differences between the left- and right-eye stimuli, and a symbol detection and recognition procedure based on SSVEP response of the left and right occipital lobes of the user's scalp, where the SSVEP response is obtained as electroencephalography (EEG) signaling. The symbols are coded as phase differences and maintain the same frequency of the sine wave-modulated light provided to the user's left and right eyes as a binocular visual stimulation. Based on this method, a basic system setup is presented to explore the possibilities of binocular phase-coded visual stimuli for virtual or augmented reality applications, where the binocular visual stimulation was achieved by the specially designed head-mounted displays. Multiple visually coded targets are realized as eight different phase-coded binocular symbols and further evaluated as a random sequence of single targets, thus representing the situations in virtual or augmented reality, where multiple visually coded targets are present but not visualized to the user simultaneously within the same field of view. The offline results obtained from ten healthy subjects revealed that an average symbol recognition accuracy of 90.63% and an information transfer rate (ITR) of 70.55 bits/min were achieved for a symbol stimulation time of 2 s. The results of this paper demonstrate the feasibility of using binocular visual stimuli for SSVEP-based BCIs, where reasonable ITR is achieved using single-frequency binocular phase-coded symbols. The proposed method indicates the possibility of combining it with 3D wearable visualization technologies, such as binocular head-mounted displays (HMDs), in order to improve the intuitiveness of the interaction with more immersive user experience using BCI modalities. © 2013 IEEE.","Brain computer interfaces; Brain stimulation; Data acquisition; Electroencephalography; Human computer interaction; Phase measurement; Steady state visually evoked potential; Three-dimensional displays; Virtual reality; Visualization; Wearable sensors","Augmented reality; Binoculars; Data acquisition; Data visualization; Electroencephalography; Electrophysiology; Flow visualization; Helmet mounted displays; Human computer interaction; Interface states; Pattern recognition; Phase measurement; Three dimensional computer graphics; Three dimensional displays; Virtual reality; Visualization; Wearable computers; Wearable sensors; Augmented reality applications; Brain computer interfaces (BCIs); Brain stimulation; Head mounted displays; Information transfer rate; Steady state visual evoked potential (SSVEPs); Steady state visually evoked potentials; Visualization technologies; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85065069612
"Moore H.F., Eiris R., Gheisari M., Esmaeili B.","57209801285;57196006104;36459705300;35388139800;","Hazard Identification Training Using 360-Degree Panorama vs. Virtual Reality Techniques: A Pilot Study",2019,"Computing in Civil Engineering 2019: Visualization, Information Modeling, and Simulation - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2019",,,,"55","62",,5,"10.1061/9780784482421.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068793185&doi=10.1061%2f9780784482421.008&partnerID=40&md5=50941e5e91da0bb940956a73a319410c","Healthcare Division, Haskell Company, 111 Riverside Ave., Jacksonville, FL  32202, United States; Human-Centered Technology in Construction (HCTC) Research Group, Rinker School of Construction Management, Univ. of Florida, PO Box 115703, Gainesville, FL  32611-5701, United States; Safety, Risk Management, and Decision-Making (SARMAD) Research Group, Sid and Reva Dewberry Dept. of Civil, Environmental, and Infrastructure Engineering, George Mason Univ., PO Box 115703, Fairfax, VA  22030, United States","Moore, H.F., Healthcare Division, Haskell Company, 111 Riverside Ave., Jacksonville, FL  32202, United States; Eiris, R., Human-Centered Technology in Construction (HCTC) Research Group, Rinker School of Construction Management, Univ. of Florida, PO Box 115703, Gainesville, FL  32611-5701, United States; Gheisari, M., Human-Centered Technology in Construction (HCTC) Research Group, Rinker School of Construction Management, Univ. of Florida, PO Box 115703, Gainesville, FL  32611-5701, United States; Esmaeili, B., Safety, Risk Management, and Decision-Making (SARMAD) Research Group, Sid and Reva Dewberry Dept. of Civil, Environmental, and Infrastructure Engineering, George Mason Univ., PO Box 115703, Fairfax, VA  22030, United States","Virtual reality (VR)-based approaches have been used to facilitate safety knowledge transfer and increase hazard awareness by providing safe and controlled experiences of unsafe scenarios for construction safety training applications. However, high computational costs, long development times, and the limited sense of presence and realism associated with existing VR methods have posed significant challenges for using such VR-based safety training platforms. Unlike the VR environments that provide computer-generated simulations of the environment, 360-degree panoramas create true-to-reality simulations of environments while maintaining a high sense of presence. The purpose of this research is to develop and compare hazard identification training scenarios using 360-degree panorama and VR techniques. A pilot study, based on OSHA's focus-four hazards, was conducted to assess the effectiveness of safety hazard identification training using these techniques. The results of this preliminary research indicate that hazard identification index scores were slightly higher on average in the VR condition comparing to the ""messier"" or ""dirtier"" as-build representation of the 360-degree panorama. © 2019 American Society of Civil Engineers.",,"E-learning; Hazardous materials; Information theory; Knowledge management; Virtual reality; Visualization; Computational costs; Computer generated; Construction safety; Hazard awareness; Hazard identification; Sense of presences; Training scenario; Virtual reality techniques; Hazards",Conference Paper,"Final","",Scopus,2-s2.0-85068793185
"Riva G., Wiederhold B.K., Mantovani F.","56962750600;7003634518;7006190897;","Neuroscience of Virtual Reality: From Virtual Exposure to Embodied Medicine",2019,"Cyberpsychology, Behavior, and Social Networking","22","1",,"82","96",,74,"10.1089/cyber.2017.29099.gri","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055999466&doi=10.1089%2fcyber.2017.29099.gri&partnerID=40&md5=cd9711ea83953e848a8ad11e7c485d1c","Applied Technology for Neuro-Psychology Lab, IRCCS Istituto Auxologico Italiano, Milan, Italy; Department of Psychology, Università Cattolica Del Sacro Cuore, Largo Gemelli 1, Milan, 20123, Italy; Virtual Reality Medical Center, San Diego, CA, United States; Virtual Reality Medical Institute, Brussels, Belgium; Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy","Riva, G., Applied Technology for Neuro-Psychology Lab, IRCCS Istituto Auxologico Italiano, Milan, Italy, Department of Psychology, Università Cattolica Del Sacro Cuore, Largo Gemelli 1, Milan, 20123, Italy; Wiederhold, B.K., Virtual Reality Medical Center, San Diego, CA, United States, Virtual Reality Medical Institute, Brussels, Belgium; Mantovani, F., Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy","Is virtual reality (VR) already a reality in behavioral health? To answer this question, a meta-review was conducted to assess the meta-analyses and systematic and narrative reviews published in this field in the last twenty-two months. Twenty-five different articles demonstrated the clinical potential of this technology in both the diagnosis and the treatment of mental health disorders: VR compares favorably to existing treatments in anxiety disorders, eating and weight disorders, and pain management, with long-term effects that generalize to the real world. But why is VR so effective? Here, the following answer is suggested: VR shares with the brain the same basic mechanism: embodied simulations. According to neuroscience, to regulate and control the body in the world effectively, the brain creates an embodied simulation of the body in the world used to represent and predict actions, concepts, and emotions. VR works in a similar way: the VR experience tries to predict the sensory consequences of an individual's movements, providing to him/her the same scene he/she will see in the real world. To achieve this, the VR system, like the brain, maintains a model (simulation) of the body and the space around it. If the presence in the body is the outcome of different embodied simulations, concepts are embodied simulations, and VR is an embodied technology, this suggests a new clinical approach discussed in this article: the possibility of altering the experience of the body and facilitating cognitive modeling/change by designing targeted virtual environments able to simulate both the external and the internal world/body. © Giuseppe Riva et al. 2019; Published by Mary Ann Liebert, Inc.",,"analgesia; anxiety disorder; cognitive neuroscience; human; mental disease; meta analysis; virtual reality; virtual reality exposure therapy; Anxiety Disorders; Cognitive Neuroscience; Humans; Mental Disorders; Pain Management; Virtual Reality; Virtual Reality Exposure Therapy",Article,"Final","",Scopus,2-s2.0-85055999466
"Tayeh R., Bademosi F.M., Issa R.R.A.","57190000479;57202742349;35587852800;","Interactive Holograms for Better Construction Information Communication",2019,"Computing in Civil Engineering 2019: Visualization, Information Modeling, and Simulation - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2019",,,,"112","119",,2,"10.1061/9780784482421.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068736374&doi=10.1061%2f9780784482421.015&partnerID=40&md5=fe1f62c82c74ceab7f1b674f2ef38188","Rinker School of Construction Management, Univ. of Florida, Gainesville, FL  32603, United States","Tayeh, R., Rinker School of Construction Management, Univ. of Florida, Gainesville, FL  32603, United States; Bademosi, F.M., Rinker School of Construction Management, Univ. of Florida, Gainesville, FL  32603, United States; Issa, R.R.A., Rinker School of Construction Management, Univ. of Florida, Gainesville, FL  32603, United States","Over the last few decades, the investments in more complex construction projects have increased the need for better communication means. Effective means of communications should be employed on construction projects to help aggregate dispersed information between project stakeholders. Advancements in information technology have resulted in new construction visualization techniques aiming at enhancing communication between project stakeholders. Some of these technologies include virtual reality, augmented reality, and holography. The aim of this paper is to further investigate the use of holography in the architecture, engineering, construction, and operations (AECO) industry. The holographic technology used in this research was previously developed by the authors through a game engine and an add-in for Revit. The add-in allows an easy transfer of information from a building information model in Revit to be used in the game engine. In addition, the developed hologram allows the user to interact with the hologram using voice commands and/or hand gestures. Both the game and the add-in were the subject of an experimental study conducted to evaluate their ease of use and effectiveness. The results proved that the developed add-in is easier to use and more effective than traditional methods used to export building models from modeling platforms into a gaming environment. Moreover, the interactive hologram was able to enhance the human-human interaction between multiple users. Future research will focus on further developing the software and on investigating the use of the holographic experience on job sites. © 2019 American Society of Civil Engineers.",,"Augmented reality; Information theory; Lithography; Virtual reality; Visualization; Building Information Model - BIM; Complex construction project; Construction information; Construction projects; Holographic technology; Human-human interactions; Project stakeholders; Transfer of information; Holograms",Conference Paper,"Final","",Scopus,2-s2.0-85068736374
"Oberdörfer S., Latoschik M.E.","57192160404;6602976914;","Knowledge encoding in game mechanics: Transfer-oriented knowledge learning in desktop-3D and VR",2019,"International Journal of Computer Games Technology","2019",, 7626349,"","",,4,"10.1155/2019/7626349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065769392&doi=10.1155%2f2019%2f7626349&partnerID=40&md5=0741462efb8e93e61183958445a8371b","Human-Computer Interaction, University of Würzburg, Würzburg, Germany","Oberdörfer, S., Human-Computer Interaction, University of Würzburg, Würzburg, Germany; Latoschik, M.E., Human-Computer Interaction, University of Würzburg, Würzburg, Germany","Affine Transformations (ATs) are a complex and abstract learning content. Encoding the AT knowledge in Game Mechanics (GMs) achieves a repetitive knowledge application and audiovisual demonstration. Playing a serious game providing these GMs leads to motivating and effective knowledge learning. Using immersive Virtual Reality (VR) has the potential to even further increase the serious game's learning outcome and learning quality. This paper compares the effectiveness and efficiency of desktop-3D and VR in respect to the achieved learning outcome. Also, the present study analyzes the effectiveness of an enhanced audiovisual knowledge encoding and the provision of a debriefing system. The results validate the effectiveness of the knowledge encoding in GMs to achieve knowledge learning. The study also indicates that VR is beneficial for the overall learning quality and that an enhanced audiovisual encoding has only a limited effect on the learning outcome. © 2019 Sebastian Oberdörfer and Marc Erich Latoschik.",,"Encoding (symbols); Knowledge management; Signal encoding; Virtual reality; Affine transformations; Effectiveness and efficiencies; Immersive virtual reality; Knowledge application; Knowledge learning; Learning contents; Learning outcome; Learning quality; Serious games",Article,"Final","",Scopus,2-s2.0-85065769392
"Stadnicka D., Litwin P., Antonelli D.","55933349000;36170708200;21833419400;","Human factor in intelligent manufacturing systems - Knowledge acquisition and motivation",2019,"Procedia CIRP","79",,,"718","723",,11,"10.1016/j.procir.2019.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065432391&doi=10.1016%2fj.procir.2019.02.023&partnerID=40&md5=26afb7d550671426374448b4567ec952","Rzeszow University of Technology, Faculty of Mechnical Engineering and Aeronautics, Al. Powstancow Warszawn 12, Rzeszow, 35-959, Poland; Politecnico di Torino, Corso Duca degli Abruzzi 24, Torino, I 10129, Italy","Stadnicka, D., Rzeszow University of Technology, Faculty of Mechnical Engineering and Aeronautics, Al. Powstancow Warszawn 12, Rzeszow, 35-959, Poland; Litwin, P., Rzeszow University of Technology, Faculty of Mechnical Engineering and Aeronautics, Al. Powstancow Warszawn 12, Rzeszow, 35-959, Poland; Antonelli, D., Politecnico di Torino, Corso Duca degli Abruzzi 24, Torino, I 10129, Italy","People play a central role in intelligent manufacturing systems because of two reasons: their knowledge is indispensable to create and improve intelligent manufacturing systems; and their motivation is very important to identify and solve causes of the problems which may occur in order to prevent them in the future. Therefore, adequate learning methods are required to accomplish these two goals: empower and motivate people. In this paper innovative methods such as learning by doing, simulations and virtual reality will be presented as the ways to transfer the knowledge about intelligent manufacturing systems and to increase motivation concerning their improvements. © 2019 The Author(s).","Human factor; Intelligent manufacturing system; Learning by doing; Simulations; Virtual reality","Engineering education; Human engineering; Motivation; Virtual reality; Innovative method; Intelligent manufacturing system; Learning by doing; Learning methods; Simulations; Manufacture",Conference Paper,"Final","",Scopus,2-s2.0-85065432391
"Resch T., Böhm C., Weinzierl S.","57195258353;57195266339;35231501100;","A cross platform C-library for efficient dynamic binaural synthesis on mobile devices",2019,"Proceedings of the AES International Conference","2019-August",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074585301&partnerID=40&md5=d6b4d812b97a67157f5f8c9ce4462328","Hochschule für Musik FHNW, Research and Development, Germany; Technische Universität Berlin, Audio Communication Group, Germany","Resch, T., Hochschule für Musik FHNW, Research and Development, Germany, Technische Universität Berlin, Audio Communication Group, Germany; Böhm, C., Technische Universität Berlin, Audio Communication Group, Germany; Weinzierl, S., Technische Universität Berlin, Audio Communication Group, Germany","Virtual Acoustic Spaces (VAS) is a lightweight C library for three-dimensional sound reproduction via headphones. It offers standard objects for dynamic binaural synthesis based on head-related transfer functions, delays for the simulation of reflections and FIR filters for headphone equalization and reverberation. Moreover, the library implements a real-time audio synthesis based on binaural room impulse response datasets, and a data reduction method for the corresponding room impulse responses. Depending on the number and timing of early reflections, binaural room synthesis becomes possible with reduced memory and processor usage, to be used in mobile augmented and virtual reality applications even on mid-range smartphones. © 2019 Audio Engineering Society. All rights reserved.",,"Cell proliferation; FIR filters; Headphones; Impulse response; Sound reproduction; Virtual reality; Augmented and virtual realities; Binaural room impulse response; Binaural synthesis; Head related transfer function; Reduction method; Room impulse response; Three-dimensional sound reproductions; Virtual acoustic space; Digital libraries",Conference Paper,"Final","",Scopus,2-s2.0-85074585301
"Riva G., Wiederhold B.K., Di Lernia D., Chirico A., Riva E.F.M., Mantovani F., Cipresso P., Gaggioli A.","56962750600;7003634518;57189076325;56755080200;57200803512;7006190897;36717478000;6603138127;","Virtual reality meets artificial intelligence: The emergence of advanced digital therapeutics and digital biomarkers",2019,"Annual Review of CyberTherapy and Telemedicine","17",,,"3","7",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085609366&partnerID=40&md5=96fce33c0e8b3450c84334fd77801a83","Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy; Interactive Media Institute, San Diego, CA, United States; Department of Cultural Heritage and Environment, Università degli Studi di Milano, Milano, Italy; Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy","Riva, G., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy; Wiederhold, B.K., Interactive Media Institute, San Diego, CA, United States; Di Lernia, D., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Chirico, A., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Riva, E.F.M., Department of Cultural Heritage and Environment, Università degli Studi di Milano, Milano, Italy; Mantovani, F., Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy; Cipresso, P., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy; Gaggioli, A., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy","In the past 25 years, researchers have discovered that Virtual Reality (VR) is an effective tool for mental health treatment and assessment in anxiety disorders, eating and weight disorders, and pain management with long-term effects that generalize to the real world. Moreover, VR is also an effective assessment tool with practical applications that range from social and cognitive deficits to addiction. Nevertheless, despite progress, evidence-based psychological treatments still need improvement. In this paper we suggest that the integration of VR with another emerging technology – Artificial Intelligence (AI) – will provide clinicians with two new powerful tools for improving evidence-based psychological treatments: advanced digital therapeutics and digital biomarkers. The term “Digital Therapeutics” indicates the use of digital/online health technologies to treat a medical or psychological condition. Following this definition, any VR clinical application can be defined as a form of digital therapeutics. However, the integration between VR al AI allows a critical feature for any digital therapeutic: personalization. On one side, VR allows the collection of “Digital Biomarkers”-physiological, and behavioral data that are collected by means of digital technologies and used as an indicator of biologic processes or biological responses to therapeutic interventions – that are directly connected to the brain functioning and can be altered to correct the specific dysfunctions of the predictive coding mechanisms in the individual’s brain. On the other side AI, by applying machine learning techniques to the individual's digital biomarkers, allows the optimization the individual treatment strategy facilitating the transition to a personalized, effective and engaging medicine. © 2019, Interactive Media Institute. All rights reserved.","Behavioral health; Digital biomarkers; Digital therapeutics; Embodied medicine; Neuroscience; Virtual Reality","biological marker; analgesia; anxiety disorder; Article; artificial intelligence; body weight disorder; digital therapeutics; eating disorder; human; machine learning; mental health; neuroscience; telemedicine; virtual reality",Article,"Final","",Scopus,2-s2.0-85085609366
"Lin C.-X., Pradhananga N., Vassigh S.","8608727000;35099310500;57203133538;","An evaluation of the effects of team projects and augmented reality on student learning in sustainable building science",2019,"ASME International Mechanical Engineering Congress and Exposition, Proceedings (IMECE)","8",,,"","",,,"10.1115/IMECE2019-11982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078833744&doi=10.1115%2fIMECE2019-11982&partnerID=40&md5=6167eed586eda4cd83a461828c9775e1","Department of Mechanical and Materials Engineering, Florida International University, Miami, FL  33174, United States; School of Construction Management, Florida International University, Miami, FL  33174, United States; Department of Architecture, Florida International University, Miami, FL  33199, United States","Lin, C.-X., Department of Mechanical and Materials Engineering, Florida International University, Miami, FL  33174, United States; Pradhananga, N., School of Construction Management, Florida International University, Miami, FL  33174, United States; Vassigh, S., Department of Architecture, Florida International University, Miami, FL  33199, United States","Sustainable building design and construction involves complex systems that require multidisciplinary teams from engineering, construction, and architecture, to design and analyze the systems at every stage of the process during the building’s life cycle. However, students who are the future work force are often trained in different disciplines across different colleges. When these students are grouped together to work on the building design and analysis, learning in a multidisciplinary environment could be both beneficial and challenging due to the difference in their background. In this paper, we report our experience and analysis of data examining the learning effectiveness of the undergraduate students from three cross-college departments in architecture, construction, and engineering. Using pre- and post-semester tests on selected building science problems, we have investigated how the student’s understanding of building science had changed through team projects. Particularly, for mechanical engineering students in the design of thermal/fluid systems classes, we analyzed whether a cross-college multidisciplinary team could do better as compared to a disciplinary-specific team within the same class. We also examined the potential effects of emerging technology, augmented reality, on student learning in the same team environment. It was interesting to find that students’ learning in discipline-specific teams can be improved as in the multidisciplinary teams, due to the challenges in the complexity of the projects. Copyright © 2019 ASME.",,"Architectural design; Augmented reality; College buildings; Heat transfer; Intelligent buildings; Life cycle; Sustainable development; Thermal Engineering; Emerging technologies; Learning effectiveness; Mechanical engineering students; Multi-disciplinary teams; Potential effects; Sustainable building; Sustainable building design; Undergraduate students; Students",Conference Paper,"Final","",Scopus,2-s2.0-85078833744
"Sifonis C.M.","6507193080;","Examining Game Transfer Phenomena in the Hybrid Reality Game, Ingress",2019,"International Journal of Human-Computer Interaction","35","17",,"1557","1568",,,"10.1080/10447318.2018.1555735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058783038&doi=10.1080%2f10447318.2018.1555735&partnerID=40&md5=b0e53077a55716cf5696922dcfa2a73b","Department of Psychology, Oakland University, Rochester, United States","Sifonis, C.M., Department of Psychology, Oakland University, Rochester, United States","Game Transfer Phenomena (GTP) is a common video gaming experience in which extended gameplay causes game-related automatic thoughts, involuntary behaviors and perceptions. Currently, there is little research examining GTP in location-based, augmented/hybrid reality (HR) gaming. Based on prior GTP research, HR gamers were predicted to experience all types of GTP while also experiencing GTP specific to HR gaming. 867 Ingress players responded to a survey examining the degree to which they experienced the types of GTP. 31 % of the sample was female and the average age was 36. Ingress players exhibited all types of GTP and were most likely to experience automatic thought process related to the game. Female Ingress players were significantly more likely to experience GTP than males and increases in age were associated with decreases in GTP. 18 % of respondents experienced HR-specific phenomena in which players unintentionally use physical actions in the real world to interact with virtual aspects of the game. Future augmented/HR research should examine the cognitive and physical components underlying HR-specific GTP and how gender and age are associated with experiencing this and GTP related to different platforms and media. © 2018, © 2018 Taylor & Francis Group, LLC.",,"Human computer interaction; User interfaces; Average ages; Location based; Most likely; Physical action; Physical components; Thought process; Transfer phenomenon; Video gaming; Mixed reality",Article,"Final","",Scopus,2-s2.0-85058783038
"Miersch P., Jochem R.","57202990521;6603204236;","Designing digital learning environments",2019,"WMSCI 2019 - 23rd World Multi-Conference on Systemics, Cybernetics and Informatics, Proceedings","1",,,"28","30",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074341983&partnerID=40&md5=099abf0610c6985ab5374e3d279eac17","Department of Machine Tools and Factory Management, Technical University of Berlin, Berlin, 10587, Germany","Miersch, P., Department of Machine Tools and Factory Management, Technical University of Berlin, Berlin, 10587, Germany; Jochem, R., Department of Machine Tools and Factory Management, Technical University of Berlin, Berlin, 10587, Germany","The motivation and qualification of employees is a decisive success factor for companies. In addition, keeping pace in times of digitalization requires a constant build-up of skills, especially in small and medium-sized enterprises (SMEs) in the manufacturing sector. Such companies therefore often enter a market that offers external further trainings but in a not very transparent way. Often these offers are cost-intensive and often meet only a fraction of the actual expectations. The current technology makes it possible to transfer learning objects such as real production machines including their process flows into the virtual environment and to make them tangible. This offers great potential for digital education, as employees could acquire additional knowledge about processes or technologies in a cost-effective and flexible way. A recommendation algorithm should enable SMEs to identify their training needs and develop individual offers, especially with regard to the learning arrangement. © 2019 by the International Institute of Informatics and Systemics.","Digital education; Human resource development; Learning arrangement; Quality assurance; Recommendation algorithm; SMEs","Computer aided instruction; Cost effectiveness; Learning algorithms; Personnel training; Quality assurance; Virtual reality; Additional knowledge; Digital learning environment; Human resource development; Learning arrangement; Manufacturing sector; Recommendation algorithms; Small and medium-sized enterprise; SMEs; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85074341983
"Barshad Y., Murphy D., Ku M., Peterson N., Spiro S., Stylianos N., Wu Q.","57220329449;57221029595;57219164534;56438994400;57219165419;56015213100;57219164810;","AI in sulfur recovery - Integration of analyzers to achieve tighter control",2019,"Sulphur 2019 + Sulphuric Acid",,,,"271","279",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091483956&partnerID=40&md5=7c08c317a66cb11d6c8455e01e823b1d","Applied Analytics, Inc., Burlington, MA, United States","Barshad, Y., Applied Analytics, Inc., Burlington, MA, United States; Murphy, D., Applied Analytics, Inc., Burlington, MA, United States; Ku, M., Applied Analytics, Inc., Burlington, MA, United States; Peterson, N., Applied Analytics, Inc., Burlington, MA, United States; Spiro, S., Applied Analytics, Inc., Burlington, MA, United States; Stylianos, N., Applied Analytics, Inc., Burlington, MA, United States; Wu, Q., Applied Analytics, Inc., Burlington, MA, United States","The control of a sulfur recovery Claus plant has been using simple concentration feedback control for most if not all of the past 70 years. While the original British patent was issued in 1883 the Claus process was implemented on a large scale about 100 years ago and was using simple photometers to make the measurement since the 1960s. By combining better math capable of understanding the process and learning it, a more precise spectral engine, a more sophisticated sampling probe it will be demonstrated that the actual measurement can be augmented using dimensional knowledge of the system and the process as well as other concentration information, temperature and pressures. In other words using all that information can create an augmented reality by accounting for known residence times and transportation lags and correct for those in real time providing an enhanced control signal quality. This is only an example and the use of other parameters will be demonstrated. We will be discussing the implementation of smarter algorithms combined with the sampling probe and spectrophotometer that take into account the real life behavior of the system and corrects for those thus reducing or eliminating the measurement related issues from the response. Better modeling of the probe and the measurement combined with learning capabilities and available process parameters provides feedback and feedforward by using transfer function approach for better modeling of both the catalyst bed and the measurement system. © 2019Sulphur 2019 + Sulphuric Acid. All rights reserved.",,"Augmented reality; Feedback; Learning systems; Probes; Sulfur; Sulfuric acid; Actual measurements; Learning capabilities; Measurement system; Process parameters; Residence time; Sampling probes; Sulfur recovery; Temperature and pressures; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85091483956
"Marchal-Crespo L., Tsangaridis P., Obwegeser D., Maggioni S., Riener R.","26422829400;57044845400;57204662517;55977900000;7003404145;","Haptic error modulation outperforms visual error amplification when learning a modified gait pattern",2019,"Frontiers in Neuroscience","13","FEB", 61,"","",,9,"10.3389/fnins.2019.00061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065839983&doi=10.3389%2ffnins.2019.00061&partnerID=40&md5=ee71ed6705c98e1f13b84ea34aae9031","Gerontechnology and Rehabilitation Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland; Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland; Reharobotics Group, Spinal Cord Injury Center, Balgrist University Hospital, Medical Faculty, University of Zurich, Zurich, Switzerland; Hocoma AG, Volketswil, Switzerland","Marchal-Crespo, L., Gerontechnology and Rehabilitation Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland, Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland; Tsangaridis, P., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland; Obwegeser, D., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland; Maggioni, S., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland, Reharobotics Group, Spinal Cord Injury Center, Balgrist University Hospital, Medical Faculty, University of Zurich, Zurich, Switzerland, Hocoma AG, Volketswil, Switzerland; Riener, R., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), Department of Health Sciences and Technology (D-HEST), ETH Zürich, Zurich, Switzerland, Reharobotics Group, Spinal Cord Injury Center, Balgrist University Hospital, Medical Faculty, University of Zurich, Zurich, Switzerland","Robotic algorithms that augment movement errors have been proposed as promising training strategies to enhance motor learning and neurorehabilitation. However, most research effort has focused on rehabilitation of upper limbs, probably because large movement errors are especially dangerous during gait training, as they might result in stumbling and falling. Furthermore, systematic large movement errors might limit the participants' motivation during training. In this study, we investigated the effect of training with novel error modulating strategies, which guarantee a safe training environment, on motivation and learning of a modified asymmetric gait pattern. Thirty healthy young participants walked in the exoskeletal robotic system Lokomat while performing a foot target-tracking task, which required an increased hip and knee flexion in the dominant leg. Learning the asymmetric gait pattern with three different strategies was evaluated: (i) No disturbance: no robot disturbance/guidance was applied, (ii) haptic error amplification: unsafe and discouraging large errors were limited with haptic guidance, while haptic error amplification enhanced awareness of small errors relevant for learning, and (iii) visual error amplification: visually observed errors were amplified in a virtual reality environment. We also evaluated whether increasing the movement variability during training by adding randomly varying haptic disturbances on top of the other training strategies further enhances learning. We analyzed participants' motor performance and self-reported intrinsic motivation before, during and after training. We found that training with the novel haptic error amplification strategy did not hamper motor adaptation and enhanced transfer of the practiced asymmetric gait pattern to free walking. Training with visual error amplification, on the other hand, increased errors during training and hampered motor learning. Participants who trained with visual error amplification also reported a reduced perceived competence. Adding haptic disturbance increased the movement variability during training, but did not have a significant effect on motor adaptation, probably because training with haptic disturbance on top of visual and haptic error amplification decreased the participants' feelings of competence. The proposed novel haptic error modulating controller that amplifies small task-relevant errors while limiting large errors outperformed visual error augmentation and might provide a promising framework to improve robotic gait training outcomes in neurological patients. Copyright © 2019 Marchal-Crespo, Tsangaridis, Obwegeser, Maggioni and Riener. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.","Error amplification; Force disturbance; Haptic guidance; Motor adaptation; Motor learning; Rehabilitation robotics; Robotic gait-training; Visual feedback","adult; article; awareness; clinical article; controlled study; error; eye tracking; female; foot; gait; hip; human; knee function; male; modulation; motivation; motor learning; motor performance; nervous system; outcome assessment; robotics; virtual reality; visual feedback",Article,"Final","",Scopus,2-s2.0-85065839983
"Torre I.","57220754587;","Keynote talk: Augmented, adaptive, accessible, and inclusive things",2019,"CEUR Workshop Proceedings","2327",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063196799&partnerID=40&md5=77dcccbf2068b947019e5f909dcfd3db","Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Italy","Torre, I., Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Italy","In this keynote talk, I will illustrate the main principles of the W3C Web of Things (WoT) paradigm and I will discuss the issue of enhanced accessibility through the WoT and adaptation techniques. In this new age of the Internet of Things (IoT), people and things are increasingly immersed in a computing environment that is aimed to simplify and improve daily activities. Meanwhile, the Web is evolving, application logic and data is increasingly distributed, the new rich web applications and liquid software are offering enhanced user experience [3]. Web-based methods and open standards are seen as a mean to augment the interaction with things and to increase the interoperability across IoT platforms, respectively [2]. In 2017 the W3C launched the Web of Things (WoT) Working Group aimed to provide standards that describe things as a basis for semantic interoperability and discovery and that simplify application development through a common interaction model independent of the underlying protocols. The WoT is based upon the fundamentals of Web architecture. WoT applications are programs that either expose a thing and implement a thing’s behavior, or interact with a thing using APIs for control of sensors and actuators and access to associated metadata. The Thing Description (TD) is a central building block of the WoT; its core component is the interaction model defined in terms of properties, actions and events, whose semantics is specified in the TD vocabulary. In the WoT paradigm, things are virtual representations of physical digital objects but also of non-digital things, such as people, places, and everyday objects. This is an extraordinary opportunity to increase accessibility and usability of real world objects. The idea is that through the virtualization of physical objects, even (digital or non-digital) objects which are not natively accessible and inclusive, can become accessible if proper adaptations are performed, for instance by changing the user interface and the interaction modalities in order to fit the user’s needs [4–6]. This would enable an open ecosystem of digitally augmented physical objects that I like to call AAAI Things (Augmented, Adaptive, Accessible and Inclusive Things) with a clear reference in the acronym to the role of AI1. AI is indeed a key component for the adaptation task, aimed to tune and personalize the user interface and the interaction with the objects. Current W3C\WAI standards offer support for accessibility and universality. Moreover, Accessibility APIs are used to communicate semantic information about the user interface to assistive technology software used by people with disabilities. However, this could be not enough to support fine-tuned adaptations. Besides the fact that the standards above might require extensions to manage accessibility features on IoT devices [1], a further problem is that disabilities are heterogeneous and often a subject has more than one disability, which requires specific adaptations. This is also the case of cognitive impairment, where for instance content adaptation might be required2. To address this issue, already in 2014 [2] we proposed an approach that virtualizes physical objects on the WoT and adapts the interaction with the virtual side of cyber-physical objects in order to make physical objects accessible. Then, in [3] we also described how our approach of object virtualization and annotation could exploit and extend the Global Public Inclusive Infrastructure (GPII), which enables the transfer of platform-independent user preferences and needs from one device to another via a cloud service. Much research must still be done to finalize WoT standardization and to update web accessibility standards. Personalization for the WoT is still at early stage as well, but a fast development is expected, driven by the rapid evolution of the IoT. © 2019 for the individual papers by the papers’ authors. Copying permitted for private and academic purposes. This volume is published and copyrighted by its editors.","Accessibility; Adaptive systems; Linked data; Web of things","Adaptive systems; Application programming interfaces (API); Application programs; Computation theory; Interoperability; Linked data; Semantics; User interfaces; Virtual addresses; Virtual reality; Virtualization; Websites; Accessibility; Application development; Computing environments; Internet of thing (IOT); People with disabilities; Semantic interoperability; Sensors and actuators; Virtual representations; Internet of things",Conference Paper,"Final","",Scopus,2-s2.0-85063196799
"Zhu D., Zhou Q., Han T., Chen Y.","57213510302;35096857600;57195952219;57221457399;","360 Degree Panorama Synthesis from Sequential Views Based on Improved FC-Densenets",2019,"IEEE Access","7",, 8926406,"180503","180511",,1,"10.1109/ACCESS.2019.2958111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077180697&doi=10.1109%2fACCESS.2019.2958111&partnerID=40&md5=fbd2a5bd98ad8ca7fcca16c8373f3968","Artificial Intelligence Institute, Shanghai Jiao Tong University, Shanghai, 200240, China; School of Information and Computer, Shanghai Business School, Shanghai, 201400, China; Department of Computer Science, Stevens Institute of Technology, Hoboken, NJ  07030, United States; Hainan Air Traffic Management Sub-Bureau, Haikou, 570000, China","Zhu, D., Artificial Intelligence Institute, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhou, Q., School of Information and Computer, Shanghai Business School, Shanghai, 201400, China; Han, T., Department of Computer Science, Stevens Institute of Technology, Hoboken, NJ  07030, United States; Chen, Y., Hainan Air Traffic Management Sub-Bureau, Haikou, 570000, China","Inspired by the effectiveness of deep learning model, many panorama saliency prediction models based on deep learning began to emerge and achieved significant performance improvement. However, this kind of model requires a large number of labeled ground-truth data, and the existing panorama datasets are small-scale and difficult to train the deep learning models. To address this problem, we propose a novel panorama generative model for synthesizing realistic and sharp-looking panorama. In particular, our proposed panorama generative model consists of two sub-networks of generator and discriminator. At first, in order to make the synthesized panorama more realistic, we employ the improved Fully-Convolutional Densely Connected Convolutional Networks (FC-DenseNets) as the generator network. Secondly, we design a new correlation layer in the discriminator network, which can calculate the similarity between the generated image and the ground-truth image, and achieve the pixel level accuracy. The experimental results show that our proposed method outperforms other baseline work and has superior generalization ability to synthesize real-world data. © 2013 IEEE.","correlation layer; generative model; panorama; saliency prediction; Virtual reality","Convolution; Discriminators; Large dataset; Virtual reality; Convolutional networks; Generalization ability; Generative model; Ground truth data; Learning models; New correlations; panorama; Prediction model; Deep learning",Article,"Final","",Scopus,2-s2.0-85077180697
"Ali Z., Jiao L., Baker T., Abbas G., Abbas Z.H., Khaf S.","57190403100;35868065100;34267505000;57203423498;35202913400;57213265576;","A deep learning approach for energy efficient computational offloading in mobile edge computing",2019,"IEEE Access","7",, 8866714,"149623","149633",,16,"10.1109/ACCESS.2019.2947053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077747923&doi=10.1109%2fACCESS.2019.2947053&partnerID=40&md5=214484a32c8ec52cb16f1ac205932a2d","Telecommunications and Networking (TeleCoN) Research Laboratory, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan; Department of Information and Communication Technology, University of Agder (UiA), Grimstad, 4898, Norway; Department of Computer Science, Faculty of Engineering and Technology, Liverpool John Moores University, Liverpool, L3 3AF, United Kingdom; Faculty of Computer Sciences and Engineering, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan; Faculty of Electrical Engineering, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan","Ali, Z., Telecommunications and Networking (TeleCoN) Research Laboratory, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan; Jiao, L., Department of Information and Communication Technology, University of Agder (UiA), Grimstad, 4898, Norway; Baker, T., Department of Computer Science, Faculty of Engineering and Technology, Liverpool John Moores University, Liverpool, L3 3AF, United Kingdom; Abbas, G., Faculty of Computer Sciences and Engineering, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan; Abbas, Z.H., Faculty of Electrical Engineering, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan; Khaf, S., Faculty of Electrical Engineering, GIK Institute of Engineering Sciences and Technology, Topi, 23640, Pakistan","Mobile edge computing (MEC) has shown tremendous potential as a means for computationally intensive mobile applications by partially or entirely offloading computations to a nearby server to minimize the energy consumption of user equipment (UE). However, the task of selecting an optimal set of components to offload considering the amount of data transfer as well as the latency in communication is a complex problem. In this paper, we propose a novel energy-efficient deep learning based offloading scheme (EEDOS) to train a deep learning based smart decision-making algorithm that selects an optimal set of application components based on remaining energy of UEs, energy consumption by application components, network conditions, computational load, amount of data transfer, and delays in communication. We formulate the cost function involving all aforementioned factors, obtain the cost for all possible combinations of component offloading policies, select the optimal policies over an exhaustive dataset, and train a deep learning network as an alternative for the extensive computations involved. Simulation results show that our proposed model is promising in terms of accuracy and energy consumption of UEs. © 2013 IEEE.","Computational offloading; deep learning; energy efficient offloading; mobile edge computing; user equipment","Computational efficiency; Cost functions; Data transfer; Decision making; Edge computing; Energy efficiency; Energy utilization; Green computing; Application components; Computational loads; Computational offloading; Decision-making algorithms; Energy efficient; Mobile applications; Offloading computations; User equipments; Deep learning",Article,"Final","",Scopus,2-s2.0-85077747923
"Martin-Brualla R., Pandey R., Yang S., Pid-Lypenskyi P., Taylor J., Valentin J., Khamis S., Davidson P., Tkach A., Lincoln P., Kowdle A., Rhemann C., Goldman D.B., Keskin C., Seitz S., Izadi S., Fanello S.","36028239400;57204394554;57195590659;57208902558;55365332000;55194433600;55365976700;7202139211;56954663700;57208428325;35173101000;8380954300;13007964200;24477101000;7004774929;16426108500;36170215300;","Lookingood: Enhancing performance capture with real-time neural re-rendering",2018,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",,, 255,"","",,17,"10.1145/3272127.3275099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066100552&doi=10.1145%2f3272127.3275099&partnerID=40&md5=05498ecd06d7b289b7346712c0962ecc","Google Inc., United States","Martin-Brualla, R., Google Inc., United States; Pandey, R., Google Inc., United States; Yang, S., Google Inc., United States; Pid-Lypenskyi, P., Google Inc., United States; Taylor, J., Google Inc., United States; Valentin, J., Google Inc., United States; Khamis, S., Google Inc., United States; Davidson, P., Google Inc., United States; Tkach, A., Google Inc., United States; Lincoln, P., Google Inc., United States; Kowdle, A., Google Inc., United States; Rhemann, C., Google Inc., United States; Goldman, D.B., Google Inc., United States; Keskin, C., Google Inc., United States; Seitz, S., Google Inc., United States; Izadi, S., Google Inc., United States; Fanello, S., Google Inc., United States","Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in real-time performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach neural (re-)rendering, and our live system “LookinGood"". Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation. We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a salient reweighing scheme of the loss function that is able to discard outliers. We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360◦ capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects. The supplementary video is available at http://youtu.be/Md3tdAKoLGU. © 2018 Copyright held by the owner/author(s).","Image denoising; Image enhancement; Re-rendering; Super-resolution","Augmented reality; Image denoising; Image enhancement; Interactive computer graphics; Optical resolving power; Real time systems; Semantics; Textures; Virtual reality; Visual communication; Augmented and virtual realities; Low-resolution textures; Real time constraints; Real time performance; Reconstruction error; Semantic information; Super resolution; Virtual and augmented reality; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-85066100552
"Singh J.M., Wasnik P., Ramachandra R.","17435449200;57194217687;57190835798;","Hessian-based robust ray-tracing of implicit surfaces on GPU",2018,"SIGGRAPH Asia 2018 Technical Briefs, SA 2018",,, a16,"","",,1,"10.1145/3283254.3283287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060537719&doi=10.1145%2f3283254.3283287&partnerID=40&md5=dc03f74a4914aced5756612d1d6787ac","NTNU, Gjøvik, Norway","Singh, J.M., NTNU, Gjøvik, Norway; Wasnik, P., NTNU, Gjøvik, Norway; Ramachandra, R., NTNU, Gjøvik, Norway","In recent years, the Ray Tracing of Implicit Surfaces on a GPU has been studied by many researchers. However, the existing methods have challenges that mainly includes solving for self-intersecting surfaces. General solutions for Ray Tracing suffer from the problem of false roots, and robust solutions are hard to generalize. In this paper, we present a robust algorithm based on Extended Taylor-Test Adaptive Marching Points, which allows a robust rendering of Self-Intersecting Implicit Surfaces on a GPU. We are using the Second Order Taylor Series expansion to alleviate the problem of double-roots in Self-Intersecting Implicit Surfaces. Our approach is simple to implement and is based on the Hessian Matrix of the Implicit Surface that can be attributed to the Hessian Matrix can be used to obtain second-order Taylor Series expansion for the univariate ray-equation. We compare our results using the simulatedground-truth with the smallest step-size possible with the proposed algorithm, and our proposed algorithm gives the best visual results as well as highest SSIM percentage than other approaches. © 2018 Association for Computing Machinery.","Augmented reality; Hessian matrix; Implicit surfaces; Mean curvature; Ray-tracing; Taylor series; Virtual reality","Augmented reality; Graphics processing unit; Interactive computer graphics; Matrix algebra; Taylor series; Virtual reality; General solutions; Hessian matrices; Implicit surfaces; Intersecting surfaces; Mean curvature; Robust algorithm; Robust solutions; Taylor series expansions; Ray tracing",Conference Paper,"Final","",Scopus,2-s2.0-85060537719
"Borisov A., Sieck J., Ashikoto L., Kamenye G., Mwenyo J., Likando N.","57205722205;24722952700;57205718881;57205719402;57205714230;57205717169;","Development of an Efficient, Cost-Reducing Content Management System for Augmented Reality Applications",2018,"ACM International Conference Proceeding Series",,, 3283471,"188","191",,,"10.1145/3283458.3283471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061273218&doi=10.1145%2f3283458.3283471&partnerID=40&md5=74626ef6ba77afd953cd42a0ffea9e5e","Hochschule für Technik und Wirtschaft, Berlin, Germany; Hochschule für Technik und Wirtschaft, Namibia University of Science and Technology, Berlin, Germany; Namibia University of Science and Technology, Windhoek, Namibia","Borisov, A., Hochschule für Technik und Wirtschaft, Berlin, Germany; Sieck, J., Hochschule für Technik und Wirtschaft, Namibia University of Science and Technology, Berlin, Germany; Ashikoto, L., Namibia University of Science and Technology, Windhoek, Namibia; Kamenye, G., Namibia University of Science and Technology, Windhoek, Namibia; Mwenyo, J., Namibia University of Science and Technology, Windhoek, Namibia; Likando, N., Namibia University of Science and Technology, Windhoek, Namibia","In recent years, the use of smartphones as well as other handheld devices has increased and spread exponentially. Despite the increased popularity and distribution of AR, development of AR applications is not a simple endeavor. Not only because it is difficult for people without the technical know-how to implement their own AR application, but also because of the high costs of the maintenance and development of such applications. This paper outlines and details a possible design of a content management system (CMS) for AR-applications, which can be a potential solution to the problem mentioned above. Considering some limitations of the mobile applications, many particularities need to be considered while developing this system. © 2018 Copyright held by the owner/author(s).","Augmented Reality; Content Management System; Mobile Application; Unity; Vuforia","Augmented reality; Knowledge engineering; Mobile computing; Technology transfer; AR application; Augmented reality applications; Content management system; Hand held device; Mobile applications; Technical know hows; Unity; Vuforia; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85061273218
"Radhamani R., Divakar A., Nair A.A., Sivadas A., Mohan G., Nizar N., Nair B., Achuthan K., Diwakar S.","56442543800;57205438385;57205437765;57205436840;57205433917;56565556600;36657466300;35483077800;24511900300;","Virtual Laboratories in Biotechnology are Significant Educational Informatics Tools",2018,"2018 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2018",,, 8554596,"1547","1551",,1,"10.1109/ICACCI.2018.8554596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060014689&doi=10.1109%2fICACCI.2018.8554596&partnerID=40&md5=2c6296b75e803053dd95b7a06f3b75b8","Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Amrita Vishwa Vidyapeetham, Center for Cybersecurity Systems and Networks, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India","Radhamani, R., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Divakar, A., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Nair, A.A., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Sivadas, A., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Mohan, G., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Nizar, N., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Nair, B., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Achuthan, K., Amrita Vishwa Vidyapeetham, Center for Cybersecurity Systems and Networks, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India; Diwakar, S., Amrita Vishwa Vidyapeetham, School of Biotechnology, Amritapuri Campus, Clappana P.O., Kollam, Kerala, 690 525, India","With recent advances in information and communication technologies (ICT), visual perception of knowledge has been crucial in education system. In developing nations, such as India, Government has added initiatives for improving the quality of education, by providing computer-based education to geographically constrained and economically challenged areas, at a minimal cost. For augmenting laboratory training, a national mission project, Virtual laboratories, was launched by India's Ministry of Human Resource Development (MHRD)'s National Mission on Education through Information and Communication Technology (NME-ICT). Through user-interactive animations, mathematical simulations and remote experiments, over 360+ experiments for more than 30 topics, web-based labs were developed and deployed in science and engineering discipline for usage by student-teacher communities. Using pedagogical aspects, this paper reviews the impact of virtual laboratories as an adaptive learning tool and its role in knowledge transfer in a blended classroom environment using cell biology and molecular biology virtual tools. Studies indicate the use of ICT-based virtual lab repository in complementing laboratory skillset and as teaching tools for disseminating online education via Massive Open Online Courses (MOOCs). © 2018 IEEE.","Blended learning; Knowledge transfer; Online platform; Skill training; Virtual laboratories","Cytology; Knowledge management; Laboratories; Molecular biology; Virtual reality; Blended learning; Knowledge transfer; Online platforms; Skill training; Virtual laboratories; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85060014689
"Khuwaja K.S.A., Chowdhry B.S., Khuwaja K.F., Mihalca V.O., Ţarcǎ R.C.","57204898223;18633334900;57204898222;57204126708;36667746600;","Virtual Reality Based Visualization and Training of a Quadcopter by using RC Remote Control Transmitter",2018,"IOP Conference Series: Materials Science and Engineering","444","5", 052008,"","",,2,"10.1088/1757-899X/444/5/052008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057736065&doi=10.1088%2f1757-899X%2f444%2f5%2f052008&partnerID=40&md5=cd03df9af193243d09d10a166f130a6b","Mechatronics Engineering Department, University of Oradea, Romania; Electronics Engineering Department, MUET, Jamshoro, Pakistan; Biomedical Engineering Department, MUET, Jamshoro, Pakistan","Khuwaja, K.S.A., Mechatronics Engineering Department, University of Oradea, Romania; Chowdhry, B.S., Electronics Engineering Department, MUET, Jamshoro, Pakistan; Khuwaja, K.F., Biomedical Engineering Department, MUET, Jamshoro, Pakistan; Mihalca, V.O., Mechatronics Engineering Department, University of Oradea, Romania; Ţarcǎ, R.C., Mechatronics Engineering Department, University of Oradea, Romania","The idea of this research paper is to simulate the flight of a quadcopter-type UAV, which can help to train drone pilots before the actual flight. We designed the quadcopter's model to help improve the future pilot's accuracy up to the point of a stable control, because it is necessary to know how to control the parameters of the quadcopter prior to a real-world scenario. Our system is based on a MATLAB Simulink model. The remote sensing input is given by the RC remote control, based on virtual reality, in order to train the data and output in 3D visualization environment. It has been developed and used for the measurement of the simulated system behavior and its performance. The objective of this research paper is to create a training environment of the quadcopter's flight parameters for a newly trained pilot. The pilot will be able to follow the simulated model and receive visual feedback. Following the simulated model implies stabilizing the moment of input-output. A visual system with real-time human-computer interaction could provide the intended training experience using this model. © 2018 Institute of Physics Publishing. All rights reserved.",,"Data visualization; E-learning; Machinery; MATLAB; Remote control; Remote sensing; Technology transfer; Three dimensional computer graphics; Unmanned aerial vehicles (UAV); Virtual reality; Visual communication; Visual servoing; Visualization; 3D Visualization; Flight parameters; Matlab Simulink models; Real-world scenario; Simulated model; Simulated system; Training experiences; Visual feedback; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85057736065
"Lin H.J., Huang S.-W., Lai S.-H., Chiang C.-K.","57201856600;57204286622;7402937330;55462217500;","Indoor Scene Layout Estimation from a Single Image",2018,"Proceedings - International Conference on Pattern Recognition","2018-August",, 8546278,"842","847",,5,"10.1109/ICPR.2018.8546278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059749828&doi=10.1109%2fICPR.2018.8546278&partnerID=40&md5=bb28a1b626abafb0e384e1c0f5b1c2d0","Dept. Of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Dept. Of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan","Lin, H.J., Dept. Of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Huang, S.-W., Dept. Of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Lai, S.-H., Dept. Of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Chiang, C.-K., Dept. Of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan","With the popularity of the hand devices and intelligent agents, many aimed to explore machine's potential in interacting with reality. Scene understanding, among the many facets of reality interaction, has gained much attention for its relevance in applications such as augmented reality (AR). Scene understanding can be partitioned into several sub tasks (i.e., layout estimation, scene classification, saliency prediction, etc). In this paper, we propose a deep learning-based approach for estimating the layout of a given indoor image in real-time. Our method consists of a deep fully convolutional network, a novel layout-degeneration augmentation method, and a new training pipeline which integrate an adaptive edge penalty and smoothness terms into the training process. Unlike previous deep learning-based methods that depend on post-processing refinement (e.g., proposal ranking and optimization), our method motivates the generalization ability of the network and the smoothness of estimated layout edges without deploying postprocessing techniques. Moreover, the proposed approach is time-efficient since it only takes the model one forward pass to render accurate layouts. We evaluate our method on LSUN Room Layout and Hedau dataset and obtain estimation results comparable with the state-of-the-art methods. © 2018 IEEE.",,"Augmented reality; Intelligent agents; Pattern recognition; Augmentation methods; Convolutional networks; Generalization ability; Learning-based approach; Learning-based methods; Post-processing techniques; Scene classification; State-of-the-art methods; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85059749828
"Deherkar K., Martin G., George N., Maurya V.","55575267000;57206929971;57205293919;57205293847;","Gesture Controlled Virtual Reality Based Conferencing",2018,"2018 International Conference on Smart City and Emerging Technology, ICSCET 2018",,, 8537334,"","",,,"10.1109/ICSCET.2018.8537334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059411790&doi=10.1109%2fICSCET.2018.8537334&partnerID=40&md5=6aa4b5f07daebc5b4567fca9b157c741","Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India","Deherkar, K., Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; Martin, G., Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; George, N., Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India; Maurya, V., Computer Engineering Department, Don Bosco Institute of Technology, Mumbai, India","The technology available today for interacting with a virtual environment involves wired or wireless hand controls with limited buttons or a large setup involving a camera and/or a sensor to capture movements. The cost of such a setup is such that it makes it inaccessible to most. The project aims at providing a cost effective VR solution which can produce the same effect with precision and flexibility, which is accessible to all. The proposed idea is to provide a hardware device that provides the user with an immersive VR experience and uses hand gestures captured via a camera placed on the device to control and interact with the VR environment. As an application of the project, an interactive workspace environment would be simulated, using a single hardware component that provides the user the viewing interface as well as can be controlled by simple hand gestures eliminating the need for additional hand-held devices controls. The project will also delve into the field of supervised learning to make possible the implementation of gesture recognition. © 2018 IEEE.","File transfer; Gesture Recognition; Hand Gesture Controls; Sensor; Supervised learning; Virtual Reality; VOIP; VR","Cameras; Cost effectiveness; Hardware; Sensors; Smart city; Supervised learning; Virtual reality; Cost effective; File transfers; Hand gesture control; Hand held device; Hardware components; Hardware devices; Interactive workspace; VOIP; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85059411790
"Valdez M.T., Ferreira C.M., Maciel Barbosa F.P.","36006535100;55146902800;6603186154;","Application of Virtual Reality Tools in the Teaching of Concepts in Projects of Fast Loading Stations",2018,"2018 28th EAEEIE Annual Conference, EAEEIE 2018",,, 8534239,"","",,,"10.1109/EAEEIE.2018.8534239","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059042151&doi=10.1109%2fEAEEIE.2018.8534239&partnerID=40&md5=a3e0043e2ff361bd98e544fd3fd729fa","Departamento de Engenharia Eletrotecnica, Coimbra Polytechnic - ISEC, Coimbra, Portugal; Faculdade de Engenharia da Universidade Do Porto and INESC TEC Porto, Departamento de Engenharia Eletrotecnica e de Computadores, Porto, Portugal","Valdez, M.T., Departamento de Engenharia Eletrotecnica, Coimbra Polytechnic - ISEC, Coimbra, Portugal; Ferreira, C.M., Departamento de Engenharia Eletrotecnica, Coimbra Polytechnic - ISEC, Coimbra, Portugal; Maciel Barbosa, F.P., Faculdade de Engenharia da Universidade Do Porto and INESC TEC Porto, Departamento de Engenharia Eletrotecnica e de Computadores, Porto, Portugal","The classroom will always present great challenges to the teachers. It is important to know how to provide learning to a new generation of students and to try to understand how students learn today. Each student arrives at school with a diversity of knowledge resulting from their own life experience, probably divergent from that of their colleagues. On the other hand, the domain of the new Information and Communication Technologies (ICTs) is part of their own experience. The teacher/student relationship is becoming more and digital and, therefore, the limitations of space are more and more reduced. ICTS have greatly altered the way students have access to knowledge, how they share it or how they simply organize the work. Thus, traditional pedagogical strategies and processes must integrate technologies, as a way to apply new work methodologies that value the students' reflection and critical thinking. In return, technology will bring them closer to knowledge, increasing their autonomy and consequent motivation. This work aimed at introducing the topic of sustainable mobility, so that electric vehicles become a growing reality in everyday life. A power generation system was proposed using a grid of photovoltaic panels, which in turn will allow the sustained feeding of a quick charging station. The surplus of electricity produced by the grid of panels can be used to meet the energy needs of a service station. A varied range of software tools were used to address the challenge. This article aims at discussing the use of different software in the educational environment, highlighting the advantages, as well as the difficulties associated with the approach. Several educational software types were used in the final project of a master's degree in ISEC (Instituto Superior de Engenharia de Coimbra). Software tools are an effective way to train students. The use of virtual reality tools in concept teaching of fast charging stations was quite effective in the improvement of students. © 2018 IEEE.","Fast loading stations; Photovoltaic panels; Skelion; Sketch Up; Virtual reality tools","Computer software; E-learning; Electric power transmission networks; Photovoltaic cells; Technology transfer; Virtual reality; Fast loading; Photovoltaic panels; Skelion; Sketch Up; Virtual reality tools; Students",Conference Paper,"Final","",Scopus,2-s2.0-85059042151
"Martin-Brualla R., Pandey R., Yang S., Pidlypenskyi P., Taylor J., Valentin J., Khamis S., Davidson P., Tkach A., Lincoln P., Kowdle A., Rhemann C., Goldman D.B., Keskin C., Seitz S., Izadi S., Fanello S.","36028239400;57204394554;57195590659;57204393393;55365332000;55194433600;55365976700;7202139211;56954663700;57208428325;35173101000;8380954300;13007964200;24477101000;7004774929;16426108500;36170215300;","LookinGood: Enhancing performance capture with real-time neural re-rendering",2018,"ACM Transactions on Graphics","37","6", 255,"","",,15,"10.1145/3272127.3275099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064840980&doi=10.1145%2f3272127.3275099&partnerID=40&md5=1fd678ed6e1773f50cb1f650b6898252","Google Inc., United States","Martin-Brualla, R., Google Inc., United States; Pandey, R., Google Inc., United States; Yang, S., Google Inc., United States; Pidlypenskyi, P., Google Inc., United States; Taylor, J., Google Inc., United States; Valentin, J., Google Inc., United States; Khamis, S., Google Inc., United States; Davidson, P., Google Inc., United States; Tkach, A., Google Inc., United States; Lincoln, P., Google Inc., United States; Kowdle, A., Google Inc., United States; Rhemann, C., Google Inc., United States; Goldman, D.B., Google Inc., United States; Keskin, C., Google Inc., United States; Seitz, S., Google Inc., United States; Izadi, S., Google Inc., United States; Fanello, S., Google Inc., United States","Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in real-time performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach neural (re-)rendering, and our live system ""LookinGood"". Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation.We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a salient reweighing scheme of the loss function that is able to discard outliers.We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360° capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects. The supplementary video is available at http://youtu.be/Md3tdAKoLGU. © 2018 Copyright held by the owner/author(s).","Image denoising; Image enhancement; Re-rendering; Super-resolution","Augmented reality; Image denoising; Image enhancement; Optical resolving power; Real time systems; Semantics; Textures; Virtual reality; Visual communication; Augmented and virtual realities; Low-resolution textures; Real time constraints; Real time performance; Reconstruction error; Semantic information; Super resolution; Virtual and augmented reality; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85064840980
"Sun L., Zhou Y., Hansen P., Geng W., Li X.","55492960600;57202023332;8862059000;35205280800;57202025304;","Cross-objects user interfaces for video interaction in virtual reality museum context",2018,"Multimedia Tools and Applications","77","21",,"29013","29041",,4,"10.1007/s11042-018-6091-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046717130&doi=10.1007%2fs11042-018-6091-5&partnerID=40&md5=e8d9ecfa78ebb388228c207d63d520a5","International Design Institute, Zhejiang University, Hangzhou, China; Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China; Department of Computer Science and Systems, Stockholm University, Kista, Sweden; Department of Digital Media, Zhejiang University, Hangzhou, China","Sun, L., International Design Institute, Zhejiang University, Hangzhou, China; Zhou, Y., International Design Institute, Zhejiang University, Hangzhou, China, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China; Hansen, P., Department of Computer Science and Systems, Stockholm University, Kista, Sweden; Geng, W., Department of Digital Media, Zhejiang University, Hangzhou, China; Li, X., Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China, Department of Digital Media, Zhejiang University, Hangzhou, China","Museums are good places for learning and nowadays many museums are integrating digital media such as video and increasingly moving towards using virtual reality. In the physical world people used to seek information from object surfaces e.g. posters on the wall and this has been used as a metaphor in the virtual reality museum: numerous videos were inhabited within virtual objects and shaped cross-objects user interfaces (COUIs). However, how such interfaces perform for video interactions still needs more investigations. In this study we implemented and investigated COUIs in comparison with the conventional card-style user interfaces and the plain virtual reality user interfaces in the virtual reality museum. The results reported no significant differences in the perceived usability or learning experience between these user interfaces, except the COUIs had a lower level of satisfaction than the card-style user interfaces. However, the COUIs showed greater efficiency with shorter eye fixation durations and higher saccade frequencies, and within these COUIs instances, namely the fully-detached, semi-attached, and fully-attached COUIs, the fully-attached instance was closest to the form of interacting with physical object surfaces and it reported highest efficiency as well. Rationales behind these results and implications generalising for the future design of COUIs, are discussed. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Cross-objects user interfaces; Museum learning experience; Perceived usability; Video interaction; Virtual reality museum","Digital storage; Efficiency; Virtual reality; Future designs; Learning experiences; Level of satisfaction; Museum learning; Perceived usability; Physical objects; Video interactions; Virtual objects; User interfaces",Article,"Final","",Scopus,2-s2.0-85046717130
"Bortman J., Baribeau Y., Jeganathan J., Amador Y., Mahmood F., Shnider M., Ahmed M., Hess P., Matyal R.","56986258900;57193893121;57044628400;57189227780;36600995600;56231210600;7402831318;23977969900;12039930500;","Improving Clinical Proficiency Using a 3-Dimensionally Printed and Patient-Specific Thoracic Spine Model as a Haptic Task Trainer",2018,"Regional Anesthesia and Pain Medicine","43","8",,"819","824",,9,"10.1097/AAP.0000000000000821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055078763&doi=10.1097%2fAAP.0000000000000821&partnerID=40&md5=0816a9202de3e154fab5040559966c8b","Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Division of Cardiac Surgery, Department of Surgery, United States; Department of Vascular and Interventional Radiology, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States","Bortman, J., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Baribeau, Y., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Jeganathan, J., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Amador, Y., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Mahmood, F., Division of Cardiac Surgery, Department of Surgery, United States; Shnider, M., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Ahmed, M., Department of Vascular and Interventional Radiology, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States; Hess, P., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States; Matyal, R., Department of Anesthesia, Critical Care and Pain Medicine, Beth Israel Deaconess Medical Center, 330 Brookline Ave, Boston, MA  02215, United States","Background and Objectives Advanced haptic simulators for neuraxial training are expensive, have a finite life, and are not patient specific. We sought to demonstrate the feasibility of developing a custom-made, low-cost, 3-dimensionally printed thoracic spine simulator model from patient computed tomographic scan data. This study assessed the model's practicality, efficiency as a teaching tool, and the transfer of skill set into patient care. Methods A high-fidelity, patient-specific thoracic spine model was used for the study. Thirteen residents underwent a 1-hour 30-minute training session prior to performing thoracic epidural analgesia (TEA) on patients. We observed another group of 14 residents who were exposed to the traditional method of training during their regional anesthesia rotation for thoracic epidural placement. The TEA was placed for patients under the supervision of attending anesthesiologists, who were blinded to the composition of the study and control groups. As a primary outcome, data were collected on successful TEAs, which was defined as a TEA that provided full relief of sensation across the entire surgical area as assessed by both a pinprick and temperature test. Secondary outcomes included whether any assistance from the attending physician was required and failed epidurals. Results A total of 27 residents completed the study (14 in the traditional training, 13 in the study group). We found that the residents who underwent training with the simulator had a significantly higher success rate (11 vs 4 successful epidural attempts, P = 0.002) as compared with the traditional training group. The control group also required significantly more assistance from the supervising anesthesiologist compared with the study group (5 vs 1 attempt requiring guidance). The number needed to treat (NNT) for the traditional training group was 1.58 patients over the study period with a 95% confidence interval of 1.55 to 1.61. Conclusions By using patient-specific, 3-dimensionally printed, thoracic spine models, we demonstrated a significant improvement in clinical proficiency as compared with traditional teaching models. © 2018 Lippincott Williams & Wilkins.",,"anesthesiologist; Article; curriculum; epidural analgesia; feasibility study; human; patient care; pinprick test; priority journal; regional anesthesia; residency education; simulation training; skill; temperature; thoracic epidural analgesia; thoracic spine; three dimensional printing; anatomic model; anatomy and histology; anesthesia; clinical competence; medical education; point of care system; procedures; standards; thoracic vertebra; three dimensional printing; Anesthesia; Clinical Competence; Humans; Internship and Residency; Models, Anatomic; Point-of-Care Systems; Printing, Three-Dimensional; Thoracic Vertebrae",Article,"Final","",Scopus,2-s2.0-85055078763
"Burloiu G.","56512445500;","Adapting the SoundThimble movement sonification system for young motion-impaired users",2018,"Proceedings - 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing, ICCP 2018",,, 8516435,"153","157",,,"10.1109/ICCP.2018.8516435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057376244&doi=10.1109%2fICCP.2018.8516435&partnerID=40&md5=96039f87304984a469af1c8b37887ea2","CINETic, UNATC I.L. Caragiale, Bucharest, Romania","Burloiu, G., CINETic, UNATC I.L. Caragiale, Bucharest, Romania","SoundThimble is an interactive sound installation which uses motion capture and machine learning to establish relationships between human movement and virtual objects in 3D space. This paper documents the strategy for adapting this system for interacting with children with physical and cognitive impairments. Starting from a specific child subject, we show how the hardware, software and interaction design can be modified, with the view of generalising to a wider range of young disabled users. The project's ultimate goal is threefold: inclusion, entertainment, and rehabilitation. © 2018 IEEE.",,"Learning systems; Cognitive impairment; Human movements; Interaction design; Interactive sound installation; Motion capture; Motion-impaired; Paper documents; Virtual objects; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85057376244
"Plecher D.A., Eichhorn C., Kindl J., Kreisig S., Wintergerst M., Klinker G.","55923283100;57203128334;57205079540;57205075707;57205083238;6603530980;","Dragon tale – A serious game for learning Japanese kanji",2018,"CHI PLAY 2018 - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts",,,,"577","583",,9,"10.1145/3270316.3271536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058490255&doi=10.1145%2f3270316.3271536&partnerID=40&md5=fcea34c79a5b82d0f144427776c3bf82","Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany","Plecher, D.A., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany; Eichhorn, C., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany; Kindl, J., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany; Kreisig, S., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany; Wintergerst, M., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany; Klinker, G., Technical University of Munich, Chair for Computer Aided Medical Procedures and Augmented, Reality Institute for Informatics / I16, Garching b. München, D-85748, Germany","To counter the difficult task of studying kanji when learning the Japanese language, we suggest an adventure style Serious Game. Previous solutions are focused on small 2D games or gamification of virtual kanji trainers. Our goal is to utilize the concept of flow by immersing the player in a rich storyline with Japanese mythology, turn-based encounters and various mini games to train all aspects of kanji: stroke order, meaning, pronunciations and compound words. We further introduce Augmented Reality to the language acquisition as an innovative way to combine virtual content with the real world. A first test revealed great potential of such an approach and feedback for further development. © 2018 Copyright is held by the owner/author(s).","Augmented Reality; Japanese; Kanji; Knowledge Transfer; Language; Mobile Devices; Role Playing Game; Serious Game","Augmented reality; Human computer interaction; Interactive computer systems; Knowledge management; Mobile devices; Japanese; Kanji; Knowledge transfer; Language; Role-playing game; Serious games",Conference Paper,"Final","",Scopus,2-s2.0-85058490255
"Zou Q., Li H., Zhang R.","54406213100;57201675742;7404861962;","Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling",2018,"IEEE Intelligent Vehicles Symposium, Proceedings","2018-June",, 8500666,"1245","1250",,2,"10.1109/IVS.2018.8500666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056785788&doi=10.1109%2fIVS.2018.8500666&partnerID=40&md5=2956ed31f8466b3b023f7e03611d0a13","BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Zou, Q., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Li, H., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Zhang, R., Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Inverse Reinforcement Learning(IRL) is formulated within the framework of Markov decision process(MDP) where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. Then the expert as trying to maximize a reward function that is expressible as a linear combination of known features specifying the reward function. However, in autonomous driving tasks, due to the difference of scene factor, such as obstacle and weather, the state spaces are frequently large and demonstrations can hardly visit all the states. it's hard to get an optimal policy with RL method to express driver behavior model based on the reward which recovered with IRL method in this large-scale state space. In this paper, we focus on driving behavior modeling with IRL method which introduces the convolutional neural network to extract the associated state feature automatically, and express the policy by neural network to generalize the expert's behaviors. Experimental results compared with the traditional end-to-end method on simulated vehicle show that the accuracy of decision-making greatly improved in the train curve, and in the new curve scene with a large number of unvisited state, this method shows a perfect generalization efficiency. © 2018 IEEE.","Advanced Driver Assistance Systems; Automated Vehicles; Human Factors and Human Machine Interaction","Automobile drivers; Behavioral research; Decision making; Human computer interaction; Inverse problems; Markov processes; Neural networks; Reinforcement learning; Vehicles; Accuracy of decision makings; Automated vehicles; Convolutional neural network; Driver behavior modeling; Human machine interaction; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85056785788
"Škola F., Liarokapis F.","57189368470;7801416785;","Embodied VR environment facilitates motor imagery brain–computer interface training",2018,"Computers and Graphics (Pergamon)","75",,,"59","71",,12,"10.1016/j.cag.2018.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049323107&doi=10.1016%2fj.cag.2018.05.024&partnerID=40&md5=561e34077968d8ba1953764e02e33b25","Faculty of Informatics, Masaryk University, Botanická 68a, Brno, Czech Republic","Škola, F., Faculty of Informatics, Masaryk University, Botanická 68a, Brno, Czech Republic; Liarokapis, F., Faculty of Informatics, Masaryk University, Botanická 68a, Brno, Czech Republic","Motor imagery (MI) is the predominant control paradigm for brain–computer interfaces (BCIs). After sufficient effort is invested to the training, the accuracy of commands mediated by mental imagery of bodily movements grows to a satisfactory level. However, many issues with the MI-BCIs persist; e.g., low bit transfer rate, BCI illiteracy, sub-optimal training procedure. Especially the training process for the MI-BCIs requires improvements. Currently, the training has an inappropriate form, resulting in a high mental and temporal demand on the users (weeks of training are required for the control). This study aims at addressing the issues with the MI-BCI training. To support the learning process, an embodied training environment was created. Participants were placed into a virtual reality environment observed from a first-person view of a human-like avatar, and their rehearsal of MI actions was reflected by the corresponding movements performed by the avatar. Leveraging extension of the sense of ownership, agency, and self-location towards a non-body object (principles known from the rubber hand illusion and the body transfer illusions) has already been proven to help in producing stronger EEG correlates of MI. These principles were used to facilitate the MI-BCI training process for the first time. Performance of 30 healthy participants after two sessions of training was measured using an on-line BCI scenario. The group trained using our embodied VR environment gained significantly higher average accuracy for BCI actions (58.3%) than the control group, trained with a standard MI-BCI training protocol (52.9%). © 2018 Elsevier Ltd","Body transfer illusion; Brain–Computer interfaces; Embodiment; Motor imagery; Rubber hand illusion; Virtual reality","Interfaces (computer); Rubber; Virtual reality; Bit transfer rates; Body transfer illusion; Embodiment; Learning process; Motor imagery; Optimal training; Predominant control; Virtual-reality environment; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85049323107
"Peterson S.M., Rios E., Ferris D.P.","57194682956;57203844599;35236545200;","Transient visual perturbations boost short-term balance learning in virtual reality by modulating electrocortical activity",2018,"Journal of Neurophysiology","120","4",,"1998","2010",,12,"10.1152/jn.00292.2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054775411&doi=10.1152%2fjn.00292.2018&partnerID=40&md5=6bb8b6acedd0dbcf8cee246dcfe15e9e","Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville, FL, United States","Peterson, S.M., Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; Rios, E., Department of Biomedical Engineering, School of Engineering, University of Michigan, Ann Arbor, MI, United States; Ferris, D.P., J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville, FL, United States","Immersive virtual reality can expose humans to novel training and sensory environments, but motor training with virtual reality has not been able to improve motor performance as much as motor training in real-world conditions. An advantage of immersive virtual reality that has not been fully leveraged is that it can introduce transient visual perturbations on top of the visual environment being displayed. The goal of this study was to determine whether transient visual perturbations introduced in immersive virtual reality modify electrocortical activity and behavioral outcomes in human subjects practicing a novel balancing task during walking. We studied three groups of healthy young adults (5 male and 5 female for each) while they learned a balance beam walking task for 30 min under different conditions. Two groups trained while wearing a virtual reality headset, and one of those groups also had half-second visual rotation perturbations lasting ~10% of the training time. The third group trained without virtual reality. We recorded high-density electroencephalography (EEG) and movement kinematics. We hypothesized that virtual reality training with perturbations would increase electrocortical activity and improve balance performance compared with virtual reality training without perturbations. Our results confirmed the hypothesis. Brief visual perturbations induced increased theta spectral power and decreased alpha spectral power in parietal and occipital regions and improved balance performance in posttesting. Our findings indicate that transient visual perturbations during immersive virtual reality training can boost short-term motor learning by inducing a cognitive change, minimizing the negative effects of virtual reality on motor training. NEW & NOTEWORTHY We found that transient visual perturbations in virtual reality during balance training can boost short-term motor learning by inducing a cognitive change, overcoming the negative effects of immersive virtual reality. As a result, subjects training in immersive virtual reality with visual perturbations have equivalent performance improvement as training in real-world conditions. Visual perturbations elicited cortical responses in occipital and parietal regions and may have improved the brain’s ability to adapt to variations in sensory input. © 2018 American Physiological Society. All rights reserved.","Adaptive generalization; Balance training; Electroencephalography; Motor learning; Virtual reality","adult; article; case report; clinical article; electroencephalography; female; human; human experiment; kinematics; male; motor learning; occipital cortex; rotation; sensory stimulation; virtual reality; walking; young adult",Article,"Final","",Scopus,2-s2.0-85054775411
"Sharma R., Biookaghazadeh S., Li B., Zhao M.","57202992244;57188843878;7410080679;55130283100;","Are existing knowledge transfer techniques effective for deep learning with edge devices?",2018,"Proceedings - 2018 IEEE International Conference on Edge Computing, EDGE 2018 - Part of the 2018 IEEE World Congress on Services",,, 8473375,"42","49",,12,"10.1109/EDGE.2018.00013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055667496&doi=10.1109%2fEDGE.2018.00013&partnerID=40&md5=aa4acdcdd5622964234c89931b58b313","Arizona State University, Tempe, AZ, United States","Sharma, R., Arizona State University, Tempe, AZ, United States; Biookaghazadeh, S., Arizona State University, Tempe, AZ, United States; Li, B., Arizona State University, Tempe, AZ, United States; Zhao, M., Arizona State University, Tempe, AZ, United States","With the emergence of edge computing paradigm, many applications such as image recognition and augmented reality require to perform machine learning (ML) and artificial intelligence (AI) tasks on edge devices. Most AI and ML models are large and computational-heavy, whereas edge devices are usually equipped with limited computational and storage resources. Such models can be compressed and reduced for deployment on edge devices, but they may lose their capability and not perform well. Recent works used knowledge transfer techniques to transfer information from a large network (termed teacher) to a small one (termed student) in order to improve the performance of the latter. This approach seems to be promising for learning on edge devices, but a thorough investigation on its effectiveness is lacking. This paper provides an extensive study on the performance (in both accuracy and convergence speed) of knowledge transfer, considering different student architectures and different techniques for transferring knowledge from teacher to student. The results show that the performance of KT does vary by architectures and transfer techniques. A good performance improvement is obtained by transferring knowledge from both the intermediate layers and last layer of the teacher to a shallower student. But other architectures and transfer techniques do not fare so well and some of them even lead to negative performance impact. © 2018 IEEE.","Cloud computing; Deep neural networks; Edge computing; Knowledge transfer","Artificial intelligence; Augmented reality; Cloud computing; Deep neural networks; Edge computing; Image recognition; Knowledge management; Network architecture; Students; Computing paradigm; Intermediate layers; Knowledge transfer; Performance impact; Performance improvements; Storage resources; Transfer information; Transfer technique; Teaching",Conference Paper,"Final","",Scopus,2-s2.0-85055667496
"Panerai S., Catania V., Rundo F., Ferri R.","6506285735;57159156400;9633894700;8759218700;","Remote home-based virtual training of functional living skills for adolescents and young adults with intellectual disability: Feasibility and preliminary results",2018,"Frontiers in Psychology","9","SEP", 1730,"","",,3,"10.3389/fpsyg.2018.01730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053467322&doi=10.3389%2ffpsyg.2018.01730&partnerID=40&md5=62167ff72b289c53805c7d984d6a547e","Unit of Psychology, Oasi Research Institute - IRCCS, Troina, Italy; Unit of Neurology, Oasi Research Institute - IRCCS, Troina, Italy","Panerai, S., Unit of Psychology, Oasi Research Institute - IRCCS, Troina, Italy; Catania, V., Unit of Psychology, Oasi Research Institute - IRCCS, Troina, Italy; Rundo, F., Unit of Neurology, Oasi Research Institute - IRCCS, Troina, Italy; Ferri, R., Unit of Neurology, Oasi Research Institute - IRCCS, Troina, Italy","Background: Virtual Reality (VR) is acquiring increasing credibility as a tool for teaching independent living skills to people with Intellectual Disability (ID). Generalization of skills acquired during VR training into real environment seems to be feasible. Objective: To assess feasibility and verify effectiveness of a remote home-based rehabilitation, focused on functional living skills, for adolescents and young adults with ID, by using virtual apps installed on tablets. In particular, to assess if this tool can be managed independently, if it is enjoyable and simple to be used, and if the acquired skills can be generalized to the real environment of everyday life. Subjects and method: A single group, pre- and post-test research design was used. Sixteen participants with ID were included. A digital system was arranged, with a server managing communication between the database and the apps installed on tablets. In vivo tests were performed before and after the eleven sessions of VR training. Satisfaction questionnaires were also administered. Results: Statistically significant improvements were found between the pre- and post-in vivo tests, as well as between the VR training sessions, in almost all the parameters taken into account, for each app. Final questionnaires showed a good satisfaction level for both the participants and their families. Conclusion: The highly technological system was managed independently by participants with ID, who found it simple to be used, useful and even fun; generalization across settings was obtained. Results obtained require to be confirmed by future controlled studies, with larger samples. © 2018 Panerai, Catania, Rundo and Ferri.","Functional living skills; Home-based rehabilitation; Intellectual disability; Remote rehabilitation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85053467322
"Ortiz-Catalan M.","55253692000;","The stochastic entanglement and phantom motor execution hypotheses: A theoretical framework for the origin and treatment of Phantom limb pain",2018,"Frontiers in Neurology","9","SEP", 748,"","",,12,"10.3389/fneur.2018.00748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053024377&doi=10.3389%2ffneur.2018.00748&partnerID=40&md5=1d680cf611abc081e5d51ff41a8177f6","Biomechatronics and Neurorehabilitation Laboratory, Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Integrum AB, Mölndal, Sweden","Ortiz-Catalan, M., Biomechatronics and Neurorehabilitation Laboratory, Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden, Integrum AB, Mölndal, Sweden","Phantom limb pain (PLP) is a debilitating condition common after amputation that can considerably hinder patients' quality of life. Several treatments have reported promising results in alleviating PLP. However, clinical evaluations are usually performed in small cohorts and rigorous clinical trials are scarce. In addition, the underlying mechanisms by which novel interventions alleviate PLP are often unclear, potentially because the condition itself is poorly understood. This article presents a theoretical framework of PLP that can be used as groundwork for hypotheses of novel treatments. Current hypotheses on the origins of PLP are discussed in relation to available clinical findings. Stochastic entanglement of the pain neurosignature, or connectome, with impaired sensorimotor circuitry is proposed as an alternative hypothesis for the genesis of PLP, and the implications and predictions this hypothesis entails are examined. In addition, I present a hypothesis for the working mechanism of Phantom Motor Execution (PME) as a treatment of PLP, along with its relation to the aforementioned stochastic entanglement hypothesis, which deals with PLP's incipience. PME aims to reactivate the original central and peripheral circuitry involved in motor control of the missing limb, along with increasing dexterity of stump muscles. The PME hypothesis entails that training of phantom movements induces gradual neural changes similar to those of perfecting a motor skill, and these purposefully induced neural changes disentangle pain processing circuitry by competitive plasticity. This is a testable hypothesis that can be examined by brain imaging and behavioral studies on subjects undergoing PME treatment. The proposed stochastic entanglement hypothesis of PLP can be generalized to neuropathic pain due to sensorimotor impairment, and can be used to design suitable therapeutic treatments. © 2018 Ortiz-Catalan.","Myoelectric pattern recognition; Neuropathic pain; Phantom limb pain; Phantom Motor Execution; Stochastic entanglement; Virtual reality","Article; conceptual framework; connectome; dexterity test; human; motor control; motor performance; nerve cell plasticity; neuralgia; neuropathic pain; phantom motor execution; phantom pain; stochastic entanglement; stochastic model",Article,"Final","",Scopus,2-s2.0-85053024377
"Wang R., Zhang M., Meng X., Geng Z., Wang F.-Y.","57190014750;55584795803;57190034646;24922975700;57211758869;","3-D Tracking for Augmented Reality Using Combined Region and Dense Cues in Endoscopic Surgery",2018,"IEEE Journal of Biomedical and Health Informatics","22","5", 8096997,"1540","1551",,8,"10.1109/JBHI.2017.2770214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034259438&doi=10.1109%2fJBHI.2017.2770214&partnerID=40&md5=dec6957713aa8c399c72d031ebaf99fe","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100190, China; Qingdao Academy of Intelligent Industries, Qingdao Shandong, 266000, China","Wang, R., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100190, China, Qingdao Academy of Intelligent Industries, Qingdao Shandong, 266000, China; Zhang, M., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, Qingdao Academy of Intelligent Industries, Qingdao Shandong, 266000, China; Meng, X., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100190, China, Qingdao Academy of Intelligent Industries, Qingdao Shandong, 266000, China; Geng, Z., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Wang, F.-Y., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, Qingdao Academy of Intelligent Industries, Qingdao Shandong, 266000, China","An augmented reality (AR) technique has recently gained its popularity in minimally invasive surgery. Tracking is a crucial step to achieve precise AR. Besides optical tracking in traditional medical AR, visual tracking attracts a lot of attention due to its generality. Moreover, when the target organ's 3-D model can be obtained from preoperative images and under the model rigidity assumption, tracking is then converted into a problem of computing the six-degree-of-freedom pose of the 3-D model. In this paper, we introduce a robust tracking algorithm in our endoscopic AR system, where we combine the benefits of both region and dense cues in a unified framework. Each kind of cues alone may not be adequate for tracking in endoscopic surgery. However, they have complementary characteristics, with region cues being more robust to motion blur and fast motion, and dense cues being more accurate when motion is not large. We also propose an appearance model adaption method and an occlusion processing method to effectively handle occlusions. Experiments on both synthetic dataset and simulated surgical environment show the effectiveness and robustness of our proposed method. This work presents a novel tracking strategy in medical AR applications. © 2013 IEEE.","Augmented reality; dense cues; endoscopic surgery; region cues; tracking","Augmented reality; Endoscopy; Medical imaging; Robustness (control systems); Surface discharges; Surgery; Three dimensional displays; Biomedical imaging; dense cues; Endoscopic surgery; region cues; Solid model; Target tracking; algorithm; analytic method; analytical parameters; Article; augmented reality technique; combined region; computer assisted tomography; computer simulation; dense cues; endoscopic surgery; human; information processing; light detection; mathematical analysis; process optimization; three dimensional model; computer assisted surgery; endoscopy; procedures; three dimensional imaging; virtual reality; Algorithms; Computer Simulation; Endoscopy; Humans; Imaging, Three-Dimensional; Surgery, Computer-Assisted; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85034259438
"Grooms D.R., Kiefer A.W., Riley M.A., Ellis J.D., Thomas S., Kitchen K., DiCesare C.A., Bonnette S., Gadd B., Foss K.D.B., Yuan W., Silva P., Galloway R., Diekfuss J.A., Leach J., Berz K., Myer G.D.","55616564700;35316086800;7203009785;57202255000;55772425700;57195460949;55620685100;55004035700;57003350800;54400953600;35305992600;16302548400;57198796785;56784267000;56823049500;6507468619;6701852696;","Brain-behavior mechanisms for the transfer of neuromuscular training adaptions to simulated sport: Initial findings from the train the brain project",2018,"Journal of Sport Rehabilitation","27","5",,"1","5",,11,"10.1123/jsr.2017-0241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053299844&doi=10.1123%2fjsr.2017-0241&partnerID=40&md5=e94610f1e711d651c5d65400cc6d4981","Ohio Musculoskeletal and Neurological Institute, Ohio University, Athens, OH, United States; Division of Athletic Training, School of Applied Health Sciences and Wellness, College of Health Sciences and Professions, Ohio University, Athens, OH, United States; The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; University of Cincinnati College of Medicine, Cincinnati, OH, United States; Center for Cognition, Action and Perception, Department of Psychology, University of Cincinnati, Cincinnati, OH, United States; Pediatric Neuroimaging Research Consortium, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Duke University School of Medicine, Durham, NC, United States; Division of Radiology, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Department of Pediatrics and Orthopaedic Surgery, University of Cincinnati, Cincinnati, OH, United States; Micheli Center for Sports Injury Prevention, Waltham, MA, United States; Department of Orthopaedics, University of Pennsylvania, Philadelphia, PA, United States","Grooms, D.R., Ohio Musculoskeletal and Neurological Institute, Ohio University, Athens, OH, United States, Division of Athletic Training, School of Applied Health Sciences and Wellness, College of Health Sciences and Professions, Ohio University, Athens, OH, United States; Kiefer, A.W., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States, University of Cincinnati College of Medicine, Cincinnati, OH, United States, Center for Cognition, Action and Perception, Department of Psychology, University of Cincinnati, Cincinnati, OH, United States; Riley, M.A., Center for Cognition, Action and Perception, Department of Psychology, University of Cincinnati, Cincinnati, OH, United States; Ellis, J.D., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States, University of Cincinnati College of Medicine, Cincinnati, OH, United States; Thomas, S., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Kitchen, K., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; DiCesare, C.A., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Bonnette, S., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Gadd, B., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Foss, K.D.B., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Yuan, W., University of Cincinnati College of Medicine, Cincinnati, OH, United States, Pediatric Neuroimaging Research Consortium, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Silva, P., Center for Cognition, Action and Perception, Department of Psychology, University of Cincinnati, Cincinnati, OH, United States; Galloway, R., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States, Duke University School of Medicine, Durham, NC, United States; Diekfuss, J.A., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Leach, J., Division of Radiology, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Berz, K., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States; Myer, G.D., The SPORT Center, Division of Sports Medicine, Cincinnati Children's Hospital Medical Center, Cincinnati, OH, United States, University of Cincinnati College of Medicine, Cincinnati, OH, United States, Department of Pediatrics and Orthopaedic Surgery, University of Cincinnati, Cincinnati, OH, United States, Micheli Center for Sports Injury Prevention, Waltham, MA, United States, Department of Orthopaedics, University of Pennsylvania, Philadelphia, PA, United States","Context: A limiting factor for reducing anterior cruciate ligament injury risk is ensuring that the movement adaptions made during the prevention program transfer to sport-specific activity. Virtual reality provides a mechanism to assess transferability, and neuroimaging provides a means to assay the neural processes allowing for such skill transfer. Objective: To determine the neural mechanisms for injury risk-reducing biomechanics transfer to sport after anterior cruciate ligament injury prevention training. Design: Cohort study. Setting: Research laboratory. Participants: Four healthy high school soccer athletes. Interventions: Participants completed augmented neuromuscular training utilizing real-time visual feedback. An unloaded knee extension task and a loaded leg press task were completed with neuroimaging before and after training. A virtual reality soccerspecific landing task was also competed following training to assess transfer of movementmechanics. Main OutcomeMeasures: Landing mechanics during the virtual reality soccer task and blood oxygen level-dependent signal change during neuroimaging. Results: Increased motor planning, sensory and visual region activity during unloaded knee extension and decreased motor cortex activity during loaded leg press were highly correlated with improvements in landing mechanics (decreased hip adduction and knee rotation). Conclusion: Changes in brain activity may underlie adaptation and transfer of injury risk-reducing movement mechanics to sport activity. Clinicians may be able to target these specific brain processes with adjunctive therapy to facilitate intervention improvements transferring to sport. © 2018 Human Kinetics, Inc.","Anterior cruciate ligament; Injury prevention; Motor control; Motor learning","accident prevention; adduction; adult; anterior cruciate ligament; article; athlete; BOLD signal; brain function; case report; clinical article; clinician; cohort analysis; female; high school; hip; human; male; motor control; motor cortex; motor learning; neuroimaging; rotation; simulation; soccer; virtual reality; visual feedback; adaptation; anterior cruciate ligament injury; biomechanics; blood; brain; exercise; motor cortex; movement (physiology); nerve cell plasticity; physiology; soccer; sport; oxygen; Adaptation, Physiological; Anterior Cruciate Ligament Injuries; Athletes; Biomechanical Phenomena; Brain; Cohort Studies; Female; Humans; Motor Cortex; Movement; Neuroimaging; Neuronal Plasticity; Oxygen; Physical Conditioning, Human; Soccer; Sports; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85053299844
"Caluya N.R., Plopski A., Ty J.F., Sandor C., Taketomi T., Kato H.","57190392364;56023098100;56534116200;15061666200;25422748800;7406497359;","Transferability of Spatial Maps: Augmented Versus Virtual Reality Training",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,, 8447561,"387","393",,2,"10.1109/VR.2018.8447561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053846424&doi=10.1109%2fVR.2018.8447561&partnerID=40&md5=0c9b1b8fb3541286613ed87d95c21344","Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan","Caluya, N.R., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan; Plopski, A., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan; Ty, J.F., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan; Sandor, C., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan; Taketomi, T., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan; Kato, H., Nara Institute of Science and Technology, Interactive Media Design Laboratory, Japan","Work space simulations help trainees acquire skills necessary to perform their tasks efficiently without disrupting the workflow, forgetting important steps during a procedure, or the location of important information. This training can be conducted in Augmented and Virtual Reality (AR, VR) to enhance its effectiveness and speed. When the skills are transferred to the actual application, it is referred to as positive training transfer. However, thus far, it is unclear which training, AR or VR, achieves better results in terms of positive training transfer. We compare the effectiveness of AR and VR for spatial memory training in a control-room scenario, where users have to memorize the location of buttons and information displays in their surroundings. We conducted a within-subject study with 16 participants and evaluated the impact the training had on short-Term and long-Term memory. Results of our study show that VR outperformed AR when tested in the same medium after the training. In a memory transfer test conducted two days later AR outperformed VR. Our findings have implications on the design of future training scenarios and applications. © 2018 IEEE.","and virtual realities; augmented; Evaluation/methodology; H.5.1-Information Interfaces and Presentation: Multimedia Information Systems-Artificial; H.5.2-Information Interfaces and Presentation: Multimedia Information Systems-Ergonomics; Theory and methods","E-learning; Ergonomics; Information systems; Information use; Virtual reality; augmented; Evaluation/methodology; MultiMedia Information Systems; Multimedia information systems-artificial; Theory and methods; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85053846424
"Krum D.M., Kang S.-H., Phan T.","6701630874;22835064900;55869140500;","Influences on the Elicitation of Interpersonal Space with Virtual Humans",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,, 8446235,"223","229",,1,"10.1109/VR.2018.8446235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053822218&doi=10.1109%2fVR.2018.8446235&partnerID=40&md5=efee792ec391075882d1fad42a20ac03","University of Southern California, Institute for Creative Technologies, United States","Krum, D.M., University of Southern California, Institute for Creative Technologies, United States; Kang, S.-H., University of Southern California, Institute for Creative Technologies, United States; Phan, T., University of Southern California, Institute for Creative Technologies, United States","The emergence of low cost virtual and augmented reality systems has encouraged the development of immersive training applications for medical, military, and many other fields. Many of the training scenarios for these various fields may require the presentation of realistic interactions with virtual humans. It is thus vital to determine the critical factors of fidelity required in those interactions to elicit naturalistic behavior on the part of trainees. Negative training may occur if trainees are inadvertently influenced to react in ways that are unexpected and unnatural, hindering proper learning and transfer of skills and knowledge back into real world contexts. In this research, we examined whether haptic priming (presenting an illusion of virtual human touch at the beginning of the virtual experience) and different locomotion techniques (either joystick or physical walking) might affect proxemic behavior in human users. The results of our study suggest that locomotion techniques can alter proxemic behavior in significant ways. Haptic priming did not appear to impact proxemic behavior, but did increase rapport and other subjective social measures. The results suggest that designers and developers of immersive training systems should carefully consider the impact of even simple design and fidelity choices on trainee reactions in social interactions. © 2018 IEEE.","fidelity; haptic priming; immersive training; locomotion techniques; proxemics; Virtual humans; virtual reality","Augmented reality; Behavioral research; Biocommunications; Human computer interaction; Military applications; User interfaces; fidelity; haptic priming; Immersive; Locomotion technique; proxemics; Virtual humans; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85053822218
"Karatsidis A., Richards R.E., Konrath J.M., Van Den Noort J.C., Schepers H.M., Bellusci G., Harlaar J., Veltink P.H.","57192812821;57037821600;57189991841;26644544500;15623758700;23007370300;6701663252;7006060993;","Validation of wearable visual feedback for retraining foot progression angle using inertial sensors and an augmented reality headset",2018,"Journal of NeuroEngineering and Rehabilitation","15","1", 78,"","",,24,"10.1186/s12984-018-0419-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051667667&doi=10.1186%2fs12984-018-0419-2&partnerID=40&md5=861c4fb0e465747960f724d9f249c8d0","Xsens Technologies B.V, Pantheon 6, Enschede, 7521 PR, Netherlands; Department of Biomedical Signals and Systems (BSS), Technical Medical Centre, University of Twente, Enschede, Netherlands; Department of Rehabilitation Medicine, Amsterdam Movement Sciences, VU University Medical Center, Amsterdam, Netherlands; Academic Medical Center, Musculoskeletal Imaging Quantification Center (MIQC), Department of Radiology and Nuclear Medicine, Amsterdam Movement Sciences, Amsterdam, Netherlands; Department of Biomechanical Engineering, Delft University of Technology, Delft, Netherlands","Karatsidis, A., Xsens Technologies B.V, Pantheon 6, Enschede, 7521 PR, Netherlands, Department of Biomedical Signals and Systems (BSS), Technical Medical Centre, University of Twente, Enschede, Netherlands; Richards, R.E., Department of Rehabilitation Medicine, Amsterdam Movement Sciences, VU University Medical Center, Amsterdam, Netherlands; Konrath, J.M., Xsens Technologies B.V, Pantheon 6, Enschede, 7521 PR, Netherlands; Van Den Noort, J.C., Department of Rehabilitation Medicine, Amsterdam Movement Sciences, VU University Medical Center, Amsterdam, Netherlands, Academic Medical Center, Musculoskeletal Imaging Quantification Center (MIQC), Department of Radiology and Nuclear Medicine, Amsterdam Movement Sciences, Amsterdam, Netherlands; Schepers, H.M., Xsens Technologies B.V, Pantheon 6, Enschede, 7521 PR, Netherlands; Bellusci, G., Xsens Technologies B.V, Pantheon 6, Enschede, 7521 PR, Netherlands; Harlaar, J., Department of Rehabilitation Medicine, Amsterdam Movement Sciences, VU University Medical Center, Amsterdam, Netherlands, Department of Biomechanical Engineering, Delft University of Technology, Delft, Netherlands; Veltink, P.H., Department of Biomedical Signals and Systems (BSS), Technical Medical Centre, University of Twente, Enschede, Netherlands","Background: Gait retraining interventions using real-time biofeedback have been proposed to alter the loading across the knee joint in patients with knee osteoarthritis. Despite the demonstrated benefits of these conservative treatments, their clinical adoption is currently obstructed by the high complexity, spatial demands, and cost of optical motion capture systems. In this study we propose and evaluate a wearable visual feedback system for gait retraining of the foot progression angle (FPA). Methods: The primary components of the system are inertial measurement units, which track the human movement without spatial limitations, and an augmented reality headset used to project the visual feedback in the visual field. The adapted gait protocol contained five different target angles ranging from 15 degrees toe-out to 5 degrees toe-in. Eleven healthy participants walked on an instrumented treadmill, and the protocol was performed using both an established laboratory visual feedback driven by optical motion capture, and the proposed wearable system. Results and conclusions: The wearable system tracked FPA with an accuracy of 2.4 degrees RMS and ICC=0.94 across all target angles and subjects, when compared to an optical motion capture reference. In addition, the effectiveness of the biofeedback, reflected by the number of steps with FPA value ±2 degrees from the target, was found to be around 50% in both wearable and laboratory approaches. These findings demonstrate that retraining of the FPA using wearable inertial sensing and visual feedback is feasible with effectiveness matching closely an established laboratory method. The proposed wearable setup may reduce the complexity of gait retraining applications and facilitate their transfer to routine clinical practice. © 2018 The Author(s).","Augmented reality headset; Foot progression angle; Gait retraining; Inertial sensors; Knee osteoarthritis; Real-time biofeedback","adult; Article; biomechanics; female; foot progression angle; gait; human; human experiment; male; mathematical analysis; measurement accuracy; musculoskeletal system parameters; normal human; priority journal; virtual reality; visual feedback; walking; electronic device; foot; gait; knee; knee osteoarthritis; physiology; sensory feedback; validation study; Adult; Biomechanical Phenomena; Feedback, Sensory; Female; Foot; Gait; Humans; Knee Joint; Male; Osteoarthritis, Knee; Virtual Reality; Walking; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85051667667
"Ahn S., Gorlatova M., Naghizadeh P., Chiang M., Mittal P.","57203146162;24775927000;56176019100;7102873398;24825217800;","Adaptive fog-based output security for augmented reality",2018,"VR/AR Network 2018 - Proceedings of the 2018 Morning Workshop on Virtual Reality and Augmented Reality Network, Part of SIGCOMM 2018",,,,"1","6",,13,"10.1145/3229625.3229626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056428916&doi=10.1145%2f3229625.3229626&partnerID=40&md5=74ed627af7deca908b65e7738a0b4b72","Department of Electrical Engineering, Princeton University, Princeton, NJ, United States; Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, United States","Ahn, S., Department of Electrical Engineering, Princeton University, Princeton, NJ, United States; Gorlatova, M., Department of Electrical Engineering, Princeton University, Princeton, NJ, United States; Naghizadeh, P., Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, United States; Chiang, M., Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, United States; Mittal, P., Department of Electrical Engineering, Princeton University, Princeton, NJ, United States","Augmented reality (AR) technologies are rapidly being adopted across multiple sectors, but little work has been done to ensure the security of such systems against potentially harmful or distracting visual output produced by malicious or bug-ridden applications. Past research has proposed to incorporate manually specified policies into AR devices to constrain their visual output. However, these policies can be cumbersome to specify and implement, and may not generalize well to complex and unpredictable environmental conditions. We propose a method for generating adaptive policies to secure visual output in AR systems using deep reinforcement learning. This approach utilizes a local fog computing node, which runs training simulations to automatically learn an appropriate policy for filtering potentially malicious or distracting content produced by an application. Through empirical evaluations, we show that these policies are able to intelligently displace AR content to reduce obstruction of real-world objects, while maintaining a favorable user experience. © 2018 Copyright held by the owner/author(s).","Augmented reality; Edge computing; Fog computing; Policy optimization; Reinforcement learning; Visual output security","Augmented reality; Deep learning; Edge computing; Fog; Reinforcement learning; Virtual reality; Adaptive policy; Empirical evaluations; Environmental conditions; Policy optimization; Real-world objects; Training simulation; User experience; Visual outputs; Fog computing",Conference Paper,"Final","",Scopus,2-s2.0-85056428916
"Bothén S., Font J., Nilsson P.","57204435557;56344008800;57204432176;","An analysis and comparative user study on interactions in mobile virtual reality games",2018,"ACM International Conference Proceeding Series",,, 4,"","",,2,"10.1145/3235765.3235772","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055561057&doi=10.1145%2f3235765.3235772&partnerID=40&md5=bceeeb8cb311e53abe43c92aa24f856e","Malmö University, Malmö, Sweden","Bothén, S., Malmö University, Malmö, Sweden; Font, J., Malmö University, Malmö, Sweden; Nilsson, P., Malmö University, Malmö, Sweden","Mobile Virtual Reality (MVR) makes Virtual Reality games more accessible to a broader audience. Interaction design guidelines and best practices for MVR experiences are available for developers. In this paper, we specifically explore interactions in MVR games, a particular subset of MVR experiences that is becoming popular. A set of MVR games is analyzed with a special focus on head gaze, categorizing and isolating their mechanics implemented with this common MVR technique. This analysis is the basis of a test application in the MVR interactions are implemented and later compared to a traditional game pad controller in three different challenges. A comparative user study has been carried out from the perspective of both gamers and non-gamers facing these challenges. Results show the preferences and performances of the players using all the interactions, highlighting an interesting generalized preference for MVR interactions over the traditional controller in some of the analyzed cases. © 2018 ACM.","Head gaze; Mobile virtual reality; Video games","Computer games; Interactive computer graphics; Virtual reality; Best practices; Head gaze; Interaction design; Test applications; User study; Video game; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85055561057
"Abernethy M., Sinnen O., Adams J., De Ruvo G., Giacaman N.","57201299216;57203063083;7404782866;56948173700;24528603500;","ParallelAR: An augmented reality app and instructional approach for learning parallel programming scheduling concepts",2018,"Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018",,, 8425430,"324","331",,2,"10.1109/IPDPSW.2018.00063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052244534&doi=10.1109%2fIPDPSW.2018.00063&partnerID=40&md5=a616f759cdca09547c42dd74e95fb3a2","Parallel and Reconfigurable Computing Lab, Department of Electrical and Computer Engineering, Auckland, New Zealand; Department of Computer Science, Calvin College, Grand Rapids, United States","Abernethy, M., Parallel and Reconfigurable Computing Lab, Department of Electrical and Computer Engineering, Auckland, New Zealand; Sinnen, O., Parallel and Reconfigurable Computing Lab, Department of Electrical and Computer Engineering, Auckland, New Zealand; Adams, J., Department of Computer Science, Calvin College, Grand Rapids, United States; De Ruvo, G., Parallel and Reconfigurable Computing Lab, Department of Electrical and Computer Engineering, Auckland, New Zealand; Giacaman, N., Parallel and Reconfigurable Computing Lab, Department of Electrical and Computer Engineering, Auckland, New Zealand","Parallel programming has rapidly moved from a special-purpose technique to standard practice. This newfound ubiquity needs to be matched by improved parallel programming education. As parallel programming involves higher level concepts, students tend to struggle with turning the abstract information into concrete mental models. Analogies are known to aid in this knowledge transfer, by providing an existing schema as the basis for the formation of a new schema. Additionally, technology has been proven to increase motivation and engagement in students, which ultimately improves learning. Combining these ideas, this paper presents several contributions that enhance aspects of parallel programming education. These contributions include a set of collaborative learning activities to target fundamental scheduling concepts, a detailed analogy to assist in the understanding of the scheduling concepts, and an augmented reality application to facilitate the collaborative learning activity by bringing the analogy to life. © 2018 IEEE.","Analogies; Augmented reality; Collaborative learning; Scheduling policies","Augmented reality; Distributed computer systems; Education computing; Knowledge management; Scheduling; Students; Analogies; Augmented reality applications; Collaborative learning; Collaborative learning activities; Knowledge transfer; Motivation and engagements; Programming education; Scheduling policies; Parallel programming",Conference Paper,"Final","",Scopus,2-s2.0-85052244534
"Bian F., Li R., Zhao L., Liu Y., Liang P.","57195946919;57219000160;57191521088;57188732686;56341925400;","Interface design of a human-robot interaction system for dual-manipulators teleoperation based on virtual reality",2018,"2018 IEEE International Conference on Information and Automation, ICIA 2018",,, 8812457,"1361","1366",,,"10.1109/ICInfA.2018.8812457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072346085&doi=10.1109%2fICInfA.2018.8812457&partnerID=40&md5=43216cdf6d19c5918cb062d16f281d16","State Key Lab of Robotics and System, Harbin Institute of Technology, 2, Yikuangjie, Nangang, Harbin, Heilongjiang, China; Quanzhou HIT Institute of Engineering and Technology, Fengze, Quanzhou, Fujian, China","Bian, F., State Key Lab of Robotics and System, Harbin Institute of Technology, 2, Yikuangjie, Nangang, Harbin, Heilongjiang, China; Li, R., State Key Lab of Robotics and System, Harbin Institute of Technology, 2, Yikuangjie, Nangang, Harbin, Heilongjiang, China; Zhao, L., State Key Lab of Robotics and System, Harbin Institute of Technology, 2, Yikuangjie, Nangang, Harbin, Heilongjiang, China; Liu, Y., Quanzhou HIT Institute of Engineering and Technology, Fengze, Quanzhou, Fujian, China; Liang, P., Quanzhou HIT Institute of Engineering and Technology, Fengze, Quanzhou, Fujian, China","This paper presents a human-robot interaction interface for dual-manipulators teleoperation based on virtual reality. Using this method, the human operator is able to control the robot at a distance to complete complicated tasks in unstructured environment. Virtual reality technology is integrated into the system to provide the first person perspective of the robot to the operator. Based on the three-dimensional coordinates of shoulder, elbow, wrist and hand captured by Kinect, a geometric model of the human arm is built. Then a geometric vector method is proposed to calculate the joint angles of human upper limbs which are translated into movement commands for a robot. To access the performance of our proposed human-robot interaction interface, teleoperation experiments are conducted on the Baxter robot, illustrating the effectiveness and feasibility of our proposed method. © 2018 IEEE.","Human-Robot Interaction; Motion transfer; Teleoperation; Virtual reality","Machine design; Man machine systems; Manipulators; Remote control; Virtual reality; Dual manipulators; First-person perspectives; Geometric modeling; Interface designs; Motion transfer; Three dimensional coordinate; Unstructured environments; Virtual reality technology; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85072346085
"Mollahosseini A., Abdollahi H., Sweeny T.D., Cole R., Mahoor M.H.","57204081302;57193025926;23029335500;7401590871;8423254700;","Role of embodiment and presence in human perception of robots’ facial cues",2018,"International Journal of Human Computer Studies","116",,,"25","39",,10,"10.1016/j.ijhcs.2018.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046493075&doi=10.1016%2fj.ijhcs.2018.04.005&partnerID=40&md5=1ab8eaacd53d69d281b34e80f5314e28","Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Boulder Learning Inc., Boulder, CO  80301, United States; Department of Psychology, University of Denver, Denver, CO  80208, United States","Mollahosseini, A., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Abdollahi, H., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Sweeny, T.D., Department of Psychology, University of Denver, Denver, CO  80208, United States; Cole, R., Boulder Learning Inc., Boulder, CO  80301, United States; Mahoor, M.H., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States","Both robotic and virtual agents could one day be equipped with social abilities necessary for effective and natural interaction with human beings. Although virtual agents are relatively inexpensive and flexible, they lack the physical embodiment present in robotic agents. Surprisingly, the role of embodiment and physical presence for enriching human-robot-interaction is still unclear. This paper explores how these unique features of robotic agents influence three major elements of human-robot face-to-face communication, namely the perception of visual speech, facial expression, and eye-gaze. We used a quantitative approach to disentangle the role of embodiment from the physical presence of a social robot, called Ryan, with three different agents (robot, telepresent robot, and virtual agent), as well as with an actual human. We used a robot with a retro-projected face for this study, since the same animation from a virtual agent could be projected to this robotic face, thus allowing comparison of the virtual agent's animation behaviors with both telepresent and the physically present robotic agents. The results of our studies indicate that the eye gaze and certain facial expressions are perceived more accurately when the embodied agent is physically present than when it is displayed on a 2D screen either as a telepresent or a virtual agent. Conversely, we find no evidence that either the embodiment or the presence of the robot improves the perception of visual speech, regardless of syntactic or semantic cues. Comparison of our findings with previous studies also indicates that the role of embodiment and presence should not be generalized without considering the limitations of the embodied agents. © 2018 Elsevier Ltd","Embodiment; Physical presence; Retro-projected robots; Social robot","Animation; Robotics; Semantics; Speech communication; Virtual reality; Embodiment; Facial Expressions; Human perception; Natural interactions; Physical presence; Quantitative approach; Social abilities; Social robots; Human robot interaction",Article,"Final","",Scopus,2-s2.0-85046493075
"Hannola L., Richter A., Richter S., Stocker A.","34871993100;35556968300;57200540103;24767000800;","Empowering production workers with digitally facilitated knowledge processes–a conceptual framework",2018,"International Journal of Production Research","56","14",,"4729","4743",,20,"10.1080/00207543.2018.1445877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044441545&doi=10.1080%2f00207543.2018.1445877&partnerID=40&md5=03a1b244ca4d281610d9f25219429e27","Department of Industrial Engineering and Management, Lappeenranta University of Technology, Lappeenranta, Finland; Department of Business IT, IT University of Copenhagen, Copenhagen, Denmark; Department of Informatics, University of Zurich, Zurich, Switzerland; Information Technology and Software Engineering Department, Auckland University of Technology, Auckland, New Zealand; Virtual Vehicle Research Center, Graz, Austria","Hannola, L., Department of Industrial Engineering and Management, Lappeenranta University of Technology, Lappeenranta, Finland; Richter, A., Department of Business IT, IT University of Copenhagen, Copenhagen, Denmark, Department of Informatics, University of Zurich, Zurich, Switzerland; Richter, S., Information Technology and Software Engineering Department, Auckland University of Technology, Auckland, New Zealand; Stocker, A., Virtual Vehicle Research Center, Graz, Austria","Recent digital advancements, including social software, mobile technologies and augmented reality, offer promising opportunities to empower knowledge workers in their production environment by leveraging their knowledge processes, decision-making skills and social interaction practices. This paper proposes a conceptual framework for empowering workers in industrial production environments with digitally facilitated knowledge management processes. The framework explores four concrete facets of digital advancements that apply to a wide range of knowledge processes and production strategies in manufacturing companies. Each of these advancements are capable of supporting one specific facet of the individual knowledge management processes of workers; knowledge transfer, discovery, acquisition and sharing. The study contributes to the production research community by aligning emerging digital technologies and current trends in advanced manufacturing environments to benefit workers and improve job satisfaction, efficiency and productivity. The paper also contains suggestions about developing innovative solutions for production environments that support workers with digital technologies for flexible production. © 2018, © 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","augmented reality; digital technology; information systems; information technology; knowledge management; manufacturing systems; production management; production models","Augmented reality; Decision making; Human resource management; Industrial research; Information systems; Information technology; Job satisfaction; Manufacture; Advanced manufacturing; Decision-making skills; Digital technologies; Knowledge management process; Manufacturing companies; Production environments; Production management; Production models; Knowledge management",Article,"Final","",Scopus,2-s2.0-85044441545
"Huang M., Zhang X.","57194328240;57194344679;","MAC scheduling for multiuser wireless virtual reality in 5G MIMO-OFDM Systems",2018,"2018 IEEE International Conference on Communications Workshops, ICC Workshops 2018 - Proceedings",,,,"1","6",,7,"10.1109/ICCW.2018.8403486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050263021&doi=10.1109%2fICCW.2018.8403486&partnerID=40&md5=0609bb9d4c8a4ace1e1290998f36b396","Intel Corporation, United States","Huang, M., Intel Corporation, United States; Zhang, X., Intel Corporation, United States","Wireless Virtual Reality (VR) is a new-arising technology to enable the untethered connection between VR server and VR client, which needs to support simultaneously ultra-high data rate and ultra-high transfer reliability for video streaming, and also ultra-high responsive speed for motion-to-photon latency. Such three ultra-high (3UH) requirements constitute the basic characteristics of the generalized tactile internet. This paper proposes a multiuser MAC scheduling scheme for VR service in 5G MIMO-OFDM system, which can maximize the number of simultaneous VR clients while guaranteeing their 3UH quality-of-experience (QoE). Specifically, this scheme is composed of three novel functions, including video frame differentiation and delay-based weight calculation, spatial-frequency user selection based on maximum aggregate delay-capacity utility (ADCU), and link adaptation with dynamic block-error-rate (BLER) target. In addition, a low-complexity downlink MIMO user selection algorithm is developed, which can reduce the calculation amount with one order. It is demonstrated by the simulation results that the proposed scheme increases 31.6% for the maximum number of simultaneously served VR users than the traditional scheme with maximum-sum-capacity based scheduling and fixed BLER target based link adaptation. © 2018 IEEE.","Deep Packet Inspection; Delay-Based; Link Adaptation; MAC Scheduling; MIMO-OFDM; Multiuser Wireless Virtual Reality","Gain control; MIMO systems; Orthogonal frequency division multiplexing; Quality of service; Scheduling; Virtual reality; Deep packet inspection; Delay-Based; Link adaptation; MAC scheduling; MIMO-OFDM; Multi-user; 5G mobile communication systems",Conference Paper,"Final","",Scopus,2-s2.0-85050263021
"Rivière E., Saucier D., Lafleur A., Lacasse M., Chiniara G.","36653247600;7005302177;56490345600;8603839300;55583067300;","Twelve tips for efficient procedural simulation",2018,"Medical Teacher","40","7",,"743","751",,7,"10.1080/0142159X.2017.1391375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032220031&doi=10.1080%2f0142159X.2017.1391375&partnerID=40&md5=66da187a9c5e396befcffe0b2603d71c","Department of Internal Medicine, Haut-Leveque Hospital, University Hospital Centre of Bordeaux, Pessac, France; Apprentiss Centre (Simulation Centre), Laval University, Quebec City, Canada; Centre of Applied Research to Educative Methods (CAREM), University of Bordeaux, Bordeaux, France; Department of Family and Emergency Medicine, Laval University, Quebec City, Canada; Office of Education and Continuing Professional Development (Vice-décanat à la pédagogie et au développement professional continu), Laval University, Quebec City, Canada; Department of Medicine, Laval University, Quebec City, Canada; Department of Anaesthesiology and Intensive Care, Laval University, Quebec City, Canada","Rivière, E., Department of Internal Medicine, Haut-Leveque Hospital, University Hospital Centre of Bordeaux, Pessac, France, Apprentiss Centre (Simulation Centre), Laval University, Quebec City, Canada, Centre of Applied Research to Educative Methods (CAREM), University of Bordeaux, Bordeaux, France; Saucier, D., Department of Family and Emergency Medicine, Laval University, Quebec City, Canada, Office of Education and Continuing Professional Development (Vice-décanat à la pédagogie et au développement professional continu), Laval University, Quebec City, Canada; Lafleur, A., Office of Education and Continuing Professional Development (Vice-décanat à la pédagogie et au développement professional continu), Laval University, Quebec City, Canada, Department of Medicine, Laval University, Quebec City, Canada; Lacasse, M., Office of Education and Continuing Professional Development (Vice-décanat à la pédagogie et au développement professional continu), Laval University, Quebec City, Canada, Department of Medicine, Laval University, Quebec City, Canada; Chiniara, G., Apprentiss Centre (Simulation Centre), Laval University, Quebec City, Canada, Department of Anaesthesiology and Intensive Care, Laval University, Quebec City, Canada","Procedural simulation (PS) is increasingly being used worldwide in healthcare for training caregivers in psychomotor competencies. It has been demonstrated to improve learners’ confidence and competence in technical procedures, with consequent positive impacts on patient outcomes and safety. Several frameworks can guide healthcare educators in using PS as an educational tool. However, no theory-informed practical framework exists to guide them in including PS in their training programs. We present 12 practical tips for efficient PS training that translates educational concepts from theory to practice, based on the existing literature. In doing this, we aim to help healthcare educators to adequately incorporate and use PS both for optimal learning and for transfer into professional practice. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group.",,"human; professional practice; simulation training; clinical competence; constructive feedback; education; learning; medical education; problem based learning; procedures; program development; simulation training; standards; Clinical Competence; Education, Medical; Educational Measurement; Formative Feedback; Humans; Learning; Problem-Based Learning; Program Development; Simulation Training",Article,"Final","",Scopus,2-s2.0-85032220031
"Werrlich S., Nguyen P.-A., Notni G.","57195069564;57209915720;7004204934;","Evaluating the training transfer of Head-Mounted Display based training for assembly tasks",2018,"ACM International Conference Proceeding Series",,,,"297","302",,10,"10.1145/3197768.3201564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049917107&doi=10.1145%2f3197768.3201564&partnerID=40&md5=cf6d53b805f90786790e6809e4a4b063","BMW AG, Taunusstraße 41, München, 80807, Germany; Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","Werrlich, S., BMW AG, Taunusstraße 41, München, 80807, Germany; Nguyen, P.-A., BMW AG, Taunusstraße 41, München, 80807, Germany; Notni, G., Technical University Ilmenau, Postfach 100565, Ilmenau, 98684, Germany","The automotive industry is growing constantly and more and more assembly workers are needed to negotiate the production volume. The training of new employees is essential to ensure premium quality products and processes. New technologies for training such as head-mounted displays (HMDs) receive a growing amount of attention by the scientific community, especially in the industrial domain. Due to its possibility to work hands-free while providing users with necessary augmented information, HMDs can enhance the quality and efficiency of assembly training tasks. However, comprehensive evaluations in industrial environments regarding the training transfer using augmented reality (AR) technologies are still very limited. In this paper, we aim to close this gap by conducting a user study with two groups and 30 participants, measuring the training transfer. We compare the effects of two slightly different HMD-based training applications. The first group complete a tutorial, beginner, intermediate and expert training level, while the second group received an additional quiz level. Results show that group two needed 17% more time to complete the training but made 79% less sequence mistakes compared to the first group. Additionally, we compare the user satisfaction by using the system usability scale (SUS) and the perceived workload by measuring the NASA-TLX. © 2018 Association for Computing Machinery.","Assembly; Augmented reality; Evaluation; Head-mounted display; Training","Assembly; Augmented reality; Automotive industry; NASA; Personnel training; Sensory perception; Street traffic control; Comprehensive evaluation; Evaluation; Head mounted displays; Industrial environments; Premium-quality products; Scientific community; System Usability Scale (SUS); Training applications; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85049917107
"Sharma R., Biookaghazadeh S., Zhao M.","57202992244;57188843878;55130283100;","Are existing knowledge transfer techniques effective for deep learning on edge devices?",2018,"HPDC 2018 - Proceedings of The 27th International Symposium on High-Performance Parallel and Distributed Computing Posters/Doctoral Consortium",,,,"15","16",,2,"10.1145/3220192.3220459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050115844&doi=10.1145%2f3220192.3220459&partnerID=40&md5=0391184a4aa83a8477e7fe5b38f426f2","Arizona State University, United States","Sharma, R., Arizona State University, United States; Biookaghazadeh, S., Arizona State University, United States; Zhao, M., Arizona State University, United States","With the emergence of edge computing paradigm, many applications such as image recognition and augmented reality require to perform machine learning (ML) and artificial intelligence (AI) tasks on edge devices. Most AI and ML models are large and computational-heavy, whereas edge devices are usually equipped with limited computational and storage resources. Such models can be compressed and reduced for deployment on edge devices, but they may loose their capability and not perform well. Recent works used knowledge transfer techniques to transfer information from a large network (termed teacher) to a small one (termed student) in order to improve the performance of the latter. This approach seems to be promising for learning on edge devices, but a thorough investigation on its effectiveness is lacking. This paper provides an extensive study on the performance (in both accuracy and convergence speed) of knowledge transfer, considering different student architectures and different techniques for transferring knowledge from teacher to student. The results show that the performance of KT does vary by architectures and transfer techniques. A good performance improvement is obtained by transferring knowledge from both the intermediate layers and last layer of the teacher to a shallower student. But other architectures and transfer techniques do not fare so well and some of them even lead to negative performance impact. © 2018 Association for Computing Machinery.","Deep learning; Edge computing; Knowledge transfer; Neural networks","Artificial intelligence; Augmented reality; Edge computing; Image recognition; Knowledge management; Network architecture; Neural networks; Students; Teaching; Computing paradigm; Intermediate layers; Knowledge transfer; Performance impact; Performance improvements; Storage resources; Transfer information; Transfer technique; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85050115844
"Krause L., Farrow D., Reid M., Buszard T., Pinder R.","56231073000;7006613807;15136884800;55605085300;26423249500;","Helping coaches apply the principles of representative learning design: validation of a tennis specific practice assessment tool",2018,"Journal of Sports Sciences","36","11",,"1277","1286",,17,"10.1080/02640414.2017.1374684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029516924&doi=10.1080%2f02640414.2017.1374684&partnerID=40&md5=f0ced1364e6f615cc29f7d5b588975a3","Institute of Sport, Exercise and Active Living (ISEAL), Victoria University, Melbourne, Australia; Athlete and Coach Services, Australian Institute of SportACT, Australia; Game Insight Group, Tennis Australia, Richmond, Australia; Australian Paralympic Committee, Adelaide, Australia","Krause, L., Institute of Sport, Exercise and Active Living (ISEAL), Victoria University, Melbourne, Australia, Athlete and Coach Services, Australian Institute of SportACT, Australia; Farrow, D., Institute of Sport, Exercise and Active Living (ISEAL), Victoria University, Melbourne, Australia, Game Insight Group, Tennis Australia, Richmond, Australia; Reid, M., Institute of Sport, Exercise and Active Living (ISEAL), Victoria University, Melbourne, Australia, Athlete and Coach Services, Australian Institute of SportACT, Australia; Buszard, T., Institute of Sport, Exercise and Active Living (ISEAL), Victoria University, Melbourne, Australia, Athlete and Coach Services, Australian Institute of SportACT, Australia; Pinder, R., Australian Paralympic Committee, Adelaide, Australia","Representative Learning Design (RLD) is a framework for assessing the degree to which experimental or practice tasks simulate key aspects of specific performance environments (i.e. competition). The key premise being that when practice replicates the performance environment, skills are more likely to transfer. In applied situations, however, there is currently no simple or quick method for coaches to assess the key concepts of RLD (e.g. during on-court tasks). The aim of this study was to develop a tool for coaches to efficiently assess practice task design in tennis. A consensus-based tool was developed using a 4-round Delphi process with 10 academic and 13 tennis-coaching experts. Expert consensus was reached for the inclusion of seven items, each consisting of two sub-questions related to (i) the task goal and (ii) the relevance of the task to competition performance. The Representative Practice Assessment Tool (RPAT) is proposed for use in assessing and enhancing practice task designs in tennis to increase the functional coupling between information and movement, and to maximise the potential for skill transfer to competition contexts. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Coaching; practice; representative learning design; skill; tennis","competition; consensus; Delphi study; human; learning; skill; tennis; validation process; competitive behavior; learning; mentoring; motor performance; physiology; task performance; tennis; transfer of learning; Competitive Behavior; Delphi Technique; Humans; Mentoring; Motor Skills; Practice (Psychology); Task Performance and Analysis; Tennis; Transfer (Psychology)",Article,"Final","",Scopus,2-s2.0-85029516924
"Riva G., Wiederhold B.K., Chirico A., Di Lernia D., Mantovani F., Gaggioli A.","56962750600;7003634518;56755080200;57189076325;7006190897;6603138127;","Brain and virtual reality: What do they have in common and how to exploit their potential",2018,"Annual Review of CyberTherapy and Telemedicine","2018","16",,"3","8",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067840584&partnerID=40&md5=3a878bb1f64f53b14d1ef91b66876123","Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy; Interactive Media Institute, San Diego, CA, United States; Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy","Riva, G., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy; Wiederhold, B.K., Interactive Media Institute, San Diego, CA, United States; Chirico, A., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Di Lernia, D., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy; Mantovani, F., Department of Human Sciences for Education, Università degli Studi di Milano-Bicocca, Milan, Italy; Gaggioli, A., Department of Psychology, Università Cattolica del Sacro Cuore, Milan, Italy, Applied Technology for Neuro-Psychology Lab. Istituto Auxologico Italiano, Milan, Italy","Different studies suggest that Virtual Reality (VR) is an effective tool for behavioural health, with long-term effects that generalize to the real world. Here we suggest that the efficacy of VR can be explained by how it works. Specifically, VR shares with our brain the same basic mechanism: embodied simulations. Different major discoveries in the field of neuroscience suggest that our brain produces and updates an embodied simulation of the body in the world. This simulation is actively used by different cognitive processes to represent and predict actions, concepts, and emotions. VR works in a similar way: through the integration of data from trackers and contents of a simulated 3D world, a VR system builds a model (simulation) of the body and the space around it. Like the brain, the VR system uses the simulation to predict the sensory consequences of the individual’s movements. In this view, the more the VR model is similar to the brain model, the more the individual feels present in the VR world. The paper discusses the potential of this link, by suggesting the emergence of a new clinical approach that uses the simulative potential of VR to exploit/empower (transformation of flow) and/or correct/update (embodied medicine) the predictive/simulative mechanisms of the brain. © 2018, Interactive Media Institute. All rights reserved.","Behavioural health; Embodied medicine; Embodied simulation; Neuroscience; Predictive coding; Virtual reality","article; brain; drug efficacy; human; human experiment; medical decision making; neuroscience; simulation; virtual reality",Article,"Final","",Scopus,2-s2.0-85067840584
"Rupprecht F.A., Kasakow G., Aurich J.C., Hamann B., Ebert A.","56225646600;57191727562;7005485993;7005775949;22834440100;","Improving collaboration efficiency via diverse networked mobile devices",2018,"Journal on Multimodal User Interfaces","12","2",,"91","108",,1,"10.1007/s12193-017-0251-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034640068&doi=10.1007%2fs12193-017-0251-1&partnerID=40&md5=e364ed970324575cc5d1618e5e2b5201","Computer Graphics and HCI, University of Kaiserslautern, Kaiserslautern, Germany; Institute for Manufacturing Technology and Production Systems, University of Kaiserslautern, Kaiserslautern, Germany; Department of Computer Science, University of California, Davis, Davis, CA, United States","Rupprecht, F.A., Computer Graphics and HCI, University of Kaiserslautern, Kaiserslautern, Germany; Kasakow, G., Institute for Manufacturing Technology and Production Systems, University of Kaiserslautern, Kaiserslautern, Germany; Aurich, J.C., Institute for Manufacturing Technology and Production Systems, University of Kaiserslautern, Kaiserslautern, Germany; Hamann, B., Department of Computer Science, University of California, Davis, Davis, CA, United States; Ebert, A., Computer Graphics and HCI, University of Kaiserslautern, Kaiserslautern, Germany","We introduce a framework for distributed or co-located teams to collaborate highly efficiently using diverse mobile devices for design and assessment of complex systems. Our framework enhances the efficiency of collaborations arising in design, simulation or data analysis, including visualization. First, we investigate which requirements on such a collaboration framework exist and which influences between task models have to be taken into account; afterwards we transfer those findings into a prototypical system. The devices provide three views of data to be processed collaboratively: (1) a simulation view; (2) a status report view; and (3) a status update view. These views serve the purpose of providing overview, detail and performance views. A smart watch view shows at-a-glance information, the environment or the process being inspected, possibly influenced by a user. Users can use their mobile devices as control interfaces. The framework is especially effective for combining the synergistic, complementary competencies of a team. We describe the design of our framework and discuss specific applications. © 2017, The Author(s).","Computer supported collaborative work; Groupwork; HCI; Inferdependence; Mobile devices; Overview and Detail view; Secondary displays; Smart devices","Computer supported cooperative work; Data visualization; Efficiency; Human computer interaction; Mobile devices; Collaboration framework; Control interfaces; Groupwork; Inferdependence; Overview and detail; Smart devices; Smart watches; Status updates; Display devices",Article,"Final","",Scopus,2-s2.0-85034640068
"Hai W., Jain N., Wydra A., Thalmann N.M., Thalmann D.","57201904238;35498072000;57201913246;55218502200;7005885082;","Increasing the feeling of social presence by incorporating realistic interactions in multi-party VR",2018,"ACM International Conference Proceeding Series",,,,"7","10",,2,"10.1145/3205326.3205345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048396292&doi=10.1145%2f3205326.3205345&partnerID=40&md5=e227e300e07f40bfe7197535609178cd","Institute for Media Innovation, Nanyang Technological University, Singapore; École polytechnique fédérale de Lausanne, Switzerland","Hai, W., Institute for Media Innovation, Nanyang Technological University, Singapore; Jain, N., Institute for Media Innovation, Nanyang Technological University, Singapore; Wydra, A., Institute for Media Innovation, Nanyang Technological University, Singapore; Thalmann, N.M., Institute for Media Innovation, Nanyang Technological University, Singapore; Thalmann, D., École polytechnique fédérale de Lausanne, Switzerland","Behavioral realism and realistic interactions are major criteria for improving social presence in virtual reality environments. We focus on multi-party VR applications where computer agents and avatars interact, share and collaborate with each other using objects. Our formulation employs realistic animations to simulate human-like behavioral motions of computer agents while they interact with avatars to enhance the sense of social presence in the VR environment. We exemplify our proposed model in a VR volleyball game setup. We model specific underlying interactions like gazing, collision detection and miscellaneous reactions (like how to pick a volleyball, how to transfer the ball to server) between computers players and avatars in the VR Volleyball game. We conduct a preliminary user survey to illustrate the significance of inclusion of realistic interactions for improving sense of social presence in a multi-party VR environment. © 2018 Copyright held by the owner/author(s).","Agent; Avatar; Behavior realism; Interactions; Multiparty; Social presence; VR","Agents; Animation; Beam plasma interactions; Behavioral research; Interactive computer graphics; Sports; Virtual reality; Avatar; Behavior realism; Collision detection; Miscellaneous reactions; Multiparty; Social presence; Virtual-reality environment; VR applications; Computer games",Conference Paper,"Final","",Scopus,2-s2.0-85048396292
"Anderson-Hanley C., Barcelos N.M., Zimmerman E.A., Gillen R.W., Dunnam M., Cohen B.D., Yerokhin V., Miller K.E., Hayes D.J., Arciero P.J., Maloney M., Kramer A.F.","6507336640;56733043300;56662793000;7004376865;37005585800;57214384712;56857334200;35553389600;57201982242;6603014422;56732599900;57203070893;","The Aerobic and Cognitive Exercise Study (ACES) for community-dwelling older adults with or at-risk for mild cognitive impairment (MCI): Neuropsychological, neurobiological and neuroimaging outcomes of a randomized clinical trial",2018,"Frontiers in Aging Neuroscience","10","MAY", 76,"","",,39,"10.3389/fnagi.2018.00076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046698936&doi=10.3389%2ffnagi.2018.00076&partnerID=40&md5=c9bd9fd15d36be22229d63418ee78d3b","The Healthy Aging and Neuropsychology Lab, Union College, Schenectady, NY, United States; Alzheimer's Disease Center, Albany Medical Center, Albany, NY, United States; Sunnyview Rehabilitation Hospital, Schenectady, NY, United States; Stratton VA Medical Center, Albany, NY, United States; Department of Biology, Union College, Schenectady, NY, United States; Biomedical Sciences Department, Oklahoma State University, Tulsa, OK, United States; Department of Anatomy and Cell Biology, Oklahoma State University, Tulsa, OK, United States; Department of Health and Human Physiological Sciences, Skidmore College, Saratoga Springs, NY, United States; Beckman Institute, University of Illinois Urbana-Champaign, Champaign, IL, United States","Anderson-Hanley, C., The Healthy Aging and Neuropsychology Lab, Union College, Schenectady, NY, United States; Barcelos, N.M., The Healthy Aging and Neuropsychology Lab, Union College, Schenectady, NY, United States; Zimmerman, E.A., Alzheimer's Disease Center, Albany Medical Center, Albany, NY, United States; Gillen, R.W., Sunnyview Rehabilitation Hospital, Schenectady, NY, United States; Dunnam, M., Stratton VA Medical Center, Albany, NY, United States; Cohen, B.D., Department of Biology, Union College, Schenectady, NY, United States; Yerokhin, V., Biomedical Sciences Department, Oklahoma State University, Tulsa, OK, United States; Miller, K.E., Department of Anatomy and Cell Biology, Oklahoma State University, Tulsa, OK, United States; Hayes, D.J., The Healthy Aging and Neuropsychology Lab, Union College, Schenectady, NY, United States; Arciero, P.J., Department of Health and Human Physiological Sciences, Skidmore College, Saratoga Springs, NY, United States; Maloney, M., The Healthy Aging and Neuropsychology Lab, Union College, Schenectady, NY, United States; Kramer, A.F., Beckman Institute, University of Illinois Urbana-Champaign, Champaign, IL, United States","Prior research has found that cognitive benefits of physical exercise and brain health in older adults may be enhanced when mental exercise is interactive simultaneously, as in exergaming. It is unclear whether the cognitive benefit can be maximized by increasing the degree of mental challenge during exercise. This randomized clinical trial (RCT), the Aerobic and Cognitive Exercise Study (ACES) sought to replicate and extend prior findings of added cognitive benefit from exergaming to those with or at risk for mild cognitive impairment (MCI). ACES compares the effects of 6 months of an exer-tour (virtual reality bike rides) with the effects of a more effortful exer-score (pedaling through a videogame to score points). Fourteen community-dwelling older adults meeting screening criteria for MCI (sMCI) were adherent to their assigned exercise for 6 months. The primary outcome was executive function, while secondary outcomes included memory and everyday cognitive function. Exer-tour and exer-score yielded significant moderate effects on executive function (Stroop A/C; d's = 0.51 and 0.47); there was no significant interaction effect. However, after 3 months the exer-tour revealed a significant and moderate effect, while exer-score showed little impact, as did a game-only condition. Both exer-tour and exer-score conditions also resulted in significant improvements in verbal memory. Effects appear to generalize to self-reported everyday cognitive function. Pilot data, including salivary biomarkers and structural MRI, were gathered at baseline and 6 months; exercise dose was associated with increased BDNF as well as increased gray matter volume in the PFC and ACC. Improvement in memory was associated with an increase in the DLPFC. Improved executive function was associated with increased expression of exosomal miRNA-9. Interactive physical and cognitive exercise (both high and low mental challenge) yielded similarly significant cognitive benefit for adherent sMCI exercisers over 6 months. A larger RCT is needed to confirm these findings. Further innovation and clinical trial data are needed to develop accessible, yet engaging and effective interventions to combat cognitive decline for the growing MCI population. © 2018 Anderson-Hanley, Barcelos, Zimmerman, Gillen, Dunnam, Cohen, Yerokhin, Miller, Hayes, Arciero, Maloney and Kramer.","Aging; Alzheimer's disease; Cognitive; Dementia; Exercise; Exergame; MCI; Neuropsychological","brain derived neurotrophic factor; C reactive protein; interleukin 6; microRNA 9; somatomedin C; vasculotropin; aged; anterior cingulate; Article; brain size; cognition; cognitive exercise; controlled study; dorsomedial prefrontal cortex; executive function; exercise; female; gene expression; gray matter; human; major clinical study; male; mild cognitive impairment; neurobiology; neuroimaging; neuropsychology; nuclear magnetic resonance imaging; pilot study; prefrontal cortex; protein blood level; randomized controlled trial; verbal memory",Article,"Final","",Scopus,2-s2.0-85046698936
"Jung J., Ahn Y.J.","57220995238;57201853639;","Effects of interface on procedural skill transfer in virtual training: Lifeboat launching operation study",2018,"Computer Animation and Virtual Worlds","29","3-4", e1812,"","",,7,"10.1002/cav.1812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046267906&doi=10.1002%2fcav.1812&partnerID=40&md5=0f4881ff1662eaf041a1adb3184611b7","Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","Jung, J., Korea Research Institute of Ships and Ocean Engineering, Daejeon, South Korea; Ahn, Y.J., Korea Institute of Maritime and Fisheries Technology, Busan, South Korea","A comparative study assessing the effect of interface type on procedural skill transfer during virtual training is presented. The aim of this research is to evaluate the transferability of two aspects of procedural skills, that is, procedural knowledge and technical skills. We established one group with a lecture and three virtual training groups with a combination of output and input devices: a monitor and keyboard/mouse, a head-mounted display (HMD) and joypad, and an HMD and wearable sensors. The task for assessment was a lifeboat launching operation that requires a participant to memorize a 10-step procedure utilizing 14 different pieces of equipment that should be manipulated in each step. Before and after training, we evaluated the participants' procedural knowledge and technical skill on a real lifeboat. The monitor and keyboard/mouse group showed the best performance in a procedural knowledge assessment that addressed visually induced recollections from the real lifeboat. Alternatively, in the assessment of technical skills that determined manipulation ability that requires word-based mnemonics, the HMD and wearable sensors group outperformed the other groups. Moreover, the results showed that the virtual training was a more efficient training format for short-term training than a lecture due to the freedom of observation viewpoint, despite simulator sickness. Copyright © 2018 John Wiley & Sons, Ltd.","interface; maritime safety; procedural skill transfer; virtual reality; virtual training","Helmet mounted displays; Interfaces (materials); Lifeboats; Typewriter keyboards; Virtual reality; Wearable sensors; Comparative studies; Head mounted displays; Lifeboat launching; Maritime safety; Procedural knowledge; Simulator sickness; Skill transfer; Virtual training; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85046267906
"Ortiz de Gortari A.B.","55977387100;","Empirical study on Game Transfer Phenomena in a location-based augmented reality game",2018,"Telematics and Informatics","35","2",,"382","396",,13,"10.1016/j.tele.2017.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039748060&doi=10.1016%2fj.tele.2017.12.015&partnerID=40&md5=3318b1b629244bc8df96a94cfc610306","University of Liège, Psychology and Neuroscience of Cognition Research Unit, Belgium; University of Bergen, Centre for the Science of Learning and Technology, Vektergården, Christies gate 13, Bergen, 5015, Norway","Ortiz de Gortari, A.B., University of Liège, Psychology and Neuroscience of Cognition Research Unit, Belgium, University of Bergen, Centre for the Science of Learning and Technology, Vektergården, Christies gate 13, Bergen, 5015, Norway","Research on Game Transfer Phenomena (GTP) has demonstrated that playing video games can lead to re-experiencing images, sounds, tactile sensations, spontaneous thoughts and actions, sometimes triggered by physical objects/events associated with the game. Location-based augmented reality games posit interesting questions regarding GTP, particularly because they use physical locations, they overlay digital images in physical contexts and the gameplay shifts between the virtual and the physical world. This study aims to investigate the prevalence of GTP and the role of immersion, augmented reality and sound in a sample of English- (EnS) and Spanish- (SpS) speaking gamers of the game Pokémon Go (PoGo). A total of 1313 gamers (Mage = 31.47) were recruited online. GTP was less common than in previous studies; however, 82.4% had experienced GTP at least once. The SpS showed higher prevalence of GTP and played more intensively. Automatic mental processes predominated in the EnS, while behaviours and actions were more common in the SpS. The absence or presence of video game features seems important for the way GTP manifests. For instance, tactile hallucinations were more prevalent, while sensations of self-motion were less reported. Playing with augmented reality (AR) and sounds showed significant correlations with various GTP types, but not with re-experiencing images from the game. More gamers who reported the sensation that Pokémon were physically present or looked for Pokémon outside the screen while playing, as connotations of immersion, had experienced GTP. Experiencing GTP while playing may be more common in location-based augmented reality games, compared to other games. © 2017 Elsevier Ltd","Auditory cues; Augmented reality games; Game Transfer Phenomena; Immersion; Location-based games; Pokémon Go","Augmented reality; Interactive computer graphics; Location; Auditory cues; Empirical studies; Immersion; Location based games; Physical locations; Physical objects; Tactile sensation; Transfer phenomenon; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85039748060
"Massetti T., Fávero F.M., De Menezes L.D.C., Alvarez M.P.B., Crocetta T.B., Guarnieri R., Nunes F.L.S., Monteiro C.B.D.M., Silva T.D.D.","55546122700;36113108600;57191485293;57193085957;54898013000;57193692889;7102392843;55481862300;55546962700;","Achievement of Virtual and Real Objects Using a Short-Term Motor Learning Protocol in People with Duchenne Muscular Dystrophy: A Crossover Randomized Controlled Trial",2018,"Games for Health Journal","7","2",,"107","115",,12,"10.1089/g4h.2016.0088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045379157&doi=10.1089%2fg4h.2016.0088&partnerID=40&md5=fe7ff5c326b3e0fd633cb2d730cfe4f7","Faculty of Medicine, University of São Paulo, Rua: Cipotanea, 51. Cidade Universitaria, Sao Paulo, CEP 05360-160, Brazil; Paulista School of Medicine, Federal University of São Paulo, UNIFESP, São Paulo, Brazil; Faculty of Medicine, ABC, Santo André, Brazil; School of Arts, Sciences and Humanities, University of São Paulo, EACH-USP, São Paulo, Brazil","Massetti, T., Faculty of Medicine, University of São Paulo, Rua: Cipotanea, 51. Cidade Universitaria, Sao Paulo, CEP 05360-160, Brazil; Fávero, F.M., Paulista School of Medicine, Federal University of São Paulo, UNIFESP, São Paulo, Brazil; De Menezes, L.D.C., Faculty of Medicine, University of São Paulo, Rua: Cipotanea, 51. Cidade Universitaria, Sao Paulo, CEP 05360-160, Brazil; Alvarez, M.P.B., Faculty of Medicine, University of São Paulo, Rua: Cipotanea, 51. Cidade Universitaria, Sao Paulo, CEP 05360-160, Brazil; Crocetta, T.B., Faculty of Medicine, ABC, Santo André, Brazil; Guarnieri, R., Faculty of Medicine, ABC, Santo André, Brazil; Nunes, F.L.S., School of Arts, Sciences and Humanities, University of São Paulo, EACH-USP, São Paulo, Brazil; Monteiro, C.B.D.M., Faculty of Medicine, University of São Paulo, Rua: Cipotanea, 51. Cidade Universitaria, Sao Paulo, CEP 05360-160, Brazil; Silva, T.D.D., Paulista School of Medicine, Federal University of São Paulo, UNIFESP, São Paulo, Brazil","Objective: To evaluate whether people with Duchenne muscular dystrophy (DMD) practicing a task in a virtual environment could improve performance given a similar task in a real environment, as well as distinguishing whether there is transference between performing the practice in virtual environment and then a real environment and vice versa. Methods: Twenty-two people with DMD were evaluated and divided into two groups. The goal was to reach out and touch a red cube. Group A began with the real task and had to touch a real object, and Group B began with the virtual task and had to reach a virtual object using the Kinect system. Results: ANOVA showed that all participants decreased the movement time from the first (M = 973 ms) to the last block of acquisition (M = 783 ms) in both virtual and real tasks and motor learning could be inferred by the short-term retention and transfer task (with increasing distance of the target). However, the evaluation of task performance demonstrated that the virtual task provided an inferior performance when compared to the real task in all phases of the study, and there was no effect for sequence. Conclusions: Both virtual and real tasks promoted improvement of performance in the acquisition phase, short-term retention, and transfer. However, there was no transference of learning between environments. In conclusion, it is recommended that the use of virtual environments for individuals with DMD needs to be considered carefully. © Copyright 2018, Mary Ann Liebert, Inc. 2018.","Duchenne muscular dystrophy; Virtual reality game","adolescent; analysis of variance; Brazil; child; computer interface; controlled study; crossover procedure; Duchenne muscular dystrophy; human; male; motor performance; physiology; psychology; randomized controlled trial; standards; task performance; transfer of learning; trends; video game; virtual reality; young adult; Adolescent; Analysis of Variance; Brazil; Child; Cross-Over Studies; Humans; Male; Motor Skills; Muscular Dystrophy, Duchenne; Task Performance and Analysis; Transfer (Psychology); User-Computer Interface; Video Games; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85045379157
"Murcia-López M., Steed A.","57194036190;18435050200;","A comparison of virtual and physical training transfer of bimanual assembly tasks",2018,"IEEE Transactions on Visualization and Computer Graphics","24","4",,"1574","1583",,22,"10.1109/TVCG.2018.2793638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041644744&doi=10.1109%2fTVCG.2018.2793638&partnerID=40&md5=df74850c0558ed09574af1eb79529d4e","University College London, United Kingdom","Murcia-López, M., University College London, United Kingdom; Steed, A., University College London, United Kingdom","As we explore the use of consumer virtual reality technology for training applications, there is a need to evaluate its validity compared to more traditional training formats. In this paper, we present a study that compares the effectiveness of virtual training and physical training for teaching a bimanual assembly task. In a between-subjects experiment, 60 participants were trained to solve three 3D burr puzzles in one of six conditions comprised of virtual and physical training elements. In the four physical conditions, training was delivered via paper-And video-based instructions, with or without the physical puzzles to practice with. In the two virtual conditions, participants learnt to assemble the puzzles in an interactive virtual environment, with or without 3D animations showing the assembly process. After training, we conducted immediate tests in which participants were asked to solve a physical version of the puzzles. We measured performance through success rates and assembly completion testing times. We also measured training times as well as subjective ratings on several aspects of the experience. Our results show that the performance of virtually trained participants was promising. A statistically significant difference was not found between virtual training with animated instructions and the best performing physical condition (in which physical blocks were available during training) for the last and most complex puzzle in terms of success rates and testing times. Performance in retention tests two weeks after training was generally not as good as expected for all experimental conditions. We discuss the implications of the results and highlight the validity of virtual reality systems in training. © 2018 IEEE Computer Society. All rights reserved.","Assembly; Learning transfer; Training; Virtual reality","Assembly; Ergonomics; Haptic interfaces; Personnel training; Testing; Virtual reality; Experimental conditions; Interactive virtual environments; Learning Transfer; Statistically significant difference; Three-dimensional display; Training applications; Virtual reality system; Virtual reality technology; E-learning; adult; comparative study; computer interface; female; human; male; physiology; task performance; transfer of learning; virtual reality; young adult; Adult; Female; Humans; Male; Task Performance and Analysis; Transfer (Psychology); User-Computer Interface; Virtual Reality; Young Adult",Article,"Final","",Scopus,2-s2.0-85041644744
"Fromberger P., Jordan K., Müller J.L.","22957487700;7202963516;7404871741;","Virtual reality applications for diagnosis, risk assessment and therapy of child abusers",2018,"Behavioral Sciences and the Law","36","2",,"235","244",,10,"10.1002/bsl.2332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045625005&doi=10.1002%2fbsl.2332&partnerID=40&md5=556f40ad3c484a23a287c5d8d5e87f1a","Clinic for Psychiatry and Psychotherapy - Forensic Psychiatry, Human Medical Center Göttingen, Georg-August-Universität Göttingen, Germany","Fromberger, P., Clinic for Psychiatry and Psychotherapy - Forensic Psychiatry, Human Medical Center Göttingen, Georg-August-Universität Göttingen, Germany; Jordan, K., Clinic for Psychiatry and Psychotherapy - Forensic Psychiatry, Human Medical Center Göttingen, Georg-August-Universität Göttingen, Germany; Müller, J.L., Clinic for Psychiatry and Psychotherapy - Forensic Psychiatry, Human Medical Center Göttingen, Georg-August-Universität Göttingen, Germany","Despite the successful application of virtual reality (VR) in a wide variety of mental disorders and the obvious potentials that VR provides, the use of VR in the context of criminology and forensic psychology is sparse. For forensic mental health professionals, VR provides some advantages that outrun general advantages of VR, e.g., ecological validity and controllability of social situations. Most important seems to be the unique possibility to expose offenders and to train coping skills in virtual situations, which are able to elicit disorder-relevant behavior—without endangering others. VR has already been used for the assessment of deviant sexual interests, for testing the ability to transfer learned coping skills communicated during treatment to behavior, and for risk assessment of child abusers. This article reviews and discusses these innovative research projects with regard to their impact on current clinical practice regarding risk assessment and treatment as well as other implementations of VR applications in forensic mental health. Finally, ethical guidelines for VR research in forensic mental health are provided. Copyright © 2018 John Wiley & Sons, Ltd.",,"child; child abuse; human; mental disease; risk assessment; virtual reality; Child; Child Abuse; Humans; Mental Disorders; Risk Assessment; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85045625005
"Kim Y., Hong S., Kim G.J.","55699533800;57192155448;7403061980;","Augmented reality-based remote coaching for fast-paced physical task",2018,"Virtual Reality","22","1",,"25","36",,1,"10.1007/s10055-017-0315-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019101055&doi=10.1007%2fs10055-017-0315-2&partnerID=40&md5=e40202059b42698cc108a85ea29bb3d0","Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, South Korea","Kim, Y., Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, South Korea; Hong, S., Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, South Korea; Kim, G.J., Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, South Korea","One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks, presentation of such guidance in a way that the recipient can make immediate recognition and quick understanding can be an especially challenging problem. In this paper, we present an AR-based tele-coaching system applied to the game of tennis, called the AR coach, and explore for interface design guidelines through a user study. We have evaluated the player’s performance for instruction understanding when the coaching instruction was presented in four different modalities: (1) Visual—visual only, (2) Sound—aural only/mono, (3) 3D Sound—aural only/3D and (4) Multimodal—both visual and aural/mono. Results from the experiment suggested that, among the three, the visual-only augmentation was the most effective and least distracting for the given pace of information transfer (e.g., under every 3 s). We attribute such a result to the characteristic of the visual modality to encode and present a lot of information at once and the human’s limited capability in handling and fusing multimodal information at a relatively fast rate. © 2017, Springer-Verlag London.","Augmented reality; Multimodal feedback; Pre-attentive recognition; Tele-coaching","Software engineering; Virtual reality; Information transfers; Interface designs; Multi-modal information; Multimodal feedback; Pre-attentive; Remote experts; Tele-coaching; Visual modalities; Augmented reality",Article,"Final","",Scopus,2-s2.0-85019101055
"Brito P.Q., Stoyanova J., Coelho A.","16678683600;55992151400;23089899600;","Augmented reality versus conventional interface: Is there any difference in effectiveness?",2018,"Multimedia Tools and Applications","77","6",,"7487","7516",,3,"10.1007/s11042-017-4658-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018835569&doi=10.1007%2fs11042-017-4658-1&partnerID=40&md5=41cff006e4398a3d761d6a60896d3c3a","Faculdade de Economia, LIAAD/INESC-Tec, Universidade do Porto, Rua Dr. Roberto Frias, Porto, 4200-464, Portugal; Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal","Brito, P.Q., Faculdade de Economia, LIAAD/INESC-Tec, Universidade do Porto, Rua Dr. Roberto Frias, Porto, 4200-464, Portugal; Stoyanova, J., Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal; Coelho, A., Faculdade de Engenharia, Universidade do Porto, Rua Dr. Roberto Frias s/n, Porto, 4200-464, Portugal","The moment immediately before the “add to cart” decision is very critical in online shopping. Drawing on theories of transfer, spreading activation and human-computer interaction, the superiority of markerless Augmented Reality (AR) and Marker-based augmented reality (M) over Conventional Interactive (CI) is hypothesized. Although those multimedia tools are not part of the product/brand motivating the consumer interest they interfere in the interactive performance of the ecommerce. 150 consumers in a lab experiment showed higher emotional response, interactive response and brand evaluation in M and AR than CI. Contrary to what was expected the usability results were the inverse. That is, usability of CI outperforms M and AR. Considering only AR and M interfaces their effect on psychological variables was not statistically significant. A sophisticated or a simple interface had no impact on intention to buy the target brand, but the brand recommendation improved from M to AR. The differing effect of those three interface systems was mediated by brand familiarity, perceived risk, opinion leadership and positive emotional traits. © 2017, Springer Science+Business Media New York.","Augmented reality; Brand evaluation; Effectiveness; Emotion; Experimental design; Online shopping","Augmented reality; Behavioral research; Design of experiments; Electronic commerce; Brand evaluation; Effectiveness; Emotion; Emotional response; Interactive performance; Online shopping; Psychological variables; Spreading activations; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85018835569
"Serafin S., Geronazzo M., Erkut C., Nilsson N.C., Nordahl R.","6603367536;36720522500;6507065675;54993660100;32867973300;","Sonic Interactions in Virtual Reality: State of the Art, Current Challenges, and Future Directions",2018,"IEEE Computer Graphics and Applications","38","2",,"31","43",,27,"10.1109/MCG.2018.193142628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045191223&doi=10.1109%2fMCG.2018.193142628&partnerID=40&md5=d0263b24cb2f433025831c345d9340bd","Aalborg University, Department of Architecture, Design, and Media Technology, Denmark; Aalborg University, Denmark","Serafin, S., Aalborg University, Department of Architecture, Design, and Media Technology, Denmark; Geronazzo, M., Aalborg University, Department of Architecture, Design, and Media Technology, Denmark; Erkut, C., Aalborg University, Denmark; Nilsson, N.C., Aalborg University, Denmark; Nordahl, R., Aalborg University, Department of Architecture, Design, and Media Technology, Denmark","A high-fidelity but efficient sound simulation is an essential element of any VR experience. Many of the techniques used in virtual acoustics are graphical rendering techniques suitably modified to account for sound generation and propagation. In recent years, several advances in hardware and software technologies have been facilitating the development of immersive interactive sound-rendering experiences. In this article, we present a review of the state of the art of such simulations, with a focus on the different elements that, combined, provide a complete interactive sonic experience. This includes physics-based simulation of sound effects and their propagation in space together with binaural rendering to simulate the position of sound sources. We present how these different elements of the sound design pipeline have been addressed in the literature, trying to find the trade-off between accuracy and plausibility. Recent applications and current challenges are also presented. © 1981-2012 IEEE.","computer graphics; head-related transfer function; sonic interaction; sound rendering","Economic and social effects; Object recognition; Immersive; Information interfaces and representations; Information technology and systems; Sonic interactions; Sound and music computing; Virtual reality",Article,"Final","",Scopus,2-s2.0-85045191223
"Mattausch O., Goksel O.","8312955600;8836175700;","Image-Based Reconstruction of Tissue Scatterers Using Beam Steering for Ultrasound Simulation",2018,"IEEE Transactions on Medical Imaging","37","3",,"767","780",,8,"10.1109/TMI.2017.2770118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034214894&doi=10.1109%2fTMI.2017.2770118&partnerID=40&md5=5df81d48c6654d0f3f2356efacf0f919","Computer-Assisted Applications in Medicine Group, Computer Vision Laboratory, ETH Zurich, Zürich, 8092, Switzerland","Mattausch, O., Computer-Assisted Applications in Medicine Group, Computer Vision Laboratory, ETH Zurich, Zürich, 8092, Switzerland; Goksel, O., Computer-Assisted Applications in Medicine Group, Computer Vision Laboratory, ETH Zurich, Zürich, 8092, Switzerland","Numerical simulation of ultrasound images can facilitate the training of sonographers. An efficient and realistic model for the simulation of ultrasonic speckle is the convolution of the ultrasound point-spread function with a distribution of point scatterers. Nevertheless, for a given arbitrary tissue type, a scatterer map that would generate a realistic appearance of that tissue is not known a priori. In this paper, we introduce a principled approach to estimate (reconstruct) such a scatterer map from images, by solving the inverse-problem of ultrasound speckle formation, such that images from arbitrary view angles and transducer settings can be generated from those scatterer maps later in simulations. Robust reconstructions are achieved by using multiple measurements of the same tissue with different viewing parameters. For this purpose, a novel use of beam-steering to rapidly and conveniently acquire multiple images of the same scene is proposed. We demonstrate in numerical and physical phantoms and in vivo images that the appearance of synthesized images closely match real images for a range of viewing parameters and probe settings. We also present a scene editing scenario exploiting these scatterer representations to create realistic images of augmented anatomy. © 2017 IEEE.","Biomedical imaging; deconvolution; graphics; inverse problems; medical diagnostic imaging; virtual reality","Acoustics; Convolution; Imaging techniques; Inverse problems; Optical transfer function; Personnel training; Speckle; Tissue; Ultrasonic imaging; Multiple measurements; Realistic images; Robust reconstruction; Synthesized images; Ultrasound images; Ultrasound simulation; Ultrasound speckle; Viewing parameters; Image reconstruction; Article; beam steering; human; image reconstruction; in vivo study; radiological procedures; simulation; ultrasound; algorithm; biological model; breast; breast tumor; computer simulation; diagnostic imaging; echography; female; image processing; imaging phantom; procedures; virtual reality; Algorithms; Breast; Breast Neoplasms; Computer Simulation; Female; Humans; Image Processing, Computer-Assisted; Models, Biological; Phantoms, Imaging; Ultrasonography; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85034214894
"Berger C.C., Gonzalez-Franco M., Tajadura-Jiménez A., Florencio D., Zhang Z.","57210476689;36080251200;23569030500;6602865660;13609600600;","Generic HRTFs may be good enough in virtual reality. Improving source localization through cross-modal plasticity",2018,"Frontiers in Neuroscience","12","FEB", 21,"","",,17,"10.3389/fnins.2018.00021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041845498&doi=10.3389%2ffnins.2018.00021&partnerID=40&md5=3453eac39fc20cf19e5f21f4ee10c783","Microsoft Research, Redmond, WA, United States; Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, CA, United States; UCL Interaction Centre, University College London, London, United Kingdom; Interactive Systems DEI-Lab, Universidad Carlos III de Madrid, Madrid, Spain; Department Electrical Engineering, University of Washington, Seattle, WA, United States","Berger, C.C., Microsoft Research, Redmond, WA, United States, Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, CA, United States; Gonzalez-Franco, M., Microsoft Research, Redmond, WA, United States; Tajadura-Jiménez, A., UCL Interaction Centre, University College London, London, United Kingdom, Interactive Systems DEI-Lab, Universidad Carlos III de Madrid, Madrid, Spain; Florencio, D., Microsoft Research, Redmond, WA, United States; Zhang, Z., Microsoft Research, Redmond, WA, United States, Department Electrical Engineering, University of Washington, Seattle, WA, United States","Auditory spatial localization in humans is performed using a combination of interaural time differences, interaural level differences, as well as spectral cues provided by the geometry of the ear. To render spatialized sounds within a virtual reality (VR) headset, either individualized or generic Head Related Transfer Functions (HRTFs) are usually employed. The former require arduous calibrations, but enable accurate auditory source localization, which may lead to a heightened sense of presence within VR. The latter obviate the need for individualized calibrations, but result in less accurate auditory source localization. Previous research on auditory source localization in the real world suggests that our representation of acoustic space is highly plastic. In light of these findings, we investigated whether auditory source localization could be improved for users of generic HRTFs via cross-modal learning. The results show that pairing a dynamic auditory stimulus, with a spatio-temporally aligned visual counterpart, enabled users of generic HRTFs to improve subsequent auditory source localization. Exposure to the auditory stimulus alone or to asynchronous audiovisual stimuli did not improve auditory source localization. These findings have important implications for human perception as well as the development of VR systems as they indicate that generic HRTFs may be enough to enable good auditory source localization in VR. © 2018 Berger, Gonzalez-Franco, Tajadura-Jiménez, Florencio and Zhang.","Auditory perception; Auditory training; Cross-modal perception; Cross-modal plasticity; HRTF (head related transfer function); Spatial audio; Virtual reality","article; auditory stimulation; calibration; head; hearing; human; learning; plasticity; virtual reality",Article,"Final","",Scopus,2-s2.0-85041845498
"Li X., Yi W., Chi H.-L., Wang X., Chan A.P.C.","57193211001;55125204800;35096047900;8945580300;56844258500;","A critical review of virtual and augmented reality (VR/AR) applications in construction safety",2018,"Automation in Construction","86",,,"150","162",,176,"10.1016/j.autcon.2017.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034043364&doi=10.1016%2fj.autcon.2017.11.003&partnerID=40&md5=2ee7532a8fe659b1dda070548e3767b4","Department of Building and Real Estate, The Hong Kong Polytechnic University, Hong Kong; School of Engineering and Advanced Technology, College of Sciences, Massey University, New Zealand; The Australasian Joint Research Centre for Building Information Modelling, School of Built Environment, Curtin University, Perth, Australia; The International Scholar, Department of Housing and Interior Design, Kyung Hee University, Australia","Li, X., Department of Building and Real Estate, The Hong Kong Polytechnic University, Hong Kong; Yi, W., School of Engineering and Advanced Technology, College of Sciences, Massey University, New Zealand; Chi, H.-L., Department of Building and Real Estate, The Hong Kong Polytechnic University, Hong Kong; Wang, X., The Australasian Joint Research Centre for Building Information Modelling, School of Built Environment, Curtin University, Perth, Australia, The International Scholar, Department of Housing and Interior Design, Kyung Hee University, Australia; Chan, A.P.C., Department of Building and Real Estate, The Hong Kong Polytechnic University, Hong Kong","Construction is a high hazard industry which involves many factors that are potentially dangerous to workers. Safety has always been advocated by many construction companies, and they have been working hard to make sure their employees are protected from fatalities and injuries. With the advent of Virtual and Augmented Reality (VR/AR), there has been a witnessed trend of capitalizing on sophisticated immersive VR/AR applications to create forgiving environments for visualizing complex workplace situations, building up risk-preventive knowledge and undergoing training. To better understand the state-of-the-art of VR/AR applications in construction safety (VR/AR-CS) and from which to uncover the related issues and propose possible improvements, this paper starts with a review and synthesis of research evidence for several VR/AR prototypes, products and the related training and evaluation paradigms. Predicated upon a wide range of well-acknowledged scholarly journals, this paper comes up with a generic taxonomy consisting of VR/AR technology characteristics, application domains, safety scenarios and evaluation methods. According to this taxonomy, a number of technical features and types that could be implemented in the context of construction safety enhancement are derived and further elaborated, while significant application domains and trends regarding the VR/AR-CS research are generalized, i.e., hazards recognition and identification, safety training and education, safety instruction and inspection, and so on. Last but not least, this study sets forth a list of gaps derived from the in-depth review and comes up with the prospective research works. It is envisioned that the outcomes of this paper could assist both researchers and industrial practitioners with appreciating the research and practice frontier of VR/AR-CS and soliciting the latest VR/AR applications. © 2017 Elsevier B.V.","Augmented reality; Construction; Review; Safety; Virtual reality","Accident prevention; Augmented reality; Construction; Construction industry; Hazards; Industrial research; Occupational risks; Petroleum reservoir evaluation; Reviews; Taxonomies; Construction companies; Construction safety; Evaluation methods; Industrial practitioners; Safety instructions; Scholarly journals; Technology characteristics; Virtual and augmented reality; Virtual reality",Article,"Final","",Scopus,2-s2.0-85034043364
"Zahiri M., Nelson C.A., Oleynikov D., Siu K.-C.","55873128300;15073221300;35474063300;57192938181;","Evaluation of Augmented Reality Feedback in Surgical Training Environment",2018,"Surgical Innovation","25","1",,"81","87",,4,"10.1177/1553350617739425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040807090&doi=10.1177%2f1553350617739425&partnerID=40&md5=c7c07553e3902821b9d9c1895dbfdfec","University of Nebraska–Lincoln, Lincoln, NE, United States; University of Nebraska Medical Center, Omaha, NE, United States","Zahiri, M., University of Nebraska–Lincoln, Lincoln, NE, United States; Nelson, C.A., University of Nebraska–Lincoln, Lincoln, NE, United States; Oleynikov, D., University of Nebraska Medical Center, Omaha, NE, United States; Siu, K.-C., University of Nebraska Medical Center, Omaha, NE, United States","Providing computer-based laparoscopic surgical training has several advantages that enhance the training process. Self-evaluation and real-time performance feedback are 2 of these advantages, which avoid dependency of trainees on expert feedback. The goal of this study was to investigate the use of a visual time indicator as real-time feedback correlated with the laparoscopic surgical training. Twenty novices participated in this study working with (and without) different presentations of time indicators. They performed a standard peg transfer task, and their completion times and muscle activity were recorded and compared. Also of interest was whether the use of this type of feedback induced any side effect in terms of motivation or muscle fatigue. Results. Of the 20 participants, 15 (75%) preferred using a time indicator in the training process rather than having no feedback. However, time to task completion showed no significant difference in performance with the time indicator; furthermore, no significant differences in muscle activity or muscle fatigue were detected with/without time feedback. Conclusion. The absence of significant difference between task performance with/without time feedback shows that using visual real-time feedback can be included in surgical training based on user preference. Trainees may benefit from this type of feedback in the form of increased motivation. The extent to which this can influence training frequency leading to performance improvement is a question for further study. © 2017, © The Author(s) 2017.","biomedical engineering; simulation; surgical education","Article; clinical article; electromyography; environment; female; Fourier transformation; human; laparoscopy; male; muscle contraction; muscle fatigue; stress; surgical training; task performance; clinical competence; education; feedback system; laparoscopy; questionnaire; teaching; virtual reality; Clinical Competence; Computer-Assisted Instruction; Feedback; Female; Humans; Laparoscopy; Male; Surveys and Questionnaires; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85040807090
"Harada H., Kanaji S., Nishi M., Otake Y., Hasegawa H., Yamamoto M., Matsuda Y., Yamashita K., Matsuda T., Oshikiri T., Sumi Y., Nakamura T., Suzuki S., Sato Y., Kakeji Y.","57201492908;6602447690;56410427800;57204280185;55793187513;57207791395;35603877700;37008161100;55825472500;6603679030;36928186800;56183829400;55732211900;56579458400;55403089300;","The learning effect of using stereoscopic vision in the early phase of laparoscopic surgical training for novices",2018,"Surgical Endoscopy","32","2",,"582","588",,10,"10.1007/s00464-017-5654-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021093563&doi=10.1007%2fs00464-017-5654-2&partnerID=40&md5=73cb7807f5ec4f2a91383fe50a7e6b08","Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan","Harada, H., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Kanaji, S., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Nishi, M., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Otake, Y., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Hasegawa, H., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Yamamoto, M., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Matsuda, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Yamashita, K., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Matsuda, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Oshikiri, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Sumi, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Nakamura, T., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Suzuki, S., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan; Sato, Y., Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Kakeji, Y., Division of Gastrointestinal Surgery, Department of Surgery, Graduate School of Medicine, Kobe University, 7-5-2, Kusunoki-cho, Chuo-ku, Kobe, Hyogo  650-0017, Japan","Background: Recently to improve depth perception, the performance of three-dimensional (3D) laparoscopic surgeries has increased. However, the effects of laparoscopic training using 3D are still unclear. This study aimed to clarify the effects of using a 3D monitor among novices in the early phase of training. Methods: Participants were 40 novices who had never performed laparoscopic surgery (20 medical students and 20 junior residents). Three laparoscopic phantom tasks (task 1: touching markers on a flat disk with a rod; task 2: straight rod transfer through a single loop; and task 3: curved rod transfer through two loops) in the training box were performed ten times, respectively. Performances were recorded by an optical position tracker. The participants were randomly divided into two groups: one group performed each task five times initially under a 2D system (2D start group), and the other group performed each task five times under a 3D system (3D start group). Both groups then performed the same task five times. After the trial, we evaluated the performance scores (operative time, path length of forceps, and technical errors) and the learning curves for both groups. Results: Scores for all tasks performed under the 3D system were significantly better than scores for tasks using the 2D system. Scores for each task in the 2D start group improved after switching to the 3D system. However, scores for each task in the 3D start group were worse after switching to the 2D system, especially scores related to technical errors. Conclusions: The stereoscopic vision improved laparoscopic surgical techniques of novices from the early phase of training. However, the performance of novices trained only by 3D worsened by changing to the 2D environment. © 2017, The Author(s).","2D laparoscopy; 3D laparoscopy; Learning effect; Novice; Task performance; Training","Article; depth perception; eye tracking; human; laparoscopic surgery; learning curve; medical student; operation duration; outcome assessment; priority journal; resident; stereoscopic vision; surgical training; three dimensional imaging; clinical competence; education; laparoscopy; procedures; simulation training; task performance; Clinical Competence; Depth Perception; Humans; Imaging, Three-Dimensional; Laparoscopy; Learning Curve; Simulation Training; Students, Medical; Task Performance and Analysis",Article,"Final","",Scopus,2-s2.0-85021093563
"Zimmermann T., Taetz B., Bleser G.","7103341642;37006091800;22950167800;","IMU-to-segment assignment and orientation alignment for the lower body using deep learning",2018,"Sensors (Switzerland)","18","1", 302,"","",,24,"10.3390/s18010302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040931637&doi=10.3390%2fs18010302&partnerID=40&md5=184e5c8e507847e9a952cd9b93f80d25","Junior Research Group wearHEALTH, University of Kaiserslautern, Gottlieb-Daimler-Str. 48, Kaiserslautern, 67663, Germany; Augmented Vision Department, DFKI, Trippstadter Str. 122, Kaiserslautern, 67663, Germany","Zimmermann, T., Junior Research Group wearHEALTH, University of Kaiserslautern, Gottlieb-Daimler-Str. 48, Kaiserslautern, 67663, Germany, Augmented Vision Department, DFKI, Trippstadter Str. 122, Kaiserslautern, 67663, Germany; Taetz, B., Junior Research Group wearHEALTH, University of Kaiserslautern, Gottlieb-Daimler-Str. 48, Kaiserslautern, 67663, Germany, Augmented Vision Department, DFKI, Trippstadter Str. 122, Kaiserslautern, 67663, Germany; Bleser, G., Junior Research Group wearHEALTH, University of Kaiserslautern, Gottlieb-Daimler-Str. 48, Kaiserslautern, 67663, Germany, Augmented Vision Department, DFKI, Trippstadter Str. 122, Kaiserslautern, 67663, Germany","Human body motion analysis based on wearable inertial measurement units (IMUs) receives a lot of attention from both the research community and the and industrial community. This is due to the significant role in, for instance, mobile health systems, sports and human computer interaction. In sensor based activity recognition, one of the major issues for obtaining reliable results is the sensor placement/assignment on the body. For inertial motion capture (joint kinematics estimation) and analysis, the IMU-to-segment (I2S) assignment and alignment are central issues to obtain biomechanical joint angles. Existing approaches for I2S assignment usually rely on hand crafted features and shallow classification approaches (e.g., support vector machines), with no agreement regarding the most suitable features for the assignment task. Moreover, estimating the complete orientation alignment of an IMU relative to the segment it is attached to using a machine learning approach has not been shown in literature so far. This is likely due to the high amount of training data that have to be recorded to suitably represent possible IMU alignment variations. In this work, we propose online approaches for solving the assignment and alignment tasks for an arbitrary amount of IMUs with respect to a biomechanical lower body model using a deep learning architecture and windows of 128 gyroscope and accelerometer data samples. For this, we combine convolutional neural networks (CNNs) for local filter learning with long-short-term memory (LSTM) recurrent networks as well as generalized recurrent units (GRUs) for learning time dynamic features. The assignment task is casted as a classification problem, while the alignment task is casted as a regression problem. In this framework, we demonstrate the feasibility of augmenting a limited amount of real IMU training data with simulated alignment variations and IMU data for improving the recognition/estimation accuracies. With the proposed approaches and final models we achieved 98.57% average accuracy over all segments for the I2S assignment task (100% when excluding left/right switches) and an average median angle error over all segments and axes of 2.91° for the I2S alignment task. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Automatic sensor alignment; Automatic sensor placement; CNN; Deep learning; Inertial sensors; LSTM; Neural networks","Biomechanics; Human computer interaction; Industrial research; Learning systems; Long short-term memory; Neural networks; Recurrent neural networks; Units of measurement; Wearable sensors; Automatic sensor placement; Convolutional neural network; Inertial measurement unit; Inertial sensor; Joint kinematics estimation; LSTM; Machine learning approaches; Sensor alignment; Deep learning; artificial neural network; biomechanics; human; machine learning; motion; Biomechanical Phenomena; Humans; Machine Learning; Motion; Neural Networks (Computer)",Article,"Final","",Scopus,2-s2.0-85040931637
"Kobayashi L., Zhang X.C., Collins S.A., Karim N., Merck D.L.","6602256106;56893437900;56669204500;57192209428;8856652000;","Exploratory application of augmented reality/mixed reality devices for acute care procedure training",2018,"Western Journal of Emergency Medicine","19","1",,"158","164",,15,"10.5811/westjem.2017.10.35026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041089107&doi=10.5811%2fwestjem.2017.10.35026&partnerID=40&md5=5fcbe34650588f7189423a37de4950a2","Alpert Medical School, Brown University, Department of Emergency Medicine, Providence, RI, United States; Rhode Island Hospital, CT Scan Department, Providence, RI, United States; Alpert Medical School, Brown University, Department of Diagnostic Imaging, Providence, RI, United States; Lifespan Medical Simulation Center, Coro West Building, 1 Hoppin Street, Providence, RI  02903, United States","Kobayashi, L., Alpert Medical School, Brown University, Department of Emergency Medicine, Providence, RI, United States, Lifespan Medical Simulation Center, Coro West Building, 1 Hoppin Street, Providence, RI  02903, United States; Zhang, X.C., Alpert Medical School, Brown University, Department of Emergency Medicine, Providence, RI, United States; Collins, S.A., Rhode Island Hospital, CT Scan Department, Providence, RI, United States; Karim, N., Alpert Medical School, Brown University, Department of Emergency Medicine, Providence, RI, United States; Merck, D.L., Alpert Medical School, Brown University, Department of Diagnostic Imaging, Providence, RI, United States","Introduction: Augmented reality (AR), mixed reality (MR), and virtual reality devices are enabling technologies that may facilitate effective communication in healthcare between those with information and knowledge (clinician/specialist; expert; educator) and those seeking understanding and insight (patient/family; non-expert; learner). Investigators initiated an exploratory program to enable the study of AR/MR use-cases in acute care clinical and instructional settings. Methods: Academic clinician educators, computer scientists, and diagnostic imaging specialists conducted a proof-of-concept project to 1) implement a core holoimaging pipeline infrastructure and open-access repository at the study institution, and 2) use novel AR/MR techniques on off-the-shelf devices with holoimages generated by the infrastructure to demonstrate their potential role in the instructive communication of complex medical information. Results: The study team successfully developed a medical holoimaging infrastructure methodology to identify, retrieve, and manipulate real patients’ de-identified computed tomography and magnetic resonance imagesets for rendering, packaging, transfer, and display of modular holoimages onto AR/MR headset devices and connected displays. Holoimages containing key segmentations of cervical and thoracic anatomic structures and pathology were overlaid and registered onto physical task trainers for simulation-based “blind insertion” invasive procedural training. During the session, learners experienced and used task-relevant anatomic holoimages for central venous catheter and tube thoracostomy insertion training with enhanced visual cues and haptic feedback. Direct instructor access into the learner’s AR/MR headset view of the task trainer was achieved for visual-axis interactive instructional guidance. Conclusion: Investigators implemented a core holoimaging pipeline infrastructure and modular open-access repository to generate and enable access to modular holoimages during exploratory pilot stage applications for invasive procedure training that featured innovative AR/MR techniques on off-the-shelf headset devices.",,"algorithm; Article; association; augmented reality; cervical spine; computer assisted tomography; emergency care; holoimaging infrastructure; human; invasive procedure; medical education; mixed reality; nuclear magnetic resonance imaging; proof of concept; tactile feedback; thoracic spine; thoracostomy; videoconferencing; virtual reality; computer interface; feedback system; image processing; learning; procedures; teaching; Computer-Assisted Instruction; Feedback; Humans; Image Processing, Computer-Assisted; Learning; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85041089107
"Sampaio A.Z.","35609919400;","Education in engineering: Bim and VR technologies improving collaborative projects",2018,"EUCEET 2018 - 4th International Conference on Civil Engineering Education: Challenges for the Third Millennium",,,,"48","57",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088275381&partnerID=40&md5=38f979b5018bc196a916d87bf260165d","University of Lisbon, IST, Dep. Civil Engineering, Av. Rovisco Pais, Lisbon, 1049-001, Portugal","Sampaio, A.Z., University of Lisbon, IST, Dep. Civil Engineering, Av. Rovisco Pais, Lisbon, 1049-001, Portugal","Building Information Modelling (BIM) is defined as the process of generating, storing, managing, exchanging, and sharing building information. The potential of BIM methodology to support a transformation of the processes of design and construction has been evident in the construction industry. A current topic that requires attention is the integration of BIM with Virtual Reality (VR) where the user visualizes a virtual world through interactive devices or a total immersion. VR combines several devices for interaction, creating virtual environment, and this must followed by studies concerning how to use devices or how to establish links for the presentation of information contained in a BIM model. By adding VR, the BIM solution can address retrieving and presenting information and increasing efficiency on communication and problem solving in an interactive and collaborative project. BIM + VR allow two main capacities: walkthrough and consulting data, and currently BIM tools allow links to VR plugins in order to achieve both capacities. As such, it is expected to be further explored in the near future. The text presents a review of actual perspective of the VR use applied over 3D/BIM models to supports multi-dimensional BIM applications, namely, 4D/BIM and 7D/BIM models. The objective of the study is to report the improvement of BIM uses with the addition of interactive capacities allowed by VR technology. Being the school the main actor in the formation of new engineers, it has the mission prepare students for the professional activity, giving the most advanced technology knowhow allowing them to make a difference in the job market. © 2018 EUCEET 2018 - 4th International Conference on Civil Engineering Education: Challenges for the Third Millennium. All rights reserved.","BIM methodology; Construction; Education; Maintenance; VR technology","Architectural design; Construction industry; Education computing; Engineering education; Engineers; Technology transfer; Three dimensional computer graphics; Advanced technology; Building Information Modelling; Collaborative projects; Design and construction; Education in engineerings; Multi dimensional; Presenting informations; Professional activities; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85088275381
"Ankomah P., Vangorp P.","57217224949;18435394900;","Virtual reality: A literature review and metrics-based classification",2018,"Computer Graphics and Visual Computing, CGVC 2018",,,,"173","181",,,"10.2312/cgvc.20181222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086820445&doi=10.2312%2fcgvc.20181222&partnerID=40&md5=67913b689e62c8521ffe6266f4e15c46","Edge Hill University, United Kingdom","Ankomah, P., Edge Hill University, United Kingdom; Vangorp, P., Edge Hill University, United Kingdom","This paper presents a multi-disciplinary overview of research evaluating virtual reality (VR). The main aim is to review and classify VR research based on several metrics: presence and immersion, navigation and interaction, knowledge improvement, performance and usability. With the continuous development and consumerisation of VR, several application domains have studied the impact of VR as an enhanced alternative environment for performing tasks. However, VR experiment results often cannot be generalised but require specific datasets and tasks suited to each domain. This review and classification of VR metrics presents an alternative metrics-based view of VR experiments and research. © 2018 The Author(s) Eurographics Proceedings © 2018 The Eurographics Association.",,"Consumerisation; Continuous development; Literature reviews; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85086820445
"Ivanov V., Pavlenko I., Trojanowska J., Zuban Y., Samokhvalov D., Bun P.","55769747343;56435834300;55780752800;57204692056;57204689470;57188870522;","Using the augmented reality for training engineering students",2018,"4th International Conference of the Virtual and Augmented Reality in Education, VARE 2018",,,,"57","64",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056744077&partnerID=40&md5=cf7112b66a3fba9f32549caf5859778a","Sumy State University, Faculty of Technical Systems and Energy Efficient Technologies, Department of Manufacturing Engineering, Machines and Tools, Ukraine; Sumy State University, Faculty of Technical Systems and Energy Efficient Technologies, Department of General Mechanics and Machine Dynamics, Ukraine; Poznan University of Technology, Faculty of Mechanical Engineering and Management, Chair of Management and Production Engineering, Poland; Sumy State University, Organizational-Methodical Center of E-Learning Technologies, Denmark","Ivanov, V., Sumy State University, Faculty of Technical Systems and Energy Efficient Technologies, Department of Manufacturing Engineering, Machines and Tools, Ukraine; Pavlenko, I., Sumy State University, Faculty of Technical Systems and Energy Efficient Technologies, Department of General Mechanics and Machine Dynamics, Ukraine; Trojanowska, J.; Zuban, Y., Sumy State University, Organizational-Methodical Center of E-Learning Technologies, Denmark; Samokhvalov, D., Sumy State University, Organizational-Methodical Center of E-Learning Technologies, Denmark; Bun, P., Poznan University of Technology, Faculty of Mechanical Engineering and Management, Chair of Management and Production Engineering, Poland","Preparation of highly qualified engineers allows solving complex tasks of development, implementation and service of innovative technical systems and technologies, requires new approaches to the educational process. Improving the competitiveness of graduates on the labor market, acquiring and accumulating production experience are needed tasks of higher education system. In the present article, the interdisciplinary relationships between disciplines of specialty “Applied Mechanics” are being presented. Approaches for students' professional competences for the disciplines “Descriptive Geometry”, “Engineering Graphics”, “Computer Graphics in Engineering” are being proposed. The importance of computer disciplines and their correspondence with professional disciplines were confirmed with regard the necessity of increasing the quality of the educational process. Engineering graphics training for modern engineer is the transfer of graphic information from an idea to manufacturing. The ability to design graphic images is an integral skill of an engineer. Due to limited lecture time, the presenting of material on the relationship between 3D geometry and their 2D projections by means of traditional approach seems to be rather complicated. In the present paper, a mobile application based on Augmented Reality is being proposed and allows representing 2D drawings in 3D models. The program is aimed on improving spatial skills and increase the students' educational motivation. Implementation of the results in the educational process shows the effectiveness and the possibility of this application to support training activities in other areas of engineering. Mobile application “AR in Engineering Graphics” based on using the technology of augmented reality was developed and implemented into the educational process for overcoming the challenges related to the graphic training of future engineers. Copyright © (2015) by CAL-TEK S.r.l.All rights reserved. . All Rights Reserved.","Augmented reality; Computer discipline; Engineering; Engineering education; Graphics; Spatial skills","Augmented reality; Education computing; Engineering; Engineering education; Engineers; Mobile computing; Personnel training; Three dimensional computer graphics; Educational motivations; Engineering graphics; Graphics; Higher education system; Professional competence; Professional disciplines; Spatial skills; Traditional approaches; Students",Conference Paper,"Final","",Scopus,2-s2.0-85056744077
"Chiu C.-C., Lee L.-C.","57193612023;10041031000;","System satisfaction survey for the App to integrate search and augmented reality with geographical information technology",2018,"Microsystem Technologies","24","1",,"319","341",,1,"10.1007/s00542-017-3333-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015152235&doi=10.1007%2fs00542-017-3333-9&partnerID=40&md5=ff931c732aa01a197036895987067a85","National Taipei University of Technology, No 1, Sec. 3, Zhongxiao E. Rd., Taipei, 10608, Taiwan","Chiu, C.-C., National Taipei University of Technology, No 1, Sec. 3, Zhongxiao E. Rd., Taipei, 10608, Taiwan; Lee, L.-C., National Taipei University of Technology, No 1, Sec. 3, Zhongxiao E. Rd., Taipei, 10608, Taiwan","Digital humanities are unfamiliar words in Taiwan. With the popularity of digital technology and mobile computing, Digital Human Geography in both theory and practice have emerged many new potentials and development opportunities. However, the research field regarding digital human geography can cover many areas, such as: mobile augmented reality, local media and service design, urban space interaction, virtual mixed space, space humanistic experience, historical materials reproduction, creativity of digital local sense, cultural narrative and art experience, local action games, mobile learning, and location-related business applications etc. This study aims to transfer information to specific groups through a simple and straightforward manner. With digital technology in combination with humane digital contents, people under the scenario of interactive interface are allowed to apply geographic information system (GIS) and geo-fencing program to convert data into visual images and through today’s Infographics to clearly use visual design characteristics in content presentation. Therefore, our study was to develop a new App, called “Ai Guang Zhan”. People can use “Ai Guang Zhan” App to integrate all exhibition related services. The App can be downloaded from smartphone or tablet PC for users to search in any time the arts and cultural activities and public facilities in the nearest place and receive instant mobile navigation services among various cultural and creative parks. To understand the preference and satisfaction by the people who use the UI interface for the APP designed from the study, a random questionnaire survey will be conducted in Huashan Cultural and Creative Industry Park and Songshan Cultural and Creative Park. The survey results will be used as reference for function improvement of next APP version. The primary purpose of this research is to propose a model for the development of applications, as well as methods for a clearer and easier visual presentation of information to be used as a reference for practitioners or application designers. The application, developed by the study through the proposed processes, was found to be effective; the developed application could be utilized as a new and useful tool to facilitate the utilization of academic and cultural resources. Additionally, users’ specific comments and suggestions collected by the study could be adopted as a reference for future application development. The application developed by this study was found to serve effectively as a method to promote the development of art and cultural activities of all sizes in various regions, and stimulate the exchange of arts and humanities between urban and rural areas. Furthermore, the application has the ability to attract younger people to participate in art and cultural activities, find their own interests, and explore more corresponding activities with its assistance. Future applications associated to art and cultural activities are suggested to effectively integrate AR, GIS, and visual design (such as Infographics) technologies, and thereby offer users more diversified, convenient, and useful functions. Restricted by the resources of labor and limited time, the functional design and development of the application still requires improvement. Specifically, the combination of AR technology and information presentation of art and cultural activities, as well as the calculation of relative distance and travel time were not presented effectively. The next step of the project would involve revising the content and functionalities of the application based on users’ feedback collected by this study, reviewing the acceptance of market responses of the application, and seeking a development company or foundation for technology transfer. Moreover, we intend to combine more technologies such as diversified cross-platform functions, visualized design of information, and interactive devices to further improve the application, and thereby stimulate social, human, and geographical interactions. © 2017, Springer-Verlag Berlin Heidelberg.",,"Augmented reality; Computation theory; Data handling; Exhibitions; Geographic information systems; Personal computers; Surveys; Technology transfer; Travel time; Virtual reality; Business applications; Cultural and creative industries; Developed applications; Geographical information; Information presentation; Interactive interfaces; Mobile augmented reality; Questionnaire surveys; Search engines",Article,"Final","",Scopus,2-s2.0-85015152235
"Zou Z., Arruda L., Ergan S.","57195301528;57208333256;55220455300;","Characteristics of models that impact transformation of BIMS to virtual environments to support facility management operations",2018,"Journal of Civil Engineering and Management","24","6",,"481","498",,4,"10.3846/jcem.2018.5689","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064496403&doi=10.3846%2fjcem.2018.5689&partnerID=40&md5=35a032e52c6b450afc702c6dc1657507","Department of Civil and Urban Engineering, New York University, New York City, United States","Zou, Z., Department of Civil and Urban Engineering, New York University, New York City, United States; Arruda, L., Department of Civil and Urban Engineering, New York University, New York City, United States; Ergan, S., Department of Civil and Urban Engineering, New York University, New York City, United States","Building information models (BIMs) have been used by the Architectural/Engineering/Construction (AEC) industry with a focus on storing and exchanging digital information about building components. However, the untapped potential of BIMs in facility operations and the experience of facility operators while they interact with digital building information have not been understood widely. One of the underlying bottlenecks in the use of BIMs in the FM phase is the lack of interactions with components to easily access information of interest, and the lack of ways to navigate in models with full spatial understanding. Virtual environments (VEs), which represent physical spaces digitally in virtual worlds, enable interactions with virtual components to access information with spatial understanding. The underlying challenges in the conversion of BIMs to VE hinder a streamlined process. This paper provides a detailed analysis of building size, geometric complexities of discipline models and level of geometric granularity as factors contributing to inefficient transformation of BIMs to VE. The paper also provides research findings on a set of computational approaches such as polygon reduction and occlusion culling to overcome challenges and improve the data transfer faced in converting BIMs into VEs over a range and size of facility models. © 2018 The Author(s).","BIM; Facility management; Virtual reality","Data transfer; Mathematical transformations; Office buildings; Virtual reality; Building Information Model - BIM; Computational approach; Digital information; Facility management; Facility operations; Geometric complexity; Support facilities; Virtual components; Architectural design",Article,"Final","",Scopus,2-s2.0-85064496403
"Loch F., Ziegler U., Vogel-Heuser B.","55193685600;57203760799;6603480302;","Integrating Haptic Interaction into a Virtual Training System for Manual Procedures in Industrial Environments",2018,"IFAC-PapersOnLine","51","11",,"60","65",,4,"10.1016/j.ifacol.2018.08.235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052855877&doi=10.1016%2fj.ifacol.2018.08.235&partnerID=40&md5=4dd09d028ffbbd0208d11a259f82b266","Institute for Automation and Information Systems, Technical University of Munich, Garching, Germany","Loch, F., Institute for Automation and Information Systems, Technical University of Munich, Garching, Germany; Ziegler, U., Institute for Automation and Information Systems, Technical University of Munich, Garching, Germany; Vogel-Heuser, B., Institute for Automation and Information Systems, Technical University of Munich, Garching, Germany","Many virtual training systems have been proposed to address the increasing complexity of manufacturing environments. However, existing training systems focus on visual interaction and neglect the haptic sense, which is a crucial component of manual tasks. This paper introduces a concept for a virtual training system for industrial procedures that introduces physical components to improve the training process. Introducing physical components is expected to improve the efficiency of training systems by transporting a sense for the haptic properties of tools and components and facilitate the transfer of the skills to the real work environment. The system allows practicing physical tasks using real tools and components. The didactic concept is based on the idea of multimodal learning and provides an environment that allows active experimentation. The application of the concept in an exemplary procedure is demonstrated. © 2018","Adaptive systems; Haptic interaction; Human-machine interface; Interaction mechanisms; Maintenance; Training; Virtual reality","Adaptive systems; Haptic interfaces; Maintenance; Personnel training; Virtual addresses; Virtual reality; Active experimentation; Haptic interactions; Human Machine Interface; Industrial environments; Interaction mechanisms; Manufacturing environments; Multi-modal learning; Virtual training systems; E-learning",Article,"Final","",Scopus,2-s2.0-85052855877
"Cañón Bermúdez G.S., Karnaushenko D.D., Karnaushenko D., Lebanov A., Bischoff L., Kaltenbrunner M., Fassbender J., Schmidt O.G., Makarov D.","57200690661;24829214100;42061560400;57207636688;7006109722;13106193000;9247546900;7201468610;12647134800;","Magnetosensitive e-skins with directional perception for augmented reality",2018,"Science Advances","4","1", eaao2623,"","",,39,"10.1126/sciadv.aao2623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042188666&doi=10.1126%2fsciadv.aao2623&partnerID=40&md5=b19495e7c356ffd0c550d768b1793ea1","Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany; Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany; Soft Electronics Laboratory, Linz Institute of Technology, Johannes Kepler University Linz, Altenbergerstrasse 69, Linz, 4040, Austria; Material Systems for Nanoelectronics, Technische Universität Chemnitz, Reichenhainer Strasse 70, Chemnitz, 09111, Germany","Cañón Bermúdez, G.S., Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany, Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany; Karnaushenko, D.D., Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany; Karnaushenko, D., Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany; Lebanov, A., Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany; Bischoff, L., Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany; Kaltenbrunner, M., Soft Electronics Laboratory, Linz Institute of Technology, Johannes Kepler University Linz, Altenbergerstrasse 69, Linz, 4040, Austria; Fassbender, J., Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany; Schmidt, O.G., Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany, Material Systems for Nanoelectronics, Technische Universität Chemnitz, Reichenhainer Strasse 70, Chemnitz, 09111, Germany; Makarov, D., Helmholtz-Zentrum Dresden-Rossendorf, Institute of Ion Beam Physics and Materials Research, Bautzner Landstrasse 400, Dresden, 01328, Germany, Institute for Integrative Nanosciences, Leibniz Institute for Solid State and Materials Research Dresden, IFW Dresden, Helmholtzstrasse 20, Dresden, 01069, Germany","Electronic skins equipped with artificial receptors are able to extend our perception beyond the modalities that have naturally evolved. These synthetic receptors offer complimentary information on our surroundings and endow us with novel means of manipulating physical or even virtual objects. We realize highly compliant magnetosensitive skins with directional perception that enable magnetic cognition, body position tracking, and touchless object manipulation. Transfer printing of eight high-performance spin valve sensors arranged into two Wheatstone bridges onto 1.7-mm-thick polyimide foils ensures mechanical imperceptibility. This resembles a new class of interactive devices extracting information from the surroundings through magnetic tags. We demonstrate this concept in augmented reality systems with virtual knob-turning functions and the operation of virtual dialing pads, based on the interaction with magnetic fields. This technology will enable a cornucopia of applications from navigation, motion tracking in robotics, regenerative medicine, and sports and gaming to interaction in supplemented reality. Copyright © 2018 The Authors.",,"Augmented reality; Magnetism; Robots; Sports medicine; Valves (mechanical); Artificial receptors; Augmented reality systems; Extracting information; Magneto sensitives; Object manipulation; Spin-valve sensors; Synthetic receptors; Wheatstone bridges; Regenerative Medicine; electronic device; human; magnetism; robotics; skin; Humans; Magnetics; Robotics; Skin; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85042188666
"Shyshkina M.P.","56289490700;","The problems of personnel training for STEM education in the modern innovative learning and research environment",2018,"CEUR Workshop Proceedings","2257",,,"61","65",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058194857&partnerID=40&md5=cb0d344bd2ef9f41c0787f4056464be2","Institute of Information Technologies and Learning Tools, NAES of Ukraine, 9, M. Berlynskoho St., Kyiv, 04060, Ukraine","Shyshkina, M.P., Institute of Information Technologies and Learning Tools, NAES of Ukraine, 9, M. Berlynskoho St., Kyiv, 04060, Ukraine","The aim of the article is to describe the problems of personnel training that arise in view of extension of the STEM approach to education, development of innovative technologies, in particular, virtualization, augmented reality, the use of ICT outsourcing in educational systems design. The object of research is the process of formation and development of the educational and scientific environment of educational institution. The subject of the study is the formation and development of the cloud-based learning and research environment for STEM education. The methods of research are: the analysis of publications on the problem; generalization of domestic and foreign experience; theoretical analysis, system analysis, systematization and generalization of research facts and laws for the development and design of the model of the cloud-based learning environment, substantiation of the main conclusions. The results of the research are the next: the concepts and the model of the cloud-based environment of STEM education is substantiated, the problems of personnel training at the present stage are outlined. © 2018 CEUR-WS. All Rights Reserved.","Augmented reality; Cloud technologies; Learning environment; Pedagogical personnel","Augmented reality; Computer aided instruction; Environmental regulations; Personnel training; Systems analysis; Cloud technologies; Educational institutions; Educational systems; Innovative learning; Innovative technology; Learning environments; Methods of researches; Research environment; STEM (science, technology, engineering and mathematics)",Conference Paper,"Final","",Scopus,2-s2.0-85058194857
"Chen L., Jung C.R., Musse S.R., Moneimne M., Wang C., Fruchter R., Bazjanac V., Chen G., Badler N.I.","57203761615;7402016327;6508070752;57203428592;57203433694;7006019058;7801664347;57203431468;57207599359;","Crowd simulation incorporating thermal environments and responsive behaviors",2018,"Presence: Teleoperators and Virtual Environments","26","4",,"436","452",,5,"10.1162/PRES_a_00308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051706939&doi=10.1162%2fPRES_a_00308&partnerID=40&md5=12d7c1a0687d1419ce4fd3beb3753dc2","Ocean University of China, China; Federal University of Rio Grande do Sul, Brazil; Pontifical Catholic University of Rio Grande do Sul; University of Pennsylvania, United States; Stanford University, United States","Chen, L., Ocean University of China, China; Jung, C.R., Federal University of Rio Grande do Sul, Brazil; Musse, S.R., Pontifical Catholic University of Rio Grande do Sul; Moneimne, M., University of Pennsylvania, United States; Wang, C., University of Pennsylvania, United States; Fruchter, R., Stanford University, United States; Bazjanac, V., Stanford University, United States; Chen, G., Ocean University of China, China; Badler, N.I., University of Pennsylvania, United States","Crowd simulation addresses algorithmic approaches to steering, navigation, perception, and behavioral models. Significant progress has been achieved in modeling interactions between agents and the environment to avoid collisions, exploit empirical local decision data, and plan efficient paths to goals. We address a relatively unexplored dimension of virtual human behavior: thermal perception, comfort, and appropriate behavioral responses. Thermal comfort is associated with the ambient environment, agent density factors, and interpersonal thermal feedback. A key feature of our approach is the temporal integration of both thermal exposure and occupant density to directly influence agent movements and behaviors (e.g., clothing changes) to increase thermal comfort. Empirical thermal comfort models are incorporated as a validation basis. Simple heat transfer models are used to model environment, agent, and interpersonal heat exchange. Our model’s generality makes it applicable to any existing crowd steering algorithm as it adds additional integrative terms to any cost function. Examples illustrate distinctive emergent behaviors such as balancing agent density with thermal comfort, hysteresis in responding to localized or brief thermal events, and discomfort and likely injury produced by extreme packing densities. © 2018 by the Massachusetts Institute of Technology.",,"Cost functions; Heat transfer; Thermal comfort; Virtual addresses; Virtual reality; Algorithmic approach; Ambient environment; Modeling environments; Responsive behaviors; Steering algorithms; Temporal integration; Thermal comfort models; Thermal environment; Behavioral research",Article,"Final","",Scopus,2-s2.0-85051706939
"Vargas González A.N., Kapalo K., Koh S.L., Laviola J.J., Jr.","57190589688;57210987038;55641302700;6602792780;","Exploring the virtuality continuum for complex rule-set education in the context of soccer rule comprehension",2017,"Multimodal Technologies and Interaction","1","4", 30,"","",,7,"10.3390/mti1040030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068838830&doi=10.3390%2fmti1040030&partnerID=40&md5=fcf9fab289bca459397f7138eadf5e18","Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd., Orlando, FL  32816, United States","Vargas González, A.N., Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd., Orlando, FL  32816, United States; Kapalo, K., Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd., Orlando, FL  32816, United States; Koh, S.L., Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd., Orlando, FL  32816, United States; Laviola, J.J., Jr., Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd., Orlando, FL  32816, United States","We present an exploratory study to assess the benefits of using Augmented Reality (AR) in training sports rule comprehension. Soccer is the chosen context for this study due to the wide range of complexity in the rules and regulations. Observers must understand and holistically evaluate the proximity of players in the game to the ball and other visual objects, such as the goal, penalty area, and other players. Grounded in previous literature investigating the effects of Virtual Reality (VR) scenarios on transfer of training (ToT), we explore how three different interfaces influence user perception using both qualitative and quantitative measures. To better understand how effective augmented reality technology is when combined with learning systems, we compare results on the effects of learning outcomes in three interface conditions: AR, VR and a traditional Desktop interface. We also compare these interfaces as measured by user experience, engagement, and immersion. Results show that there were no significance difference among VR and AR; however, these participants outperformed the Desktop group which needed a higher number of adaptations to acquire the same knowledge. © 2017 by the authors.","Augmented reality; Intelligent tutoring systems; Soccer; Sports training; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85068838830
"Johnson-Glenberg M.C., Megowan-Romanowicz C.","6507238066;26537877500;","Embodied science and mixed reality: How gesture and motion capture affect physics education",2017,"Cognitive Research: Principles and Implications","2","1", 24,"","",,30,"10.1186/s41235-017-0060-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038820372&doi=10.1186%2fs41235-017-0060-9&partnerID=40&md5=f5b98014a9c22b3850ab96e5474bf458","Department of Psychology, Arizona State University, Tempe, AZ, United States; Embodied Games LLC, Tempe, AZ, United States; Modeling Instruction Institute, Sacramento, CA, United States","Johnson-Glenberg, M.C., Department of Psychology, Arizona State University, Tempe, AZ, United States, Embodied Games LLC, Tempe, AZ, United States; Megowan-Romanowicz, C., Modeling Instruction Institute, Sacramento, CA, United States","A mixed design was created using text and game-like multimedia to instruct in the content of physics. The study assessed which variables predicted learning gains after a 1-h lesson on the electric field. The three manipulated variables were: (1) level of embodiment; (2) level of active generativity; and (3) presence of story narrative. Two types of tests were administered: (1) a traditional text-based physics test answered with a keyboard; and (2) a more embodied, transfer test using the Wacom large tablet where learners could use gestures (long swipes) to create vectors and answers. The 166 participants were randomly assigned to four conditions: (1) symbols and text; (2) low embodied; (3) high embodied/active; or (4) high embodied/active with narrative. The last two conditions were active because the on-screen content could be manipulated with gross body gestures gathered via the Kinect sensor. Results demonstrated that the three groups that included embodiment learned significantly more than the symbols and text group on the traditional keyboard post-test. When knowledge was assessed with the Wacom tablet format that facilitated gestures, the two active gesture-based groups scored significantly higher. In addition, engagement scores were significantly higher for the two active embodied groups. The Wacom results suggest test sensitivity issues; the more embodied test revealed greater gains in learning for the more embodied conditions. We recommend that as more embodied learning comes to the fore, more sensitive tests that incorporate gesture be used to accurately assess learning. The predicted differences in engagement and learning for the condition with the graphically rich story narrative were not supported. We hypothesize that a narrative effect for motivation and learning may be difficult to uncover in a lab experiment where participants are primarily motivated by course credit. Several design principles for mediated and embodied science education are proposed. © 2017, The Author(s).","Embodied science; Game-based learning; Gesture and learning; Mixed reality; Narrative; Physics; Science education; STEM; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85038820372
"Alameddine H.A., Sebbah S., Assi C.","57190291297;23010220700;6603706480;","On the interplay between network function mapping and scheduling in VNF-based networks: A column generation approach",2017,"IEEE Transactions on Network and Service Management","14","4", 8052155,"860","874",,18,"10.1109/TNSM.2017.2757266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030782415&doi=10.1109%2fTNSM.2017.2757266&partnerID=40&md5=2733ade79cca8af5dc8d960e8eecb696","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC  H3H2G6, Canada","Alameddine, H.A., Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC  H3H2G6, Canada; Sebbah, S., Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC  H3H2G6, Canada; Assi, C., Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC  H3H2G6, Canada","Middleboxes (i.e., firewall, cache, proxy, etc.) are hardware appliances designed to enforce security and performance policies. Being an integral part of today’s cloud and enterprise networks, these middleboxes are expensive, hard to manage and to maintain. Network function virtualization has emerged as a promising technology that replaces these hardware appliances by software ones known as virtual network functions (VNFs). Unlike hardware middleboxes, VNFs can be instantiated and deployed on virtual machines running on commodity servers which ensures their flexibility, manageability, cost-efficiency, and reduce their time-to-market. However, efficiently processing services through an ordered chain of VNFs, called service function chaining (SFC), is not trivial. It requires solving three inter-related sub-problems; the network functions (NFs) mapping sub-problem, the traffic routing sub-problem and the service scheduling sub-problem. This paper first highlights the existing interplay between the three sub-problems and then presents a formulation of the SFC scheduling (SFCS) which exploits interactions between NFs mapping onto VNFs, service scheduling and traffic routing. Given the complexity of the SFCS problem, we present a novel primal–dual decomposition using column generation that solves exactly a relaxed version of the problem and can serve as a benchmark approach. We enhance our solution methodology with a diversification technique to help improve the quality of the obtained solutions. We evaluate numerically our method and show that it can attain optimal solutions substantially faster. Finally, we present several engineering insights for improving the network performance. © 2017 IEEE.",,"Computer hardware; Computer software; Hardware; Linear programming; Mapping; Network routing; Optimization; Proxy caches; Scheduling; Transfer functions; Virtual reality; Virtualization; Bandwidth guarantee; Cloud networks; Column generation; Delays; Middleboxes; Routing; Service functions; Virtual networks; Network function virtualization",Article,"Final","",Scopus,2-s2.0-85030782415
"Bork F., Barmaki R., Eck U., Yu K., Sandor C., Navab N.","57188681620;57079124100;6507331080;57202881272;15061666200;7003458998;","Empirical study of non-reversing magic mirrors for augmented reality anatomy learning",2017,"Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2017",,, 8115415,"169","176",,13,"10.1109/ISMAR.2017.33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041662291&doi=10.1109%2fISMAR.2017.33&partnerID=40&md5=675b96017db2c791bfef11db7746c384","Technische Universität München, Munich, Germany; Johns Hopkins University, Baltimore, MD, United States; Nara Institute of Technology, Nara, Japan","Bork, F., Technische Universität München, Munich, Germany; Barmaki, R., Johns Hopkins University, Baltimore, MD, United States; Eck, U., Technische Universität München, Munich, Germany; Yu, K., Technische Universität München, Munich, Germany; Sandor, C., Nara Institute of Technology, Nara, Japan; Navab, N., Technische Universität München, Munich, Germany, Johns Hopkins University, Baltimore, MD, United States","Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)' In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains. © 2017 IEEE.",,"Augmented reality; Knowledge management; Mirrors; Empirical studies; Knowledge transfer; Learning resource; Medical students; Perceptual difference; Psychological effects; Teaching resources; User perceptions; Education",Conference Paper,"Final","",Scopus,2-s2.0-85041662291
"Palestini C., Basso A.","56541484100;57194550137;","The photogrammetric survey methodologies applied to low cost 3D virtual exploration in multidisciplinary field",2017,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2W8",,"195","202",,8,"10.5194/isprs-archives-XLII-2-W8-195-2017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051478984&doi=10.5194%2fisprs-archives-XLII-2-W8-195-2017&partnerID=40&md5=b9204e423480a9407c0ec4a93ea08647","Dipartimento di Architettura, Università degli Studi g. d'Annunzio, Pescara, Italy","Palestini, C., Dipartimento di Architettura, Università degli Studi g. d'Annunzio, Pescara, Italy; Basso, A., Dipartimento di Architettura, Università degli Studi g. d'Annunzio, Pescara, Italy","In recent years, an increase in international investment in hardware and software technology to support programs that adopt algorithms for photomodeling or data management from laser scanners significantly reduced the costs of operations in support of Augmented Reality and Virtual Reality, designed to generate real-time explorable digital environments integrated to virtual stereoscopic headset. The research analyzes transversal methodologies related to the acquisition of these technologies in order to intervene directly on the phenomenon of acquiring the current VR tools within a specific workflow, in light of any issues related to the intensive use of such devices , outlining a quick overview of the possible ""virtual migration"" phenomenon, assuming a possible integration with the new internet hyper-speed systems, capable of triggering a massive cyberspace colonization process that paradoxically would also affect the everyday life and more in general, on human space perception. The contribution aims at analyzing the application systems used for low cost 3d photogrammetry by means of a precise pipeline, clarifying how a 3d model is generated, automatically retopologized, textured by color painting or photo-cloning techniques, and optimized for parametric insertion on virtual exploration platforms. Workflow analysis will follow some case studies related to photomodeling, digital retopology and ""virtual 3d transfer"" of some small archaeological artifacts and an architectural compartment corresponding to the pronaus of Aurum, a building designed in the 1940s by Michelucci. All operations will be conducted on cheap or free licensed software that today offer almost the same performance as their paid counterparts, progressively improving in the data processing speed and management. © 2017 Authors.","3d; Interaction; Photogrammetric survey; Virtual empathy; Virtual reality","Augmented reality; Costs; Data handling; Information management; Photogrammetry; Software engineering; Stereo image processing; Surveys; Three dimensional computer graphics; 3d photogrammetries; Archaeological artifacts; Colonization process; Digital environment; Hardware and software; Interaction; International investments; Virtual empathy; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85051478984
"Singh G.","55341188800;","Using virtual reality for scaffolding computer programming learning",2017,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST","Part F131944",, a79,"","",,,"10.1145/3139131.3141225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038585905&doi=10.1145%2f3139131.3141225&partnerID=40&md5=99b7d4b1e9a858171933c6cc5638d4ff","Fairleigh Dickinson University, Madison, NJ, United States","Singh, G., Fairleigh Dickinson University, Madison, NJ, United States","Learning how to analyze computational problems, to think critically, and to transfer algorithmic logic into language-specific code is central to computer programming. A critical step towards acquiring these skills is analyzing and debugging existing code usually starting with the famous “Hello World” program. Even after learning basic structure and syntax of a computer language, new learners struggle to understand algorithmic process, to mentally visualize effects of algorithms on data, and to remain engaged while learning it. We explore the use of virtual reality that teaches introductory concepts of computer programming to students in a 3D interactive space, while scaffolding their progression. © 2017 Copyright held by the owner/author(s).","Computer programming; Interactive learning environments; Technology-enhanced learning; Virtual reality in education","Algorithmic languages; Computation theory; Computer aided instruction; Computer programming; Education; Educational technology; Learning systems; Program debugging; Scaffolds; Virtual reality; Algorithmic process; Basic structure; Computational problem; Critical steps; Interactive learning environment; Interactive spaces; Technology enhanced learning; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85038585905
"Svensson L., Svensson S., Nyström I., Nysjö F., Nysjö J., Laloeuf A., den Hollander L., Brun A., Masich S., Sandblad L., Sani M., Sintorn I.-M.","57139708000;7202278853;6701460799;55212267800;54393831900;36165083700;6507377372;8726451900;6507172029;15725904000;57192406645;7801464619;","ProViz: a tool for explorative 3-D visualization and template matching in electron tomograms",2017,"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization","5","6",,"446","454",,,"10.1080/21681163.2016.1154483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006208815&doi=10.1080%2f21681163.2016.1154483&partnerID=40&md5=da6e7f2a91fd7a58b645ccecf6efc29e","Centre for Image Analysis, Uppsala University, Uppsala, Sweden; RaySearch Laboratories, Stockholm, Sweden; Department of Cell and Molecular Biology, Karolinska Institutet, Stockholm, Sweden; Department of Molecular Biology, Umeå University, Umeå, Sweden; Vironova AB, Stockholm, Sweden","Svensson, L., Centre for Image Analysis, Uppsala University, Uppsala, Sweden; Svensson, S., RaySearch Laboratories, Stockholm, Sweden; Nyström, I., Centre for Image Analysis, Uppsala University, Uppsala, Sweden; Nysjö, F., Centre for Image Analysis, Uppsala University, Uppsala, Sweden; Nysjö, J., Centre for Image Analysis, Uppsala University, Uppsala, Sweden; Laloeuf, A., Department of Cell and Molecular Biology, Karolinska Institutet, Stockholm, Sweden; den Hollander, L., Department of Cell and Molecular Biology, Karolinska Institutet, Stockholm, Sweden; Brun, A., Centre for Image Analysis, Uppsala University, Uppsala, Sweden; Masich, S., Department of Cell and Molecular Biology, Karolinska Institutet, Stockholm, Sweden; Sandblad, L., Department of Molecular Biology, Umeå University, Umeå, Sweden; Sani, M., Vironova AB, Stockholm, Sweden; Sintorn, I.-M., Centre for Image Analysis, Uppsala University, Uppsala, Sweden, Vironova AB, Stockholm, Sweden","Visual understanding is a key aspect when studying electron tomography data-sets, aside quantitative assessments such as registration of high-resolution structures. We here present the free software tool ProViz (Protein Visualization) for visualisation and template matching in electron tomograms of biological samples. The ProViz software contains methods and tools which we have developed, adapted and computationally optimised for easy and intuitive visualisation and analysis of electron tomograms with low signal-to-noise ratio. ProViz complements existing software in the application field and serves as an easy and convenient tool for a first assessment and screening of the tomograms. It provides enhancements in three areas: (1) improved visualisation that makes connections as well as intensity differences between and within objects or structures easier to see and interpret, (2) interactive transfer function editing with direct visual result feedback using both piecewise linear functions and Gaussian function elements, (3) computationally optimised template matching and tools to visually assess and interactively explore the correlation results. The visualisation capabilities and features of ProViz are demonstrated on various biological volume data-sets: bacterial filament structures in vitro, a desmosome and the transmembrane cadherin connections therein in situ, and liposomes filled with doxorubicin in solution. The explorative template matching is demonstrated on a synthetic IgG data-set. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","connected component filtering; direct volume rendering; Electron tomography; image registration; visualisation and analysis software","cadherin; doxorubicin; immunoglobulin G; liposome; algorithm; Article; desmosome; electron tomography; normal distribution; priority journal; Protein Visualization software; signal noise ratio; simulation; software; three dimensional imaging",Article,"Final","",Scopus,2-s2.0-85006208815
"Al-Saud L.M., Mushtaq F., Allsop M.J., Culmer P.C., Mirghani I., Yates E., Keeling A., Mon-Williams M.A., Manogue M.","55361574200;56999078200;35602715400;34978234000;57191096853;57191095015;55969850600;7006287402;6701334811;","Feedback and motor skill acquisition using a haptic dental simulator",2017,"European Journal of Dental Education","21","4",,"240","247",,25,"10.1111/eje.12214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993660524&doi=10.1111%2feje.12214&partnerID=40&md5=1d900e9f061f9a90f8b701789ab21d3f","School of Dentistry, University of Leeds, Leeds, West Yorkshire, United Kingdom; School of Psychology, University of Leeds, Leeds, United Kingdom; College of Dentistry, King Saud University, Riyadh, Saudi Arabia; Leeds Institute of Health Sciences, University of Leeds, Leeds, United Kingdom; School of Mechanical Engineering, University of Leeds, Leeds, United Kingdom; School of Dentistry, The University of Western Australia, Crawley, Australia","Al-Saud, L.M., School of Dentistry, University of Leeds, Leeds, West Yorkshire, United Kingdom, School of Psychology, University of Leeds, Leeds, United Kingdom, College of Dentistry, King Saud University, Riyadh, Saudi Arabia; Mushtaq, F., School of Psychology, University of Leeds, Leeds, United Kingdom; Allsop, M.J., Leeds Institute of Health Sciences, University of Leeds, Leeds, United Kingdom; Culmer, P.C., School of Mechanical Engineering, University of Leeds, Leeds, United Kingdom; Mirghani, I., School of Dentistry, University of Leeds, Leeds, West Yorkshire, United Kingdom, School of Psychology, University of Leeds, Leeds, United Kingdom; Yates, E., School of Dentistry, The University of Western Australia, Crawley, Australia; Keeling, A., School of Dentistry, University of Leeds, Leeds, West Yorkshire, United Kingdom; Mon-Williams, M.A., School of Psychology, University of Leeds, Leeds, United Kingdom; Manogue, M., School of Dentistry, University of Leeds, Leeds, West Yorkshire, United Kingdom","Aim: To investigate the effect of qualitatively different types of pedagogical feedback (FB) on the training, transfer and retention of basic manual dexterity dental skills using a virtual reality (VR) haptic dental simulator. Methods: Sixty-three participants (M = 22.7 years; SD = 3.4 years), with no previous dental training, were randomly allocated to one of three groups (n = 21 each). Group 1 received device-only feedback during the training phase, that is the visual display of the simulator (DFB); Group 2 received verbal feedback from a qualified dental instructor (IFB); and Group 3 received a combination of instructor and device feedback (IDFB). Participants completed four tasks during which feedback was given according to group allocation as well as two skills transfer tests. Skill retention was examined immediately after training, at 1 week and at 1 month post-test. Results: Statistically significant differences were found between the groups in overall performance (P < 0.001) and error (P = 0.006). Post hoc comparisons revealed the IDFB group produced substantially better performance and fewer errors in comparison with DFB and IFB training. This difference translated to improved performance in skill retention and generalisation of knowledge to novel tasks. Conclusion: These data indicate that the acquisition and retention of basic dental motor skills in novice trainees is best optimised through a combination of instructor and visual display (VR)-driven feedback. The results have implications for the utility and implementation of VR haptic technology in dental education. © 2016 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd","dentistry; feedback; motor learning; skill acquisition; undergraduate dental education; virtual reality","computer simulation; controlled study; dental education; human; motor performance; procedures; randomized controlled trial; sensory feedback; teaching; touch; young adult; Computer Simulation; Computer-Assisted Instruction; Education, Dental; Feedback, Sensory; Humans; Motor Skills; Touch; Young Adult",Article,"Final","",Scopus,2-s2.0-84993660524
"Oleksy T., Wnuk A.","55791634600;57007996900;","Catch them all and increase your place attachment! The role of location-based augmented reality games in changing people - place relations",2017,"Computers in Human Behavior","76",,,"3","8",,39,"10.1016/j.chb.2017.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020705107&doi=10.1016%2fj.chb.2017.06.008&partnerID=40&md5=b97f59578ab8135e1f3df4901a999a13","Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, Warsaw, 00-183, Poland","Oleksy, T., Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, Warsaw, 00-183, Poland; Wnuk, A., Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, Warsaw, 00-183, Poland","We examined how playing a game employing augmented reality (AR) technology increases attachment to the place of playing. Place attachment refers to the relationship between people and places, which has numerous benefits for individual well-being. Popular location-based AR games often include elements that are known to predict place attachment: exploration, social relations or the experience of enjoyment in a place. We argue that positive emotions triggered by playing can influence players’ place attachment via the process of gamification. We tested this hypothesis in a correlational study conducted among Pokémon Go players. Our analyses showed that satisfaction from playing and the social relations made during play positively predict place attachment, but the amount of time spent on playing does not. A series of mediation analyses showed that relations among game satisfaction, social relations, and place attachment were mediated by the appraisal of the place as exciting. This study demonstrated a mechanism of emotional transfer between positive experiences from playing and place attachment, which may prove useful in other domains, such as education, land conservation, or marketing. © 2017 Elsevier Ltd","Augmented reality; Gamification; Location-based AR games; Place attachment; Pokémon Go","Augmented reality; Conservation; Gamification; Land conservation; Location based; Mediation analysis; Place attachment; Positive emotions; Positive experiences; Social relations; Location; correlational study; education; emotional attachment; human; human experiment; marketing; satisfaction; social interaction; wellbeing",Article,"Final","",Scopus,2-s2.0-85020705107
"Haghighi M., Moghadamfalahi M., Akcakaya M., Shinn-Cunningham B.G., Erdogmus D.","56102733500;56157493700;25229769500;6701765863;7004584113;","A graphical model for online auditory scene modulation using EEG evidence for attention",2017,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","25","11", 7940107,"1970","1977",,4,"10.1109/TNSRE.2017.2712419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021821336&doi=10.1109%2fTNSRE.2017.2712419&partnerID=40&md5=c2121b74000a290ee6e68865c64ef830","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA  02115, United States; Honeywell Laboratories, Plymouth, MN  55441, United States; University of Pittsburgh, Pittsburgh, PA  15260, United States; Boston University, Boston, MA  02215, United States; Northeastern University, Boston, MA  02115, United States","Haghighi, M., Department of Electrical and Computer Engineering, Northeastern University, Boston, MA  02115, United States; Moghadamfalahi, M., Honeywell Laboratories, Plymouth, MN  55441, United States; Akcakaya, M., University of Pittsburgh, Pittsburgh, PA  15260, United States; Shinn-Cunningham, B.G., Boston University, Boston, MA  02215, United States; Erdogmus, D., Northeastern University, Boston, MA  02115, United States","Recent findings indicate that brain interfaces have the potential to enable attention-guided auditory scene analysis and manipulation in applications, such as hearing aids and augmented/virtual environments. Specifically, noninvasively acquired electroencephalography (EEG) signals have been demonstrated to carry some evidence regarding, which of multiple synchronous speech waveforms the subject attends to. In this paper, we demonstrate that: 1) using data- and model-driven cross-correlation features yield competitive binary auditory attention classification results with at most 20 s of EEG from 16 channels or even a single well-positioned channel; 2) a model calibrated using equal-energy speech waveforms competing for attention could perform well on estimating attention in closed-loop unbalanced-energy speech waveform situations, where the speech amplitudes are modulated by the estimated attention posterior probability distribution; 3) such a model would perform even better if it is corrected (linearly, in this instance) based on EEG evidence dependence on speech weights in the mixture; and 4) calibrating a model based on population EEG could result in acceptable performance for new individuals/users; therefore, EEG-based auditory attention classifiers may generalize across individuals, leading to reduced or eliminated calibration time and effort. © 2017 IEEE.","Auditory attention detection; Brain interface; EEG","Audition; Brain models; Calibration; Electrophysiology; Graphic methods; Hearing aids; Modulation; Patient rehabilitation; Population statistics; Probability distributions; Speech; Speech recognition; Virtual reality; Acceptable performance; Auditory attention; Auditory Scene Analysis; Brain interfaces; Classification results; Context modeling; Cross correlations; GraphicaL model; Electroencephalography; adult; algorithm; analytic method; Article; attention; auditory scene analysis; discriminant analysis; electroencephalography; female; graphical model; human; human experiment; male; signal processing; speech; stimulus; waveform; attention; biological model; brain computer interface; calibration; hearing; online system; physiology; prosthesis design; speech perception; transfer of learning; wavelet analysis; Adult; Algorithms; Attention; Auditory Perception; Brain-Computer Interfaces; Calibration; Electroencephalography; Female; Humans; Male; Models, Neurological; Online Systems; Prosthesis Design; Signal Processing, Computer-Assisted; Speech Perception; Transfer (Psychology); Wavelet Analysis",Article,"Final","",Scopus,2-s2.0-85021821336
"Gao Y., Li S., Yang L., Qin H., Hao A.","57191277328;35107382000;55549616400;56161957400;36445833300;","An efficient heat-based model for solid-liquid-gas phase transition and dynamic interaction",2017,"Graphical Models","94",,,"14","24",,6,"10.1016/j.gmod.2017.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031750544&doi=10.1016%2fj.gmod.2017.09.001&partnerID=40&md5=ee222a685f5f54000fab8d2804cc97d9","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China; Beihang University Qingdao Research Institute, China; Amazon China Investment Corporation, China; Department of Computer Science, Stony Brook University, United States","Gao, Y., State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China; Li, S., State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China, Beihang University Qingdao Research Institute, China; Yang, L., Amazon China Investment Corporation, China; Qin, H., Department of Computer Science, Stony Brook University, United States; Hao, A., State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China","For melting simulation, solid-liquid coupling, liquid-gas interaction, bubble/foam generation, etc., many new methods have been emerging in recent years in computer graphics. To further push advance the technical frontier of the aforementioned phenomena, our novel solution is to focus on an efficient heat-based method towards faithful simulation of physical procedures pertinent to phase transitions and their dynamic interactions. On the methodology aspect, this paper details a simplified temperature-based model to animate the phase transitions and their dynamic interactions, including melting, freezing, and vaporization, by integrating the latent heat model with relevant governing physical laws. On the numerical aspect, our framework supports a new algorithm aiming at tight coupling of heat transfer and multiphase FLIP-based fluids. Specifically for liquid-gas phase transition, we take into account the dissolved gas involved in liquid which further enhances the bubble generation effects. Besides the unique feature of heat transfer, we also devise a SPH-FLIP coupled model to simulate sub-grid bubbles, which enables three-phase dynamic interactions among solid, liquid, and gas. The extensive experiments show that our hybrid approach can simultaneously handle multi-phase transition driven by physics-based heat conditions, as well as the multi-phase dynamic interactions with high fidelity and visual appeal. © 2017 Elsevier Inc.","Bubble simulation; FLIP; Multi-phase fluid; Phase transition and interaction; Simplified heat transfer model; SPH","Computer graphics; Dynamics; Gases; Heat transfer; Liquefied gases; Liquids; Melting; Vaporization; Bubble simulation; FLIP; Heat transfer model; Liquid-gas phase transition; Melting simulations; Multi phase transitions; Multiphase fluids; Solid-liquid coupling; Fluids; algorithm; dissolved gas; experimental study; heat transfer; numerical model; phase transition; simulation",Article,"Final","",Scopus,2-s2.0-85031750544
"Makarov I., Aliev V., Gerasimova O.","57203060623;57197847593;57190962423;","Semi-dense depth interpolation using deep convolutional neural networks",2017,"MM 2017 - Proceedings of the 2017 ACM Multimedia Conference",,,,"1407","1415",,8,"10.1145/3123266.3123360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035198467&doi=10.1145%2f3123266.3123360&partnerID=40&md5=a15a1ead6fad9b428b6dd918391829e0","National Research University, Higher School of Economics, School of Data Analysis and Artificial Intelligence, Moscow, Russian Federation","Makarov, I., National Research University, Higher School of Economics, School of Data Analysis and Artificial Intelligence, Moscow, Russian Federation; Aliev, V., National Research University, Higher School of Economics, School of Data Analysis and Artificial Intelligence, Moscow, Russian Federation; Gerasimova, O., National Research University, Higher School of Economics, School of Data Analysis and Artificial Intelligence, Moscow, Russian Federation","With advances of recent technologies, augmented reality systems and autonomous vehicles gained a lot of interest from academics and industry. Both these areas rely on scene geometry understanding, which usually requires depth map estimation. However, in case of systems with limited computational resources, such as smart-phones or autonomous robots, high resolution dense depth map estimation may be challenging. In this paper, we study the problem of semi-dense depth map interpolation along with low resolution depth map upsampling. We present an end-to-end learnable residual convolutional neural network architecture that achieves fast interpolation of semi-dense depth maps with different sparse depth distributions: uniform, sparse grid and along intensity image gradient. We also propose a loss function combining classical mean squared error with perceptual loss widely used in intensity image super-resolution and style transfer tasks. We show that with some modifications, this architecture can be used for depth map superresolution. Finally, we evaluate our results on both synthetic and real data, and consider applications for autonomous vehicles and creating AR/MR video games. © 2017 Copyright held by the owner/author(s).","Augmented reality; Autonomous vehicles; Computer vision; Convolutional neural networks; Depth map; Mixed reality","Augmented reality; Computer vision; Convolution; Deep neural networks; Interactive computer graphics; Interpolation; Mean square error; Neural networks; Optical resolving power; Smartphones; Vehicles; Virtual reality; Augmented reality systems; Autonomous Vehicles; Computational resources; Convolutional neural network; Depth Map; Depth map up samplings; Mixed reality; Synthetic and real data; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85035198467
"McClelland J.C., Teather R.J., Girouard A.","57197781412;24588246800;16303494200;","HaptoBend: Shape-changing passive haptic feedback in virtual reality",2017,"SUI 2017 - Proceedings of the 2017 Symposium on Spatial User Interaction",,,,"82","90",,20,"10.1145/3131277.3132179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037047945&doi=10.1145%2f3131277.3132179&partnerID=40&md5=f5343c613cffb410c595b59c94070b0e","Carleton University, Ottawa, Canada","McClelland, J.C., Carleton University, Ottawa, Canada; Teather, R.J., Carleton University, Ottawa, Canada; Girouard, A., Carleton University, Ottawa, Canada","We present HaptoBend, a novel shape-changing input device providing passive haptic feedback (PHF) for a wide spectrum of objects in virtual reality (VR). Past research in VR shows that PHF increases presence and improves user task performance. However, providing PHF for multiple objects usually requires complex, immobile systems, or multiple props. HaptoBend addresses this problem by allowing users to bend the device into 2D plane-like shapes and multi-surface 3D shapes. We believe HaptoBend’s physical approximations of virtual objects can provide realistic haptic feedback through research demonstrating the dominance of human vision over other senses in VR. To test the effectiveness of HaptoBend in matching 2D planar and 3D multi-surface shapes, we conducted an experiment modeled after gesture elicitation studies with 20 participants. High goodness and ease scores show shape-changing passive haptic devices, like HaptoBend, are an effective approach to generalized haptics. Further analysis supports the use of physical approximations for realistic haptic feedback. © 2017 Copyright is held by the owner/author(s).","Haptic feedback; Shape-changing interactions; Virtual Reality","Haptic interfaces; Effective approaches; Haptic feedbacks; Multiple objects; Passive haptic devices; Physical approximations; Task performance; Virtual objects; Wide spectrum; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85037047945
"Li J., Van Der Spek E., Hu J., Feijs L.","57195065369;24833884000;56442891500;7004096151;","SEE ME ROAR: Self-determination enhanced engagement for math education relying on augmented reality",2017,"CHI PLAY 2017 Extended Abstracts - Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play",,,,"345","351",,6,"10.1145/3130859.3131316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034740419&doi=10.1145%2f3130859.3131316&partnerID=40&md5=be02f6b9b7ff36595adf9102fe297150","Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven, 5612 AZ, Netherlands","Li, J., Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven, 5612 AZ, Netherlands; Van Der Spek, E., Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven, 5612 AZ, Netherlands; Hu, J., Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven, 5612 AZ, Netherlands; Feijs, L., Department of Industrial Design, Eindhoven University of Technology, Den Dolech 2, Eindhoven, 5612 AZ, Netherlands","Contemporary primary school students generally spend a lot of time playing digital games, but may be less interested in their schoolwork, such as learning mathematics. Mathematics includes many abstract concepts that can be difficult to grasp for some students. Augmented reality as a technology makes it possible to transfer the abstract knowledge into concrete and situated contexts so that the students can better comprehend the information. In addition, the social interactions can also improve the students' learning experience. In order to better understand how to design such AR games for learning, we design an AR-based social game platform for primary school students in mathematics learning, called SEE ME ROAR. This paper describes the design and implementation process of the first prototype. We report early results from one primary school teacher and two primary school students, which indicate the game is fun and might be helpful for their study. Future work will focus on the specific effects of different game mechanics on the students' learning experience. The game platform will become more generalized for other subjects in the future. © 2017 Copyright is held by the owner/author(s).","Augmented reality; Game; Game design; Mathematics learning; Serious game","Abstracting; Augmented reality; Human computer interaction; Interactive computer systems; Serious games; Students; Teaching; Design and implementations; Game; Game design; Games for learning; Learning Mathematics; Mathematics learning; Social interactions; Students' learning experiences; Education",Conference Paper,"Final","",Scopus,2-s2.0-85034740419
"Plessas A.","55056455600;","Computerized Virtual Reality Simulation in Preclinical Dentistry: Can a Computerized Simulator Replace the Conventional Phantom Heads and Human Instruction?",2017,"Simulation in Healthcare","12","5",,"332","338",,17,"10.1097/SIH.0000000000000250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023160701&doi=10.1097%2fSIH.0000000000000250&partnerID=40&md5=52857df93dec0dd424ee35ade8b818fb","NIHR Academic and Clinical Fellow in General Dental Practice, Peninsula Dental School, Plymouth University, Plymouth, PL4 8AA, United Kingdom","Plessas, A., NIHR Academic and Clinical Fellow in General Dental Practice, Peninsula Dental School, Plymouth University, Plymouth, PL4 8AA, United Kingdom","Summary Statement In preclinical dental education, the acquisition of clinical, technical skills, and the transfer of these skills to the clinic are paramount. Phantom heads provide an efficient way to teach preclinical students dental procedures safely while increasing their dexterity skills considerably. Modern computerized phantom head training units incorporate features of virtual reality technology and the ability to offer concurrent augmented feedback. The aims of this review were to examine and evaluate the dental literature for evidence supporting their use and to discuss the role of augmented feedback versus the facilitator's instruction. Adjunctive training in these units seems to enhance student's learning and skill acquisition and reduce the required faculty supervision time. However, the virtual augmented feedback cannot be used as the sole method of feedback, and the facilitator's input is still critical. Well-powered longitudinal randomized trials exploring the impact of these units on student's clinical performance and issues of cost-effectiveness are warranted. Copyright © 2017 Society for Simulation in Healthcare.","faculty; Key Words Dental education; simulation training","clinical competence; computer interface; constructive feedback; cost benefit analysis; dental education; education; human; manikin; oral surgery; problem based learning; procedures; simulation training; time factor; virtual reality; Clinical Competence; Cost-Benefit Analysis; Education, Dental; Formative Feedback; Humans; Manikins; Oral Surgical Procedures; Problem-Based Learning; Simulation Training; Time Factors; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85023160701
"Ge C., Wang N., Foster G., Wilson M.","55499837400;8883516500;57209971017;57202530152;","Toward QoE-Assured 4K Video-on-Demand Delivery Through Mobile Edge Virtualization with Adaptive Prefetching",2017,"IEEE Transactions on Multimedia","19","10", 8000391,"2222","2237",,50,"10.1109/TMM.2017.2735301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028915861&doi=10.1109%2fTMM.2017.2735301&partnerID=40&md5=f76d7afc642a614d4531bc5130d814be","5GIC, Institute for Communication Systems, University of Surrey, Guildford, GU2 7XH, United Kingdom; Fujitsu Laboratories of Europe, Hayes, UB4 8FE, United Kingdom","Ge, C., 5GIC, Institute for Communication Systems, University of Surrey, Guildford, GU2 7XH, United Kingdom; Wang, N., 5GIC, Institute for Communication Systems, University of Surrey, Guildford, GU2 7XH, United Kingdom; Foster, G., 5GIC, Institute for Communication Systems, University of Surrey, Guildford, GU2 7XH, United Kingdom; Wilson, M., Fujitsu Laboratories of Europe, Hayes, UB4 8FE, United Kingdom","Internet video streaming applications have been demanding more bandwidth and higher video quality, especially with the advent of virtual reality and augmented reality appli-cations. While adaptive strea ming protocols like MPEG-DASH (dynamic adaptive streaming over HTTP) allows video quality to be flexibly adapted, e.g., degraded when mobile network condition deteriorates, this is not an option if the application itself requires guaranteed 4K quality at all time. On the other hand, conventional end-to-end transmission control protocol (TCP) has been struggling in supporting 4K video delivery across long-distance Internet paths containing both fixed and mobile network segments with heterogeneous characteristics. In this paper, we present a novel and practically feasible system architecture named MVP (mobile edge virtualization with adaptive prefetching), which enables content providers to embed their content intelligence as a virtual network function into the mobile network operator's infrastructure edge. Based on this architecture, we present a context-aware adaptive video prefetching scheme in order to achieve quality of experience (QoE)-assured 4K video on demand (VoD) delivery across the global Internet. Through experiments based on a real LTE-A network infrastructure, we demonstrate that our proposed scheme is able to achieve QoE-assured 4K VoD streaming, especially when the video source is located remotely in the public Internet, in which case none of the state-of-the-art solutions is able to support such an objective at global Internet scale. © 1999-2012 IEEE.","Mobile edge computing; MPEG-DASH; network function virtualization; prefetching; quality of experience (QoE); video on demand (VoD)","Augmented reality; HTTP; Mobile telecommunication systems; Motion Picture Experts Group standards; Network architecture; Network function virtualization; Quality of service; Transfer functions; Video streaming; Virtual reality; Virtualization; Wireless networks; Wireless telecommunication systems; Dynamic Adaptive Streaming over HTTP; Edge computing; Heterogeneous characteristic; Mobile network operators; Mpeg dashes; Prefetching; Quality of experience (QoE); Video on demands (VoD); Video on demand",Article,"Final","",Scopus,2-s2.0-85028915861
"Fouché R.C.","57196321009;","Head mouse: Generalisability of research focused on the disabled to able bodied users",2017,"ACM International Conference Proceeding Series","Part F130806",, a14,"","",,1,"10.1145/3129416.3129442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032636511&doi=10.1145%2f3129416.3129442&partnerID=40&md5=a9fd8886d1977bcfcca42e87f32d3820","University of the Free State, South Africa","Fouché, R.C., University of the Free State, South Africa","This paper discusses a study which aimed to determine whether the results obtained from studies focusing on users with disabilities are comparable with the results from users without disabilities for head operated cursor control. Four categories of research findings were investigated namely: the optimum sensitivity setting, the amount of neck fatigue, learnability and the level of user satisfaction. A shooting genre game was developed, and a task was included during development to simulate gaming actions with moving targets. Data for four efficiency metrics was gathered during two phases of user testing. The low sensitivity setting resulted in the fastest gameplay for users without disabilities, which contradicted research results from studies focusing on individuals with disabilities. It was also found that the amount of neck fatigue decreased after several sessions for users without disabilities. The efficient use of the head mouse by individuals without disabilities showed significant improvement over time for certain elements, indicating that learning took place. The level of user satisfaction also increased with extended use of the head mouse by individuals without disabilities. Although some findings can be generalized to both types of users, this study shows that new knowledge was gained in terms of able bodied use of a head mouse. © 2017 Association for Computing Machinery.","Cursor control; Gaming; Head mouse; Human-computer Interaction; Natural User Interfaces; Usability","Engineers; Mammals; User interfaces; Cursor control; Gaming; Head mouse; Natural user interfaces; Usability; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85032636511
"Wakisaka S., Hiyama A., Inami M.","55604107400;8866503200;9640047900;","Transmission of experiences with augmented human techniques",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,,,"740","744",,,"10.1145/3123024.3129276","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030833467&doi=10.1145%2f3123024.3129276&partnerID=40&md5=f0d290c5cb1d850bd07dbff7bc5feb58","Information Somatics Laboratory, Research Center for Advanced Science and Technology, Eng.1-403,7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan","Wakisaka, S., Information Somatics Laboratory, Research Center for Advanced Science and Technology, Eng.1-403,7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Hiyama, A., Information Somatics Laboratory, Research Center for Advanced Science and Technology, Eng.1-403,7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Inami, M., Information Somatics Laboratory, Research Center for Advanced Science and Technology, Eng.1-403,7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan","Transferring human experience is one of fundamental and vital activities in our history. Teachers, trainers, textbooks, instruction movies, etc. has been introduced for the transmission. New methods that enhance transmission of human experiences are always desired according to the social and economic situations. Recent developments of virtual reality (VR), augmented realty (AR) and augmented human (AH) technologies make us expect a future that we can instantaneously (or at least more efficiently) transfer the skills or knowledges from one to another, like in Sci-Fi movies. However, for that, we need to optimize the transmission method for each person. We propose the augmented-human based approach for the experience transmission, and review some factors that are essential but not well focused in past studies. Copyright © 2017 ACM.","Augmented human; Cognitive bias; Experimental supplement; Health care; Sports","Health care; Sports; Teaching; Ubiquitous computing; Wearable computers; Wearable technology; Augmented human; Cognitive bias; Economic situation; Experimental supplement; Human techniques; Transmission methods; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85030833467
"Hoedt S., Claeys A., Van Landeghem H., Cottyn J.","57045968300;57045712400;23013574600;35145389900;","The evaluation of an elementary virtual training system for manual assembly",2017,"International Journal of Production Research","55","24",,"7496","7508",,13,"10.1080/00207543.2017.1374572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029453228&doi=10.1080%2f00207543.2017.1374572&partnerID=40&md5=629547701b08d4d25cbd017c7ccd8fb2","Department of Industrial Systems Engineering and Product Design, Ghent University, Gent-Zwijnaarde, Belgium; Department of Agile and Human Centered Production and Robotic Systems, Flanders Make, Ghent, Belgium","Hoedt, S., Department of Industrial Systems Engineering and Product Design, Ghent University, Gent-Zwijnaarde, Belgium, Department of Agile and Human Centered Production and Robotic Systems, Flanders Make, Ghent, Belgium; Claeys, A., Department of Industrial Systems Engineering and Product Design, Ghent University, Gent-Zwijnaarde, Belgium, Department of Agile and Human Centered Production and Robotic Systems, Flanders Make, Ghent, Belgium; Van Landeghem, H., Department of Industrial Systems Engineering and Product Design, Ghent University, Gent-Zwijnaarde, Belgium, Department of Agile and Human Centered Production and Robotic Systems, Flanders Make, Ghent, Belgium; Cottyn, J., Department of Industrial Systems Engineering and Product Design, Ghent University, Gent-Zwijnaarde, Belgium, Department of Agile and Human Centered Production and Robotic Systems, Flanders Make, Ghent, Belgium","Due to the low volume high variety strategies of manufacturing companies, manual assembly operators have a much larger cognitive load than before. The expertise of the operators must be kept up to date at any time. Since the high investment and low flexibility of a real setting to perform a manual assembly training, a virtual replica is introduced in many cases. The aim of this paper is to study the effect of an elementary virtual training for manual assembly tasks. In literature, different studies on the topic can be found; nevertheless, a comparison between the different studies is not possible due to diverse evaluation methods and descriptions. A benchmark for a uniform evaluation of virtual training systems is presented and applied to this experiment. Two groups were submitted to a number of manual assembly tasks. The test group got a virtual training period in advance. A significant learning transfer during that training period was observed. When the first assembly of the reference group is counted as a real training, no significant difference can be found between the virtual and real training. The outcomes of this experiment will be used in future work to compare different virtual training systems and influential factors such as the assembly complexity. Furthermore, the application of virtual training to manual assembly in a mixed-model environment and its industrial usability are topics that still need to be studied. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Learning effect; Manual assembly; Virtual manufacturing; Virtual reality; Virtual training","Agile manufacturing systems; Virtual reality; Evaluation methods; Influential factors; Learning effects; Manual assembly; Manufacturing companies; Virtual manufacturing; Virtual training; Virtual training systems; E-learning",Article,"Final","",Scopus,2-s2.0-85029453228
"Levac D.E., Jovanovic B.B.","25937323900;57210598387;","Is children's motor learning of a postural reaching task enhanced by practice in a virtual environment?",2017,"International Conference on Virtual Rehabilitation, ICVR","2017-June",, 8007489,"","",,3,"10.1109/ICVR.2017.8007489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034263420&doi=10.1109%2fICVR.2017.8007489&partnerID=40&md5=76697453ade975ba90ce55926f2aa9cb","Dept of Physical Therapy, Movement and Rehab Sciences, Northeastern University, Boston, United States; Dept of Electronics, Faculty of Electr. Engineering, University of Niš, Niš, Serbia","Levac, D.E., Dept of Physical Therapy, Movement and Rehab Sciences, Northeastern University, Boston, United States; Jovanovic, B.B., Dept of Electronics, Faculty of Electr. Engineering, University of Niš, Niš, Serbia","To support the use of virtual environments (VEs) in pediatric rehabilitation, greater understanding of the extent and mechanisms by which practice in a VE might facilitate motor learning as compared to practice in a physical environment (PE) is required. One proposed mechanism is via enhanced user engagement and/or motivation, which may directly influence the quality of motor memory consolidation. The objectives of this study were to a) compare children's motor learning of the same novel postural reaching task in a VE versus a PE; b) evaluate differences in engagement and motivation between the two practice environments; and c) explore the relationships between practice environment, engagement, motivation, and motor learning. Thirty-six typically developing children aged 7-13 years were randomized to acquire a novel postural reaching skill in either a 2D flat-screen VE or a PE. Skin conductance level (SCL) was measured on the non-task hand during practice. Following acquisition, children completed a language-modified User Engagement Scale (UES) and the Pediatric Motivation Inventory (PMOT). Participants returned 1-7 days later for retention (same environment) and transfer (opposite environment) tests. Children who practiced in the VE demonstrated greater retention, as evidenced by higher mean scores on the retention test (t[30] =-3.72, p = 0.001, partial eta squared 0.28). Children who practiced in the PE demonstrated greater transfer to the opposite environment as compared to those who practiced in the VE (t[30] = 2.05, p = 0.001, partial eta squared = 0.238). There were no significant differences in UES total or subscale scores between the 2 groups. PMOT total motivation scores differed significantly between groups, favoring the VE (t[30]= 2.49, p = 0.018, partial eta squared = 0.154). There were no significant differences in SCL peak count or peaks per minute between groups. There was no relationship between engagement, motivation, and retention or transfer performance. Findings suggest that retention but not transfer of a new motor skill may be facilitated by practice in a VE. This may be due to unique task demands in each environment. Children were more motivated to succeed in the VE and were engaged in the task in both environments, suggesting that both constructs should be measured in subsequent studies and that VE aesthetics alone may not be a key 'active ingredient' of children's engagement. Subsequent research will more objectively quantify neurophysiological correlates of engagement and motivation, explore additional tasks, include populations with neurological impairments and compare 3D vs 2D VE displays. © 2017 IEEE.","children; engagement; motivation; motor learning; video game; Virtual reality","E-learning; Motivation; Pediatrics; Virtual reality; Active ingredients; children; engagement; Memory consolidation; Motor learning; Physical environments; Transfer performance; Video game; Education",Conference Paper,"Final","",Scopus,2-s2.0-85034263420
"Kiefer A.W., Dicesare C., Bonnette S., Kitchen K., Gadd B., Thomas S., Barber Foss K.D., Myer G.D., Riley M.A., Silva P.","35316086800;55620685100;55004035700;57195460949;57003350800;55772425700;6507308390;6701852696;7203009785;16302548400;","Sport-specific virtual reality to identify profiles of anterior cruciate ligament injury risk during unanticipated cutting",2017,"International Conference on Virtual Rehabilitation, ICVR","2017-June",, 8007511,"","",,4,"10.1109/ICVR.2017.8007511","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034266510&doi=10.1109%2fICVR.2017.8007511&partnerID=40&md5=cf5780b41e2b5c33be48505f311391ad","Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Center for Cognition Action, and Perception, University of Cincinnati, Cincinnati, OH, United States","Kiefer, A.W., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Dicesare, C., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Bonnette, S., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Kitchen, K., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Gadd, B., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Thomas, S., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Barber Foss, K.D., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Myer, G.D., Division of Sports Medicine, Cincinnati Children's Hospital, Cincinnati, OH, United States; Riley, M.A., Center for Cognition Action, and Perception, University of Cincinnati, Cincinnati, OH, United States; Silva, P., Center for Cognition Action, and Perception, University of Cincinnati, Cincinnati, OH, United States","Female athletes are at an increased risk of anterior cruciate ligament (ACL) injury in competitive sport during running, jumping and cutting tasks. This risk is due to deficits in posterior chain and hip recruitment associated with aberrant frontal knee loads. The identification of these risk factors has led to targeted neuromuscular training (NMT) interventions to enhance hip neuromuscular control during such tasks. Despite the successful modification of ACL injury risk factors following NMT, the transfer of these corrected movement patterns to the sport-specific contexts has not been directly evaluated. Sport-specific virtual reality (VR) may provide the best method to measure training transfer to realistic sport performance, while still allowing appropriate experimental control and high-fidelity performance measurements. The current study examined the effect of a biofeedback-driven augmented NMT (aNMT) on skill transfer of ACL-injury resistant movement patterns during performance of sport-specific VR scenarios. Five trained athletes participated, and their performance on an unanticipated cutting task was assessed in VR prior to and after six weeks of aNMT. A significant 87% reduction in internal hip rotation was observed on the plant leg during the loading phase of cutting (p =.05), along with an observed 116% reduction during the push-off phase (p =.02), from pre-to post-training. A non-significant trend of a 19% reduction in knee abduction was also observed (p =.15). This study is the first that has utilized free ambulatory wireless VR to assess injury risk in athletes during performance of sport-specific tasks. The reduction in internal hip rotation and knee abduction align with previous findings on laboratory based tests. The current results are the first step in the validation of sport-specific VR as a tool for understanding injury risk during simulation of real-world sport performance. © 2017 IEEE.","anterior cruciate ligament; cutting; soccer; sport; virtual reality","Biofeedback; Cutting; Health risks; Risk assessment; Virtual reality; Anterior cruciate ligament; Anterior cruciate ligament injury; Competitive sports; Experimental control; Neuromuscular control; Performance measurements; soccer; Sport performance; Sports",Conference Paper,"Final","",Scopus,2-s2.0-85034266510
"Chebat D.-R., Maidenbaum S., Amedi A.","11140655700;54585718400;6506450805;","The transfer of non-visual spatial knowledge between real and virtual mazes via sensory substitution",2017,"International Conference on Virtual Rehabilitation, ICVR","2017-June",, 8007542,"","",,8,"10.1109/ICVR.2017.8007542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034243902&doi=10.1109%2fICVR.2017.8007542&partnerID=40&md5=59cbc6b15fd74d6e809d425cc0cd7bfe","Visual and Cognitive Neuroscience Laboratory (VCN Lab), Department of Behavioral Sciences and Psychology, Ariel University, Ariel, Israel; Department of Medical Neurobiology, Institute for Medical Research Israel-Canada, Faculty of Medicine, Hebrew University of Jerusalem, Jerusalem, Israel","Chebat, D.-R., Visual and Cognitive Neuroscience Laboratory (VCN Lab), Department of Behavioral Sciences and Psychology, Ariel University, Ariel, Israel; Maidenbaum, S., Department of Medical Neurobiology, Institute for Medical Research Israel-Canada, Faculty of Medicine, Hebrew University of Jerusalem, Jerusalem, Israel; Amedi, A., Department of Medical Neurobiology, Institute for Medical Research Israel-Canada, Faculty of Medicine, Hebrew University of Jerusalem, Jerusalem, Israel","Many attempts are being made to ease navigation for people who are blind, both in terms of spatial learning and of navigation. One promising approach is the use of virtual environments for safe and versatile training. While it is known that humans can transfer non-visual spatial knowledge between real and virtual environments, limitations of these studies typically include results obtained mainly in simple environments, using mainly blindfolded-sighted participants and different methods of sensory input for real and virtual environments. In this study, participants with a wide range of visual experience use the EyeCane and Virtual EyeCane to solve complex Hebb-Williams mazes in real and virtual environments. The EyeCane and its virtual counterpart are minimalistic sensory substitution devices that code single-point distance information into sound. We test whether participants improve performance in the real-to-virtual sequence: Solve a real maze and subsequently improve performance in the virtual maze. We also test whether participants can sole a virtual maze and subsequently improve performance in the virtual world: The virtual-to-real sequence. We find that participants can use sensory substitution guided navigation to extract spatial information from the virtual world and apply it to significantly improve their behavioral performance in the real world and vice versa. Our results demonstrate transfer in both direction, strengthening and extending the existing literature in terms of complexity, parameters, input-matching and varying levels of visual experience. © 2017 IEEE.","Acquired Blindness; assistive technology; blind; Congenital Blindness; Environmental Rehabilitation; Low Vision; Maze Learning; Perceptual Learning; Sensory Substitution; Spatial knowledge; Virtual Reality; Visual Rehabilitation","Cognitive systems; Environmental technology; Eye protection; Navigation; Acquired Blindness; Assistive technology; blind; Congenital blindness; Environmental rehabilitation; Low vision; Maze Learning; Perceptual learning; Sensory substitution; Spatial knowledge; Visual rehabilitation; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85034243902
"Ragan E.D., Scerbo S., Bacim F., Bowman D.A.","26667185300;55210765300;16174310700;57203231782;","Amplified Head Rotation in Virtual Reality and the Effects on 3D Search, Training Transfer, and Spatial Orientation",2017,"IEEE Transactions on Visualization and Computer Graphics","23","8", 7547900,"1880","1895",,22,"10.1109/TVCG.2016.2601607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028407406&doi=10.1109%2fTVCG.2016.2601607&partnerID=40&md5=36f5ebc4f3b8d70469432fd11b448a6f","Texas A and M University, College Station, TX  77843, United States; Virginia Tech, Blacksburg, VA  24061, United States","Ragan, E.D., Texas A and M University, College Station, TX  77843, United States; Scerbo, S., Virginia Tech, Blacksburg, VA  24061, United States; Bacim, F., Virginia Tech, Blacksburg, VA  24061, United States; Bowman, D.A., Virginia Tech, Blacksburg, VA  24061, United States","Many types of virtual reality (VR) systems allow users to use natural, physical head movements to view a 3D environment. In some situations, such as when using systems that lack a fully surrounding display or when opting for convenient low-effort interaction, view control can be enabled through a combination of physical and virtual turns to view the environment, but the reduced realism could potentially interfere with the ability to maintain spatial orientation. One solution to this problem is to amplify head rotations such that smaller physical turns are mapped to larger virtual turns, allowing trainees to view the entire surrounding environment with small head movements. This solution is attractive because it allows semi-natural physical view control rather than requiring complete physical rotations or a fully-surrounding display. However, the effects of amplified head rotations on spatial orientation and many practical tasks are not well understood. In this paper, we present an experiment that evaluates the influence of amplified head rotation on 3D search, spatial orientation, and cybersickness. In the study, we varied the amount of amplification and also varied the type of display used (head-mounted display or surround-screen CAVE) for the VR search task. By evaluating participants first with amplification and then without, we were also able to study training transfer effects. The findings demonstrate the feasibility of using amplified head rotation to view 360 degrees of virtual space, but noticeable problems were identified when using high amplification with a head-mounted display. In addition, participants were able to more easily maintain a sense of spatial orientation when using the CAVE version of the application, which suggests that visibility of the user's body and awareness of the CAVE's physical environment may have contributed to the ability to use the amplification technique while keeping track of orientation. © 1995-2012 IEEE.","3D interaction; cybersickness; Rotation amplification; Search; Spatial orientation; Virtual reality","Caves; E-learning; Helmet mounted displays; Virtual reality; 3D interactions; Amplification technique; Cybersickness; Head mounted displays; Physical environments; Search; Spatial orientations; Surrounding environment; Rotation",Article,"Final","",Scopus,2-s2.0-85028407406
"Zou Y.-B., Chen Y.-M., Gao M.-K., Liu Q., Jiang S.-Y., Lu J.-H., Huang C., Li Z.-Y., Zhang D.-H.","25647078200;56430038800;56814140600;56813870700;57194942277;57189989390;37861305000;56814084800;9733268500;","Coronary Heart Disease Preoperative Gesture Interactive Diagnostic System Based on Augmented Reality",2017,"Journal of Medical Systems","41","8", 126,"","",,3,"10.1007/s10916-017-0768-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024362810&doi=10.1007%2fs10916-017-0768-6&partnerID=40&md5=69ba844d2f7cea7c5f6154c0253af88b","College of Information Technology, Shanghai Ocean University, No.999 Huchenghuan Road , Pudong New District, Shanghai City, China; School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; The 32nd Research Institute, China Electronics Technology Group Corporation, No.63 Chengliugong Road, Jiading District, Shanghai City, China; Ruijin Hospital, School of Medicine, Shanghai Jiao Tong University, No2 Ruijin Road, Shanghai City, China","Zou, Y.-B., College of Information Technology, Shanghai Ocean University, No.999 Huchenghuan Road , Pudong New District, Shanghai City, China; Chen, Y.-M., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; Gao, M.-K., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China, The 32nd Research Institute, China Electronics Technology Group Corporation, No.63 Chengliugong Road, Jiading District, Shanghai City, China; Liu, Q., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; Jiang, S.-Y., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; Lu, J.-H., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; Huang, C., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China; Li, Z.-Y., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China, Ruijin Hospital, School of Medicine, Shanghai Jiao Tong University, No2 Ruijin Road, Shanghai City, China; Zhang, D.-H., School of Computer Engineering and Science, Shanghai University, No.99 Shangda Road, Baoshan District, Shanghai City, China","Coronary heart disease preoperative diagnosis plays an important role in the treatment of vascular interventional surgery. Actually, most doctors are used to diagnosing the position of the vascular stenosis and then empirically estimating vascular stenosis by selective coronary angiography images instead of using mouse, keyboard and computer during preoperative diagnosis. The invasive diagnostic modality is short of intuitive and natural interaction and the results are not accurate enough. Aiming at above problems, the coronary heart disease preoperative gesture interactive diagnostic system based on Augmented Reality is proposed. The system uses Leap Motion Controller to capture hand gesture video sequences and extract the features which that are the position and orientation vector of the gesture motion trajectory and the change of the hand shape. The training planet is determined by K-means algorithm and then the effect of gesture training is improved by multi-features and multi-observation sequences for gesture training. The reusability of gesture is improved by establishing the state transition model. The algorithm efficiency is improved by gesture prejudgment which is used by threshold discriminating before recognition. The integrity of the trajectory is preserved and the gesture motion space is extended by employing space rotation transformation of gesture manipulation plane. Ultimately, the gesture recognition based on SRT-HMM is realized. The diagnosis and measurement of the vascular stenosis are intuitively and naturally realized by operating and measuring the coronary artery model with augmented reality and gesture interaction techniques. All of the gesture recognition experiments show the distinguish ability and generalization ability of the algorithm and gesture interaction experiments prove the availability and reliability of the system. © 2017, Springer Science+Business Media, LLC.","augmented reality; coronary heart disease; gesture interaction; HMM; K-means; leap motion controller; preoperative diagnosis","algorithm; Article; diagnostic equipment; gesture; hidden Markov model; ischemic heart disease; Leap Motion controller; motion analysis system; preoperative evaluation; recognition; stenosis; coronary artery disease; gesture; hand; human; reproducibility; Algorithms; Coronary Disease; Gestures; Hand; Humans; Reproducibility of Results",Article,"Final","",Scopus,2-s2.0-85024362810
"Melendez-Calderon A., Tan M., Bittmann M.F., Burdet E., Patton J.L.","24438365500;57193863931;57191576337;7004358183;35782382700;","Transfer of dynamic motor skills acquired during isometric training to free motion",2017,"Journal of Neurophysiology","118","1",,"219","233",,5,"10.1152/jn.00614.2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021973612&doi=10.1152%2fjn.00614.2016&partnerID=40&md5=633f615e0635e0e56cfdd52afea44f09","Department of Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL, United States; Rehabilitation Institute of Chicago, Chicago, IL, United States; University of Illinois at Chicago, Chicago, IL, United States; Department of Bioengineering, Imperial College of Science, Technology and Medicine, London, United Kingdom","Melendez-Calderon, A., Department of Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL, United States, Rehabilitation Institute of Chicago, Chicago, IL, United States; Tan, M., Rehabilitation Institute of Chicago, Chicago, IL, United States, University of Illinois at Chicago, Chicago, IL, United States; Bittmann, M.F., Rehabilitation Institute of Chicago, Chicago, IL, United States, University of Illinois at Chicago, Chicago, IL, United States; Burdet, E., Department of Bioengineering, Imperial College of Science, Technology and Medicine, London, United Kingdom; Patton, J.L., Department of Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL, United States, Rehabilitation Institute of Chicago, Chicago, IL, United States, University of Illinois at Chicago, Chicago, IL, United States","Recent studies have explored the prospects of learning to move without moving, by displaying virtual arm movement related to exerted force. However, it has yet to be tested whether learning the dynamics of moving can transfer to the corresponding movement. Here we present a series of experiments that investigate this isometric training paradigm. Subjects were asked to hold a handle and generate forces as their arms were constrained to a static position. A precise simulation of reaching was used to make a graphic rendering of an arm moving realistically in response to the measured interaction forces and simulated environmental forces. Such graphic rendering was displayed on a horizontal display that blocked their view to their actual (statically constrained) arm and encouraged them to believe they were moving. We studied adaptation of horizontal, planar, goal-directed arm movements in a velocity-dependent force field. Our results show that individuals can learn to compensate for such a force field in a virtual environment and transfer their new skills to the actual free motion condition, with performance comparable to practice while moving. Such nonmoving techniques should impact various training conditions when moving may not be possible. NEW & NOTEWORTHY This study provided early evidence supporting that training movement skills without moving is possible. In contrast to previous studies, our study involves 1) exploiting cross-modal sensory interactions between vision and proprioception in a motionless setting to teach motor skills that could be transferable to a corresponding physical task, and 2) evaluates the movement skill of controlling muscle-generated forces to execute arm movements in the presence of external forces that were only virtually present during training. © 2017 the American Physiological Society.","Isometric; Motor adaptation; Motor learning; Visual feedback","adaptation; adult; arm movement; Article; force; human; human experiment; isometric exercise; motion; motor performance; priority journal; simulation training; velocity; virtual reality; arm; female; learning; male; motion; muscle isometric contraction; physiological feedback; physiology; task performance; Adult; Arm; Feedback, Physiological; Female; Humans; Isometric Contraction; Learning; Male; Motion; Motor Skills; Task Performance and Analysis",Article,"Final","",Scopus,2-s2.0-85021973612
"Alam M.F., Katsikas S., Beltramello O., Hadjiefthymiades S.","57061419000;54939590000;57206137363;6603881052;","Augmented and virtual reality based monitoring and safety system: A prototype IoT platform",2017,"Journal of Network and Computer Applications","89",,,"109","119",,38,"10.1016/j.jnca.2017.03.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016438031&doi=10.1016%2fj.jnca.2017.03.022&partnerID=40&md5=fd03a592a947579fdc656ace6be24708","Department of Informatics and Telecommunications, National and Kapodistrian University of Athens, Greece; Head of Research and Development, Prisma Electronics SA, Greece; EDUSAFE project coordinator, The European Organization for Nuclear Research (CERN), Switzerland","Alam, M.F., Department of Informatics and Telecommunications, National and Kapodistrian University of Athens, Greece; Katsikas, S., Head of Research and Development, Prisma Electronics SA, Greece; Beltramello, O., EDUSAFE project coordinator, The European Organization for Nuclear Research (CERN), Switzerland; Hadjiefthymiades, S., Department of Informatics and Telecommunications, National and Kapodistrian University of Athens, Greece","This paper presents an Augmented and Virtual Reality (AR/VR) based IoT prototype system. Performing maintenance tasks in a complex environment is quite challenging and difficult due to complex, and possibly, underground facilities, uneasy access, human factors, heavy machineries, etc. Current technology is not acceptable because of significant delays in communication and data transmission, missing multi-input interfaces, and simultaneous supervision of multiple workers who are working in the extreme environment. The aim is to technically advance and combine several technologies and integrate them as integral part of a personnel safety system to improve safety, maintain availability, reduce errors and decrease the time needed for scheduled or ad hoc interventions. We emphasize on the aspects that were made “feasible” on the worker's side due to the equipment used (mobile computing equipment). We present that the demanding tasks that previously were simply undertaken on the fixed infrastructure are now possible on the mobile end. The research challenges lie in the development of real-time data-transmission, instantaneous analysis of data coming from different inputs, local intelligence in low power embedded systems, interaction with multiple on-site users, complex user interfaces, portability and wearability. This work is part EDUSAFE, a Marie Curie ITN (Initial Training Network) project focusing on research into the use of Augmented and Virtual Reality (AR/VR) during planned and ad hoc maintenance in extreme work environments. © 2017 Elsevier Ltd","AR/VR design; IoT; Maintenance; Mobile; Modular; Prototype; Safety system/application","Complex networks; Data communication systems; Data transfer; Embedded systems; Machinery; Maintenance; Real time systems; Safety engineering; Security systems; User interfaces; Virtual reality; Augmented and virtual realities; Low power embedded systems; Mobile; Modular; Performing maintenance; Personnel safety systems; Prototype; Real time data transmission; Internet of things",Article,"Final","",Scopus,2-s2.0-85016438031
"Hetherington J., Lessoway V., Gunka V., Abolmaesumi P., Rohling R.","57193795065;16145363600;6508059485;6602170125;7004322927;","SLIDE: automatic spine level identification system using a deep convolutional neural network",2017,"International Journal of Computer Assisted Radiology and Surgery","12","7",,"1189","1198",,25,"10.1007/s11548-017-1575-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016640225&doi=10.1007%2fs11548-017-1575-8&partnerID=40&md5=60a73b4ac7e3c44038440fa8a5e97e7e","Department of Electrical and Computer Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Department of Mechanical Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Department of Ultrasound, BC Women’s Hospital, 4500 Oak Street, Vancouver, BC  V6H 3N1, Canada; Department of Obstetric Anesthesia, BC Women’s Hospital, 4500 Oak Street, Vancouver, BC  V6H 3N1, Canada","Hetherington, J., Department of Electrical and Computer Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Lessoway, V., Department of Ultrasound, BC Women’s Hospital, 4500 Oak Street, Vancouver, BC  V6H 3N1, Canada; Gunka, V., Department of Obstetric Anesthesia, BC Women’s Hospital, 4500 Oak Street, Vancouver, BC  V6H 3N1, Canada; Abolmaesumi, P., Department of Electrical and Computer Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Rohling, R., Department of Electrical and Computer Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada, Department of Mechanical Engineering, The University of British Columbia, 2332 Main Mall, Vancouver, BC  V6T 1Z4, Canada","Purpose: Percutaneous spinal needle insertion procedures often require proper identification of the vertebral level to effectively and safely deliver analgesic agents. The current clinical method involves “blind” identification of the vertebral level through manual palpation of the spine, which has only 30% reported accuracy. Therefore, there is a need for better anatomical identification prior to needle insertion. Methods: A real-time system was developed to identify the vertebral level from a sequence of ultrasound images, following a clinical imaging protocol. The system uses a deep convolutional neural network (CNN) to classify transverse images of the lower spine. Several existing CNN architectures were implemented, utilizing transfer learning, and compared for adequacy in a real-time system. In the system, the CNN output is processed, using a novel state machine, to automatically identify vertebral levels as the transducer moves up the spine. Additionally, a graphical display was developed and integrated within 3D Slicer. Finally, an augmented reality display, projecting the level onto the patient’s back, was also designed. A small feasibility study (n= 20) evaluated performance. Results: The proposed CNN successfully discriminates ultrasound images of the sacrum, intervertebral gaps, and vertebral bones, achieving 88% 20-fold cross-validation accuracy. Seventeen of 20 test ultrasound scans had successful identification of all vertebral levels, processed at real-time speed (40 frames/s). Conclusion: A machine learning system is presented that successfully identifies lumbar vertebral levels. The small study on human subjects demonstrated real-time performance. A projection-based augmented reality display was used to show the vertebral level directly on the subject adjacent to the puncture site. © 2017, CARS.","Machine learning; Needle guidance; Ultrasound; Vertebral level","anatomical variation; Article; artificial neural network; automation; clinical evaluation; computer graphics; convolutional neural network; feasibility study; fifth lumbar vertebra; first lumbar vertebra; fourth lumbar vertebra; human; intermethod comparison; measurement accuracy; priority journal; real time echography; real time tracking system; real time ultrasound scanner; sacral vertebra; software validation; three dimensional imaging; ultrasound transducer; vertebra body; algorithm; diagnostic imaging; epidural anesthesia; evaluation study; image processing; lumbar vertebra; procedures; spine; Algorithms; Anesthesia, Epidural; Humans; Image Processing, Computer-Assisted; Lumbar Vertebrae; Neural Networks (Computer); Spine",Article,"Final","",Scopus,2-s2.0-85016640225
"Huang Z., Lin C., Kanai-Pak M., Maeda J., Kitajima Y., Nakamura M., Kuwahara N., Ogata T., Ota J.","55491068400;57193331359;24831500500;55211090200;55488702400;55211431100;14021573100;8636962300;7006719514;","Impact of Using a Robot Patient for Nursing Skill Training in Patient Transfer",2017,"IEEE Transactions on Learning Technologies","10","3", 7542122,"355","366",,12,"10.1109/TLT.2016.2599537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030526839&doi=10.1109%2fTLT.2016.2599537&partnerID=40&md5=86b719a11e4e7ea197134b3570d59560","School of Automation, Guangdong University of Technology, Guangzhou, 510006, China; Research into Artifacts, Center for Engineering (RACE), University of Tokyo, Chiba, 277-8568, Japan; Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, 135-0063, Japan; Department of Advanced Fibro-Science, Kyoto Institute of Technology, Kyoto, 606-8585, Japan","Huang, Z., School of Automation, Guangdong University of Technology, Guangzhou, 510006, China; Lin, C., Research into Artifacts, Center for Engineering (RACE), University of Tokyo, Chiba, 277-8568, Japan; Kanai-Pak, M., Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, 135-0063, Japan; Maeda, J., Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, 135-0063, Japan; Kitajima, Y., Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, 135-0063, Japan; Nakamura, M., Faculty of Nursing, Tokyo Ariake University of Medical and Health Sciences, Tokyo, 135-0063, Japan; Kuwahara, N., Department of Advanced Fibro-Science, Kyoto Institute of Technology, Kyoto, 606-8585, Japan; Ogata, T., Research into Artifacts, Center for Engineering (RACE), University of Tokyo, Chiba, 277-8568, Japan; Ota, J., Research into Artifacts, Center for Engineering (RACE), University of Tokyo, Chiba, 277-8568, Japan","In the past few decades, simulation training has been used to help nurses improve their patient-transfer skills. However, the effectiveness of such training remains limited because it lacks effective ways of simulating patients' actions realistically. It is difficult for nurses to use the skills learned from simulation training to transfer an actual patient. Therefore, we developed a robot patient that could simulate the behavior of patients' limbs for patient-transfer training. This study examined the performance of the robot used in training and evaluated its training effectiveness. Four nursing teachers individually transferred the robot patient and then scored the robot patient's ability to simulate patients' actions and its suitability for skill training. An experiment using pre-post control group design was carried out to examine the robot patient's training effectiveness compared with the human simulated patient. The participants were 20 nursing students and one nursing teacher who was responsible for scoring the students' skills in the pre-test and post-test. All of the students were assigned to train with either the proposed robot patient or a healthy person simulating the patient. The results show that all four nursing teachers regarded the robot patient's actions as realistic. In addition, all four teachers agreed that the robot patient was suitable for skill training. The results also show that the proposed robot patient is more challenging than the current method, which employs a healthy person to simulate the patient. Significant skill improvement (p < 0.01) was observed in the experimental group when transferring the robot patient. © 2008-2011 IEEE.","Computer uses in education; educational technology; robot patient; training","Education; Educational technology; Machine design; Medical computing; Nursing; Personnel training; Robots; Teaching; Computer uses in education; Experimental groups; Healthy persons; Nursing students; Patient transfer; Simulated patients; Simulation training; Training effectiveness; Students",Article,"Final","",Scopus,2-s2.0-85030526839
"Ferrer V., Perdomo A., Ali H.R., Fies C., Quarles J.","55949304700;55949402300;57195229030;6508009602;55868360300;","Virtual humans for temperature visualization in a tangible augmented reality educational game",2017,"2017 IEEE Virtual Reality Workshop on K-12 Embodied Learning through Virtual and Augmented Reality, KELVAR 2017",,, 7961559,"","",,7,"10.1109/KELVAR.2017.7961559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026413057&doi=10.1109%2fKELVAR.2017.7961559&partnerID=40&md5=d8f25c80f52a141d27d41e926cd27720","University of Texas at San Antonio, United States","Ferrer, V., University of Texas at San Antonio, United States; Perdomo, A., University of Texas at San Antonio, United States; Ali, H.R., University of Texas at San Antonio, United States; Fies, C., University of Texas at San Antonio, United States; Quarles, J., University of Texas at San Antonio, United States","Our primary objective is to enable effective game based learning approaches in tangible augmented reality. In game based learning there is often a tradeoff in motivation between the educational aspects and game aspects. For example, consider our previous work - a tangible augmented reality application for passive solar energy education (AR-SEE), in which users learn about the science behind architectural design by interacting with a tangible model house and an augmented reality-based visualization of energy transfer within the house. This research extends AR-SEE to begin to convert this educational simulation into an effective educational game by introducing gaming elements, such as interactive virtual humans. Although it is known that AR-SEE does enable learning, it is unknown how the addition of interactive virtual humans will affect user perception of temperature data and learning. In this paper, the goal was to compare user perception of two approaches to temperature data visualization in in tangible augmented reality on mobile phones: 1) the current particle-based visualization (i.e., based on the science of energy transfer) and 2) novel virtual human-based visualizations. The game was intended for high school students. However, as a preliminary study, we conducted a user study with 27 3rd and 4th year architecture students that compared these two visualization approaches and their impact on temperature estimation, motivation, and perceived learning effectiveness. In the future, we plan to integrate this game into high school curricula. © 2017 IEEE.","Augmented reality; education; visualization","Augmented reality; Data visualization; Education; Education computing; Energy transfer; Flow visualization; Motivation; Solar energy; Students; Virtual reality; Visualization; Augmented reality applications; Educational aspects; Educational simulations; Energy educations; Game-based Learning; High school students; Perceived learning; Temperature estimation; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85026413057
"Jacob M.","56443422700;","Towards lifelong interactive learning for open-ended embodied narrative improvisation",2017,"C and C 2017 - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition",,,,"502","507",,3,"10.1145/3059454.3078699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025630461&doi=10.1145%2f3059454.3078699&partnerID=40&md5=515927b3642f52741258cd1ae89a2170","Georgia Institute of Technology, Atlanta, GA  30305, United States","Jacob, M., Georgia Institute of Technology, Atlanta, GA  30305, United States","This paper describes a doctoral research plan modeling collaborative embodied narrative improvisation, using lifelong interactive learning to mitigate the knowledgeauthoring bottleneck. Research methodology involves building interactive system models of the improvisation process, public installation/exhibition, user experience studies in public/lab settings, and ablation experiments. The article concludes with a research timeline. © 2017 ACM.","Embodied narrative; Generalized hypergraph; Imitation learning; Improvisation; Modeling creativity; VR","Educational technology; Learning systems; Ablation experiments; Embodied narrative; Hypergraph; Imitation learning; Improvisation; Interactive learning; Interactive system; Research methodologies; Education",Conference Paper,"Final","",Scopus,2-s2.0-85025630461
"Song S., Yang J.","55340188800;37045870100;","Configurable component framework supporting motion platform-based VR simulators",2017,"Journal of Mechanical Science and Technology","31","6",,"2985","2996",,2,"10.1007/s12206-017-0542-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025633040&doi=10.1007%2fs12206-017-0542-1&partnerID=40&md5=3864684d5c2c89bba0d90acd7c4c99e6","Department of Industrial Engineering, Ajou University, 206 Worldcup-ro, Suwon, 16499, South Korea","Song, S., Department of Industrial Engineering, Ajou University, 206 Worldcup-ro, Suwon, 16499, South Korea; Yang, J., Department of Industrial Engineering, Ajou University, 206 Worldcup-ro, Suwon, 16499, South Korea","This paper classifies functional elements of a motion platform-based VR simulator by its component types, and proposes a VR simulator component framework (VSCF) that can be used in various VR simulators by integrating associated components. The VSCF consists of a VSCF component manager (VCM), VSCF components (VCs), and a VSCF data interface (VDI). The functional elements of a VR simulator are defined by the VC units that are registered to the VCM and operated on by the VR simulator. The VCM manages the registered VCs and plays a role in controlling information exchange between VCs. The information for VCs is defined at the VDI while the VCM stores essential elements necessary to collect and transfer information into the VDI and provide it to VCs. Simulator developers configure VCs depending on functional elements required by the VR simulator and define the VDI for information exchange between VCs from which they are able to build various motion platform-based VR simulators by integrating the VCs through the VCM. In this study, two VR simulators were developed to verify the applicability of the VSCF: The first was a VR simulator for firefighting robot training and the second was a VR simulator for electrical wheelchair operation. © 2017, The Korean Society of Mechanical Engineers and Springer-Verlag GmbH Germany.","Component framework; Component framework; Motion platform; Virtual reality simulator","Information dissemination; Virtual reality; Component framework; Fire-fighting robot; Functional elements; Information exchanges; Motion platforms; Simulator components; Transfer information; Virtual reality simulator; Simulators",Article,"Final","",Scopus,2-s2.0-85025633040
"Vergara D., Rubio M.P., Lorenzo M.","57211856936;56817934900;34979055000;","On the design of virtual reality learning environments in engineering",2017,"Multimodal Technologies and Interaction","1","2", 11,"","",,43,"10.3390/mti1020011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043994822&doi=10.3390%2fmti1020011&partnerID=40&md5=874bccfd998af002acd8c1ac19fa2e93","Technological Department, University Catholic of Ávila, C/Canteros, s/n, Avila, 05005, Spain; Construction Department, University of Salamanca, Campus Viriato, Zamora, 37008, Spain; Department of Mechanical Engineering, University of Salamanca, ETSII, Avda. Fernando Ballesteros, 2, Béjar, Salamanca  37700, Spain","Vergara, D., Technological Department, University Catholic of Ávila, C/Canteros, s/n, Avila, 05005, Spain; Rubio, M.P., Construction Department, University of Salamanca, Campus Viriato, Zamora, 37008, Spain; Lorenzo, M., Department of Mechanical Engineering, University of Salamanca, ETSII, Avda. Fernando Ballesteros, 2, Béjar, Salamanca  37700, Spain","Currently, the use of virtual reality (VR) is being widely applied in different fields, especially in computer science, engineering, and medicine. Concretely, the engineering applications based on VR cover approximately one half of the total number of VR resources (considering the research works published up to last year, 2016). In this paper, the capabilities of different computational software for designing VR applications in engineering education are discussed. As a result, a general flowchart is proposed as a guide for designing VR resources in any application. It is worth highlighting that, rather than this study being based on the applications used in the engineering field, the obtained results can be easily extrapolated to other knowledge areas without any loss of generality. This way, this paper can serve as a guide for creating a VR application. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Design; Education; Engineering; Instruction; Virtual laboratory; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85043994822
"Fratamico L., Conati C., Kardan S., Roll I.","56024878700;6602976668;35810317700;8987398200;","Applying a Framework for Student Modeling in Exploratory Learning Environments: Comparing Data Representation Granularity to Handle Environment Complexity",2017,"International Journal of Artificial Intelligence in Education","27","2",,"320","352",,11,"10.1007/s40593-016-0131-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018702396&doi=10.1007%2fs40593-016-0131-y&partnerID=40&md5=ac526f000728d7f76ecf2fad91b188b5","University of British Columbia, Vancouver, Canada","Fratamico, L., University of British Columbia, Vancouver, Canada; Conati, C., University of British Columbia, Vancouver, Canada; Kardan, S., University of British Columbia, Vancouver, Canada; Roll, I., University of British Columbia, Vancouver, Canada","Interactive simulations can facilitate inquiry learning. However, similarly to other Exploratory Learning Environments, students may not always learn effectively in these unstructured environments. Thus, providing adaptive support has great potential to help improve student learning with these rich activities. Providing adaptive support requires a student model that can both evaluate learning as well inform relevant feedback. Building such a model for interactive simulations is especially challenging because the exploratory nature of the interaction makes it hard to know a priori which behaviors are conducive to learning. To address this problem, in this paper we leverage the student modeling framework proposed in (Kardan and Conati, 2011) to specifically address the challenge of modeling students in interactive simulations. The framework has already been successfully applied to build a student model and to give adaptive interventions for an interactive simulation for constraint satisfaction. We seek to investigate the generality of the framework by building student models for a more complex simulation on electric circuits called Circuit Construction Kit (CCK). We evaluate alternative representations of logged interaction data with CCK, capturing different amounts of granularity and feature engineering. We then apply the student modeling framework proposed in (Kardan and Conati, 2011) to group students based on their interaction behaviors, map these behaviors into learning outcomes and leverage the resulting clusters to classify new learners. Data collected from 100 college students working with the CCK simulation indicates that the proposed framework is able to successfully classify students in groups of high and low learners and identify patterns of productive behaviors that are common across representations that can inform real-time feedback. In addition to presenting these results, we discuss trade-offs between levels of granularity and feature engineering in the tested interaction representations in terms of their ability to evaluate learning, classify students, and inform feedback. © 2017, International Artificial Intelligence in Education Society.","Clustering; Educational data mining; Exploratory learning environments; Interactive simulations; User modeling","Computer aided instruction; Data mining; Economic and social effects; Education; Feedback; Network function virtualization; Clustering; Educational data mining; Exploratory learning; Interactive simulations; User Modeling; Students",Article,"Final","",Scopus,2-s2.0-85018702396
"Martin K.A., Laviola J.J.","56119040300;6602792780;","The Transreality Interaction Platform: Enabling Interaction across Physical and Virtual Reality",2017,"Proceedings - 2016 IEEE International Conference on Internet of Things; IEEE Green Computing and Communications; IEEE Cyber, Physical, and Social Computing; IEEE Smart Data, iThings-GreenCom-CPSCom-Smart Data 2016",,, 7917082,"177","186",,3,"10.1109/iThings-GreenCom-CPSCom-SmartData.2016.54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020224414&doi=10.1109%2fiThings-GreenCom-CPSCom-SmartData.2016.54&partnerID=40&md5=a8854fd2389e9f894285444a67202328","Dept. of Computer Science, University of Central Florida, Orlando, FL, United States","Martin, K.A., Dept. of Computer Science, University of Central Florida, Orlando, FL, United States; Laviola, J.J., Dept. of Computer Science, University of Central Florida, Orlando, FL, United States","The convergence of the Internet of Things (IoT) and interactive systems will enable future interactive environments which transcend physical and virtual reality. Embedded Things provide sensors and actuators to virtualize the physical environment, while Interactive Things extend the virtualized environment with modalities for human interaction, ranging from tangible and wearable interfaces to immersive virtual and augmented reality interfaces. We introduce the Transreality Interaction Platform (TrIP) to enable service ecosystems which situate virtual objects alongside virtualized physical objects and allow for novel ad-hoc interactions between humans, virtual, and physical objects in a transreality environment. TrIP provides a generalized middleware platform addressing the unique challenges that arise in complex transreality systems which have yet to be fully explored in current IoT or HCI research. We describe the system architecture, data model, and query language for the platform and present a proof-of-concept implementation. We evaluate the performance of the implementation and demonstrate its use integrating embedded and interactive things for seamless interaction across physical and virtual realities. © 2016 IEEE.","Augmented Reality; Interactive Systems; Interactive Things; Internet of Things; Tangible User Interface; Transreality; Virtual Reality","Augmented reality; Green computing; Internet of things; Middleware; Query languages; Search engines; User interfaces; Interactive Environments; Interactive system; Interactive Things; Internet of thing (IOT); Tangible user interfaces; Transreality; Virtual and augmented reality; Virtualized environment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85020224414
"Liu Y.","47061795700;","Toward intelligent welding robots: Virtualized welding based learning of human welder behaviors [Verso robot di saldatura intelligenti: Saldatura virtualizzata basata sull'apprendimento dei comportamenti dei saldatori umani]",2017,"Rivista Italiana della Saldatura","69","3",,"341","356",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051390593&partnerID=40&md5=d29d2b27f7939d762a198a88eab1ebd1","University of Kentucky, United States","Liu, Y., University of Kentucky, United States","Combining human welders (with intelligence and sensing versatility) and automated welding robots (with precision and consistency) can lead to next-generation intelligent welding robots that can perform precision welding similar to or even outperform skilled welders. In this study, an innovative human-machine welding paradigm, virtualized welding, is proposed that can transfer human intelligence to welding robots. A welding robot is augmented with sensors to observe the welding process and performs actual welding. A human welder operates a virtual welding torch and freely adjusts its movement in 3D space based on the visual feedback from the sensors. The adjustments together with the reconstructed 3D weld pool surfaces are recorded, analyzed, and utilized to form models representing human welder intelligence. To demonstrate the concepts behind the virtualized welding system and modeling method proposed, human welder's adjustments are ""selectively"" learned and transferred to the welding robot to perform automated gas tungsten arc welding (GTAW). Experimental results verified the effectiveness of the learned human response models. A foundation is thus established to rapidly extract human intelligence and transfer it into welding robots. © IIW 2016.","Artificial intelligence; Gta welding; Robots","Artificial intelligence; Behavioral research; E-learning; Gas metal arc welding; Gas welding; Human form models; Industrial robots; Robot applications; Robots; Three dimensional computer graphics; Virtual reality; Visual communication; Visual servoing; Automated gas-tungsten arc welding; Automated welding; GTA welding; Human intelligence; Intelligent welding; Precision welding; Visual feedback; Weld pool surfaces; Intelligent robots",Article,"Final","",Scopus,2-s2.0-85051390593
"Bertrand J., Bhargava A., Madathil K.C., Gramopadhye A., Babu S.V.","55858839700;57194158382;37075253800;7005569103;9039004700;","The effects of presentation method and simulation fidelity on psychomotor education in a bimanual metrology training simulation",2017,"2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings",,, 7893318,"59","68",,7,"10.1109/3DUI.2017.7893318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019003187&doi=10.1109%2f3DUI.2017.7893318&partnerID=40&md5=01f06159d13212f502fbea389f779a5b","Clemson University, United States","Bertrand, J., Clemson University, United States; Bhargava, A., Clemson University, United States; Madathil, K.C., Clemson University, United States; Gramopadhye, A., Clemson University, United States; Babu, S.V., Clemson University, United States","In this study, we empirically evaluated the effects of presentation method and simulation fidelity on task performance and psychomotor skills acquisition in an immersive bimanual simulation towards precision metrology education. In a 2 × 2 experiment design, we investigated a large-screen immersive display (LSID) with a head-mounted display (HMD), and the presence versus absence of gravity. Advantages of the HMD include interacting with the simulation in a more natural manner as compared to using a large-screen immersive display due to the similarities between the interactions afforded in the virtual compared to the real-world task. Suspending the laws of physics may have an effect on usability and in turn could affect learning outcomes. Our dependent variables consisted of a pre and post cognition questionnaire, quantitative performance measures, perceived workload and system usefulness, and a psychomotor assessment to measure to what extent transfer of learning took place from the virtual to the real world. Results indicate that the HMD condition was preferable to the immersive display in several metrics while the no-gravity condition resulted in users adopting strategies that were not advantageous for task performance. © 2017 IEEE.","and virtual realities; augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial","Personnel training; Units of measurement; User interfaces; Virtual reality; augmented; Dependent variables; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; Head mounted displays; Large-screen immersive displays; Simulation fidelity; Training simulation; Transfer of learning; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85019003187
"Xu M., Murcia-Lopez M., Steed A.","57194043735;57194036190;18435050200;","Object location memory error in virtual and real environments",2017,"Proceedings - IEEE Virtual Reality",,, 7892303,"315","316",,9,"10.1109/VR.2017.7892303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018434814&doi=10.1109%2fVR.2017.7892303&partnerID=40&md5=a93916712582a06b89ab064f03b0fe94","University College London, United Kingdom","Xu, M., University College London, United Kingdom; Murcia-Lopez, M., University College London, United Kingdom; Steed, A., University College London, United Kingdom","We aim to further explore the transfer of spatial knowledge from virtual to real spaces. Based on previous research on spatial memory in immersive virtual reality (VR) we ran a study that looked at the effect of three locomotion techniques (joystick, pointing-and-teleporting and walking-in-place) on object location learning and recall. Participants were asked to learn the location of a virtual object in a virtual environment (VE). After a short period of time they were asked to recall the location by placing a real version of the object in the real-world equivalent environment. Results indicate that the average placement error, or distance between original and recalled object location, is approximately 20cm for all locomotion technique conditions. This result is similar to the outcome of a previous study on spatial memory in VEs that used real walking. We report this unexpected finding and suggest further work on spatial memory in VR by recommending the replication of this study in different environments and using objects with a wider diversity of properties, including varying sizes and shapes. © 2017 IEEE.","Augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality; Virtual Realities","Computer graphics; Three dimensional computer graphics; Virtual reality; Augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; I.3.7 [computer graphics]: three-dimensional graphics and realism - virtual realities; Immersive virtual reality; Locomotion technique; Real environments; Spatial knowledge; Walking-in-place; Location",Conference Paper,"Final","",Scopus,2-s2.0-85018434814
"Lin S., Cheng H.F., Li W., Huang Z., Hui P., Peylo C.","57193275392;57193276567;56267259500;56267502200;14029922900;36700140800;","Ubii: Physical World Interaction Through Augmented Reality",2017,"IEEE Transactions on Mobile Computing","16","3", 7469860,"872","885",,30,"10.1109/TMC.2016.2567378","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012244034&doi=10.1109%2fTMC.2016.2567378&partnerID=40&md5=48836d7858ad06e5078e4c27259c9dd2","HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Deutsche Telekom AG Laboratories, Ernst-Reuter-Platz 7, Berlin, Germany","Lin, S., HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Cheng, H.F., HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Li, W., HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Huang, Z., HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Hui, P., HKUST-DT System and Media Laboratory, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Peylo, C., Deutsche Telekom AG Laboratories, Ernst-Reuter-Platz 7, Berlin, Germany","We describe a new set of interaction techniques that allow users to interact with physical objects through augmented reality (AR). Previously, to operate a smart device, physical touch is generally needed and a graphical interface is normally involved. These become limitations and prevent the user from operating a device out of reach or operating multiple devices at once. Ubii (Ubiquitous interface and interaction) is an integrated interface system that connects a network of smart devices together, and allows users to interact with the physical objects using hand gestures. The user wears a smart glass which displays the user interface in an augmented reality view. Hand gestures are captured by the smart glass, and upon recognizing the right gesture input, Ubii will communicate with the connected smart devices to complete the designated operations. Ubii supports common inter-device operations such as file transfer, printing, projecting, as well as device pairing. To improve the overall performance of the system, we implement computation offloading to perform the image processing computation. Our user test shows that Ubii is easy to use and more intuitive than traditional user interfaces. Ubii shortens the operation time on various tasks involving operating physical devices. The novel interaction paradigm attains a seamless interaction between the physical and digital worlds. © 2002-2012 IEEE.","Augmented reality; human computer interaction; mobile computing; user interfaces; wearable computers","Augmented reality; Glass; Human computer interaction; Image processing; Mobile computing; Wearable computers; Computation offloading; Device operations; Graphical interface; Integrated interface; Interaction paradigm; Interaction techniques; Multiple devices; Ubiquitous interfaces; User interfaces",Article,"Final","",Scopus,2-s2.0-85012244034
"Dong M., Guo R.","57193733822;55211939300;","Towards understanding the capability of spatial audio feedback in virtual environments for people with visual impairments",2017,"2016 IEEE 2nd Workshop on Everyday Virtual Reality, WEVR 2016",,, 7859538,"15","20",,2,"10.1109/WEVR.2016.7859538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016202855&doi=10.1109%2fWEVR.2016.7859538&partnerID=40&md5=dcc177c18146e2d51b17f8469cf200ab","Kennesaw State University, United States","Dong, M., Kennesaw State University, United States; Guo, R., Kennesaw State University, United States","This research analyzes if and how the Head Related Transfer Function (HRTF) can be used to support effective Human-Computer Interaction when people in a Virtual Environment (VE) without visual feedback. If sounds can be located in a VE by using HRTF only, designing and developing considerably safer but diversified training environments might greatly benefit individuals with visual impairments. To investigate this, we ran 2 usability studies: 1) to ascertain whether the HRTF could provide sufficient position information in VEs; 2) to learn whether the HRTF could provide sufficient distance and direction information in VEs. The results showed that a continuous audio feedback could help navigate in a VE without vision feedback. © 2016 IEEE.","3D Audio; Assistive technology; HRTF; User study","Human computer interaction; Sound reproduction; Transfer functions; Visual communication; 3D audio; Assistive technology; Head related transfer function; HRTF; Position information; Usability studies; User study; Visual impairment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85016202855
"Alrashidi M., Gardner M., Callaghan V.","56405416200;9840290400;35561000000;","Evaluating the use of pedagogical virtual machine with augmented reality to support learning embedded computing activity",2017,"ACM International Conference Proceeding Series","Part F127852",,,"44","50",,5,"10.1145/3057039.3057088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020897950&doi=10.1145%2f3057039.3057088&partnerID=40&md5=1972383f936d31a50cf8e2cd069e1e9a","School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom","Alrashidi, M., School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom; Gardner, M., School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom; Callaghan, V., School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom","Embedded computing is often considered as a hidden technology where learners can require more assistance to inspect processes and activities hidden within the technologies, making use of debugging, monitoring, and visual tools. To the student, this kind of technology often has abstract behaviours where the only information/things people can see is the final action, and they do not know how the internal processes work and communicate inside the embedded computing device to achieve the desired result. Augmented reality (AR) can overcome this issue and produce a magic-lens view for revealing hidden embedded computing activities. This can result in learners achieving a better level of knowledge and awareness of the technology, as well as higher learning outcomes. AR on its own will not improve the learning processes without first considering how to manage and represent the hidden information. Therefore, a pedagogical virtual machine (PVM) model was employed, and to evaluate the learning effectiveness of the proposed model. We conducted an experiment based on a problem-solving educational mobile robot task. Twenty students participated in the experimental (AR approach) and control (conventional approach) group. The result showed that the augmented reality approach was more effective in increasing students' computational thinking and learning outcomes. In addition, the augmented reality approach reduced both time completion and debugging times. © 2017 ACM.","Augmented reality; Embedded computing; Learning and teaching; Learning object; Mixed reality; Pedagogical virtual machine; Real-time feedback; Robot","Augmented reality; E-learning; Education; Embedded systems; Network security; Problem solving; Program debugging; Robots; Students; Teaching; Technology transfer; Virtual machine; Virtual reality; Embedded computing; Learning and teachings; Learning objects; Mixed reality; Real-time feedback; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-85020897950
"Cooper N., Milella F., Cant I., Pinto C., White M., Meyer G.","57193613538;6602739760;57193607743;57193603497;56315199000;7402850996;","Augmented Cues Facilitate Learning Transfer from Virtual to Real Environments",2017,"Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016",,, 7836496,"194","198",,4,"10.1109/ISMAR-Adjunct.2016.0075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015258466&doi=10.1109%2fISMAR-Adjunct.2016.0075&partnerID=40&md5=6296b41a51237548cbfed6c5ac6f6db5","University of Liverpool, United Kingdom; Virtual Engineering Centre, United Kingdom","Cooper, N., University of Liverpool, United Kingdom; Milella, F., Virtual Engineering Centre, United Kingdom; Cant, I., Virtual Engineering Centre, United Kingdom; Pinto, C., Virtual Engineering Centre, United Kingdom; White, M., University of Liverpool, United Kingdom; Meyer, G., University of Liverpool, United Kingdom","The aim of this study was to investigate whether augmented cues that have previously been shown to enhance performance and user satisfaction in VR training translate into performance improvements in real environments. Subjects were randomly allocated into 3 groups. Group 1 were trained to perform a real tyre change, group 2 were trained in a conventional VR setting, while group 3 were trained in VR with augmented cues. After training participants were tested on a real tyre change task. Overall time to completion was recorded as objective measure; subjective ratings of presence, perceived workload and discomfort were recorded using questionnaires. The performances of the three groups were compared. Overall, participants who received VR training performed significantly faster on the real task than participants who completed the real tyre change only. The difference between the virtual reality training groups was found to be not significant. However, participants who were trained with augmented cues performed the real tyre change with fewer errors than participants in the minimal cues training group. Systematic differences in subjective ratings that reflected objective performance were also observed. © 2016 IEEE.","augmented cues; multisensory feedback; performance; presence; simulation sickness; training transfer; virtual reality; workload","Augmented reality; Surveys; Tires; Virtual reality; augmented cues; Multi-sensory feedback; performance; presence; simulation sickness; workload; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85015258466
"Halabi O., El-Seoud S.A., Aljaam J.M., Alpona H., Al-Hemadi M., Al-Hassan D.","57203219290;6507058670;24528095900;56407207400;57194434607;57194429167;","Design of immersive virtual reality system to improve communication skills in individuals with autism",2017,"International Journal of Emerging Technologies in Learning","12","5",,"50","64",,10,"10.3991/ijet.v12i05.6766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020066717&doi=10.3991%2fijet.v12i05.6766&partnerID=40&md5=e2208a321c0b786aa1272f0ea478869e","Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt","Halabi, O., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; El-Seoud, S.A., Faculty of Informatics and Computer Science, The British University in Egypt, El Sherouk City, Cairo, Egypt; Aljaam, J.M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Alpona, H., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hemadi, M., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar; Al-Hassan, D., Department of Computer Science and Engineering, Qatar University, P.O. Box 2713, Doha, Qatar","Individuals with autism spectrum disorder (ASD) regularly experience situations in which they need to give answers but do not know how to respond; for example, questions related to everyday life activities that are asked by strangers. Research geared at utilizing technology to mend social and communication impairments in children with autism is actively underway. Immersive virtual reality (VR) is a relatively recent technology that has the potential of being an effective therapeutic tool for developing various skills in autistic children. This paper presents an interactive scenario-based VR system developed to improve the communications skills of autistic children. The system utilizes speech recognition to provide natural interaction and role-play and turntaking to evaluate and verify the effectiveness of the immersive environment on the social performance of autistic children. In experiments conducted, participants showed more improved performance with a computer augmented virtual environment (CAVE) than with a head mounted display (HMD) or a normal desktop. The results indicate that immersive VR could be more satisfactory and motivational than desktop for children with ASD.","Autism spectrum disorder; Communication skill; Immersion; Social performance; Virtual reality","Diseases; Helmet mounted displays; Speech recognition; Technology transfer; Virtual reality; Autism spectrum disorders; Children with autisms; Communication skills; Head mounted displays; Immersion; Immersive environment; Immersive virtual reality; Social performance; Education",Article,"Final","",Scopus,2-s2.0-85020066717
"Hai N.D., Chaudhary N.K., Peksi S., Ranjan R., He J., Gan W.-S.","57201183624;57201189529;56770063300;55635587900;55968222200;7103165543;","Fast HRFT measurement system with unconstrained head movements for 3D audio in virtual and augmented reality applications",2017,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2017-March",,,"6576","6577",,6,"10.1109/ICASSP.2017.8005299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043706787&doi=10.1109%2fICASSP.2017.8005299&partnerID=40&md5=2e61ea590626739ebb07bbeba7d4c3fd","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Maxim Integrated Products, Inc., Singapore","Hai, N.D., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Chaudhary, N.K., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Peksi, S., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Ranjan, R., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; He, J., Maxim Integrated Products, Inc., Singapore; Gan, W.-S., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","Binaural audio plays an indispensable role in virtual reality (VR) and augmented reality (AR). Binaural audio recreates the sensation of the three dimensional auditory experience using Head- Related Transfer Functions (HRTFs). HRTFs are as unique as our fingerprint. To achieve an immersive audio experience, HRTFs measured from every particular user is required. Nowadays, the conventional methods for HRTF measurements requires a wellcontrolled environment, hardly any movement of the user, and projecting to the user a high level of unpleasant sound in a rather long duration. Such difficulties have greatly limited the use of individually measurement HRTFs and hinder the authenticity of immersive audio. To solve these problems, we proposed a fast and convenient HRTF measurement system that is an order of magnitude faster and more importantly, it does not place any constraints on the user's movement. With the help of a head-Tracker and advanced adaptive signal processing algorithms, this system is able to achieve satisfactory HRTF measurement accuracy. In this demonstration, we will present a fast real-Time HRTF acquisition system and show how the individualized HRTFs improve the audio experience in VR/AR applications. © 2017 IEEE.","Augmented reality (AR); Binauralrendering; Fast and relaxed HRTF acquisition; Virtual reality (VR)","Augmented reality; Bins; Signal processing; Transfer functions; Virtual reality; Acquisition systems; Adaptive signal processing; Binauralrendering; Conventional methods; Fast and relaxed HRTF acquisition; Head related transfer function; Measurement accuracy; Virtual and augmented reality; Sound reproduction",Conference Paper,"Final","",Scopus,2-s2.0-85043706787
"Kaul O.B., Rohs M.","57191506409;17346497700;","HapticHead: A spherical vibrotactile grid around the head for 3D guidance in virtual and augmented reality",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-January",,,"3729","3740",,37,"10.1145/3025453.3025684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029123271&doi=10.1145%2f3025453.3025684&partnerID=40&md5=a5e7f4386fd76112d0a37da34a90f577","University of Hannover, Hannover, Germany","Kaul, O.B., University of Hannover, Hannover, Germany; Rohs, M., University of Hannover, Hannover, Germany","Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4 % vs. 54.2 % success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is - as expected - more precise (99.7 % success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a headmounted display. © 2017 ACM.","3D output; Augmented reality; Guidance; Haptic feedback; Navigation; Spatial interaction; Vibrotactile; Virtual reality","Augmented reality; Electronic guidance systems; Helmet mounted displays; Navigation; Virtual reality; Visual communication; Human engineering; Sensory perception; Street traffic control; 3D output; Haptic feedbacks; Head mounted displays; Head related transfer function; Spatial interaction; Vibro-tactile feedbacks; Vibrotactile; Virtual and augmented reality; Feedback",Conference Paper,"Final","",Scopus,2-s2.0-85029123271
"Lee H., Cho Y.-S.","57204033467;57204041517;","Virtual and mixed reality for students: How to control human factors",2017,"ICCE 2017 - 25th International Conference on Computers in Education: Technology and Innovation: Computer-Based Educational Systems for the 21st Century, Workshop Proceedings",,,,"343","354",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054174464&partnerID=40&md5=489c0f8f593877d3aeca1f316d0aa874","Korea Education and Research Information Service, South Korea","Lee, H., Korea Education and Research Information Service, South Korea; Cho, Y.-S., Korea Education and Research Information Service, South Korea","Emerging technologies, such as virtual reality and mixed reality, help teachers as well as students understand educational contents more easily. But we need to consider more deeply. Because most of the devices and contents that released recently are targeted at the game and entertainment market, it may well be doubted whether they are proper for education. Some people have difficulty in health and social aspects after experience virtual or mixed reality. In this paper, we introduce the human factors issues in the virtual and mixed reality area. If we know how to control human factors, virtual and mixed reality could be used more safely in education. We gathered the usage guides, best practices and guidelines about those. We analyze and put the parts commonly mentioned together. But the consideration of hardware itself is excluded. As a result, we propose human factor guideline for users and contents creators using virtual and mixed reality in education. © 2017 Asia-Pacific Society for Computers in Education. All rights reserved.","Augmented reality; Education; Guidelines; Human factors; Virtual reality","Augmented reality; E-learning; Education; Educational technology; Engineering research; Human engineering; Social aspects; Students; Teaching; Technology transfer; Virtual reality; Best practices; Educational contents; Emerging technologies; Guidelines; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85054174464
"Fotaris P., Pellas N., Kazanidis I., Smith P.","8681925900;55930215100;23990530500;57214269224;","A systematic review of augmented reality game-based applications in primary education",2017,"Proceedings of the 11th European Conference on Games Based Learning, ECGBL 2017",,,,"181","190",,28,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036471742&partnerID=40&md5=81279c40b7e8c474cf96a510f2c0422e","University of East London, School of Arts and Digital Industries, United Kingdom; University of the Aegean, Department of Product and Systems Design Engineering, Greece; Information Technology Department, Eastern Macedonia and Thrace Institute of Technology, Greece","Fotaris, P., University of East London, School of Arts and Digital Industries, United Kingdom; Pellas, N., University of the Aegean, Department of Product and Systems Design Engineering, Greece; Kazanidis, I., Information Technology Department, Eastern Macedonia and Thrace Institute of Technology, Greece; Smith, P., University of East London, School of Arts and Digital Industries, United Kingdom","Augmented Reality game-based learning (ARGBL) is quickly gaining momentum in the education sector worldwide as it has the potential to enable new forms of learning and transform the learning experience. However, it remains unclear how ARGBL applications can impact students' motivation and performance in primary education. This study addresses that topic by providing a systematic review, which analyses and critically appraises the current state of knowledge and practice in the use of ARGBL applications in primary education. In total, seventeen (17) studies that used either qualitative, quantitative, or mixed-methods to collect their data were analysed and were published between 2012 and 2017. The study results indicated that ARGBL applications are mainly used to document the design and development process, as well as to share preliminary findings and student feedback. Based on a comprehensive taxonomy of application areas for AR in primary education, ARGBL can potentially influence the students' attendance, knowledge transfer, skill acquisition, hands-on digital experience, and positive attitudes in laboratory experimental exercises for different courses. This review aims to offer new insights to researchers and provide educators with effective advice and suggestions on how to improve learning outcomes, as well as increase students' motivation and learning performance by incorporating this instructional model into their teaching.","Augmented reality; Game-based learning; Mixed reality; Mobile learning; Primary education","Augmented reality; Education computing; Knowledge management; Motivation; Students; Teaching; Virtual reality; Design and development process; Game-based Learning; Learning experiences; Learning performance; Mixed reality; Mobile Learning; Motivation and performance; Primary education; Education",Conference Paper,"Final","",Scopus,2-s2.0-85036471742
"Spicer R.P., Russell S.M., Rosenberg E.S.","25723810300;57190403263;23991764900;","The mixed reality of things: Emerging challenges for human-information interaction",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10207",, 102070A,"","",,4,"10.1117/12.2268004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025639079&doi=10.1117%2f12.2268004&partnerID=40&md5=d839fefcd114f2d66f187a208b9a6873","University of Southern California, United States; U.S. Army Research Lab, United States","Spicer, R.P., University of Southern California, United States; Russell, S.M., U.S. Army Research Lab, United States; Rosenberg, E.S., University of Southern California, United States","Virtual and mixed reality technology has advanced tremendously over the past several years. This nascent medium has the potential to transform how people communicate over distance, train for unfamiliar tasks, operate in challenging environments, and how they visualize, interact, and make decisions based on complex data. At the same time, the marketplace has experienced a proliferation of network-connected devices and generalized sensors that are becoming increasingly accessible and ubiquitous. As the nternet of Things"" expands to encompass a predicted 50 billion connected devices by 2020, the volume and complexity of information generated in pervasive and virtualized environments will continue to grow exponentially. The convergence of these trends demands a theoretically grounded research agenda that can address emerging challenges for human-information interaction (HII). Virtual and mixed reality environments can provide controlled settings where HII phenomena can be observed and measured, new theories developed, and novel algorithms and interaction techniques evaluated. In this paper, we describe the intersection of pervasive computing with virtual and mixed reality, identify current research gaps and opportunities to advance the fundamental understanding of HII, and discuss implications for the design and development of cyber-human systems for both military and civilian use. © 2017 SPIE.","Human-Information Interaction; Mixed Reality; Virtual Reality","Complex networks; Ubiquitous computing; Design and Development; Human-information interaction; Interaction techniques; Mixed reality; Mixed reality technologies; Mixed-reality environment; Novel algorithm; Virtualized environment; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85025639079
"Mastmeyer A., Wilms M., Handels H.","14056412100;55211200500;6701686055;","Interpatient respiratory motion model transfer for virtual reality simulations of liver punctures",2017,"Journal of WSCG","25","1",,"1","9",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027045452&partnerID=40&md5=c502a443c54f29ad47c54a49d5d6a223","Univ. of Luebeck, Inst. Of Med. Inform, Ratzeburger Allee 160, Luebeck, 23568, Germany","Mastmeyer, A., Univ. of Luebeck, Inst. Of Med. Inform, Ratzeburger Allee 160, Luebeck, 23568, Germany; Wilms, M., Univ. of Luebeck, Inst. Of Med. Inform, Ratzeburger Allee 160, Luebeck, 23568, Germany; Handels, H., Univ. of Luebeck, Inst. Of Med. Inform, Ratzeburger Allee 160, Luebeck, 23568, Germany","Current virtual reality (VR) training simulators of liver punctures often rely on static 3D patient data and use an unrealistic (sinusoidal) periodic animation of the respiratory movement. Existing methods for the animation of breathing motion support simple mathematical or patient-specific, estimated breathing models. However with personalized breathing models for each new patient, a heavily dose relevant or expensive 4D data acquisition is mandatory for keyframe-based motion modeling. Given the reference 4D data, first a model building stage using linear regression motion field modeling takes place. Then the methodology shown here allows the transfer of existing reference respiratory motion models of a 4D reference patient to a new static 3D patient. This goal is achieved by using non-linear inter-patient registration to warp one personalized 4D motion field model to new 3D patient data. This cost- and dose-saving new method is shown here visually in a qualitative proof-of-concept study. © 2017, Vaclav Skala Union Agency. All rights reserved.","4D motion models; Inter-patient registration of motion models; Liver puncture training; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85027045452
"Palaigeorgiou G., Politou F., Tsirika F., Kotabasis G.","8853389200;57198347564;57198359195;57198347649;","FingerDetectives: Affordable augmented interactive miniatures for embodied vocabulary acquisition in second language learning",2017,"Proceedings of the 11th European Conference on Games Based Learning, ECGBL 2017",,,,"523","530",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036468993&partnerID=40&md5=860c4de8f2e74defe0381c1f89cb671a","University of Western Macedonia, Greece","Palaigeorgiou, G., University of Western Macedonia, Greece; Politou, F., University of Western Macedonia, Greece; Tsirika, F., University of Western Macedonia, Greece; Kotabasis, G., University of Western Macedonia, Greece","New computer-assisted language learning approaches provoke that learning a second language should be treated as a communication tool with a specific goal, incorporated in real communication activities. There are several relevant theoretical approaches such as Content and Language Integrated Learning (CLIL) or Task-Based Language Learning (TBLL). Additionally, many researchers support that learning a new language should engage learners' kinesthetic intelligence since embodied learning seems to improve learning efficiency and effectiveness. For these two reasons, mixed reality environments seem adequate for language learners and teachers alike as they support situated learning practices which promote enjoyment, motivation, and confidence, as well as autonomous and authentic learning experiences. In this study, we propose augmented game-based miniatures as affordable vocabulary learning platforms for capitalizing on external, pseudo-physical context to address task-based and embodied learning. To demonstrate their value, we have constructed a 3d miniature house augmented by a projector in which an invasion robbery has occurred, and the students have to follow instructions in the targeted language to find the thief. Students' fingers become the detectives of the game and the detectives walk inside the house on a path that is smoothly engraved on the model. The goal and the instructions of the game were developed and written in a way that the words from the vocabulary were systematically repeated and most times these words were related to objects which the students had to seek for, examine or transfer. 37 students played with the environment in groups of 2 or 3 people in sessions lasting about 20 minutes. The students' attitudes toward the game were collected with mini AttrakDiff questionnaire and variables examining their concentration on the task at hand, autotelic experience and perceived learning. Additionally, after each game session, short interviews were conducted with the participants. Students considered the experience as efficient and exciting and stressed that this is an innovative and effective way of familiarizing with a new set of words. The mixed reality game was constructed with low-cost prototyping hardware, it can be easily replicated by students and teachers and aims to provide a prototype solution to vocabulary learning.","Augmented miniatures; Embodied learning; Mixed reality environments; Second language learning; Vocabulary acquisition","Computer aided instruction; E-learning; Learning systems; Students; Teaching; Virtual reality; Augmented miniatures; Embodied learning; Mixed-reality environment; Second language; Vocabulary acquisition; Education",Conference Paper,"Final","",Scopus,2-s2.0-85036468993
"Fechter M., Wartzack S.","57115088300;6506007420;","Natural finger interaction for cad assembly modeling",2017,"Proceedings of the ASME Design Engineering Technical Conference","1",, 67555,"","",,2,"10.1115/DETC2017-67555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034773383&doi=10.1115%2fDETC2017-67555&partnerID=40&md5=72d41fefc5239e3e102d2bdfca44f30d","Department of Mechanical Engineering, Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany","Fechter, M., Department of Mechanical Engineering, Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany; Wartzack, S., Department of Mechanical Engineering, Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany","In current CAD software the process of assembly modeling is hindered by a large number of separate rotation and translation actions necessary, especially in case of larger assemblies. Additionally matching faces, edges or points must be selected by clicking to define the appropriate constraint. In contrast to that, the process of assembling two normal sized physical parts in the real world seems to be rather simple. That is because we know how to grasp and move objects with our hands intuitively from our everyday experience. The idea behind this contribution is to enable the product developer to assemble CAD parts in a virtual environment through natural finger interaction like in reality. Therefore we present an overall method that combines the natural finger interaction with virtual objects and the insertion of constraints between rotationally symmetric CAD parts. The developed algorithms identify matching surfaces on the basis of the geometry as well as position and orientation of the parts in 3D space. This paper highlights the method to use a combination of real-Time physics simulation and a heuristic approach to achieve an intuitive interaction interface. Additionally, we describe the detection algorithms developed to find assembly relationships between rotationally symmetric CAD parts without prior constraint definition. We also present a prototype system to demonstrate the functionality of the overall method. Furthermore, challenges for future research, such as extending the functionality of the detection algorithms on additional part types, like non-rotationally symmetric shapes, are discussed. Draft Video: https://vimeo.com/203437638. © 2017 ASME.","Assembly modeling; Computer-Aided design; Natural user interface; Virtual reality","Geometry; Heuristic methods; Signal detection; Technology transfer; User interfaces; Virtual reality; Assembly model; Detection algorithm; Finger interactions; Heuristic approach; Intuitive interaction; Natural user interfaces; Position and orientations; Product developers; Computer aided design",Conference Paper,"Final","",Scopus,2-s2.0-85034773383
"Gilbert K.A.","57195204967;","Investigating the use and design of immersive simulation to improve self-efficacy for aspiring principals",2017,"Journal of Information Technology Education: Innovations in Practice","16","1",,"127","169",,2,"10.28945/3726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026251032&doi=10.28945%2f3726&partnerID=40&md5=4a6ea259d7cabedc8f81be6143e3afb1","Department of Teaching and Leading, Augusta University, Augusta, GA, United States","Gilbert, K.A., Department of Teaching and Leading, Augusta University, Augusta, GA, United States","Aim/Purpose Improving public schools is a focus of federal legislation in the United States with much of the burden placed on principals. However, preparing principals for this task has proven elusive despite many changes in programming by insti-tutions of higher learning. Emerging technologies that rely on augmented and virtual realities are posited to be powerful pedagogical tools for closing this gap. Background This study investigated the effects of immersive simulation technologies on principals' self-efficacy after treatment and the perceived significance of the design of the immersive simulation experience as an effective tool for adult learners. Methodology The investigator employed a multiple-methods study that relied on a purposive sample of graduate students enrolled in educational leadership programs at two small universities in the southeastern United States. Participants completed a two-hour module of immersive simulation designed to facilitate transfer of knowledge to skills thereby increasing their self-efficacy. Contribution This paper contributes to a small body of literature that examines the use of immersive simulation to prepare aspiring principals. Findings The findings indicate moderate effect sizes in changes in self-efficacy, positive attitudes toward immersive simulation as a pedagogical tool, and significance in the design of immersive simulation modules. This suggests that immersive sim-ulation, when properly designed, aids principals in taking action to improve schools. Recommendations for Practitioners Educational leadership programs might consider the use of immersive simula-tions to enhance principals' ability to meet the complex demands of leading in the 21st century. Impact on Society Principals may be more adept at improving schools if preparation programs provided consistent opportunities to engage in immersive simulations.Future Research Future research should be conducted with larger sample sizes and longitudinally to determine the effectiveness of this treatment.","Action re-view cycle; Critical pedagogy; Immersive simulation; Principals; School improvement; Self-efficacy; Situated learning","Knowledge management; Virtual reality; Action re-view cycle; Critical pedagogies; Immersive; Principals; School improvement; Self efficacy; Situated learning; Students",Article,"Final","",Scopus,2-s2.0-85026251032
"Sylaiou S., Mania K., Paliokas I., Pujol-Tost L., Killintzis V., Liarokapis F.","14828467500;6602471750;6506588780;36873398800;24722814200;7801416785;","Exploring the educational impact of diverse technologies in online virtual museums",2017,"International Journal of Arts and Technology","10","1",,"58","84",,13,"10.1504/IJART.2017.083907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019019344&doi=10.1504%2fIJART.2017.083907&partnerID=40&md5=fd1e9051eb8ee4e5fa17dfb48b26ba25","School of Social Sciences, Hellenic Open University, Greece; Department of Electronic and Computer Engineering, Technical University of Crete, Greece; Department of Electrical and Computer Engineering, Democritus University of Thrace, Greece; Department of Humanities, Universitat Pompeu Fabra, Spain; Laboratory of Medical Informatics, Aristotle University of Thessaloniki, Greece; Faculty of Informatics, Masaryk University, Brno, Czech Republic","Sylaiou, S., School of Social Sciences, Hellenic Open University, Greece; Mania, K., Department of Electronic and Computer Engineering, Technical University of Crete, Greece; Paliokas, I., Department of Electrical and Computer Engineering, Democritus University of Thrace, Greece; Pujol-Tost, L., Department of Humanities, Universitat Pompeu Fabra, Spain; Killintzis, V., Laboratory of Medical Informatics, Aristotle University of Thessaloniki, Greece; Liarokapis, F., Faculty of Informatics, Masaryk University, Brno, Czech Republic","This research explores the learning outcomes of online virtual museums employing diverse technologies such as images, videos, 3D reconstructions, etc. It presents the selection criteria (imageability, interactivity, navigability, personalisation, communication) of the five online virtual museums (VMs) involved in the analysis, each of which brings forward a prominent visualisation technology. Then, it describes the methodology of the evaluation process, in which a group of 164 (n = 164) participants, after exploration of the virtual museum websites, answered a self-administered questionnaire including 12 questions based on the concept of generic learning outcomes: a) knowledge and understanding; b) skills; c) change in attitude and values; d) enjoyment, inspiration and creativity; e) action, behaviour and progression. The results of a statistical analysis investigating the educational impact of each VM are analysed. The potential educational advantage of incorporating complex 3D reconstructions in a VM is questioned. A new methodology for analysing VMs is required. This paper contributes to the understanding of the educational impact of VMs in relation to their underlying technology and human computer interaction (HCI) features. Therefore, conclusions have a wider impact and can be generalised to be relevant even after design changes of the VMs selected for the evaluation. Copyright © 2017 Inderscience Enterprises Ltd.","Educational impact; Generic learning outcomes; GLO; Virtual museums","E-learning; Engineering education; Human computer interaction; Image reconstruction; Virtual machine; 3D reconstruction; Educational advantages; Educational impact; Human computer interaction (HCI); Learning outcome; Selection criteria; Self-administered questionnaire; Virtual museum; Virtual reality",Article,"Final","",Scopus,2-s2.0-85019019344
"Smit D., Spruit E., Dankelman J., Tuijthof G., Hamming J., Horeman T.","57213648181;55818023600;7003296968;6603186033;55382767000;35322481100;","Improving training of laparoscopic tissue manipulation skills using various visual force feedback types",2017,"Surgical Endoscopy","31","1",,"299","308",,7,"10.1007/s00464-016-4972-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969884818&doi=10.1007%2fs00464-016-4972-0&partnerID=40&md5=25f3761988616c2fd958b2f3a1b59e9d","Cognitive Psychology, Institute of Psychology, Leiden University, Leiden, Netherlands; BioMechanical Engineering, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, Netherlands; Department of Orthopedic Surgery, Academic Medical Center, Amsterdam, Netherlands; Department of Surgery, Leiden University Medical Center, Leiden, Netherlands","Smit, D., Cognitive Psychology, Institute of Psychology, Leiden University, Leiden, Netherlands; Spruit, E., Cognitive Psychology, Institute of Psychology, Leiden University, Leiden, Netherlands, Department of Surgery, Leiden University Medical Center, Leiden, Netherlands; Dankelman, J., BioMechanical Engineering, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, Netherlands; Tuijthof, G., BioMechanical Engineering, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, Netherlands, Department of Orthopedic Surgery, Academic Medical Center, Amsterdam, Netherlands; Hamming, J., Department of Surgery, Leiden University Medical Center, Leiden, Netherlands; Horeman, T., BioMechanical Engineering, Delft University of Technology, Mekelweg 2, Delft, 2628 CD, Netherlands, Department of Orthopedic Surgery, Academic Medical Center, Amsterdam, Netherlands","Background: Visual force feedback allows trainees to learn laparoscopic tissue manipulation skills. The aim of this experimental study was to find the most efficient visual force feedback method to acquire these skills. Retention and transfer validity to an untrained task were assessed. Methods: Medical students without prior experience in laparoscopy were randomized in three groups: Constant Force Feedback (CFF) (N = 17), Bandwidth Force Feedback (BFF) (N = 16) and Fade-in Force Feedback (N = 18). All participants performed a pretest, training, post-test and follow-up test. The study involved two dissimilar tissue manipulation tasks, one for training and one to assess transferability. Participants performed six trials of the training task. A force platform was used to record several force parameters. Results: A paired-sample t test showed overall lower force parameter outcomes in the post-test compared to the pretest (p < .001). A week later, the force parameter outcomes were still significantly lower than found in the pretest (p < .005). Participants also performed the transfer task in the post-test (p < .02) and follow-up (p < .05) test with lower force parameter outcomes compared to the pretest. A one-way MANOVA indicated that in the post-test the CFF group applied 50 % less Mean Absolute Nonzero Force (p = .005) than the BFF group. Conclusion: All visual force feedback methods showed to be effective in decreasing tissue manipulation force as no major differences were found between groups in the post and follow-up trials. The BFF method is preferred for it respects individual progress and minimizes distraction. © 2016, The Author(s).","Force; Hybrid box trainer; Laparoscopy; Learning curve; Tissue manipulation skills; Visual feedback","adolescent; adult; Article; bandwidth force feedback; clinical competence; comparative study; constant force feedback; controlled study; experimental study; fade in force feedback; female; follow up; human; laparoscopy; learning curve; male; medical student; pretest posttest design; priority journal; randomized controlled trial; surgical training; tissue preparation; visual feedback; young adult; clinical competence; education; laparoscopy; sensory feedback; simulation training; Adolescent; Adult; Clinical Competence; Educational Measurement; Feedback, Sensory; Female; Humans; Laparoscopy; Learning Curve; Male; Simulation Training; Students, Medical; Young Adult",Article,"Final","",Scopus,2-s2.0-84969884818
"Chellali A., Mentis H., Miller A., Ahn W., Arikatla V.S., Sankaranarayanan G., De S., Schwaitzberg S.D., Cao C.G.L.","26767578800;57203381862;55956855100;9334513400;26654027600;15623319200;7202304567;7007036892;25957557800;","Achieving interface and environment fidelity in the Virtual Basic Laparoscopic Surgical Trainer",2016,"International Journal of Human Computer Studies","96",,,"22","37",,9,"10.1016/j.ijhcs.2016.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979747876&doi=10.1016%2fj.ijhcs.2016.07.005&partnerID=40&md5=343a4468d7b192400ee9a598468ded84","Department of Computer Engineering, IBISC Laboratory, University of Evry, Evry, France; Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States; Department of Information Systems, University of Maryland Baltimore County, Baltimore, MD, United States; Department of Surgery, Wright State University, Dayton, OH, United States; Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic InstituteNY, United States; Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, Dayton, OH, United States","Chellali, A., Department of Computer Engineering, IBISC Laboratory, University of Evry, Evry, France, Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States; Mentis, H., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States, Department of Information Systems, University of Maryland Baltimore County, Baltimore, MD, United States; Miller, A., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States, Department of Surgery, Wright State University, Dayton, OH, United States; Ahn, W., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic InstituteNY, United States; Arikatla, V.S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic InstituteNY, United States; Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic InstituteNY, United States; De, S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic InstituteNY, United States; Schwaitzberg, S.D., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States; Cao, C.G.L., Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, Dayton, OH, United States","Virtual reality trainers are educational tools with great potential for laparoscopic surgery. They can provide basic skills training in a controlled environment and free of risks for patients. They can also offer objective performance assessment without the need for proctors. However, designing effective user interfaces that allow the acquisition of the appropriate technical skills on these systems remains a challenge. This paper aims to examine a process for achieving interface and environment fidelity during the development of the Virtual Basic Laparoscopic Surgical Trainer (VBLaST). Two iterations of the design process were conducted and evaluated. For that purpose, a total of 42 subjects participated in two experimental studies in which two versions of the VBLaST were compared to the accepted standard in the surgical community for training and assessing basic laparoscopic skills in North America, the FLS box-trainer. Participants performed 10 trials of the peg transfer task on each trainer. The assessment of task performance was based on the validated FLS scoring method. Moreover, a subjective evaluation questionnaire was used to assess the fidelity aspects of the VBLaST relative to the FLS trainer. Finally, a focus group session with expert surgeons was conducted as a comparative situated evaluation after the first design iteration. This session aimed to assess the fidelity aspects of the early VBLaST prototype as compared to the FLS trainer. The results indicate that user performance on the earlier version of the VBLaST resulting from the first design iteration was significantly lower than the performance on the standard FLS box-trainer. The comparative situated evaluation with domain experts permitted us to identify some issues related to the visual, haptic and interface fidelity on this early prototype. Results of the second experiment indicate that the performance on the second generation VBLaST was significantly improved as compared to the first generation and not significantly different from that of the standard FLS box-trainer. Furthermore, the subjects rated the fidelity features of the modified VBLaST version higher than the early version. These findings demonstrate the value of the comparative situated evaluation sessions entailing hands on reflection by domain experts to achieve the environment and interface fidelity and training objectives when designing a virtual reality laparoscopic trainer. This suggests that this method could be used successfully in the future to enhance the value of VR systems as an alternative to physical trainers for laparoscopic surgery skills. Some recommendations on how to use this method to achieve the environment and interface fidelity of a VR laparoscopic surgical trainer are identified. © 2016 Elsevier Ltd","Interaction design; Iterative design; Simulator fidelity; surgical training; Virtual reality (VR)","Design; Haptic interfaces; Iterative methods; Laparoscopy; Surgery; User interfaces; Virtual reality; Controlled environment; Interaction design; Iterative design; Performance assessment; Simulator fidelity; Subjective evaluations; Surgical training; Virtual reality trainer; Surgical equipment",Article,"Final","",Scopus,2-s2.0-84979747876
"Van Dijk L., Van Der Sluis C.K., Van Dijk H.W., Bongers R.M.","55986349800;6701309500;14523705200;6602524481;","Task-Oriented Gaming for Transfer to Prosthesis Use",2016,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","24","12",,"1384","1394",,28,"10.1109/TNSRE.2015.2502424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990958782&doi=10.1109%2fTNSRE.2015.2502424&partnerID=40&md5=d3f04d30a4122c08af8e6dfbedf89e9c","University Medical Center Groningen, Center for Human Movement Science, University of Groningen, Groningen, 9700 AD, Netherlands; University Medical Center Groningen, Department of Rehabilitation Medicine, University of Groningen, Groningen, 9700 RB, Netherlands; Faculty of Engineering, Serious Gaming Group, NHL University of Applied Sciences, Leeuwarden, 8900 CB, Netherlands","Van Dijk, L., University Medical Center Groningen, Center for Human Movement Science, University of Groningen, Groningen, 9700 AD, Netherlands; Van Der Sluis, C.K., University Medical Center Groningen, Department of Rehabilitation Medicine, University of Groningen, Groningen, 9700 RB, Netherlands; Van Dijk, H.W., Faculty of Engineering, Serious Gaming Group, NHL University of Applied Sciences, Leeuwarden, 8900 CB, Netherlands; Bongers, R.M., University Medical Center Groningen, Center for Human Movement Science, University of Groningen, Groningen, 9700 AD, Netherlands","The aim of this study is to establish the effect of task-oriented video gaming on using a myoelectric prosthesis in a basic activity of daily life (ADL). Forty-one able-bodied right-handed participants were randomly assigned to one of four groups. In three of these groups the participants trained to control a video game using the myosignals of the flexors and extensors of the wrist: in the Adaptive Catching group participants needed to catch falling objects by opening and closing a grabber and received ADL-relevant feedback during performance. The Free Catching group used the same game, but without augmented feedback. The Interceptive Catching group trained a game where the goal was to intercept a falling object by moving a grabber to the left and right. They received no additional feedback. The control group played a regular Mario computer game. All groups trained 20 minutes a day for four consecutive days. Two tests were conducted before and after training: one level of the training game was performed, and participants grasped objects with a prosthesis simulator. Results showed all groups improved their game performance over controls. In the prosthesis-simulator task, after training the Adaptive Catching group outperformed the other groups in their ability to adjust the hand aperture to the size of the objects and the degree of compression of compressible objects. This study is the first to demonstrate transfer effects from a serious game to a myoelectric prosthesis task. The specificity of the learning effects suggests that research into serious gaming will benefit from placing ADL-specific constraints on game development. © 2001-2011 IEEE.","Artificial limbs; electromyography; transfer of training; video games","Artificial limbs; Computer games; Electromyography; Feedback; Human computer interaction; Interactive computer graphics; Prosthetics; Serious games; Software design; Activity of daily lives; Adaptive catching; Augmented feedback; Game development; Myoelectric prosthesis; Relevant feedback; Transfer of trainings; Video game; Myoelectrically controlled prosthetics; arm; computer interface; daily life activity; devices; female; human; limb prosthesis; male; neurorehabilitation; physiology; procedures; psychomotor performance; teaching; video game; young adult; Activities of Daily Living; Arm; Artificial Limbs; Computer-Assisted Instruction; Female; Humans; Male; Neurological Rehabilitation; Psychomotor Performance; User-Computer Interface; Video Games; Young Adult",Article,"Final","",Scopus,2-s2.0-84990958782
"Silva P.M., Pappas T.N., Atkins J., West J.E.","47062047800;35566614500;36627852700;7402746449;","Perceiving Graphical and Pictorial Information via Hearing and Touch",2016,"IEEE Transactions on Multimedia","18","12", 7544538,"2432","2445",,3,"10.1109/TMM.2016.2601029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000624459&doi=10.1109%2fTMM.2016.2601029&partnerID=40&md5=d5e947da8102defd5a56699a7aa1015e","Intel Corporation, Hillsboro, OR  97124, United States; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL  60208, United States; Apple Inc., Culver City, CA  90232, United States; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD  21218, United States","Silva, P.M., Intel Corporation, Hillsboro, OR  97124, United States; Pappas, T.N., Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL  60208, United States; Atkins, J., Apple Inc., Culver City, CA  90232, United States; West, J.E., Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD  21218, United States","We propose a dynamic interactive system for conveying visual information via hearing and touch to blind and visually impaired people. The system is implemented with a touch screen that allows the user to actively explore a two-dimensional layout consisting of one or more objects with the finger while listening to auditory feedback. Sound is used as the primary source of information for object localization, identification, and shape, while touch is used for pointing and kinesthetic feedback. A static overlay of raised-dot tactile patterns can also be added. The head-related transfer function is used for rendering sound directionality, and variations of sound intensity or other features are used for rendering proximity. The main focus is on conveying the shape of an object, but the rendering of a simple scene layout, that consists of objects in a linear arrangement, each with a distinct tapping sound, is also considered and compared to a 'virtual cane.' We consider a number of acoustic-tactile configurations and use empirical studies with visually blocked sighted participants to compare their effectiveness. Our findings demonstrate the advantages of spatial sound (directionality and proximity cues) for dynamic display of information (localization, identification, shape), while raised-dot patterns provide the best static shape rendition. We also show that the proposed configurations outperform existing techniques. The proposed approach is also expected to impact other applications where vision cannot be used. © 2016 IEEE.","Acoustic-tactile representation of visual signals; semantic mapping; sensory substitution; user interface","Human computer interaction; Semantics; Touch screens; User interfaces; Blind and visually impaired; Dynamic interactive systems; Head related transfer function; Kinesthetic feedback; Semantic mapping; Sensory substitution; Two-dimensional layout; Visual signals; Audition",Conference Paper,"Final","",Scopus,2-s2.0-85000624459
"Geronazzo M., Fantin J., Sorato G., Baldovino G., Avanzini F.","36720522500;57192160918;57192165752;57192163530;7005300654;","Acoustic selfies for extraction of external ear features in mobile audio augmented reality",2016,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST","02-04-November-2016",,,"23","26",,6,"10.1145/2993369.2993376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998673798&doi=10.1145%2f2993369.2993376&partnerID=40&md5=baaf37ec647ecc8a17f0ae21946a13bd","Dept. of Neurosciences, Biomedicine and Movement Sciences, University of Verona, Italy; Dept. of Information Engineering, University of Padova, Italy","Geronazzo, M., Dept. of Neurosciences, Biomedicine and Movement Sciences, University of Verona, Italy; Fantin, J., Dept. of Information Engineering, University of Padova, Italy; Sorato, G., Dept. of Information Engineering, University of Padova, Italy; Baldovino, G., Dept. of Information Engineering, University of Padova, Italy; Avanzini, F., Dept. of Information Engineering, University of Padova, Italy","Virtual and augmented realities are expected to become more and more important in everyday life in the next future; the role of spatial audio technologies over headphones will be pivotal for application scenarios which involve mobility. This paper introduces the SelfEar project, aimed at low-cost acquisition and personalization of Head-Related Transfer Functions (HRTFs) on mobile devices. This first version focuses on capturing individual spectral features which characterize external ear acoustics, through a self-adjustable procedure which guides users in collecting such information: their mobile device must be held with the stretched arm and positioned at several specific elevation points; acoustic data are acquired by an audio augmented reality headset which embeds a pair of microphones at listener ear-canals. A preliminary measurement session assesses the ability of the system to capture spectral features which are crucial for elevation perception. Moreover, a virtual experiment using a computational auditory model predicts clear vertical localization cues in the measured features.","Binaural audio; Computational auditory model; Head-related transfer function; Headphones; Mobile augmented reality","Augmented reality; Headphones; Loudspeakers; Mobile devices; Transfer functions; Virtual reality; Audio augmented reality; Auditory modeling; Binaural audio; Elevation perceptions; Head related transfer function; Mobile augmented reality; Vertical localization; Virtual and augmented reality; Audio acoustics",Conference Paper,"Final","",Scopus,2-s2.0-84998673798
"Gomez G., Crombie D.","56111293900;56235544200;","Bridging design prototypes in the development of games for formal learning environments",2016,"2016 8th International Conference on Games and Virtual Worlds for Serious Applications, VS-Games 2016",,, 7590340,"","",,1,"10.1109/VS-GAMES.2016.7590340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013158255&doi=10.1109%2fVS-GAMES.2016.7590340&partnerID=40&md5=7c92ae23d495ff35084af75a5efe95d7","Department of Technology and Innovation, Faculty of Engineering, University of Southern Denmark, Odense, Denmark; European Research, HKU-University of the Arts Utrecht, Utrecht, Netherlands","Gomez, G., Department of Technology and Innovation, Faculty of Engineering, University of Southern Denmark, Odense, Denmark; Crombie, D., European Research, HKU-University of the Arts Utrecht, Utrecht, Netherlands","For more than 30 years, research and applied work has been reported on the huge potential of games for learning. This paper presents Bridging Design Prototypes (BDPs) as an approach that could address the remaining challenge of the successful uptake of games in formal learning environments. BDPs are functional prototypes that bring teachers and learners into a development process early: teachers adapt, re-design, and incorporate them into real activities with students without the presence of the R&D team. Designers employ them for learning about the user community, and their context of practice, and to further inform product development. Two examples illustrate how BDPs enabling novel educational practices have put teachers in control of experimentations, in a leading design role, and dissemination. It is argued that this approach could transfer well into the development of serious games in formal education. © 2016 IEEE.","Bridging design prototypes; Educational games; Formal learning; Gamified learning; Serious games methodologies","Computer aided instruction; Education; Teaching; Virtual reality; Design prototype; Development process; Educational game; Formal learning; Formal learning environments; Functional Prototypes; Games for learning; Gamified learning; Serious games",Conference Paper,"Final","",Scopus,2-s2.0-85013158255
"Akchelov E., Galanina E.","57193342118;56960510400;","Virtual world of video games",2016,"2016 8th International Conference on Games and Virtual Worlds for Serious Applications, VS-Games 2016",,, 7590379,"","",,,"10.1109/VS-GAMES.2016.7590379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013218777&doi=10.1109%2fVS-GAMES.2016.7590379&partnerID=40&md5=b3e13812b3dbe5b8778e05541d986965","Institute of Humanities, Social Sciences and Technologies, National Research Tomsk Polytechnic University, Tomsk, Russian Federation","Akchelov, E., Institute of Humanities, Social Sciences and Technologies, National Research Tomsk Polytechnic University, Tomsk, Russian Federation; Galanina, E., Institute of Humanities, Social Sciences and Technologies, National Research Tomsk Polytechnic University, Tomsk, Russian Federation","This article is dedicated to the phenomenon of video games virtual world. We have overviewed specific literature that let us formulate the generalized definition of virtual world: computer-based three or two dimensional environment or space that can simulate real world where users represented by avatars are able to communicate or interact simultaneously or synchronically. This definition is quite limited when it comes to virtual non-ICT worlds. To understand the essence of virtual world we referred to postmodern philosophy that implies the possibility of multiple full fledged worlds. We have formulated one thesis stating that virtual world is the universum of simulacra and the second thesis stating that virtual world is fiction, imaginary and illusory, that becomes real and actual in the process of interaction between the Architect of a virtual world and the Beholders. The article proposes a new view on virtual world of video games. © 2016 IEEE.","Game studies; Postmodern philosophy; Simulacrum; Video games; Virtual world; Virtual world architect","Human computer interaction; Serious games; Virtual reality; Game studies; Postmodern philosophy; Simulacrum; Video game; Virtual worlds; Interactive computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85013218777
"Barsom E.Z., Graafland M., Schijven M.P.","57140782000;27267612800;6602492995;","Systematic review on the effectiveness of augmented reality applications in medical training",2016,"Surgical Endoscopy","30","10",,"4174","4183",,157,"10.1007/s00464-016-4800-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959096172&doi=10.1007%2fs00464-016-4800-6&partnerID=40&md5=46df8c1076fea9f35efc5138cf107b53","Department of Surgery, Academic Medical Centre, PO Box 22660, Amsterdam, 1100 DD, Netherlands; Department of Surgery, Flevo Hospital, Almere, Netherlands","Barsom, E.Z., Department of Surgery, Academic Medical Centre, PO Box 22660, Amsterdam, 1100 DD, Netherlands; Graafland, M., Department of Surgery, Academic Medical Centre, PO Box 22660, Amsterdam, 1100 DD, Netherlands, Department of Surgery, Flevo Hospital, Almere, Netherlands; Schijven, M.P., Department of Surgery, Academic Medical Centre, PO Box 22660, Amsterdam, 1100 DD, Netherlands","Background: Computer-based applications are increasingly used to support the training of medical professionals. Augmented reality applications (ARAs) render an interactive virtual layer on top of reality. The use of ARAs is of real interest to medical education because they blend digital elements with the physical learning environment. This will result in new educational opportunities. The aim of this systematic review is to investigate to which extent augmented reality applications are currently used to validly support medical professionals training. Methods: PubMed, Embase, INSPEC and PsychInfo were searched using predefined inclusion criteria for relevant articles up to August 2015. All study types were considered eligible. Articles concerning AR applications used to train or educate medical professionals were evaluated. Results: Twenty-seven studies were found relevant, describing a total of seven augmented reality applications. Applications were assigned to three different categories. The first category is directed toward laparoscopic surgical training, the second category toward mixed reality training of neurosurgical procedures and the third category toward training echocardiography. Statistical pooling of data could not be performed due to heterogeneity of study designs. Face-, construct- and concurrent validity was proven for two applications directed at laparoscopic training, face- and construct validity for neurosurgical procedures and face-, content- and construct validity in echocardiography training. In the literature, none of the ARAs completed a full validation process for the purpose of use. Conclusion: Augmented reality applications that support blended learning in medical training have gained public and scientific interest. In order to be of value, applications must be able to transfer information to the user. Although promising, the literature to date is lacking to support such evidence. © 2016, The Author(s).","Augmented reality; Medical education; Medical specialist training; Surgery; Training","Article; augmented reality application; concurrent validity; construct validity; content validity; echocardiography; face validity; human; laparoscopic surgery; medical education; neurosurgery; priority journal; simulation training; simulator; software; surgical training; systematic review; validation process; virtual reality; clinical competence; computer interface; computer simulation; education; laparoscopy; learning; software; Clinical Competence; Computer Simulation; Humans; Laparoscopy; Learning; Neurosurgical Procedures; Software; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84959096172
"Chen W., Ladeveze N., Clavel C., Bourdot P.","55918917900;26537655000;57210674262;14051447300;","Refined experiment of the altered human joystick for user cohabitation in multi-stereocopic immersive CVEs",2016,"2016 IEEE 3rd VR International Workshop on Collaborative Virtual Environments, 3DCVE 2016",,, 7563558,"1","8",,1,"10.1109/3DCVE.2016.7563558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991287566&doi=10.1109%2f3DCVE.2016.7563558&partnerID=40&md5=bb8eb089f580f84d422dbe6a238fec7c","VENISE Group, CNRS-LIMSI, Orsay, France; CPU Group, CNRS-LIMSI, Orsay, France","Chen, W., VENISE Group, CNRS-LIMSI, Orsay, France; Ladeveze, N., VENISE Group, CNRS-LIMSI, Orsay, France; Clavel, C., CPU Group, CNRS-LIMSI, Orsay, France; Bourdot, P., VENISE Group, CNRS-LIMSI, Orsay, France","Immersive multi-user virtual environments give good support for closely-coupled collaboration between co-located users. More complex collaborative tasks may require individual user navigation to achieve loosely-coupled collaboration. We designed a navigation framework based on the human joystick metaphor with some alterations for cohabitation management. This model allows each user to navigate independently using a human joystick based control while avoiding physical obstacles and staying within the usable part of the system. We conducted a series of user studies to investigate the influence of each alteration by testing their combinations on various navigation tasks. The results show that modified transfer functions and adaptive neutral orientations improve users' cohabitation performance, while the impact of adaptive neutral positions need to be further studied. © 2016 IEEE.","I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality","Computer graphics; Navigation; Virtual reality; Co-located; Collaborative tasks; I.3.7 [computer graphics]: three-dimensional graphics and realism - virtual realities; Loosely coupled; Multi-user virtual environment; Navigation tasks; User navigation; User study; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84991287566
"Ferrer-Torregrosa J., Jiménez-Rodríguez M.Á., Torralba-Estelles J., Garzón-Farinós F., Pérez-Bermejo M., Fernández-Ehrling N.","56497953600;57168816300;57190985796;57190983121;57191966548;57190981162;","Distance learning ects and flipped classroom in the anatomy learning: Comparative study of the use of augmented reality, video and notes",2016,"BMC Medical Education","16","1", 230,"","",,35,"10.1186/s12909-016-0757-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985023167&doi=10.1186%2fs12909-016-0757-3&partnerID=40&md5=2e184d9ddd34f0b6947a0cca5249f5bf","Department of Podiatry, School of Physiotherapy and Podiatry, Catholic University of Valencia, San Vicente Martir, C/ Ramiro de Maeztu 14, Torrente, 46900, Spain; Didactics and Educational Innovation, School of Psychology, Teaching and Educational Sciences, Catholic University of Valencia, San Vicente Martir, Valencia, Spain; Faculty of Nursing, Catholic University of Valencia, San Vicente Martir, Valencia, Spain; Doctoral School, Catholic University of Valencia, San Vicente Martir, Valencia, Spain","Ferrer-Torregrosa, J., Department of Podiatry, School of Physiotherapy and Podiatry, Catholic University of Valencia, San Vicente Martir, C/ Ramiro de Maeztu 14, Torrente, 46900, Spain; Jiménez-Rodríguez, M.Á., Didactics and Educational Innovation, School of Psychology, Teaching and Educational Sciences, Catholic University of Valencia, San Vicente Martir, Valencia, Spain; Torralba-Estelles, J., Department of Podiatry, School of Physiotherapy and Podiatry, Catholic University of Valencia, San Vicente Martir, C/ Ramiro de Maeztu 14, Torrente, 46900, Spain; Garzón-Farinós, F., Department of Podiatry, School of Physiotherapy and Podiatry, Catholic University of Valencia, San Vicente Martir, C/ Ramiro de Maeztu 14, Torrente, 46900, Spain; Pérez-Bermejo, M., Faculty of Nursing, Catholic University of Valencia, San Vicente Martir, Valencia, Spain; Fernández-Ehrling, N., Doctoral School, Catholic University of Valencia, San Vicente Martir, Valencia, Spain","Background: The establishment of the ECTS (European Credit Transfer System) is one of the pillars of the European Space of Higher Education. This way of accounting for the time spent in training has two essential parts, classroom teaching (work with the professor) and distance learning (work without the professor, whether in an individual or collective way). Much has been published on the distance learning part, but less on the classroom teaching section. In this work, the authors investigate didactic strategies and associated aids for distance learning work in a concept based on flipped classroom where transmitting information is carried out with aids that the professor prepares, so that the student works in an independent way before the classes, thus being able to dedicate the classroom teaching time to more complex learning and being able to count on the professor's help. Methods: Three teaching aids applied to the study of anatomy have been compared: Notes with images, videos, and augmented reality. Four dimensions have been compared: the time spent, the acquired learnings, the metacognitive perception, and the prospects of the use of augmented reality for study. Results: The results show the effectiveness, in all aspects, of augmented reality when compared with the rest of aids. The questionnaire assessed the acquired knowledge through a course exam, where 5.60 points were obtained for the notes group, 6.54 for the video group, and 7.19 for the augmented reality group. That is 0.94 more points for the video group compared with the notes and 1.59 more points for the augmented reality group compared with the notes group. Conclusions: This research demonstrates that, although technology has not been sufficiently developed for education, it is expected that it can be improved in both the autonomous work of the student and the academic training of health science students and that we can teach how to learn. Moreover, one can see how the grades of the students who studied with augmented reality are more grouped and that there is less dispersion in the marks compared with other materials. © 2016 The Author(s).","Anatomy; Augmented reality; Autonomous learning; ECTS; Flipped classroom; Metacognition","anatomy; comparative study; health science; human; human experiment; learning; metacognition; perception; questionnaire; rest; teaching; videorecording; academic achievement; anatomy; comparative study; computer interface; education; educational model; health personnel attitude; learning; medical literature; organization and management; problem based learning; procedures; program evaluation; Spain; standards; Anatomy; Attitude of Health Personnel; Computer-Assisted Instruction; Education, Distance; Education, Graduate; Educational Measurement; Humans; Learning; Medical Writing; Models, Educational; Problem-Based Learning; Program Evaluation; Spain; User-Computer Interface; Video Recording",Article,"Final","",Scopus,2-s2.0-84985023167
"Loukas C., Georgiou E.","6603074122;7004603021;","Performance comparison of various feature detector-descriptors and temporal models for video-based assessment of laparoscopic skills",2016,"International Journal of Medical Robotics and Computer Assisted Surgery","12","3",,"387","398",,9,"10.1002/rcs.1702","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985040464&doi=10.1002%2frcs.1702&partnerID=40&md5=6c1f9420c6928ef168deb2f0ce49d2ec","Medical Physics Lab-Simulation Center, School of Medicine, University of Athens, Greece","Loukas, C., Medical Physics Lab-Simulation Center, School of Medicine, University of Athens, Greece; Georgiou, E., Medical Physics Lab-Simulation Center, School of Medicine, University of Athens, Greece","Background: Despite the significant progress in hand gesture analysis for surgical skills assessment, video-based analysis has not received much attention. In this study we investigate the application of various feature detector-descriptors and temporal modeling techniques for laparoscopic skills assessment. Methods: Two different setups were designed: static and dynamic video-histogram analysis. Four well-known feature detection-extraction methods were investigated: SIFT, SURF, STAR-BRIEF and STIP-HOG. For the dynamic setup two temporal models were employed (LDS and GMMAR model). Each method was evaluated for its ability to classify experts and novices on peg transfer and knot tying. Results: STIP-HOG yielded the best performance (static: 74–79%; dynamic: 80–89%). Temporal models had equivalent performance. Important differences were found between the two groups with respect to the underlying dynamics of the video-histogram sequences. Conclusions: Temporal modeling of feature histograms extracted from laparoscopic training videos provides information about the skill level and motion pattern of the operator. Copyright © 2015 John Wiley & Sons, Ltd. Copyright © 2015 John Wiley & Sons, Ltd.","feature extraction; laparoscopy; simulation; skills assessment; surgery; video processing","Article; clinical article; controlled study; histogram; human; intermethod comparison; laparoscopic surgery; minimally invasive surgery; performance; skill; surgical training; virtual reality",Article,"Final","",Scopus,2-s2.0-84985040464
"Wu S., Jing X.-Y., Yue D., Zhang J., Yang K.J., Yang J.","24485676900;7202420489;36193477900;54987622400;57191156018;12773171100;","Unsupervised visual domain adaptation via dictionary evolution",2016,"Proceedings - IEEE International Conference on Multimedia and Expo","2016-August",, 7552896,"","",,5,"10.1109/ICME.2016.7552896","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987644080&doi=10.1109%2fICME.2016.7552896&partnerID=40&md5=3b2c19f82b47138215aba7339bb0a483","School of Automation, Nanjing University of Posts and Telecommunications, China; School of Computer Science, Wuhan University, China; School of Computing and Communications, University of Technology Sydney, Australia; School of Computer Science and Engineering, Nanjing University of Science and Technology, China","Wu, S., School of Automation, Nanjing University of Posts and Telecommunications, China; Jing, X.-Y., School of Computer Science, Wuhan University, China; Yue, D., School of Automation, Nanjing University of Posts and Telecommunications, China; Zhang, J., School of Computing and Communications, University of Technology Sydney, Australia; Yang, K.J., School of Computer Science and Engineering, Nanjing University of Science and Technology, China; Yang, J., School of Computer Science and Engineering, Nanjing University of Science and Technology, China","In real-word visual applications, distribution mismatch between samples from different domains may significantly degrade classification performance. To improve the generalization capability of classifier across domains, domain adaptation has attracted a lot of interest in computer vision. This work focuses on unsupervised domain adaptation which is still challenging because no labels are available in the target domain. Most of the attention has been dedicated to seeking domain-invariant feature by exploring the shared structure between domains, ignoring the valuable discriminative information contained in the labeled source data. In this paper, we propose a Dictionary Evolution (DE) approach to construct discriminative features robust to domain shift. Specifically, DE aims to adapt a discriminative dictionary learnt based on labeled source samples to unlabeled target samples through a gradual transition process. We show that the learnt dictionary is endowed with cross-domain data representation ability and powerful discriminant capability. Empirical results on real world data sets demonstrate the advantages of the proposed approach over competing methods. © 2016 IEEE.","dictionary learning; knowledge transfer; Unsupervised domain adaptation","Knowledge management; Virtual reality; Classification performance; Data representations; Dictionary learning; Discriminative dictionaries; Discriminative features; Domain adaptation; Generalization capability; Knowledge transfer; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84987644080
"Mastmeyer A., Fortmeier D., Handels H.","14056412100;55208146100;6701686055;","Efficient patient modeling for visuo-haptic VR simulation using a generic patient atlas",2016,"Computer Methods and Programs in Biomedicine","132",,,"161","175",,14,"10.1016/j.cmpb.2016.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966605200&doi=10.1016%2fj.cmpb.2016.04.017&partnerID=40&md5=9d6f7799cd805c496fd7c3406f417d08","Institute of Medical Informatics, University of Lübeck, Lübeck, Germany; Institute of Medical Informatics and the Graduate School for Computing in Medicine and Life Sciences, University of Lübeck, Lübeck, Germany","Mastmeyer, A., Institute of Medical Informatics, University of Lübeck, Lübeck, Germany; Fortmeier, D., Institute of Medical Informatics and the Graduate School for Computing in Medicine and Life Sciences, University of Lübeck, Lübeck, Germany; Handels, H., Institute of Medical Informatics, University of Lübeck, Lübeck, Germany","Background and Objective: This work presents a new time-saving virtual patient modeling system by way of example for an existing visuo-haptic training and planning virtual reality (VR) system for percutaneous transhepatic cholangio-drainage (PTCD). Methods: Our modeling process is based on a generic patient atlas to start with. It is defined by organ-specific optimized models, method modules and parameters, i.e. mainly individual segmentation masks, transfer functions to fill the gaps between the masks and intensity image data. In this contribution, we show how generic patient atlases can be generalized to new patient data. The methodology consists of patient-specific, locally-adaptive transfer functions and dedicated modeling methods such as multi-atlas segmentation, vessel filtering and spline-modeling. Results: Our full image volume segmentation algorithm yields median DICE coefficients of 0.98, 0.93, 0.82, 0.74, 0.51 and 0.48 regarding soft-tissue, liver, bone, skin, blood and bile vessels for ten test patients and three selected reference patients. Compared to standard slice-wise manual contouring time saving is remarkable. Conclusions: Our segmentation process shows out efficiency and robustness for upper abdominal puncture simulation systems. This marks a significant step toward establishing patient-specific training and hands-on planning systems in a clinical environment. © 2016 Elsevier Ireland Ltd.","Atlas-based segmentation; Cloud computing; Efficient image segmentation; Full body segmentation; Virtual reality simulation","Blood vessels; Cloud computing; Hospital data processing; Transfer functions; Virtual reality; Atlas-based segmentation; Clinical environments; Full body; Segmentation masks; Segmentation process; Simulation systems; Virtual patient models; Virtual reality simulations; Image segmentation; Article; blood; bone; cloud computing; computer assisted tomography; fascia; first cervical vertebra; human; liver; lumbar puncture; patient coding; percutaneous transhepatic cholangio drainage; percutaneous transhepatic drainage; skin; soft tissue; tactile stimulation; virtual reality; visual stimulation; voxel based morphometry; algorithm; computer interface; theoretical model; Algorithms; Humans; Models, Theoretical; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84966605200
"Van Dijk L., Van Der Sluis C.K., Van Dijk H.W., Bongers R.M.","55986349800;6701309500;14523705200;6602524481;","Learning an EMG controlled game: Task-specific adaptations and transfer",2016,"PLoS ONE","11","8", e0160817,"","",,17,"10.1371/journal.pone.0160817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990913286&doi=10.1371%2fjournal.pone.0160817&partnerID=40&md5=16c6e19289160026049364bd1ed0531e","University of Groningen, University Medical Center Groningen, Center for Human Movement Science, Groningen, Netherlands; University of Groningen, University Medical Center Groningen, Department of Rehabilitation Medicine, Groningen, Netherlands; NHL University of Applied Sciences, Faculty of Engineering, Serious Gaming Group, Leeuwarden, Netherlands","Van Dijk, L., University of Groningen, University Medical Center Groningen, Center for Human Movement Science, Groningen, Netherlands; Van Der Sluis, C.K., University of Groningen, University Medical Center Groningen, Department of Rehabilitation Medicine, Groningen, Netherlands; Van Dijk, H.W., NHL University of Applied Sciences, Faculty of Engineering, Serious Gaming Group, Leeuwarden, Netherlands; Bongers, R.M., University of Groningen, University Medical Center Groningen, Center for Human Movement Science, Groningen, Netherlands","Video games that aim to improve myoelectric control (myogames) are gaining popularity and are often part of the rehabilitation process following an upper limb amputation. However, direct evidence for their effect on prosthetic skill is limited. This study aimed to determine whether and how myogaming improves EMG control and whether performance improvements transfer to a prosthesis-simulator task. Able-bodied right-handed participants (N = 28) were randomly assigned to 1 of 2 groups. The intervention group was trained to control a video game (Breakout-EMG) using the myosignals of wrist flexors and extensors. Controls played a regular Mario computer game. Both groups trained 20 minutes a day for 4 consecutive days. Before and after training, two tests were conducted: one level of the Breakout-EMG game, and grasping objects with a prosthesis-simulator. Results showed a larger increase of in-game accuracy for the Breakout-EMG group than for controls. The Breakout-EMG group moreover showed increased adaptation of the EMG signal to the game. No differences were found in using a prosthesis-simulator. This study demonstrated that myogames lead to task-specific myocontrol skills. Transfer to a prosthesis task is therefore far from easy. We discuss several implications for future myogame designs. © 2016 van Dijk et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adaptation; adult; Article; controlled study; electromyogram; female; human; human experiment; male; myoelectric control; normal human; prosthesis; randomized controlled trial; simulator; task performance; video game; analysis of variance; learning; limb prosthesis; motor performance; upper limb; video game; young adult; Adult; Analysis of Variance; Artificial Limbs; Female; Humans; Learning; Male; Motor Skills; Task Performance and Analysis; Upper Extremity; Video Games; Young Adult",Article,"Final","",Scopus,2-s2.0-84990913286
"Daher S., Kim K., Lee M., Raij A., Schubert R., Bailenson J., Welch G.","57062307400;56159628700;56159565300;9735776700;55550102700;6602840468;35572266400;","Exploring social presence transfer in real-virtual human interaction",2016,"Proceedings - IEEE Virtual Reality","2016-July",, 7504705,"165","166",,3,"10.1109/VR.2016.7504705","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979780820&doi=10.1109%2fVR.2016.7504705&partnerID=40&md5=0475bd8bf66bb730025fb18d977c3e83","University of Central Florida, United States; UNC-Chapel Hill, United States; Stanford University, United States","Daher, S., University of Central Florida, United States; Kim, K., University of Central Florida, United States; Lee, M., University of Central Florida, United States; Raij, A., University of Central Florida, United States; Schubert, R., University of Central Florida, United States, UNC-Chapel Hill, United States; Bailenson, J., Stanford University, United States; Welch, G., University of Central Florida, United States","We explore whether a peripheral observation of apparent mutual social presence between a real human (RH) and a virtual human (VH) can in turn increase a subject's sense of social presence with the VH. In other words, we explore whether social presence can transfer from one RH-VH interaction to another. Specifically, we carried out an experiment where human subjects were asked to play a game with a VH. As they entered the game room, approximately half of the subjects were exposed to a brief but apparently engaging conversation between an RH and the VH. The subjects who were exposed to the brief RH-VH interaction had significantly higher measures of both emotional connection and the attentional allocation dimension of social presence for the VH, compared to those who were not. We describe the motivation, the experiment, and the results. © 2016 IEEE.","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, Augmented, and Virtual Realities; J.4 [Computer Applications]: Social and Behavioral Sciences - Psychology","Behavioral research; Emotional connections; Exposed to; H.5.1 [Information interfaces and presentation]: Multimedia Information Systems Artificial , augmented , and virtual realities; Human subjects; J.4 [computer applications]: social and behavioral sciences - psychologies; Social presence; Virtual humans; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84979780820
"Lee M., Kim K., Daher S., Raij A., Schubert R., Bailenson J., Welch G.","56159565300;56159628700;57062307400;9735776700;55550102700;6602840468;35572266400;","The Wobbly Table: Increased Social Presence via Subtle Incidental Movement of a Real-Virtual Table",2016,"Proceedings - IEEE Virtual Reality","2016-July",, 7504683,"11","17",,27,"10.1109/VR.2016.7504683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979788594&doi=10.1109%2fVR.2016.7504683&partnerID=40&md5=f824efacb20dd6e82bbe106e978d234b","University of Central Florida, United States; UNC, Chapel Hill, United States; Stanford University, United States","Lee, M., University of Central Florida, United States; Kim, K., University of Central Florida, United States; Daher, S., University of Central Florida, United States; Raij, A., University of Central Florida, United States; Schubert, R., University of Central Florida, United States, UNC, Chapel Hill, United States; Bailenson, J., Stanford University, United States; Welch, G., University of Central Florida, United States","While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced - however subtly - by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of Twenty Questions, where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the wobbly group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies. © 2016 IEEE.","Augmented and Virtual Realities; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; J.4 [Computer Applications]: Social and Behavioral Sciences - Psychology","Behavioral research; Augmented and virtual realities; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; J.4 [computer applications]: social and behavioral sciences - psychologies; Social presence; Virtual human systems; Virtual humans; Virtual objects; Virtual tables; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84979788594
"Park C.S., Le Q.T., Pedro A., Lim C.R.","55728031400;55536113800;56402928500;56163405400;","Interactive Building Anatomy Modeling for Experiential Building Construction Education",2016,"Journal of Professional Issues in Engineering Education and Practice","142","3", 4015019,"","",,23,"10.1061/(ASCE)EI.1943-5541.0000268","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975246807&doi=10.1061%2f%28ASCE%29EI.1943-5541.0000268&partnerID=40&md5=2fc6735a3beb25438346d88629acc28a","Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 06974, South Korea; Mutual Aid Project Division, Construction Workers Mutual Aid Association06151, South Korea","Park, C.S., Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 06974, South Korea; Le, Q.T., Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 06974, South Korea; Pedro, A., Dept. of Architectural Engineering, Chung-Ang Univ., Seoul, 06974, South Korea; Lim, C.R., Mutual Aid Project Division, Construction Workers Mutual Aid Association06151, South Korea","Building construction education is crucial in ensuring university students are adequately knowledgeable and competent to meet industry demands. Attributable to the changeable and complex environment, traditional pedagogical methods in building construction courses cannot equip students with concrete experience and knowledge. Despite several studies, adapting information and communication technology (ICT) tools such as virtual reality (VR) to enhance construction education, limited interaction and low detail of virtual contents still remain an unsolved issue. To address this problem, this study proposes an interactive building anatomy modeling (IBAM) system, which allows students to conveniently interact with VR environment for experiential building construction education. The IBAM system is developed based on the medical anatomy concept, including intuitive features, which support detaching and attaching components; and dissecting which enhances student-model interaction. To identify the advantages and limitations of IBAM, a prototype was developed, and its applicability was verified through a case study. Interim results suggest that the system facilitates experiential learning and provides adequate levels of interaction to effectively transfer knowledge to learners. © 2015 American Society of Civil Engineers.","Building information modeling; Construction education; Experiential learning; Virtual reality","Buildings; Education; Education computing; Students; Teaching; Virtual reality; Building construction; Building Information Model - BIM; Complex environments; Construction education; Experiential learning; Information and Communication Technologies; Pedagogical method; University students; Construction",Article,"Final","",Scopus,2-s2.0-84975246807
"Neupert C., Matich S., Scherping N., Kupnik M., Werthschutzky R., Hatzfeld C.","55987951500;55489746000;57191336069;14060455400;6603325434;35218941200;","Pseudo-Haptic Feedback in Teleoperation",2016,"IEEE Transactions on Haptics","9","3", 7457685,"397","408",,11,"10.1109/TOH.2016.2557331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988864392&doi=10.1109%2fTOH.2016.2557331&partnerID=40&md5=998959ee7ef9e5e3a237e9ddc9121afc","Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany","Neupert, C., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany; Matich, S., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany; Scherping, N., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany; Kupnik, M., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany; Werthschutzky, R., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany; Hatzfeld, C., Haptic Systems Group, Institute of Electromechanical Design, Technische Universitat Darmstadt, Darmstadt, 64283, Germany","In this paper, we develop possible realizations of pseudo-haptic feedback in teleoperation systems based on existing works for pseudo-haptic feedback in virtual reality and the intended applications. We derive four potential factors affecting the performance of haptic feedback (calculation operator, maximum displacement, offset force, and scaling factor), which are analyzed in three compliance identification experiments. First, we analyze the principle usability of pseudo-haptic feedback by comparing information transfer measures for teleoperation and direct interaction. Pseudo-haptic interaction yields well above-chance performance, while direct interaction performs almost perfectly. In order to optimize pseudo-haptic feedback, in the second study we perform a full-factorial experimental design with 36 subjects performing 6,480 trials with 36 different treatments. Information transfer ranges from 0.68 bit to 1.72 bit in a task with a theoretical maximum of 2.6 bit, with a predominant effect of the calculation operator and a minor effect of the maximum displacement. In a third study, short-and long-term learning effects are analyzed. Learning effects regarding the performance of pseudo-haptic feedback cannot be observed for single-day experiments. Tests over 10 days show a maximum increase in information transfer of 0.8 bit. The results show the feasibility of pseudo-haptic feedback for teleoperation and can be used as design basis for task-specific systems. © 2016 IEEE.","Haptics; medical robotics; pseudo-haptics; teleoperation","Design of experiments; Remote control; Virtual reality; Different treatments; Haptics; Information transfers; Maximum displacement; Medical robotics; Pseudo haptic feedback; Pseudo-haptics; Teleoperation systems; Haptic interfaces; adult; computer interface; computer simulation; devices; equipment design; feedback system; female; human; learning; male; physiology; procedures; robotic surgical procedure; robotics; telemedicine; touch; Adult; Computer Simulation; Equipment Design; Feedback; Female; Humans; Learning; Male; Robotic Surgical Procedures; Robotics; Telemedicine; Touch; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84988864392
"Liu Y.K.","47061795700;","Toward intelligent welding robots: virtualized welding based learning of human welder behaviors",2016,"Welding in the World","60","4",,"719","729",,5,"10.1007/s40194-016-0340-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975747616&doi=10.1007%2fs40194-016-0340-x&partnerID=40&md5=60cf2d874a8863e54f8e338e4e29119f","Department of Electrical and Computer Engineering, University of Kentucky, Lexington, KY  40506, United States","Liu, Y.K., Department of Electrical and Computer Engineering, University of Kentucky, Lexington, KY  40506, United States","Combining human welders (with intelligence and sensing versatility) and automated welding robots (with precision and consistency) can lead to next-generation intelligent welding robots that can perform precision welding similar to or even outperform skilled welders. In this study, an innovative human-machine welding paradigm, virtualized welding, is proposed that can transfer human intelligence to welding robots. A welding robot is augmented with sensors to observe the welding process and performs actual welding. A human welder operates a virtual welding torch and freely adjusts its movement in 3D space based on the visual feedback from the sensors. The adjustments together with the reconstructed 3D weld pool surfaces are recorded, analyzed, and utilized to form models representing human welder intelligence. To demonstrate the concepts behind the virtualized welding system and modeling method proposed, human welder’s adjustments are “selectively” learned and transferred to the welding robot to perform automated gas tungsten arc welding (GTAW). Experimental results verified the effectiveness of the learned human response models. A foundation is thus established to rapidly extract human intelligence and transfer it into welding robots. © 2016, International Institute of Welding.","Artifical intelligence; GTA welding; Robots","Behavioral research; Electric arc welding; Electric welding; Gas metal arc welding; Gas welding; Human form models; Industrial robots; Intelligent robots; Robot applications; Robots; Three dimensional computer graphics; Virtual reality; Visual communication; Artifical intelligence; Automated gas-tungsten arc welding; Automated welding; GTA welding; Human intelligence; Intelligent welding; Precision welding; Weld pool surfaces; Welding",Article,"Final","",Scopus,2-s2.0-84975747616
"Ouya S., Seyed C., Mbacke A.B., Mendy G., Niang I.","36004785600;57190401501;57190400074;57204407053;36984215800;","WebRTC platform proposition as a support to the educational system of universities in a limited Internet connection context",2016,"Proceedings of the 2015 5th World Congress on Information and Communication Technologies, WICT 2015",,, 7489643,"47","52",,6,"10.1109/WICT.2015.7489643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979777180&doi=10.1109%2fWICT.2015.7489643&partnerID=40&md5=85c0b634b33d950e560faf60919da4b8","Department Infrastructures and Information Systems, Virtual University of Senegal (U.V.S.), Dakar, Senegal; Laboratory LIRT, Ecole Supérieure Polytechnique Universite Cheikh Anta DIOP (U.C.A.D.), Dakar, Senegal","Ouya, S., Department Infrastructures and Information Systems, Virtual University of Senegal (U.V.S.), Dakar, Senegal; Seyed, C., Laboratory LIRT, Ecole Supérieure Polytechnique Universite Cheikh Anta DIOP (U.C.A.D.), Dakar, Senegal; Mbacke, A.B., Laboratory LIRT, Ecole Supérieure Polytechnique Universite Cheikh Anta DIOP (U.C.A.D.), Dakar, Senegal; Mendy, G., Laboratory LIRT, Ecole Supérieure Polytechnique Universite Cheikh Anta DIOP (U.C.A.D.), Dakar, Senegal; Niang, I., Laboratory LIRT, Ecole Supérieure Polytechnique Universite Cheikh Anta DIOP (U.C.A.D.), Dakar, Senegal","This paper proposes a collaborative system based on WebRTC technology to improve digital universities e-Learning environment. It allows teachers and students, through a web browser, to communicate via chat, audio and camera. It also supports file transfer and screen sharing for computer connected lab equipment. All these features are functional in an IP environment without need for Internet access. For its design and realization, we had to implement a WebRTC signalisation server to manage real-time applications, using the three WebRTC APIs: MediaStream for the acquisition and synchronization of audio and video, Peer Connection for communication between user's browsers and RTCDataChannel for file transfer, Chat and Screen Sharing. © 2015 IEEE.","collaborative system; e-Learning; Peer connection; virtual classroom; webRTC","E-learning; Education; Internet; Internet protocols; Teaching; Collaborative systems; E-learning environment; Educational systems; Internet connection; Peer connection; Real-time application; Virtual Classroom; webRTC; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-84979777180
"Liu K.S., Hsueh S.-L.","36663711200;9745425300;","Effects of digital teaching on the thinking styles and the transfer of learning of the students in department of interior design",2016,"Eurasia Journal of Mathematics, Science and Technology Education","12","6",,"1697","1706",,3,"10.12973/eurasia.2016.1563a","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965002747&doi=10.12973%2feurasia.2016.1563a&partnerID=40&md5=31f12f5a1030efabfb982d5c3e6c4e29","Tung Fang Design Institute, Taiwan","Liu, K.S., Tung Fang Design Institute, Taiwan; Hsueh, S.-L., Tung Fang Design Institute, Taiwan","Along with the constant advance of information technology and the rapid development of the Internet, the diverse functions and characteristics of E-Learning break through lots of limitations in traditional instruction. Properly integrating E-Learning design with Internet activities could enhance students' learning effect, and applying digital technology to assist in teachers' instruction is a future trend of instruction. A nonequivalent experimental design is applied to the Quasi-Experimental research in this study, in which total 162 students in three classes in National Taipei University of Technology, Tung Fang Design Institute, National Yunlin University of Science and Technology in Taiwan are selected as the research subjects for the 16-Week (three hours per week) experimental teaching research. The research results conclude that 1. thinking styles present significant effects on transfer of learning, 2. E-Learning shows remarkable effects on transfer of learning, 3.liberal thinking styles reveal the best effect on promoting far transfer under E-Learning, and 4.conservative thinking styles appear the best effect on promoting near transfer under E-Learning. At the end, conclusions and suggestions are proposed in this study, expecting to provide teachers with some assistance in the teaching methods. © 2016 by the author/s.","Near transfer; Simulation; Thinking styles; Transfer of learning",,Article,"Final","",Scopus,2-s2.0-84965002747
"Riechmann H., Finke A., Ritter H.","43061434800;28067619800;55964297500;","Using a cVEP-Based Brain-Computer Interface to Control a Virtual Agent",2016,"IEEE Transactions on Neural Systems and Rehabilitation Engineering","24","6", 7298447,"692","699",,25,"10.1109/TNSRE.2015.2490621","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976495754&doi=10.1109%2fTNSRE.2015.2490621&partnerID=40&md5=60c473af37697b2bcfb04b793915c6d9","Cognitive Interaction Technology-Center of Excellence, Bielefeld UniversityD-30501, Germany; Bielefeld University, Bielefeld, D-30501, Germany","Riechmann, H., Cognitive Interaction Technology-Center of Excellence, Bielefeld UniversityD-30501, Germany; Finke, A., Bielefeld University, Bielefeld, D-30501, Germany; Ritter, H., Bielefeld University, Bielefeld, D-30501, Germany","Brain-computer interfaces provide a means for controlling a device by brain activity alone. One major drawback of noninvasive BCIs is their low information transfer rate, obstructing a wider deployment outside the lab. BCIs based on codebook visually evoked potentials (cVEP) outperform all other state-of-The-Art systems in that regard. Previous work investigated cVEPs for spelling applications. We present the first cVEP-based BCI for use in real-world settings to accomplish everyday tasks such as navigation or action selection. To this end, we developed and evaluated a cVEP-based on-line BCI that controls a virtual agent in a simulated, but realistic, 3-D kitchen scenario. We show that cVEPs can be reliably triggered with stimuli in less restricted presentation schemes, such as on dynamic, changing backgrounds. We introduce a novel, dynamic repetition algorithm that allows for optimizing the balance between accuracy and speed individually for each user. Using these novel mechanisms in a 12-command cVEP-BCI in the 3-D simulation results in ITRs of 50 bits/min on average and 68 bits/min maximum. Thus, this work supports the notion of cVEP-BCIs as a particular fast and robust approach suitable for real-world use. © 2001-2011 IEEE.","brain control; Brain-computer interface (BCI); code-modulating visually-evoked potential (cVEP); electroencephalography (EEG); virtual agent","Bioelectric potentials; Brain; Electroencephalography; Electrophysiology; Human computer interaction; Interfaces (computer); Action selection; Brain controls; Information transfer rate; Real world setting; Robust approaches; State-of-the-art system; Virtual agent; Visually evoked potentials; Brain computer interface; adult; algorithm; automated pattern recognition; brain computer interface; communication aid; electroencephalography; female; human; male; man machine interaction; physiology; procedures; task performance; vision; visual evoked potential; Adult; Algorithms; Brain-Computer Interfaces; Communication Aids for Disabled; Electroencephalography; Evoked Potentials, Visual; Female; Humans; Male; Man-Machine Systems; Pattern Recognition, Automated; Task Performance and Analysis; Visual Perception",Article,"Final","",Scopus,2-s2.0-84976495754
"Davis M.C., Can D.D., Pindrik J., Rocque B.G., Johnston J.M.","56443189000;57000171300;12805714800;57212093445;55426656500;","Virtual Interactive Presence in Global Surgical Education: International Collaboration Through Augmented Reality",2016,"World Neurosurgery","86",,,"103","111",,53,"10.1016/j.wneu.2015.08.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959496176&doi=10.1016%2fj.wneu.2015.08.053&partnerID=40&md5=ab27ccbb7a0fecc1928f74721fe0b98c","University of Alabama at Birmingham, Birmingham, AL, United States; Neurosurgical Department, Children's Hospital #2, Ho Chi Minh City, Viet Nam","Davis, M.C., University of Alabama at Birmingham, Birmingham, AL, United States; Can, D.D., Neurosurgical Department, Children's Hospital #2, Ho Chi Minh City, Viet Nam; Pindrik, J., University of Alabama at Birmingham, Birmingham, AL, United States; Rocque, B.G., University of Alabama at Birmingham, Birmingham, AL, United States; Johnston, J.M., University of Alabama at Birmingham, Birmingham, AL, United States","Background Technology allowing a remote, experienced surgeon to provide real-time guidance to local surgeons has great potential for training and capacity building in medical centers worldwide. Virtual interactive presence and augmented reality (VIPAR), an iPad-based tool, allows surgeons to provide long-distance, virtual assistance wherever a wireless internet connection is available. Local and remote surgeons view a composite image of video feeds at each station, allowing for intraoperative telecollaboration in real time. Methods Local and remote stations were established in Ho Chi Minh City, Vietnam, and Birmingham, Alabama, as part of ongoing neurosurgical collaboration. Endoscopic third ventriculostomy with choroid plexus coagulation with VIPAR was used for subjective and objective evaluation of system performance. Results VIPAR allowed both surgeons to engage in complex visual and verbal communication during the procedure. Analysis of 5 video clips revealed video delay of 237 milliseconds (range, 93-391 milliseconds) relative to the audio signal. Excellent image resolution allowed the remote neurosurgeon to visualize all critical anatomy. The remote neurosurgeon could gesture to structures with no detectable difference in accuracy between stations, allowing for submillimeter precision. Fifteen endoscopic third ventriculostomy with choroid plexus coagulation procedures have been performed with the use of VIPAR between Vietnam and the United States, with no significant complications. 80% of these patients remain shunt-free. Conclusion Evolving technologies that allow long-distance, intraoperative guidance, and knowledge transfer hold great potential for highly efficient international neurosurgical education. VIPAR is one example of an inexpensive, scalable platform for increasing global neurosurgical capacity. Efforts to create a network of Vietnamese neurosurgeons who use VIPAR for collaboration are underway. © 2016 Elsevier Inc. All rights reserved.","Global Health; Neurosurgery; Pediatrics; Telecommunications","anastomosis; anatomy; choroid plexus; gesture; human; human experiment; neurosurgeon; surgical training; United States; verbal communication; videorecording; Viet Nam; case report; computer interface; female; hydrocephalus; infant; international cooperation; male; neuroendoscopy; preschool child; teleconsultation; ventriculostomy; Child, Preschool; Female; Humans; Hydrocephalus; Infant; International Cooperation; Male; Neuroendoscopy; Remote Consultation; United States; User-Computer Interface; Ventriculostomy; Vietnam",Article,"Final","",Scopus,2-s2.0-84959496176
"Zendejas B., Ruparel R.K., Cook D.A.","35849662900;55923148700;35233076600;","Validity evidence for the Fundamentals of Laparoscopic Surgery (FLS) program as an assessment tool: a systematic review",2016,"Surgical Endoscopy","30","2",,"512","520",,40,"10.1007/s00464-015-4233-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957839290&doi=10.1007%2fs00464-015-4233-7&partnerID=40&md5=49827c438d39a49d771b82a27b183644","Department of Surgery, Mayo Clinic College of Medicine, Mayo 12-W, 200 First Street SW, Rochester, MN  55905, United States; Division of General Internal Medicine, Mayo Clinic College of Medicine, Rochester, MN, United States; Multidisciplinary Simulation Center, Mayo Clinic College of Medicine, Rochester, MN, United States","Zendejas, B., Department of Surgery, Mayo Clinic College of Medicine, Mayo 12-W, 200 First Street SW, Rochester, MN  55905, United States; Ruparel, R.K., Department of Surgery, Mayo Clinic College of Medicine, Mayo 12-W, 200 First Street SW, Rochester, MN  55905, United States; Cook, D.A., Division of General Internal Medicine, Mayo Clinic College of Medicine, Rochester, MN, United States, Multidisciplinary Simulation Center, Mayo Clinic College of Medicine, Rochester, MN, United States","Background: The Fundamentals of Laparoscopic Surgery (FLS) program uses five simulation stations (peg transfer, precision cutting, loop ligation, and suturing with extracorporeal and intracorporeal knot tying) to teach and assess laparoscopic surgery skills. We sought to summarize evidence regarding the validity of scores from the FLS assessment. Methods: We systematically searched for studies evaluating the FLS as an assessment tool (last search update February 26, 2013). We classified validity evidence using the currently standard validity framework (content, response process, internal structure, relations with other variables, and consequences). Results: From a pool of 11,628 studies, we identified 23 studies reporting validity evidence for FLS scores. Studies involved residents (n = 19), practicing physicians (n = 17), and medical students (n = 8), in specialties of general (n = 17), gynecologic (n = 4), urologic (n = 1), and veterinary (n = 1) surgery. Evidence was most common in the form of relations with other variables (n = 22, most often expert–novice differences). Only three studies reported internal structure evidence (inter-rater or inter-station reliability), two studies reported content evidence (i.e., derivation of assessment elements), and three studies reported consequences evidence (definition of pass/fail thresholds). Evidence nearly always supported the validity of FLS total scores. However, the loop ligation task lacks discriminatory ability. Conclusion: Validity evidence confirms expected relations with other variables and acceptable inter-rater reliability, but other validity evidence is sparse. Given the high-stakes use of this assessment (required for board eligibility), we suggest that more validity evidence is required, especially to support its content (selection of tasks and scoring rubric) and the consequences (favorable and unfavorable impact) of assessment. © 2015, Springer Science+Business Media New York.","Assessment; Fundamentals of Laparoscopic Surgery; Simulation; Validation","Article; cholecystectomy; construct validity; human; laparoscopic surgery; priority journal; skill; surgical training; systematic review; test retest reliability; validity; work environment; clinical competence; education; laparoscopy; procedures; reproducibility; simulation training; United States; Clinical Competence; Humans; Laparoscopy; Reproducibility of Results; Simulation Training; United States",Article,"Final","",Scopus,2-s2.0-84957839290
"Turkan Y., Karabulut-Ilgu A., Radkowski R., Chen A., Behzadan A.H., Jahren C.T.","54390463700;57190805412;10043883300;57215560344;15073914400;6603866838;","Mobile augmented reality implementation for teaching structural analysis",2016,"23rd International Workshop of the European Group for Intelligent Computing in Engineering, EG-ICE 2016",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987741097&partnerID=40&md5=aca85a7504888958f54fe9be09b5c33a","Iowa State University, United States; Missouri State University, United States","Turkan, Y., Iowa State University, United States; Karabulut-Ilgu, A., Iowa State University, United States; Radkowski, R., Iowa State University, United States; Chen, A., Iowa State University, United States; Behzadan, A.H., Missouri State University, United States; Jahren, C.T., Iowa State University, United States","In this paper, augmented reality (AR) technology is used to design and launch a mobile application to visualize structural components, and their behavior under loading, which will help students transfer their abstract knowledge into real world situations. To validate results, a set of example problems are used that include questions concerning structural component behavior under loading. For these examples, 3D models are developed and overlaid on top of the book images to help students see the effect of loads on structures in action. For assessment, both control and experimental groups are deployed and students learning is measured in order to investigate the design characteristics of an effective pedagogy involving AR to teach structural analysis, how AR impacts students learning, the role of AR in creating an inclusive learning environment that caters to various learning styles, and enhancing students adaptive flexibility, and how AR impacts students engagement in the learning process.",,"Augmented reality; Computer aided instruction; Human computer interaction; Intelligent computing; Structural analysis; Students; Teaching; Design characteristics; Experimental groups; Learning environments; Mobile applications; Mobile augmented reality; Real world situations; Structural component; Students learning; Education",Conference Paper,"Final","",Scopus,2-s2.0-84987741097
"Nathanael D., Mosialos S., Vosniakos G.-C., Tsagkas V.","6506068895;14058702100;6701789136;37105230000;","Development and Evaluation of a Virtual Reality Training System Based on Cognitive Task Analysis: The Case of CNC Tool Length Offsetting",2016,"Human Factors and Ergonomics In Manufacturing","26","1",,"52","67",,12,"10.1002/hfm.20613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955099078&doi=10.1002%2fhfm.20613&partnerID=40&md5=8a1e68c2850b77abc085eb5d383bc1f5","School of Mechanical Engineering, National Technical University of Athens, Zografou, GR-15780, Greece","Nathanael, D., School of Mechanical Engineering, National Technical University of Athens, Zografou, GR-15780, Greece; Mosialos, S., School of Mechanical Engineering, National Technical University of Athens, Zografou, GR-15780, Greece; Vosniakos, G.-C., School of Mechanical Engineering, National Technical University of Athens, Zografou, GR-15780, Greece; Tsagkas, V., School of Mechanical Engineering, National Technical University of Athens, Zografou, GR-15780, Greece","This article reports on the development and evaluation of a virtual reality training system (VRTS) for a specific machining task. A cognitive task analysis of expert machinists was conducted to examine whether this can be effective in developing a VRTS concerning tool length offsetting for a machining center. This analysis provided the necessary information for development and calibration of such a system. Subsequently, the effectiveness of the VRTS was evaluated by conducting an experiment with 29 mechanical engineering students. The VRTS set-up comprised a video projection of the machining center and a physical mock-up of its interface. The system demonstrated positive training transfer for the toll length offsetting task in terms of task accomplishment and of time to complete the task. No positive transfer was observed in terms of task accuracy, probably due to perceptual biases induced by the detailed specification of the VRTS. The present work provides evidence that cognitive task analysis was effective in identifying a number of key skills pertaining to the tool length offsetting task and in implementing ways to facilitate training in such tasks in a virtual environment. This article also demonstrates that even for tasks that include subtle perceptual skills VRTS may be beneficial regardless of the level of physical fidelity, provided that the cognitive organization of a task is adequately mapped in the system. © 2014 Wiley Periodicals, Inc.","CNC machining; Perceptual skills; Simulation; Training; Virtual reality","Job analysis; Machining; Machining centers; Personnel training; Virtual reality; Cnc machining; Cognitive organization; Cognitive task analysis; Mechanical engineering students; Perceptual skills; Simulation; Task accomplishment; Virtual reality training; E-learning",Article,"Final","",Scopus,2-s2.0-84955099078
"Bojórquez E.M., Villegas O.O.V., Sánchez V.G.C., García-Alcaraz J.L., Vara J.F.","57191628024;57201650680;7103226409;55616966800;6701593883;","Study on Mobile Augmented Reality Adoption for Mayo Language Learning",2016,"Mobile Information Systems","2016",, 1069581,"","",,3,"10.1155/2016/1069581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992192233&doi=10.1155%2f2016%2f1069581&partnerID=40&md5=f41471e3f50bf4f45be40a3e0dd7077c","Departamento de Ingeniería Eléctrica y Computación, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; Departamento de Ingeniería Industrial y Manufactura, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; Departamento de Ciencias de la Computación, Centro de Investigación Científica y de Educación Superior de Ensenada, Ensenada, BC, Mexico","Bojórquez, E.M., Departamento de Ingeniería Eléctrica y Computación, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; Villegas, O.O.V., Departamento de Ingeniería Industrial y Manufactura, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; Sánchez, V.G.C., Departamento de Ingeniería Eléctrica y Computación, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; García-Alcaraz, J.L., Departamento de Ingeniería Industrial y Manufactura, Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, CHIH, Mexico; Vara, J.F., Departamento de Ciencias de la Computación, Centro de Investigación Científica y de Educación Superior de Ensenada, Ensenada, BC, Mexico","This paper presents the results of a study applied to undergraduates in order to know how the cultural dimensions affect their perceptions of the acceptance and use of new technologies in a student-centered learning environment. A total of 85 undergraduate students from the Autonomous Indigenous University of Mexico (UAIM) participated in the study. Each student was asked to use a mobile augmented reality (MAR) application designed to learn Mayo language (language spoken in Northwestern Mexico). Afterwards, the students responded to a survey with items concerning the use and technology acceptance and about cultural dimensions of individualism and uncertainty avoidance. Structural equation modeling (SEM) was used to analyze the data collected from students. Results provide evidence that the individualism contributes positively to perceived ease of use of the MAR app, and uncertainty avoidance has no impact. The findings showed that the MAR system could be easily used if it includes a natural way to promote collaborative work. In addition, to gain the trust of students, the uncertainty avoidance needs to be reduced by enriching the help information offered for app use. © 2016 Erasmo Miranda Bojórquez et al.",,"Augmented reality; Computer aided instruction; E-learning; Education; Technology transfer; Cultural dimensions; Mobile augmented reality; Perceived ease of use; Structural equation modeling; Student-centered learning; Technology acceptance; Uncertainty avoidance; Undergraduate students; Students",Article,"Final","",Scopus,2-s2.0-84992192233
"Alghamdi M., Regenbrecht H., Hoermann S., Langlotz T., Aldridge C.","36459710000;6603333462;54787745300;8250843500;57062294500;","Social presence and mode of video communication in a Collaborative Virtual Environment",2016,"Pacific Asia Conference on Information Systems, PACIS 2016 - Proceedings",,,,"","",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011117105&partnerID=40&md5=0235f7bf3ef9f4bdceecd4b42a98b91e","Department of Information Science, University of Otago, New Zealand; School of Electrical and Information Engineering, University of Sydney, Sydney, Australia","Alghamdi, M., Department of Information Science, University of Otago, New Zealand; Regenbrecht, H., Department of Information Science, University of Otago, New Zealand; Hoermann, S., School of Electrical and Information Engineering, University of Sydney, Sydney, Australia; Langlotz, T., Department of Information Science, University of Otago, New Zealand; Aldridge, C., Department of Information Science, University of Otago, New Zealand","Collaborative Virtual Environments (CVE) with co-located or remote video communication functionality require a continuous experience of social presence. If, at any stage during the experience the communication interrupts presence then the CVE experience as a whole is affected - spatial presence is then decoupled from social presence. We present a solution to this problem by introducing the concept of a virtualized version of Google Glass™ called Virtual Glass. Virtual Glass is integrated into the CVE as a real-world metaphor for a communication device, one particularly suited for collaborative instructor-performer systems. Together with domain experts we developed a prototype system based on an instructor-performer architecture. In two studies with a total number of 115 participants we showed that the concept of Virtual Glass is effective, that it supports a high level of social presence and that the social presence for the performers is rated significantly higher than a standard picture-in-picture videoconferencing approach used for the performers. We present our experimental system, our studies, and the generalizability of our approach towards future uses.","Human-computer interface; Videoconferencing; Virtual reality","Display devices; Glass; Human computer interaction; Information systems; Video conferencing; Collaborative virtual environment; Communication device; Domain experts; Experimental system; Human computer interfaces; Prototype system; Social presence; Video communications; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85011117105
"Hain V., Löffler R., Zajíček V.","57192112303;56946660500;57192112852;","Interdisciplinary Cooperation in the Virtual Presentation of Industrial Heritage Development",2016,"Procedia Engineering","161",,,"2030","2035",,6,"10.1016/j.proeng.2016.08.798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997831540&doi=10.1016%2fj.proeng.2016.08.798&partnerID=40&md5=4c461a4b0c15572e61acefa71ebabd6a","Faculty of Architecture, Slovak University of Technology in Bratislava, Námestie Slobody 19, Bratislava 1, 812 45, Slovakia","Hain, V., Faculty of Architecture, Slovak University of Technology in Bratislava, Námestie Slobody 19, Bratislava 1, 812 45, Slovakia; Löffler, R., Faculty of Architecture, Slovak University of Technology in Bratislava, Námestie Slobody 19, Bratislava 1, 812 45, Slovakia; Zajíček, V., Faculty of Architecture, Slovak University of Technology in Bratislava, Námestie Slobody 19, Bratislava 1, 812 45, Slovakia","The issue of industrial heritage conservation as well as its new use has been worldwide hot topic for a longer period. Industrial heritage provides one of the most important records of urban development and progress of human civilization in the last two centuries. Monumental industrial buildings reflect the extraordinary technical and economic development and the progress in science and technology. Even after the termination of their original function, industrial heritage buildings and equipment's with their architecture are still participating in the atmosphere of each city in a significant way. The electricity industry played a key role in the life of the mankind. Electricity and electrical currents are intangible symbols of the creative use of the natural forces by man, also a real driving force for technological progress and extensive development of human knowledge. The young generation takes it for granted, with no interest in learning about its causal impact and context. A global problem is the decreasing interest of young people in studying natural sciences and engineering, which is a prerequisite for further technological progress and socio-economic development of the life of inhabitants. This lack of interest is justified by the high abstraction and lack of clarity in the scientific and technical fields which are move about from people's everyday lives. Therefore, the current trend is developing an interactive virtual model of presentations of this rich source of knowledge and experiences. Those are able to make technical museums more attractive and allow the inspirational use for a broader audience. The article ""Interdisciplinary cooperation in the virtual presentation of industrial heritage development"" is aimed to explore opportunities for collaboration between theoretical research, monument preservation, virtual reality and architectural practice. It deals with the identified key factors that conditionally affect the quality and efficiency of architectural design process within the cooperation in the conservation process. As well as it deals with the opportunities of transfer the research results from futuristic disciplines. For this purpose, the paper examines the case study ""the reconstruction of old Power plant in city Piestany"" and describes one of the possible solutions on the basis of the operational research model so-called ""Educational polygon."" © 2016 The Authors.","architecture; augmented reality; electricity industry; future education; industrial heritage; operational research; virtual reality","Architecture; Augmented reality; Economics; Electric industry; Industrial economics; Industrial research; Office buildings; Urban growth; Urban planning; Virtual reality; Electricity industry; Industrial heritage; Interdisciplinary cooperations; Knowledge and experience; Natural sciences and engineerings; Operational research; Socio-economic development; Technological progress; Economic and social effects",Conference Paper,"Final","",Scopus,2-s2.0-84997831540
"Gustafson-Pearce O., Grant S.","26656627700;26030369100;","Supply chain knowledge networking using a 3D virtual world environment",2016,"Proceedings of the European Conference on IS Management and Evaluation, ECIME",,,,"68","75",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016188993&partnerID=40&md5=93b4e1addc66cadc4897068b5ff85b3a","Brunel University, London, United Kingdom","Gustafson-Pearce, O., Brunel University, London, United Kingdom; Grant, S., Brunel University, London, United Kingdom","The specific aims of the paper are to highlight the use of 3D Virtual World (VW) tools for knowledge sharing (explicit and tacit knowledge) within a complex supply chain managed by a principal insurer. A set of web based tools, applications and exercises supporting the formation of communities of inquiry and promoting learning through social interaction is presented. These results are from a pilot study that was run over a 4 month period across an insurance supply chain, to explore how suppliers and the principal insurer shared knowledge using these tools. With the IoT (Internet of Things) generating multiple sources of 'streamed' data, the potential for using this type of data in a format that allows users to access data that is 'understandable' to them, is expanding. The paper focusses on one of these tools, a 3D web based Virtual World environment. Within the insurance industry, and specifically home insurers, a key priority is to have current and meaningful data on physical events and conditions available to their stakeholders and members of the supply chain. This is to enable them to make correct and timely decisions on claims, for example, weather related claims. Therefore an environment was designed and created, which used live streaming data from the United States Geological Survey, and a variety of VW tools and techniques to illustrate this data, and to orient it to make it relevant to the home claims teams. The use of social media tools such as Virtual Reality (VR) presents a new set of challenges to organizations that are not used to managing knowledge and information transfer in this way, and where lessons learnt from research endeavours into the use of VR/VW technology in knowledge management, are limited.","IoT; Knowledge sharing; Streamed data; Virtual reality; Virtual World environment","Economic and social effects; Information systems; Internet of things; Knowledge management; Supply chains; Video streaming; Virtual reality; Web crawler; Websites; 3D virtual world environments; Explicit and tacit knowledge; Information transfers; Knowledge-sharing; Streamed data; Supply chain knowledge; United states geological surveys; Virtual worlds; Information management",Conference Paper,"Final","",Scopus,2-s2.0-85016188993
"Antoniou P.E., Dafli E., Bamidis P.D.","56237199800;35753237600;6603398831;","Design of novel teaching episodes in medical education using emerging experiential digital assets; technology enabled medical education beyond the Gimmicky",2015,"Proceedings - 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015",,, 7363280,"1560","1565",,2,"10.1109/CIT/IUCC/DASC/PICOM.2015.360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964308542&doi=10.1109%2fCIT%2fIUCC%2fDASC%2fPICOM.2015.360&partnerID=40&md5=bb402e284e22108407dcce7ed5955125","Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece","Antoniou, P.E., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece; Dafli, E., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece; Bamidis, P.D., Medical Physics Laboratory, Medical School Aristotle University of Thessaloniki, Thessaloniki, Greece","Medical education has always been about experiential hands on training to prepare future doctors to create the necessary skillset to deal with the sensitive and immediate nature of their work. Contemporary experiential technologies such as Virtual and Augmented reality offer a realistic but consequence free test-bed in which to interact safely and cope with emotional challenges pertaining to the realistic tasks simulated through these media. This work describes the design guidelines and workflow for incorporating these novel virtual assets in medical education through serious role play learning episodes. This approach consists of the case selection, the identification of roles, information flow and narrative requirements, implementation of technological narrative tools and implementation of the learning episode. Using a specific example of case transfer through this model we describe the process, outline implementation, assessment and technological considerations. Finally rationale for this work as a counterweight to technological hype fluctuations and integration of these media into the mainstream curricula is discussed. © 2015 IEEE.","Autgmented reality; Medical education; Serious role play; Virtual reality; Virtual worlds","Augmented reality; Curricula; Education; Medical education; Personnel training; Ubiquitous computing; Virtual reality; Autgmented reality; Case selections; Digital assets; Hands-on-trainings; Information flows; Role play; Virtual and augmented reality; Virtual worlds; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-84964308542
"Britt R.C., Scerbo M.W., Montano M., Kennedy R.A., Prytz E., Stefanidis D.","8265800600;7004570474;55583273100;55426202400;34875597400;8543232800;","Intracorporeal suturing: Transfer from Fundamentals of Laparoscopic Surgery to cadavers results in substantial increase in mental workload",2015,"Surgery (United States)","158","5", 4127,"1428","1433",,11,"10.1016/j.surg.2015.03.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944170650&doi=10.1016%2fj.surg.2015.03.032&partnerID=40&md5=84ab5e7f465d746d7326037da8e3610d","Department of Surgery, Eastern Virginia Medical School, 825 Fairfax Ave, Suite 610, Norfolk, VA  23507, United States; Department of Psychology, Old Dominion University, Norfolk, VA, United States; Carolinas Medical Center, Charlotte, NC, United States; Norfolk, VA, and Charlotte, NC","Britt, R.C., Department of Surgery, Eastern Virginia Medical School, 825 Fairfax Ave, Suite 610, Norfolk, VA  23507, United States, Norfolk, VA, and Charlotte, NC; Scerbo, M.W., Department of Psychology, Old Dominion University, Norfolk, VA, United States, Norfolk, VA, and Charlotte, NC; Montano, M., Department of Psychology, Old Dominion University, Norfolk, VA, United States, Norfolk, VA, and Charlotte, NC; Kennedy, R.A., Department of Psychology, Old Dominion University, Norfolk, VA, United States, Norfolk, VA, and Charlotte, NC; Prytz, E., Department of Psychology, Old Dominion University, Norfolk, VA, United States, Norfolk, VA, and Charlotte, NC; Stefanidis, D., Carolinas Medical Center, Charlotte, NC, United States, Norfolk, VA, and Charlotte, NC","Introduction A spatial secondary task developed by the authors was used to measure the mental workload of the participant when transferring suturing skills from a box simulator to more realistic surgical conditions using a fresh cadaver. We hypothesized that laparoscopic suturing on genuine bowel would be more challenging than on the Fundamentals of Laparoscopic Surgery (FLS)-simulated bowel as reflected in differences on both suturing and secondary task scores. Methods We trained 14 surgical assistant students to FLS proficiency in intracorporeal suturing. Participants practiced suturing on the FLS box for 30 minutes and then were tested on both the FLS box and the bowel of a fresh cadaver using the spatial, secondary dual-task conditions developed by the authors. Results Suturing times increased by >333% when moving from the FLS platform to the cadaver F(1,13) = 44.04, P <.001. The increased completion times were accompanied by a 70% decrease in secondary task scores, F(1,13) = 21.21, P <.001. Conclusion The mental workload associated with intracorporeal suturing increases dramatically when trainees transfer from the FLS platform to human tissue under more realistic conditions of suturing. The increase in mental workload is indexed by both an increase in suturing times and a decrease in the ability to attend to the secondary task. © 2015 Elsevier Inc.",,"Article; cadaver; camera; human; laparoscopic surgery; medical assistant; priority journal; simulator; surgical assistant; suture; suturing method; workload; adult; attention; audiovisual equipment; clinical competence; education; female; laparoscopy; learning; male; mental function; physiology; simulation training; suturing method; task performance; young adult; Adult; Attention; Cadaver; Clinical Competence; Female; Humans; Laparoscopy; Male; Mental Processes; Models, Anatomic; Simulation Training; Suture Techniques; Task Performance and Analysis; Transfer (Psychology); Young Adult",Article,"Final","",Scopus,2-s2.0-84944170650
"Juanes J.A., Gómez J.J., Ruisoto P., Peguero P.D.","7004223337;56014507600;55102801800;57188930691;","Practical applications of movement control technology in the acquisition of clinical skills",2015,"ACM International Conference Proceeding Series",,,,"13","17",,2,"10.1145/2808580.2808583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014742726&doi=10.1145%2f2808580.2808583&partnerID=40&md5=2033d43d140a65e68fd577f0fd124da6","Instituto Universitario de Ciencias de la Educación (IUCE), University of Salamanca, Spain; Centro de Imagen y Tecnología Del Conocimiento Biomédico (CITEC-B)., Madrid, Spain; VisualMed System Research Group, University of Salamanca, Spain; Centro para la Imagen Biomédica en, Extremadura (CIBEX), Centro de Cirugía Mínima Invasión J. Usón, Cáceres, Spain","Juanes, J.A., Instituto Universitario de Ciencias de la Educación (IUCE), University of Salamanca, Spain; Gómez, J.J., Centro de Imagen y Tecnología Del Conocimiento Biomédico (CITEC-B)., Madrid, Spain; Ruisoto, P., VisualMed System Research Group, University of Salamanca, Spain; Peguero, P.D., Centro para la Imagen Biomédica en, Extremadura (CIBEX), Centro de Cirugía Mínima Invasión J. Usón, Cáceres, Spain","Intelligent environments are increasingly becoming useful scenarios for handling computers. Technological devices are practical tools for learning and acquiring clinical skills as part of the medical training process. Within the framework of the advanced user interface, we present a technological application using Leap Motion, to enhance interaction with the user in the process of a laparoscopic surgical intervention and integrate the navigation through augmented reality images using manual gestures. Thus, we intend to achieve a more natural interaction with the objects that participate in a surgical intervention, which are augmented and related to the user's hand movements. © 2015 ACM.","Frames; Infrared light; Leap Motion; Motion capture; Surgical simulation; Technology","Augmented reality; Ecology; Ecosystems; Personnel training; Surgery; Technology; User interfaces; Frames; Infrared light; Leap Motion; Motion capture; Surgical simulation; Technology transfer",Conference Paper,"Final","",Scopus,2-s2.0-85014742726
"Vaughan N., Dubey V.N., Wainwright T.W., Middleton R.G.","55345677800;56243741100;24342473500;21644110000;","Does virtual-reality training on orthopaedic simulators improve performance in the operating room?",2015,"Proceedings of the 2015 Science and Information Conference, SAI 2015",,, 7237125,"51","54",,6,"10.1109/SAI.2015.7237125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957809956&doi=10.1109%2fSAI.2015.7237125&partnerID=40&md5=a1beac849bf9a08361e051493d14b23f","Faculty of Science and Technology, Bournemouth University (BU), Bournemouth, United Kingdom; Department of Orthopaedics, Royal Bournemouth Hospital NHS Foundation Trust, Bournemouth, United Kingdom","Vaughan, N., Faculty of Science and Technology, Bournemouth University (BU), Bournemouth, United Kingdom; Dubey, V.N., Faculty of Science and Technology, Bournemouth University (BU), Bournemouth, United Kingdom; Wainwright, T.W., Department of Orthopaedics, Royal Bournemouth Hospital NHS Foundation Trust, Bournemouth, United Kingdom; Middleton, R.G., Department of Orthopaedics, Royal Bournemouth Hospital NHS Foundation Trust, Bournemouth, United Kingdom","This paper summarises recent validation studies and evidence demonstrating whether training on virtual reality (VR) simulators directly relates to improved performance in-vivo for orthopaedic surgical procedures. This research provides a summary of transfer validity on virtual reality orthopaedic simulators. This covers studies which have shown validation of simulators and have shown the transfer of simulator-acquired skill to the operating room. The findings of this study are that there are 6 studies showing transfer of skill for VR to in-vivo However more studies assessing efficacy and transfer validity are required to conclusively quantify the transfer validity of VR orthopaedic simulators. However there is a popular positive opinion for the ability of VR training to convert into better in-vivo performance. © 2015 IEEE.","Efficacy; Orthopeadic; Performance; Simulator; Surgery; Testing; Transfer validity","E-learning; Operating rooms; Simulators; Surgery; Testing; Virtual reality; Efficacy; Improve performance; Orthopaedic surgical procedures; Orthopeadic; Performance; Transfer validity; Validation study; Virtual reality training; Surgical equipment",Conference Paper,"Final","",Scopus,2-s2.0-84957809956
"Katz B.F.G., Felinto D.Q., Touraine D., Poirier-Quinot D., Bourdot P.","35557015800;54788920100;6506895230;55613431100;14051447300;","BlenderVR: Open-source framework for interactive and immersive VR",2015,"2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings",,, 7223366,"203","204",,12,"10.1109/VR.2015.7223366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954548555&doi=10.1109%2fVR.2015.7223366&partnerID=40&md5=9cd4b67baa192a1ea65bdcc275d8639b","LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France","Katz, B.F.G., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Felinto, D.Q., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Touraine, D., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Poirier-Quinot, D., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France; Bourdot, P., LIMSI-CNRS, Campus Universitaire d'Orsay, Orsay, France","BlenderVR is an open-source project framework for interactive and immersive applications based on an extension of the Blender Game Engine to Virtual Reality applications. BlenderVR is a generalization of the BlenderCAVE project, accounting for alternate platforms (e.g., HMD, video-walls). The goal is to provide a flexible and easy to use framework for the creation of VR applications for various platforms, making use of the existing power of the BGE's graphics rendering and physics engine. Compatible with 3 major Operating Systems, BlenderVR has been developed by VR researchers with support from the Blender Community. BlenderVR currently handles multi-screen/multi-user tracked stereoscopic rendering through efficient low-level master/slave synchronization process with multimodal interactions via OSC and VRPN protocols. © 2015 IEEE.","H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities; I.3.2 [Graphics Systems]: Distributed/network graphics","Blending; Computer graphics; Stereo image processing; Three dimensional computer graphics; Virtual reality; Distributed/network graphics; H.5.1 [multimedia information systems]: artificial , augmented , and virtual realities; Immersive application; Multi-Modal Interactions; Open source frameworks; Open source projects; Stereoscopic renderings; Synchronization process; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-84954548555
"Hochreiter J., Daher S., Nagendran A., Gonzalez L., Welch G.","55821444200;57062307400;26768151000;32667638800;35572266400;","Touch sensing on non-parametric rear-projection surfaces: A physical-virtual head for hands-on healthcare training",2015,"2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings",,, 7223326,"69","74",,4,"10.1109/VR.2015.7223326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954496838&doi=10.1109%2fVR.2015.7223326&partnerID=40&md5=b931f185c4f065c4af22ada031f13d17","University of Central Florida, United States","Hochreiter, J., University of Central Florida, United States; Daher, S., University of Central Florida, United States; Nagendran, A., University of Central Florida, United States; Gonzalez, L., University of Central Florida, United States; Welch, G., University of Central Florida, United States","We demonstrate a generalizable method for unified multitouch detection and response on a human head-shaped surface with a rear-projection animated 3D face. The method helps achieve hands-on touch-sensitive training with dynamic physical-virtual patient behavior. The method, which is generalizable to other non-parametric rear-projection surfaces, requires one or more infrared (IR) cameras, one or more projectors, IR light sources, and a rear-projection surface. IR light reflected off of human fingers is captured by cameras with matched IR pass filters, allowing for the localization of multiple finger touch events. These events are tightly coupled with the rendering system to produce auditory and visual responses on the animated face displayed using the projector(s), resulting in a responsive, interactive experience. We illustrate the applicability of our physical prototype in a medical training scenario. © 2015 IEEE.","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Animations, Artificial, Augmented, and Virtual Realities; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality; I.3.8 [Computer Graphics]: Applications","Cameras; Computer graphics; Light sources; Matched filters; Virtual reality; I.3.7 [computer graphics]: three-dimensional graphics and realism - virtual realities; I.3.8 [computer graphics]: Applications; Infrared cameras; Medical training; Multimedia Information Systems - Animations; Rendering system; Tightly-coupled; Virtual patients; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84954496838
"Hristov G., Kyuchukova D., Borisov S., Zahariev P.","24472522600;56891737600;57150357500;24829549600;","Improving virtual learning environments by development and integration of 3D models of real devices",2015,"2015 International Conference on Information Technology Based Higher Education and Training, ITHET 2015",,, 7217969,"","",,,"10.1109/ITHET.2015.7217969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959536167&doi=10.1109%2fITHET.2015.7217969&partnerID=40&md5=97d62183b88d3da1033625159c84449d","Department of Telecommunications, University of Ruse 'Angel Kanchev', Ruse, Bulgaria","Hristov, G., Department of Telecommunications, University of Ruse 'Angel Kanchev', Ruse, Bulgaria; Kyuchukova, D., Department of Telecommunications, University of Ruse 'Angel Kanchev', Ruse, Bulgaria; Borisov, S., Department of Telecommunications, University of Ruse 'Angel Kanchev', Ruse, Bulgaria; Zahariev, P., Department of Telecommunications, University of Ruse 'Angel Kanchev', Ruse, Bulgaria","When it comes to education we usually imagine a traditional one, which in most cases is conducted by lectors or educators who present the learning materials face to face with their students. With the technological progress during the last years, the things have changed - new ways for learning and teaching were born. In the present days we refer to these approaches as digital education or simply e-learning. The modern educational processes include some features of the traditional ones, but they also implement new technologies for visualization of contents and for providing connectivity and access to various resources. One of the most recent and attractive ways for providing e-learning courses is through the use of virtual laboratories. In the last years many different virtual laboratories were developed and presented including our own virtual laboratory in Telecommunication networks and systems. The purpose of this laboratory is to provide means for interaction and experiments with real communication devices, which are remotely accessed. Based on our experience and the feedback from our students about the laboratory, we have come to the conclusion that one of the drawbacks of our solution is the lack of physical interaction with the telecommunication devices. This means that the users of the virtual laboratory can take advantage of the functionality of the remotely accessed devices and they can learn how to operate them, but they do not know how the devices look like or what physical characteristics they have. We believe that obtaining knowledge about the physical structure and the hardware architecture of every system is an essential part of the educational process. In order to provide such knowledge an investigation of the modern options for providing additional visual resources and the ways to integrate this content in the virtual laboratories is required. In this paper we present an approach for visualization of the communication devices in our virtual laboratory, but the same approach can be used for visualization of different objects in every virtual learning environment. Our approach uses the most attractive and provocative way for getting realistic information about the physical structure and the architecture of the devices - the use of 3D models. In the paper we investigate the trends in the area of the 3D modelling of different objects and we provide a solution for integration of 3D content in virtual learning environments. © 2015 IEEE.","3D modelling technologies; virtual and remote laboratories; virtual learning environment; virtual reality photography","Computer aided instruction; Education; Education computing; Engineering education; Laboratories; Network architecture; Students; Teaching; Technology transfer; Telecommunication networks; Virtual reality; Visualization; 3D modelling; Hardware architecture; Learning and teachings; Physical characteristics; Remote laboratories; Technological progress; Telecommunication devices; Virtual learning environments; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84959536167
"Valyukh S., Slobodyanyuk O.","6506888596;57216315766;","Assessment of minimum permissible geometrical parameters of a near-to-eye display",2015,"Applied Optics","54","21",,"6526","6533",,,"10.1364/AO.54.006526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016924003&doi=10.1364%2fAO.54.006526&partnerID=40&md5=a2e69f534666f9c9c4c0ca6367386762","Department of Physics, Chemistry and Biology, Laboratory of Applied Optics, Linköping University, Linköping, SE-581 83, Sweden; Department of Physics, Taras Shevchenko National University of Kyiv, Ukraine","Valyukh, S., Department of Physics, Chemistry and Biology, Laboratory of Applied Optics, Linköping University, Linköping, SE-581 83, Sweden; Slobodyanyuk, O., Department of Physics, Taras Shevchenko National University of Kyiv, Ukraine","Light weight and small dimensions are some of the most important characteristics of near-to-eye displays (NEDs). These displays consist of two basic parts: a microdisplay for generating an image and supplementary optics in order to see the image. Nowadays, the pixel size of microdisplays may be less than 4 μm, which makes the supplementary optics the major factor in defining restrictions on a NED dimensions or at least on the distance between the microdisplay and the eye. The goal of the present work is to find answers to the following two questions: how small this distance can be in principle and what is the microdisplay maximum resolution that stays effective to see through the supplementary optics placed in immediate vicinity of the eye. To explore the first question, we consider an aberration-free magnifier, which is the initial stage in elaboration of a real optical system. In this case, the paraxial approximation and the transfer matrix method are ideal tools for simulation of light propagation from the microdisplay through the magnifier and the human eye's optical system to the retina. The human eye is considered according to the Gullstrand model. Parameters of the magnifier, its location with respect to the eye and the microdisplay, and the depth of field, which can be interpreted as the tolerance of the microdisplay position, are determined and discussed. The second question related to the microdisplay maximum resolution is investigated by using the principles of wave optics. © 2015 Optical Society of America.",,"Geometry; Light; Optical systems; Aberration-free; Depth of field; Light weight; Major factors; Maximum resolution; Microdisplays; Near-to-Eye display; Paraxial approximations; Transfer matrix method; algorithm; computer simulation; equipment design; eye; human; light; man machine interaction; normal distribution; optical instrumentation; optics; refractometry; retina; spectacles; theoretical model; Algorithms; Computer Simulation; Equipment Design; Eye; Eyeglasses; Humans; Light; Man-Machine Systems; Models, Theoretical; Normal Distribution; Optical Devices; Optics and Photonics; Refractometry; Retina",Article,"Final","",Scopus,2-s2.0-85016924003
"Bleser G., Damen D., Behera A., Hendeby G., Mura K., Miezal M., Gee A., Petersen N., Maçães G., Domingues H., Gorecky D., Almeida L., Mayol-Cuevas W., Calway A., Cohn A.G., Hogg D.C., Stricker D.","22950167800;25654582700;8684934900;15925569800;55523862900;54788071500;24821358900;35318283600;50961471900;56765803700;36095668000;57206479891;6507899137;6602753391;7005699215;7103188000;6701489212;","Cognitive learning, monitoring and assistance of industrial workflows using egocentric sensor networks",2015,"PLoS ONE","10","6", e0127769,"","",,29,"10.1371/journal.pone.0127769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938704361&doi=10.1371%2fjournal.pone.0127769&partnerID=40&md5=e5314c0fab73a82e8d2900fcea2fd628","Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Department of Computer Science, University of Bristol, Bristol, United Kingdom; School of Computing, University of Leeds, Leeds, United Kingdom; Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden; Department of Electrical Engineering, Linköping University, Linköping, Sweden; SmartFactory KL e.V., Kaiserslautern, Germany; Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal","Bleser, G., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany, Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Damen, D., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Behera, A., School of Computing, University of Leeds, Leeds, United Kingdom, Department of Computing, Edge Hill University, Ormskirk, United Kingdom; Hendeby, G., Department Sensor Informatics, Swedish Defence Research Agency, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden; Mura, K., SmartFactory KL e.V., Kaiserslautern, Germany; Miezal, M., Department of Computer Science, Technical University of Kaiserslautern, Kaiserslautern, Germany; Gee, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Petersen, N., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany; Maçães, G., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Domingues, H., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Gorecky, D., SmartFactory KL e.V., Kaiserslautern, Germany; Almeida, L., Department Computer Vision, Interaction and Graphics, Center for Computer Graphics, Guimarães, Portugal; Mayol-Cuevas, W., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Calway, A., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Cohn, A.G., School of Computing, University of Leeds, Leeds, United Kingdom; Hogg, D.C., School of Computing, University of Leeds, Leeds, United Kingdom; Stricker, D., Department Augmented Vision, German Research Center for Artificial Intelligence, Kaiserslautern, Germany","Today, the workflows that are involved in industrial assembly and production activities are becoming increasingly complex. To efficiently and safely perform these workflows is demanding on the workers, in particular when it comes to infrequent or repetitive tasks. This burden on the workers can be eased by introducing smart assistance systems. This article presents a scalable concept and an integrated system demonstrator designed for this purpose. The basic idea is to learn workflows from observing multiple expert operators and then transfer the learnt workflow models to novice users. Being entirely learning-based, the proposed system can be applied to various tasks and domains. The above idea has been realized in a prototype, which combines components pushing the state of the art of hardware and software designed with interoperability in mind. The emphasis of this article is on the algorithms developed for the prototype: 1) fusion of inertial and visual sensor information from an on-body sensor network (BSN) to robustly track the user's pose in magnetically polluted environments; 2) learning-based computer vision algorithms to map the workspace, localize the sensor with respect to the workspace and capture objects, even as they are carried; 3) domain-independent and robust workflow recovery and monitoring algorithms based on spatiotemporal pairwise relations deduced from object and user movement with respect to the scene; and 4) context-sensitive augmented reality (AR) user feedback using a head-mounted display (HMD). A distinguishing key feature of the developed algorithms is that they all operate solely on data from the on-body sensor network and that no external instrumentation is needed. The feasibility of the chosen approach for the complete action-perception-feedback loop is demonstrated on three increasingly complex datasets representing manual industrial tasks. These limited size datasets indicate and highlight the potential of the chosen technology as a combined entity as well as point out limitations of the system. © 2015 Bleser et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; cognition; computer interface; computer program; egocentric sensor networks; feedback system; female; head mounted display; human; image display; industrial production; information system; information technology; learning algorithm; male; on body sensor network; online monitoring; operator; sensor; spatiotemporal analysis; virtual reality; visual information; workflow; algorithm; learning; occupational health; occupational medicine; system analysis; three dimensional imaging; workflow; Algorithms; Cognition; Humans; Imaging, Three-Dimensional; Learning; Occupational Health; Occupational Medicine; Systems Integration; User-Computer Interface; Workflow",Article,"Final","",Scopus,2-s2.0-84938704361
"Carlson P., Peters A., Gilbert S.B., Vance J.M., Luse A.","55211819500;39062047500;14041448900;57209452002;24776050400;","Virtual training: Learning transfer of assembly tasks",2015,"IEEE Transactions on Visualization and Computer Graphics","21","6", 7014246,"770","782",,43,"10.1109/TVCG.2015.2393871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928953556&doi=10.1109%2fTVCG.2015.2393871&partnerID=40&md5=52638cc89410113748eb702d400853b6","Department of Human-Computer Interaction (HCI), Iowa State University, Ames, IA  5001, United States; Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA  5001, United States; Department of Mechanical Engineering, Iowa State University, Ames, IA  5001, United States; Department of Management Science and Information Systems, Oklahoma State University, Stillwater, OK, United States","Carlson, P., Department of Human-Computer Interaction (HCI), Iowa State University, Ames, IA  5001, United States; Peters, A., Department of Human-Computer Interaction (HCI), Iowa State University, Ames, IA  5001, United States; Gilbert, S.B., Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA  5001, United States; Vance, J.M., Department of Mechanical Engineering, Iowa State University, Ames, IA  5001, United States; Luse, A., Department of Management Science and Information Systems, Oklahoma State University, Stillwater, OK, United States","In training assembly workers in a factory, there are often barriers such as cost and lost productivity due to shutdown. The use of virtual reality (VR) training has the potential to reduce these costs. This research compares virtual bimanual haptic training versus traditional physical training and the effectiveness for learning transfer. In a mixed experimental design, participants were assigned to either virtual or physical training and trained by assembling a wooden burr puzzle as many times as possible during a twenty minute time period. After training, participants were tested using the physical puzzle and were retested again after two weeks. All participants were trained using brightly colored puzzle pieces. To examine the effect of color, testing involved the assembly of colored physical parts and natural wood colored physical pieces. Spatial ability as measured using a mental rotation test, was shown to correlate with the number of assemblies they were able to complete in the training. While physical training outperformed virtual training, after two weeks the virtually trained participants actually improved their test assembly times. The results suggest that the color of the puzzle pieces helped the virtually trained participants in remembering the assembly process. © 2015 IEEE.","assembly; Haptics; Learning transfer; training; virtual reality","Assembly; Personnel training; Plant shutdowns; Virtual reality; Assembly process; Assembly workers; Haptics; Learning Transfer; Lost productivities; Physical training; Spatial abilities; Virtual training; E-learning",Article,"Final","",Scopus,2-s2.0-84928953556
"Bai Z., Blackwell A.F., Coulouris G.","55321604000;7006751809;6603082364;","Using augmented reality to elicit pretend play for children with autism",2015,"IEEE Transactions on Visualization and Computer Graphics","21","5", 7000596,"598","610",,50,"10.1109/TVCG.2014.2385092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926069056&doi=10.1109%2fTVCG.2014.2385092&partnerID=40&md5=c73cc86a7a9e7332a84e71daa805b92f","Computer Laboratory, University of CambridgeCB3 0FD, United Kingdom","Bai, Z., Computer Laboratory, University of CambridgeCB3 0FD, United Kingdom; Blackwell, A.F., Computer Laboratory, University of CambridgeCB3 0FD, United Kingdom; Coulouris, G., Computer Laboratory, University of CambridgeCB3 0FD, United Kingdom","Children with autism spectrum condition (ASC) suffer from deficits or developmental delays in symbolic thinking. In particular, they are often found lacking in pretend play during early childhood. Researchers believe that they encounter difficulty in generating and maintaining mental representation of pretense coupled with the immediate reality. We have developed an interactive system that explores the potential of Augmented Reality (AR) technology to visually conceptualize the representation of pretense within an open-ended play environment. Results from an empirical study involving children with ASC aged 4 to 7 demonstrated a significant improvement of pretend play in terms of frequency, duration and relevance using the AR system in comparison to a non computer-assisted situation. We investigated individual differences, skill transfer, system usability and limitations of the proposed AR system. We discuss design guidelines for future AR systems for children with ASC and other pervasive developmental disorders. © 1995-2012 IEEE.","H.5.1 Multimedia Information Systems","Diseases; Human computer interaction; Children with autisms; Developmental delay; Empirical studies; Individual Differences; Interactive system; Mental representations; MultiMedia Information Systems; Pervasive developmental disorders; Augmented reality; Asperger syndrome; Autistic Disorder; child; computer graphics; female; human; male; play therapy; preschool child; procedures; virtual reality exposure therapy; Asperger Syndrome; Autistic Disorder; Child; Child, Preschool; Computer Graphics; Female; Humans; Male; Play Therapy; Virtual Reality Exposure Therapy",Article,"Final","",Scopus,2-s2.0-84926069056
"Van Bruwaene S., Schijven M.P., Napolitano D., De Win G., Miserez M.","26536965100;6602492995;56585655500;12779954100;6603926707;","Porcine cadaver organ or virtual-reality simulation training for laparoscopic cholecystectomy: A randomized, controlled trial",2015,"Journal of Surgical Education","72","3",,"483","490",,12,"10.1016/j.jsurg.2014.11.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926660267&doi=10.1016%2fj.jsurg.2014.11.015&partnerID=40&md5=381eac06efa39e40513f76e65474c6f3","Center for Surgical Technologies, Leuven, Belgium; Department of Urology, University Hospitals Leuven, Leuven, Belgium; Department of Surgery, Academic Medical Center Amsterdam, Amsterdam, Netherlands; Universidad Catolica de Cordoba, Córdoba, Argentina; Department of Urology, University Hospitals Antwerp, Edegem, Belgium; Department of Abdominal Surgery, University Hospitals Leuven, Leuven, Belgium","Van Bruwaene, S., Center for Surgical Technologies, Leuven, Belgium, Department of Urology, University Hospitals Leuven, Leuven, Belgium; Schijven, M.P., Department of Surgery, Academic Medical Center Amsterdam, Amsterdam, Netherlands; Napolitano, D., Universidad Catolica de Cordoba, Córdoba, Argentina; De Win, G., Center for Surgical Technologies, Leuven, Belgium, Department of Urology, University Hospitals Leuven, Leuven, Belgium, Department of Urology, University Hospitals Antwerp, Edegem, Belgium; Miserez, M., Center for Surgical Technologies, Leuven, Belgium, Department of Abdominal Surgery, University Hospitals Leuven, Leuven, Belgium","Objectives As conventional laparoscopic procedural training requires live animals or cadaver organs, virtual simulation seems an attractive alternative. Therefore, we compared the transfer of training for the laparoscopic cholecystectomy from porcine cadaver organs vs virtual simulation to surgery in a live animal model in a prospective randomized trial. Design After completing an intensive training in basic laparoscopic skills, 3 groups of 10 participants proceeded with no additional training (control group), 5 hours of cholecystectomy training on cadaver organs (= organ training) or proficiency-based cholecystectomy training on the LapMentor (= virtual-reality training). Participants were evaluated on time and quality during a laparoscopic cholecystectomy on a live anaesthetized pig at baseline, 1 week (= post) and 4 months (= retention) after training. Setting All research was performed in the Center for Surgical Technologies, Leuven, Belgium. Participants In total, 30 volunteering medical students without prior experience in laparoscopy or minimally invasive surgery from the University of Leuven (Belgium). Results The organ training group performed the procedure significantly faster than the virtual trainer and borderline significantly faster than control group at posttesting. Only 1 of 3 expert raters suggested significantly better quality of performance of the organ training group compared with both the other groups at posttesting (p < 0.01). There were no significant differences between groups at retention testing. The virtual trainer group did not outperform the control group at any time. Conclusions For trainees who are proficient in basic laparoscopic skills, the long-term advantage of additional procedural training, especially on a virtual but also on the conventional organ training model, remains to be proven. © 2015 Association of Program Directors in Surgery.","Cadaver organ; Cholecystectomy; LapMentor; Porcine; Training; Virtual reality","Article; Belgium; cadaver; cholecystectomy; controlled study; experience; experimental model; human; laparoscopic surgery; medical student; minimally invasive surgery; organ; porcine cadaver organ; priority journal; prospective study; randomized controlled trial; simulation; simulator; skill; surgical technology; swine; training; virtual reality; virtual reality simulation training; animal; animal model; cadaver; clinical competence; computer interface; education; laparoscopic cholecystectomy; medical education; pig; procedures; simulation training; teaching; Animals; Belgium; Cadaver; Cholecystectomy, Laparoscopic; Clinical Competence; Computer-Assisted Instruction; Education, Medical, Undergraduate; Educational Measurement; Humans; Models, Animal; Prospective Studies; Simulation Training; Swine; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84926660267
"Mohr P., Kerbl B., Donoser M., Schmalstieg D., Kalkofen D.","57014549200;55505216500;55924307900;55101019100;24822405100;","Retargeting technical documentation to augmented reality",2015,"Conference on Human Factors in Computing Systems - Proceedings","2015-April",,,"3337","3346",,30,"10.1145/2702123.2702490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950989114&doi=10.1145%2f2702123.2702490&partnerID=40&md5=4e44d9a91fe801aad08f2c083002fb3f","Graz University of Technology, Austria","Mohr, P., Graz University of Technology, Austria; Kerbl, B., Graz University of Technology, Austria; Donoser, M., Graz University of Technology, Austria; Schmalstieg, D., Graz University of Technology, Austria; Kalkofen, D., Graz University of Technology, Austria","We present a system which automatically transfers printed technical documentation, such as handbooks, to three dimensional Augmented Reality. Our system identifies the most frequent forms of instructions found in printed documentation, such as image sequences, explosion diagrams, textual annotations and arrows indicating motion. The analysis of the printed documentation works automatically, with minimal user input. The system only requires the documentation itself and a CAD model or 3D scan of the object described in the documentation. The output is a fully interactive Augmented Reality application, presenting the information from the printed documentation in 3D, registered to the real object. © Copyright 2015 ACM.","Augmented reality; Retargeting; Virtual reality","Computer aided design; Human computer interaction; Human engineering; Virtual reality; Augmented reality applications; CAD modeling; Explosion diagrams; Image sequence; Real objects; Retargeting; Technical documentations; Textual annotations; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84950989114
"Harboe G., Huang E.M.","14035468800;7201787769;","Real-world affinity diagramming practices: Bridging the paper-digital gap",2015,"Conference on Human Factors in Computing Systems - Proceedings","2015-April",,,"95","104",,43,"10.1145/2702123.2702561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951198341&doi=10.1145%2f2702123.2702561&partnerID=40&md5=bf58aea001973ba0c363c945a69b1074","IFI, University of Zurich, Binzmühlestrasse 14, Zürich, 8050, Switzerland","Harboe, G., IFI, University of Zurich, Binzmühlestrasse 14, Zürich, 8050, Switzerland; Huang, E.M., IFI, University of Zurich, Binzmühlestrasse 14, Zürich, 8050, Switzerland","Despite the availability of computer-based alternatives both for desktop and touch screen systems, a number of cooperative work processes still commonly rely on simple paper sticky notes. In this paper, we present the first in-depth investigation of the real-world practices of people who use paper-based affinity diagrams and similar clustering processes in their work, in order to identify challenges and requirements for technology support. Findings from retrospective and artifact-based interviews with 13 participants suggest ways in which the rich interactions and material affordances offered by paper are key to the process. Instead of seeking to replicate interactions with paper on a screen, simpler transfer of information between the physical and digital worlds has the potential to address many of the most pressing problems experienced in practice. We describe different types of technology integration and augmentation, with preliminary recommendations for different situations. © Copyright 2015 ACM.","Affinity diagrams; Augmented paper; Cooperative data analysis; Interview study; Paper clustering; Sticky notes","Clustering algorithms; Human engineering; Touch screens; Affinity diagram; Augmented paper; Interview study; Real-world practice; Sticky notes; Technology Integration; Technology support; Transfer of information; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84951198341
"Wang L., Qu Y.","57189346996;55521851400;","Research on cultural ecology of modern historical architecture based on digital simulation technology",2015,"International Journal of Simulation: Systems, Science and Technology","16","2B",,"8.1","8.6",,,"10.5013/IJSSST.a.16.2B.08","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969497809&doi=10.5013%2fIJSSST.a.16.2B.08&partnerID=40&md5=7b0d20bc2dc4f1d3b7c563ba1c0bb108","College of Landscape Architecture and Arts, Northwest Agriculture & Forestry UniversityShaanxi  712100, China","Wang, L., College of Landscape Architecture and Arts, Northwest Agriculture & Forestry UniversityShaanxi  712100, China; Qu, Y., College of Landscape Architecture and Arts, Northwest Agriculture & Forestry UniversityShaanxi  712100, China","With the continuous development of modern computer and network technology, the protection and development of the national cultural heritage and cultural resources has gradually entered the digital field. First, digital acquisition and storage technology for integrity protection of national cultural resources to provide a guarantee, digital restoration and reconstruction technology for the effective inheritance provides support; secondly, the virtual reality technology for the development of these cultural resources by expanding the space; at the same time, digital display and communication technology provides the widely shared platform. Among them, with respect to the digitization of earlier books, dance and other intangible cultural resources, as an important part of the intangible cultural heritage, for historical building digital development and application has become the world a new topic, particularly the remains of the city more, construction quality is good even in the use of historical buildings, with distinct characteristics of the times, but also the important material carrier of traditional culture, in recent years, also caused the attention and research of academic circles at home and abroad. The times calling for the current digital survival can not only improve the protection level of historical and cultural resources, but also can protect the digital protection and development of historical and cultural resources. Digital protection and development of historical and cultural resources, is the trend of the development of the times. © 2016, UK Simulation Society. All rights reserved.","Digital technology; Historic buildings; Protection and development","Historic preservation; Virtual reality; Communication technologies; Development and applications; Digital technologies; Historic buildings; Historical architectures; Intangible cultural heritages; Protection and development; Virtual reality technology; Technology transfer",Article,"Final","",Scopus,2-s2.0-84969497809
"Chiappino S., Morerio P., Marcenaro L., Regazzoni C.S.","55416549200;55364178700;6603377664;35513672400;","Bio-inspired relevant interaction modelling in cognitive crowd management",2015,"Journal of Ambient Intelligence and Humanized Computing","6","2",,"171","192",,17,"10.1007/s12652-014-0224-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925537935&doi=10.1007%2fs12652-014-0224-0&partnerID=40&md5=ba4c7b132a61fe2f6d8789cf729cf569","Department of Naval, Electric, Electronic and Telecommunication Engineering, University of Genoa, Via Opera Pia 11A, Genoa, 16145, Italy","Chiappino, S., Department of Naval, Electric, Electronic and Telecommunication Engineering, University of Genoa, Via Opera Pia 11A, Genoa, 16145, Italy; Morerio, P., Department of Naval, Electric, Electronic and Telecommunication Engineering, University of Genoa, Via Opera Pia 11A, Genoa, 16145, Italy; Marcenaro, L., Department of Naval, Electric, Electronic and Telecommunication Engineering, University of Genoa, Via Opera Pia 11A, Genoa, 16145, Italy; Regazzoni, C.S., Department of Naval, Electric, Electronic and Telecommunication Engineering, University of Genoa, Via Opera Pia 11A, Genoa, 16145, Italy","Cognitive algorithms, integrated in intelligent systems, represent an important innovation in designing interactive smart environments. More in details, Cognitive Systems have important applications in anomaly detection and management in advanced video surveillance. These algorithms mainly address the problem of modelling interactions and behaviours among the main entities in a scene. A bio-inspired structure is here proposed, which is able to encode and synthesize signals, not only for the description of single entities behaviours, but also for modelling cause–effect relationships between user actions and changes in environment configurations. Such models are stored within a memory (Autobiographical Memory) during a learning phase. Here the system operates an effective knowledge transfer from a human operator towards an automatic systems called Cognitive Surveillance Node (CSN), which is part of a complex cognitive JDL-based and bio-inspired architecture. After such a knowledge-transfer phase, learned representations can be used, at different levels, either to support human decisions, by detecting anomalous interaction models and thus compensating for human shortcomings, or, in an automatic decision scenario, to identify anomalous patterns and choose the best strategy to preserve stability of the entire system. Results are presented in a video surveillance scenario, where the CSN can observe two interacting entities consisting in a simulated crowd and a human operator. These can interact within a visual 3D simulator, where crowd behaviour is modelled by means of Social Forces. The way anomalies are detected and consequently handled is demonstrated, on synthetic and also on real video sequences, in both the user-support and automatic modes. © 2014, The Author(s).","Anomalous interactions; Bio-inspired learning; Cognitive systems; Crowd monitoring; Self organizing map","Anomaly detection; Behavioral research; Biomimetics; Cognitive systems; Conformal mapping; Intelligent systems; Knowledge management; Monitoring; Personnel; Security systems; Self organizing maps; Anomalous interactions; Autobiographical memory; Bio-inspired architectures; Bio-inspired learning; Interacting entities; Interaction modelling; Interactive smart environments; Real video sequences; Learning systems",Article,"Final","",Scopus,2-s2.0-84925537935
"Sigrist R., Rauter G., Marchal-Crespo L., Riener R., Wolf P.","37073446800;26423348100;26422829400;7003404145;55627877036;","Sonification and haptic feedback in addition to visual feedback enhances complex motor task learning",2015,"Experimental Brain Research","233","3",,"909","925",,66,"10.1007/s00221-014-4167-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925536399&doi=10.1007%2fs00221-014-4167-7&partnerID=40&md5=6f004eab1cf6f20bbc98b8c50cc3d111","Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland; University of Zurich, Zurich, Switzerland","Sigrist, R., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland, University of Zurich, Zurich, Switzerland; Rauter, G., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland, University of Zurich, Zurich, Switzerland; Marchal-Crespo, L., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland, University of Zurich, Zurich, Switzerland; Riener, R., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland, University of Zurich, Zurich, Switzerland; Wolf, P., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Sonneggstrasse 3 (ML G 57), Zurich, 8092, Switzerland, University of Zurich, Zurich, Switzerland","Concurrent augmented feedback has been shown to be less effective for learning simple motor tasks than for complex tasks. However, as mostly artificial tasks have been investigated, transfer of results to tasks in sports and rehabilitation remains unknown. Therefore, in this study, the effect of different concurrent feedback was evaluated in trunk-arm rowing. It was then investigated whether multimodal audiovisual and visuohaptic feedback are more effective for learning than visual feedback only. Naïve subjects (N = 24) trained in three groups on a highly realistic virtual reality-based rowing simulator. In the visual feedback group, the subject’s oar was superimposed to the target oar, which continuously became more transparent when the deviation between the oars decreased. Moreover, a trace of the subject’s trajectory emerged if deviations exceeded a threshold. The audiovisual feedback group trained with oar movement sonification in addition to visual feedback to facilitate learning of the velocity profile. In the visuohaptic group, the oar movement was inhibited by path deviation-dependent braking forces to enhance learning of spatial aspects. All groups significantly decreased the spatial error (tendency in visual group) and velocity error from baseline to the retention tests. Audiovisual feedback fostered learning of the velocity profile significantly more than visuohaptic feedback. The study revealed that well-designed concurrent feedback fosters complex task learning, especially if the advantages of different modalities are exploited. Further studies should analyze the impact of within-feedback design parameters and the transferability of the results to other tasks in sports and rehabilitation. © 2014, Springer-Verlag Berlin Heidelberg.","Augmented feedback; Haptic guidance; Movement sonification; Multimodal feedback; Robot-assisted learning; Rowing simulator","adult; Article; auditory feedback; error; facilitation; female; human; human experiment; learning; male; normal human; perception; priority journal; rowing; tactile feedback; task performance; ultrasound; virtual reality; visual feedback; hearing; learning; motor performance; physiology; psychomotor performance; sensory feedback; touch; vision; young adult; Adult; Auditory Perception; Feedback, Sensory; Female; Humans; Learning; Male; Motor Skills; Psychomotor Performance; Touch Perception; Visual Perception; Young Adult",Article,"Final","",Scopus,2-s2.0-84925536399
"Schoob A., Kundrat D., Kleingrothe L., Kahrs L.A., Andreff N., Ortmaier T.","56183773600;55892003500;56183934300;8715886200;6602635082;6603301682;","Tissue surface information for intraoperative incision planning and focus adjustment in laser surgery",2015,"International Journal of Computer Assisted Radiology and Surgery","10","2",,"171","181",,16,"10.1007/s11548-014-1077-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938077573&doi=10.1007%2fs11548-014-1077-x&partnerID=40&md5=fbf1a9ce8418b4efa50f549d6f700c18","Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany; FEMTO-ST Institute, Université de Franche-Comté/CNRS/ENSMM/UTBM, Besançon, 25000, France","Schoob, A., Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany; Kundrat, D., Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany; Kleingrothe, L., Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany; Kahrs, L.A., Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany; Andreff, N., FEMTO-ST Institute, Université de Franche-Comté/CNRS/ENSMM/UTBM, Besançon, 25000, France; Ortmaier, T., Institute of Mechatronic Systems, Leibniz Universität Hannover, Hanover, 30167, Germany","Purpose : Introducing computational methods to laser surgery are an emerging field. Focusing on endoscopic laser interventions, a novel approach is presented to enhance intraoperative incision planning and laser focusing by means of tissue surface information obtained by stereoscopic vision.Methods : Tissue surface is estimated with stereo-based methods using nonparametric image transforms. Subsequently, laser-to-camera registration is obtained by ablating a pattern on tissue substitutes and performing a principle component analysis for precise laser axis estimation. Furthermore, a virtual laser view is computed utilizing trifocal transfer. Depth-based laser focus adaptation is integrated into a custom experimental laser setup in order to achieve optimal ablation morphology. Experimental validation is conducted on tissue substitutes and ex vivo animal tissue.Results : Laser-to-camera registration gives an error between planning and ablation of less than 0.2 mm. As a result, the laser workspace can accurately be highlighted within the live views and incision planning can directly be performed. Experiments related to laser focus adaptation demonstrate that ablation geometry can be kept almost uniform within a depth range of 7.9 mm, whereas cutting quality significantly decreases when the laser is defocused.Conclusions : An automatic laser focus adjustment on tissue surfaces based on stereoscopic scene information is feasible and has the potential to become an effective methodology for optimal ablation. Laser-to-camera registration facilitates advanced surgical planning for prospective user interfaces and augmented reality extensions. © 2014, CARS.","Er:YAG laser surgery; Laser focus adjustment; Surface reconstruction; Tissue ablation","computer assisted surgery; depth perception; endoscopy; human; laser; low level laser therapy; procedures; Depth Perception; Endoscopy; Humans; Laser Therapy; Lasers; Surgery, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-84938077573
"Li L., Dai S.","56512494300;7203008798;","Bayesian neural network approach to hand gesture recognition system",2015,"2014 IEEE Chinese Guidance, Navigation and Control Conference, CGNCC 2014",,, 7007487,"2019","2023",,1,"10.1109/CGNCC.2014.7007487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922568333&doi=10.1109%2fCGNCC.2014.7007487&partnerID=40&md5=422b97a92b9a6c149817b7d4a41c41a7","Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, 100191, China","Li, L., Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, 100191, China; Dai, S., Science and Technology on Aircraft Control Laboratory, Beihang University, Beijing, 100191, China","This paper presents a hand gesture recognition system as a part of our virtual reality system called non-contact flight auxiliary (NCFAC) system. The system is developed using Bayesian neural network to translate hand gestures to corresponding commands and utilizes one hand gesture to prepare for collision detection. Cyberglove sensory glove and Flock of Birds motion tracker are applied to this system to extract hand features. The Bayesian neural network model is trained and tested with different sample groups. Experiment shows that our system is able to recognize 16 kinds of hand gestures with the accuracy of 95.6% and greater generalization capability. The system can also be extended and use other algorithms for future works. © 2014 IEEE.","Bayesian neural network; Gesture recognition; Glove; Virtual reality","Neural networks; Palmprint recognition; Virtual reality; Bayesian neural network models; Bayesian neural networks; Collision detection; Flock of Birds; Generalization capability; Glove; Hand-gesture recognition; Virtual reality system; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84922568333
"Hou L., Li J.","57199859224;56829796200;","Landscape design system based on virtual reality",2015,"Metallurgical and Mining Industry","7","6",,"504","512",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941269544&partnerID=40&md5=eaba9c5a3e867367a34c810d13981e33","College of Art, Agricultural University of Hebei, Baoding, Hebei, 071000, China; Faculty of Art, Langfang Teachers University, Langfang, Hebei, 065000, China","Hou, L., College of Art, Agricultural University of Hebei, Baoding, Hebei, 071000, China; Li, J., Faculty of Art, Langfang Teachers University, Langfang, Hebei, 065000, China","City Park landscape design can be divided into two parts including city park green space system planning, landscape spatial planning and specific space design, it is a way of thinking and solutions dealing with the relationship between artificial and natural environment. This thesis taking virtual reality design of city park landscape design as a starting point based on augmented reality by visual computing of virtual reality of landscape design system. Aiming at the problem that the applications of the augmented reality technology in various fields are mainly dominated by the functions of browsing and displaying, lack of the interaction with users, this article studies the key technology of the augmented reality based on visual calculation. The transfer matrixes among the coordinate systems are determined through the mark points on the two already known images, which achieves the virtual-real registration of AR system. Secondly, the method of virtual-real synthesis display is designed in the environment of OpenGL, through which the images of virtual objects and real scenes are displayed in the same window. Finally, this article sums up the interactive operation functions supplied for users into the three aspects of the interaction with the real world, the interaction with the virtual object and the interaction with the history, and designs a kind of method of utilizing relevant parameters of virtual-real synthesis to store and call out the virtual-real synthesis effect in real scene, and the living examples verify the effectiveness of the method. © Metallurgical and Mining Industry, 2015.","Landscape design; Virtual computing; Virtual reality","Application programming interfaces (API); Augmented reality; Virtual reality; Augmented reality technology; Co-ordinate system; Green space system planning; Interactive operations; Landscape design; Natural environments; Virtual computing; Virtual real synthesis; Design",Article,"Final","",Scopus,2-s2.0-84941269544
"Chapoulie E., Tsandilas T., Oehlberg L., MacKay W., Drettakis G.","38662242200;6507282888;23991094100;7102699682;6603449604;","Finger-based manipulation in immersive spaces and the real world",2015,"2015 IEEE Symposium on 3D User Interfaces, 3DUI 2015 - Proceedings",,, 7131734,"109","116",,6,"10.1109/3DUI.2015.7131734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939200619&doi=10.1109%2f3DUI.2015.7131734&partnerID=40&md5=e2a02c5e045bd8cb1a45d08de3dc4280","Inria, Univ Paris-Sud, France","Chapoulie, E., Inria, Univ Paris-Sud, France; Tsandilas, T., Inria, Univ Paris-Sud, France; Oehlberg, L., Inria, Univ Paris-Sud, France; MacKay, W., Inria, Univ Paris-Sud, France; Drettakis, G., Inria, Univ Paris-Sud, France","Immersive environments that approximate natural interaction with physical 3D objects are designed to increase the user's sense of presence and improve performance by allowing users to transfer existing skills and expertise from real to virtual environments. However, limitations of current Virtual Reality technologies, e.g., low-fidelity real-time physics simulations and tracking problems, make it difficult to ascertain the full potential of finger-based 3D manipulation techniques. This paper decomposes 3D object manipulation into the component movements, taking into account both physical constraints and mechanics. We fabricate five physical devices that simulate these movements in a measurable way under experimental conditions. We then implement the devices in an immersive environment and conduct an experiment to evaluate direct finger-based against ray-based object manipulation. The key contribution of this work is the careful design and creation of physical and virtual devices to study physics-based 3D object manipulation in a rigorous manner in both real and virtual setups. © 2015 IEEE.","Finger-based manipulation; Immersive Cube-like Displays; Real/virtual world comparison","Virtual reality; 3D object manipulations; Experimental conditions; Finger-based manipulation; Immersive; Immersive environment; Physical constraints; Real/virtual world comparison; Virtual reality technology; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84939200619
"Nabioyuni M., Bowman D.A.","57218219407;7202508735;","An Evaluation of the Effects of Hyper-Natural Components of Interaction Fidelity on Locomotion Performance in Virtual Reality",2015,"International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments, ICAT-EGVE 2015",,,,"167","174",,8,"10.2312/egve.20151325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994865676&doi=10.2312%2fegve.20151325&partnerID=40&md5=f3db2d10ac35041de78cf07da799959e","Instittue Center for Human-Computer Interaction, Department of Computer Science, Virginia Tech, United States","Nabioyuni, M., Instittue Center for Human-Computer Interaction, Department of Computer Science, Virginia Tech, United States; Bowman, D.A., Instittue Center for Human-Computer Interaction, Department of Computer Science, Virginia Tech, United States","Virtual reality (VR) locomotion techniques that approximate real-world walking often have lower performance than fully natural real walking due to moderate interaction fidelity. Other techniques with moderate fidelity, however, are intentionally designedto enhance users'abilities beyond whatis possible in the realworld. We compared such hyper-natural techniques to their natural counterparts on a wide range of locomotion tasks for a variety of measures. The evaluation also considered two independent components of interaction fidelity: bio-mechanics and transfer function. The results show that hyper-natural transfer functions can improve locomotion speed and some aspects of user satisfaction, although this can come at the expense of accuracy for complicated path-following tasks. On the other hand, hyper-natural techniques designed to provide biomechanical assistance had lower performance and user acceptance than those based on natural walking movements. These results contribute to a deeper understanding of the effects of interaction fidelity and designer intent for VR interaction techniques. © The Eurographics Association 2015.",,"Biomechanics; Function evaluation; Transfer functions; Virtual reality; Independent components; Interaction techniques; Locomotion technique; Natural components; Path following; User acceptance; User satisfaction; Walking movements; Walking aids",Conference Paper,"Final","",Scopus,2-s2.0-84994865676
"Ramírez H., Mendoza E., Mendoza M., González E.","55485752300;54913546200;56374559400;57158450700;","Application of Augmented Reality in Statistical Process Control, to Increment the Productivity in Manufacture",2015,"Procedia Computer Science","75",,,"213","220",,10,"10.1016/j.procs.2015.12.240","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964022114&doi=10.1016%2fj.procs.2015.12.240&partnerID=40&md5=b72b19848e5dd425501a211e2ff40963","Automated Data Systems, Santiago Tapia #991 Pte. Col. Centro, Monterrey, 64000, Mexico; Tecnologico de Monterrey, Campus Monterrey, Ave. Eugenio Garza Sada #2501 Sur Col. Tecnologico, Monterrey, 64849, Mexico","Ramírez, H., Automated Data Systems, Santiago Tapia #991 Pte. Col. Centro, Monterrey, 64000, Mexico, Tecnologico de Monterrey, Campus Monterrey, Ave. Eugenio Garza Sada #2501 Sur Col. Tecnologico, Monterrey, 64849, Mexico; Mendoza, E., Automated Data Systems, Santiago Tapia #991 Pte. Col. Centro, Monterrey, 64000, Mexico; Mendoza, M., Automated Data Systems, Santiago Tapia #991 Pte. Col. Centro, Monterrey, 64000, Mexico; González, E., Tecnologico de Monterrey, Campus Monterrey, Ave. Eugenio Garza Sada #2501 Sur Col. Tecnologico, Monterrey, 64849, Mexico","In education from the standpoint of looking for new techniques to train staff more efficiently, especially when they have a lot of turnover of employees, the technology always helps to improve on this area. For the statistical process control, data acquisition of the manufacturing processes is very important and this can be affected if the technician who use the measure tools doesn't know how to use them correctly. On the other hand augmented reality (AR) is an alternative to the traditional training methods, where one can receive instructions on real-time on which tool use and how to use it depending of what is measuring. It's proposed on this work, if augmented reality can improve time to train a person and with this spend less money and have better prepared staff. For this experiment two groups of young engineers were given a set of instructions to do a quality process measuring. On one team a manual instructions was provided and on the other team an application of augmented reality where the subject can watch on site the instructions on real time of the process. With the analysis of the results it was determined that the group with the augmented do the process up to 30% faster and with less money than the group with the manual instructions, and also analyze the future works about this subject. © 2015 The Authors.","Augmented Reality; Education; Statistical Process Control","Augmented reality; Data acquisition; Education; Manufacture; Statistical process control; Technology transfer; Manufacturing process; Quality process; Real time; Tool use; Training methods; Young engineer; Process control",Conference Paper,"Final","",Scopus,2-s2.0-84964022114
"Levin Pt M.F., Weiss Ot P.L., Keshner Pt E.A.","56155750700;56540010800;7003942504;","Emergence of virtual reality as a tool for upper limb rehabilitation: Incorporation of motor control and motor learning Principles",2015,"Physical Therapy","95","3",,"415","425",,146,"10.2522/ptj.20130579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924225097&doi=10.2522%2fptj.20130579&partnerID=40&md5=475a42ee88266a9711d56f0f2965dda1","School of Physical and Occupational Therapy, McGill University, 3654 Promenade Sir William Osler, Montreal, Quebec  H3G 1Y5, Canada; Centre for Interdisciplinary Research in Rehabilitation, Jewish Rehabilitation Hospital, Laval, QC, Canada; Department of Occupational Therapy, University of Haifa Mount Carmel, Haifa, Israel; Department of Physical Therapy, College of Health Professions and Social Work, Temple University, Philadelphia, PA, United States","Levin Pt, M.F., School of Physical and Occupational Therapy, McGill University, 3654 Promenade Sir William Osler, Montreal, Quebec  H3G 1Y5, Canada, Centre for Interdisciplinary Research in Rehabilitation, Jewish Rehabilitation Hospital, Laval, QC, Canada; Weiss Ot, P.L., Department of Occupational Therapy, University of Haifa Mount Carmel, Haifa, Israel; Keshner Pt, E.A., Department of Physical Therapy, College of Health Professions and Social Work, Temple University, Philadelphia, PA, United States","The primary focus of rehabilitation for individuals with loss of upper limb movement as a result of acquired brain injury is the relearning of specific motor skills and daily tasks. This relearning is essential because the loss of upper limb movement often results in a reduced quality of life. Although rehabilitation strives to take advantage of neuroplastic processes during recovery, results of traditional approaches to upper limb rehabilitation have not entirely met this goal. In contrast, enriched training tasks, simulated with a wide range of low-to high-end virtual reality-based simulations, can be used to provide meaningful, repetitive practice together with salient feedback, thereby maximizing neuroplastic processes via motor learning and motor recovery. Such enriched virtual environments have the potential to optimize motor learning by manipulating practice conditions that explicitly engage motivational, cognitive, motor control, and sensory feedback-based learning mechanisms. The objectives of this article are to review motor control and motor learning principles, to discuss how they can be exploited by virtual reality training environments, and to provide evidence concerning current applications for upper limb motor recovery. The limitations of the current technologies with respect to their effectiveness and transfer of learning to daily life tasks also are discussed. © 2015 American Physical Therapy Association.",,"arm; Brain Injuries; complication; computer interface; human; motor activity; pathophysiology; physiotherapy; psychomotor performance; Brain Injuries; Humans; Motor Activity; Physical Therapy Modalities; Psychomotor Performance; Upper Extremity; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84924225097
"Silva W.H.S., Lopes G.L.B., Yano K.M., Tavares N.S.A., Rego I.A.O., Da Costa Cavalcanti F.A.","56865512600;56865589600;56865302400;56865818800;56866000000;57000087100;","Effect of a rehabilitation program using virtual reality for balance and functionality of chronic stroke patients",2015,"Motriz. Revista de Educacao Fisica","21","3",,"237","243",,6,"10.1590/S1980-65742015000300003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942080414&doi=10.1590%2fS1980-65742015000300003&partnerID=40&md5=8d318783732c5e6789c6e5378611c22a","Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil; 150 Rue St. Norbert, Montreal, QC  H2X1G6, Canada","Silva, W.H.S., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil, 150 Rue St. Norbert, Montreal, QC  H2X1G6, Canada; Lopes, G.L.B., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil; Yano, K.M., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil; Tavares, N.S.A., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil; Rego, I.A.O., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil; Da Costa Cavalcanti, F.A., Department of Physical Therapy, Brazil Universidade Federal do Rio Grande do Norte, Av. Senador Salgado Filho 3000, Natal, Rio Grande do Norte, 59072-970, Brazil","This study aimed to investigate the effect of a rehabilitation program using virtual reality (VR) in addition to conventional therapy for improvement of balance (BERG scale) and functional independence (FIM scale) in chronic stroke patients. Ten individuals, mean age of 51.4 (± 6.7 years), participated of eight 60-minute sessions comprising kinesiotherapy (15min), Nintendo Wii (30min) and Learning transfer (15min) exercises. After training, nonparametric statistical analysis showed significant improvement in total FIM (p = .01) and BERG scores (p = .00), and in some of their subitems: FIM - dressing lower body (p = .01), transfer to bathtub/shower (p = .02) and locomotion: stairs (p = .03); BERG - reaching forward with outstretched arm (p = .01), retrieving object from the floor (p = .04), turning 360 (p = .01), placing alternate foot on step (p < .01), standing with one foot in front (p = .01), and one leg stand (p = .03). These findings suggest a positive influence of virtual reality exercises adjunct to conventional therapy on rehabilitation of balance and functionality post stroke, and indicate the feasibility of the proposed VR-based rehabilitation program.","Daily living activities; Postural balance; Rehabilitation; Virtual reality",,Article,"Final","",Scopus,2-s2.0-84942080414
"Saiano M., Pellegrino L., Casadio M., Summa S., Garbarino E., Rossi V., Dall'Agata D., Sanguineti V.","56236785100;56236941700;9735851100;55388082900;56237552600;57050789200;56538437600;7003348473;","Natural interfaces and virtual environments for the acquisition of street crossing and path following skills in adults with Autism Spectrum Disorders: A feasibility study",2015,"Journal of NeuroEngineering and Rehabilitation","12","1", 17,"","",,32,"10.1186/s12984-015-0010-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924181449&doi=10.1186%2fs12984-015-0010-z&partnerID=40&md5=ec71bc2eae9ddc7ccf1fc1b8ac4c8dee","Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy; Department Primary Care, ASL3 Genovese, Genoa, Italy","Saiano, M., Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy, Department Primary Care, ASL3 Genovese, Genoa, Italy; Pellegrino, L., Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy; Casadio, M., Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy; Summa, S., Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy; Garbarino, E., Department Primary Care, ASL3 Genovese, Genoa, Italy; Rossi, V., Department Primary Care, ASL3 Genovese, Genoa, Italy; Dall'Agata, D., Department Primary Care, ASL3 Genovese, Genoa, Italy; Sanguineti, V., Department Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy","Background: Lack of social skills and/or a reduced ability to determine when to use them are common symptoms of Autism Spectrum Disorder (ASD). Here we examine whether an integrated approach based on virtual environments and natural interfaces is effective in teaching safety skills in adults with ASD. We specifically focus on pedestrian skills, namely street crossing with or without traffic lights, and following road signs. Methods: Seven adults with ASD explored a virtual environment (VE) representing a city (buildings, sidewalks, streets, squares), which was continuously displayed on a wide screen. A markerless motion capture device recorded the subjects' movements, which were translated into control commands for the VE according to a predefined vocabulary of gestures. The treatment protocol consisted of ten 45-minutes sessions (1 session/week). During a familiarization phase, the participants practiced the vocabulary of gestures. In a subsequent training phase, participants had to follow road signs (to either a police station or a pharmacy) and to cross streets with and without traffic lights. We assessed the performance in both street crossing (number and type of errors) and navigation (walking speed, path length and ability to turn without stopping). To assess their understanding of the practiced skill, before and after treatment subjects had to answer a test questionnaire. To assess transfer of the learned skill to real-life situations, another specific questionnaire was separately administered to both parents/legal guardians and the subjects' personal caregivers. Results: One subject did not complete the familiarization phase because of problems with depth perception. The six subjects who completed the protocol easily learned the simple body gestures required to interact with the VE. Over sessions they significantly improved their navigation performance, but did not significantly reduce the errors made in street crossing. In the test questionnaire they exhibited no significant reduction in the number of errors. However, both parents and caregivers reported a significant improvement in the subjects' street crossing performance. Their answers were also highly consistent, thus pointing at a significant transfer to real-life behaviors. Conclusions: Rehabilitation of adults with ASD mainly focuses on educational interventions that have an impact in their quality of life, which includes safety skills. Our results confirm that interaction with VEs may be effective in facilitating the acquisition of these skills. © 2015 Saiano et al.; licensee BioMed Central.","Adults; Autism Spectrum Disorder; Kinect; Natural interfaces; Safety skills; Virtual environments","adult; Article; autism; clinical article; depth perception; feasibility study; gesture; human; information processing; life; male; movement (physiology); natural interface; patient safety; pedestrian; priority journal; questionnaire; skill; virtual environment; virtual reality; walking speed; Autism Spectrum Disorder; clinical trial; computer interface; daily life activity; environment; female; quality of life; Activities of Daily Living; Adult; Autism Spectrum Disorder; Environment; Feasibility Studies; Female; Humans; Male; Quality of Life; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84924181449
"Li S., Chen Y., Whittinghill D., Vorvoreanu M.","56301468400;56301553700;43462227400;15053834400;","A pilot study exploring augmented reality to increase motivation of Chinese college students learning English",2015,"Computers in Education Journal","6","1",,"23","33",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052799998&partnerID=40&md5=0f3b5cdd673ccb1a1e8c6f20199654e4","Computer Graphics Technology Department, Purdue University, United States","Li, S., Computer Graphics Technology Department, Purdue University, United States; Chen, Y., Computer Graphics Technology Department, Purdue University, United States; Whittinghill, D., Computer Graphics Technology Department, Purdue University, United States; Vorvoreanu, M., Computer Graphics Technology Department, Purdue University, United States","With the advent and accelerated development of augmented reality (AR), an increasing number of studies have been conducted to test the effectiveness of this technique in education. Few, however, have investigated how AR might influence students' motivation toward the learning of a second language. To address this gap in the literature, we used a combination of convenience sampling and criterion sampling to select five Chinese college students to evaluate an English vocabulary learning application built upon augmented reality technology. To assess student motivation, the ARCS motivational model was adopted. A semi-structured interview with open-ended questions was used to collect data. Participants indicated that though they were attracted by this tool at the beginning, their motivation level decreased toward the end of the study. An interpretation of our observations in the context of the ARCS model suggests three motivational issues. First, predefined AR materials failed to establish relevance to subjects' personal interests and previous experiences. Secondly, subjects' confidence seemed to have been negatively influenced due to their difficulty in achieving the stated learning objectives. Lastly, technical issues delayed the computer quickly identifying the triggering image and thus resulted in a noticeable lack of system responsiveness. It seems this delay decreased subjects' satisfaction and distracted their attention from the learning task. These factors seemed most determinative in compromising AR's effectiveness as a tool to increase student motivation toward English vocabulary learning. It must be stressed that this study is a pilot with too low number of subjects from which to make any binding generalizations. Nonetheless, these findings should provide useful insights toward the successful application of AR in the educational realm. The authors recommend further study with a larger number of subjects with a wider range of vocabulary samples and a more powerful computer capable of more quickly identifying the trigger image. © 2015 American Society for Engineering Education. All rights reserved.","ARCS Model; Augmented reality; English vocabulary learning; Learning motivation","Augmented reality; Computer aided instruction; Education computing; Motivation; ARCS model; Augmented reality technology; Learning motivation; Learning objectives; Open-ended questions; Semi structured interviews; Student motivation; Vocabulary learning; Students",Article,"Final","",Scopus,2-s2.0-85052799998
"Korzatkowski J., Kolsch M., Sciarini L.W.","57190618799;8726439500;35181884400;","Developing a low-cost, portable virtual environment for aircraft carrier launch officers",2015,"Proceedings of the Human Factors and Ergonomics Society","2015-January",,,"1844","1848",,,"10.1177/1541931215591398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981714801&doi=10.1177%2f1541931215591398&partnerID=40&md5=198f8ca2c47201716a12bb1827be0478","Naval Postgraduate School, United States","Korzatkowski, J., Naval Postgraduate School, United States; Kolsch, M., Naval Postgraduate School, United States; Sciarini, L.W., Naval Postgraduate School, United States","The primary purpose of a United States aircraft carrier is to transport its embarked air wing in order to project combat power through the launch and recovery of various aircraft. In order to get airborne, the air wing depends upon the skills of a small number of officers responsible for the safe and rapid launch of aircraft from the carrier deck. These officers, known as ""shooters"", receive initial classroom training on the systems they use then receive qualification to be launch officers through on-the-job training. Due to scheduling complexities the training to achieve qualification is disjointed and often requires trainees to go underway with different aircraft carriers to complete their training. The current approach results in burdens on the parent command, host commands, and the trainees. Of greater concern is the lack of consistency in the training of such a high risk activity. This paper describes the results of a job task analysis conducted to provide insights into the skills required to perform the duties of a launch officer. Further, the information from the job task analysis was examined and a representative finite state machine was developed and is presented. Finally, a portable, low-cost virtual environment created based on the work described above is discussed. It is proposed that the current virtual reality system used for this demonstration faithfully recreates the required attributes and scenarios to train launch officer tasks and that the prototype system, with proof of training transfer can reduce the burden on commands, trainees, and perhaps most importantly, provide consistent training. Copyright 2015 Human Factors and Ergonomics Society.",,"Aircraft carriers; Ergonomics; Human engineering; Job analysis; Personnel training; Virtual reality; Classroom training; Launch and recoveries; On the job trainings; Prototype system; Required attribute; Scheduling complexity; Task analysis; Virtual reality system; Training aircraft",Conference Paper,"Final","",Scopus,2-s2.0-84981714801
"Lara-Prieto V., Bravo-Quirino E., Rivera-Campa M.Á., Gutiérrez-Arredondo J.E.","35488233200;57188872275;57188879779;57188879898;","An Innovative Self-learning Approach to 3D Printing Using Multimedia and Augmented Reality on Mobile Devices",2015,"Procedia Computer Science","75",,,"59","65",,3,"10.1016/j.procs.2015.12.206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964050242&doi=10.1016%2fj.procs.2015.12.206&partnerID=40&md5=743bf88296623c18db96d7ed454208a3","Tecnológico de Monterrey, Campus Monterrey, Av. Garza Sada 2501 Sur, Monterrey NL, CP 64849, Mexico","Lara-Prieto, V., Tecnológico de Monterrey, Campus Monterrey, Av. Garza Sada 2501 Sur, Monterrey NL, CP 64849, Mexico; Bravo-Quirino, E., Tecnológico de Monterrey, Campus Monterrey, Av. Garza Sada 2501 Sur, Monterrey NL, CP 64849, Mexico; Rivera-Campa, M.Á., Tecnológico de Monterrey, Campus Monterrey, Av. Garza Sada 2501 Sur, Monterrey NL, CP 64849, Mexico; Gutiérrez-Arredondo, J.E., Tecnológico de Monterrey, Campus Monterrey, Av. Garza Sada 2501 Sur, Monterrey NL, CP 64849, Mexico","Technology is evolving rapidly and it is becoming harder to keep up its pace. At a university environment, important investments are required so that students and professors can have access to novel technology. However, it is also fundamental to know how to use this novel technology to exploit its benefits. Tecnológico de Monterrey, Campus Monterrey, recently acquired several 3D printers so that the students could get familiar with this rapid prototyping technology and use them to deliver better quality projects. Nevertheless, the new challenge was to administrate the 3D printing process including 3D model verification, STL file generation, printing time and raw material. Since students were not familiar with 3D printing, the expert had to spend a lot of time with them explaining the whole process from the modelling to the printing stage, and verifying their work to make an efficient use of 3D printing time and materials. To overcome this situation, it was decided to make use of augmented reality and multimedia applications to generate tutorials for self-learning the whole process of 3D printing. Nowadays, the wide spread of mobile devices and wireless technologies brings a huge potential to e-learning changing dramatically the traditional instructor-oriented scheme. The learning process can take place in an informal setting having the tutorials available on mobile devices by just scanning a quick response code, usually known as QR code. The first QR code enables a link to download the free augmented reality Layar app. Then, several images can be scanned using Layar to open each of the video tutorials. These videos explain graphically step by step of all the processes involved and give different options of software to be used. The tutorials are easy to follow so that any engineering or design student can learn from them. This case-study offers an innovative learning approach that fosters self-learning and a more efficient use of technology resources.","3D Printing; Augmented Reality; Self-learning","Augmented reality; Codes (symbols); E-learning; Education; Education computing; Engineering education; Mobile devices; Printing; Printing presses; Students; Technology transfer; Wireless telecommunication systems; 3-D printing; Innovative learning; Multimedia applications; Rapid prototyping technology; Self-learning; Technology resources; University environment; Wireless technologies; 3D printers",Conference Paper,"Final","",Scopus,2-s2.0-84964050242
"Gogouvitis X.V., Vosniakos G.-C.","55811339800;6701789136;","Construction of a virtual reality environment for robotic manufacturing cells",2015,"International Journal of Computer Applications in Technology","51","3",,"173","184",,12,"10.1504/IJCAT.2015.069331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929628929&doi=10.1504%2fIJCAT.2015.069331&partnerID=40&md5=dd2de4093158d5e5ca6d3310c1a9423b","School of Mechanical Engineering, National Technical University of Athens, Heroon Polytehneiou 9, Athens, 15780, Greece","Gogouvitis, X.V., School of Mechanical Engineering, National Technical University of Athens, Heroon Polytehneiou 9, Athens, 15780, Greece; Vosniakos, G.-C., School of Mechanical Engineering, National Technical University of Athens, Heroon Polytehneiou 9, Athens, 15780, Greece","This paper describes the approach, algorithms, tools and implementation regarding construction from scratch of a simulation environment for robotic manufacturing cells following the virtual reality paradigm. A semantic model of robots ensures generality of the system, whereas fundamental robotics algorithms can be built into the environment, examples implemented concerning inverse kinematics for robotic arms with six rotary axes (6R), fast collision detection based on bounding boxes and interactive trajectory planning. The environment was developed in C++ making use of an open 3D graphics interface and a cross-platform user interface framework supporting different setups, i.e., enhanced desktop (monitor, headmounted display, glove) and hybrid wall (monitor, projector, game-type remote controller). Objects involved are virtual reality modelling language (VRML) versions of models originally designed in conventional 3D computer-aided design (CAD) systems. A sample application covering the design and training domains is demonstrated corresponding to selecting, positioning and programming a 6R robotic arm tending a turning centre. Copyright © 2015 Inderscience Enterprises Ltd.","Collision detection; Robot kinematics; Robotic manufacturing cells; Semantic modelling; Trajectory planning; Virtual reality","C++ (programming language); Computer aided design; Display devices; Educational robots; Flexible manufacturing systems; Inverse kinematics; Modeling languages; Remote control; Robot programming; Robotic arms; Robotics; Semantics; Three dimensional computer graphics; User interfaces; Collision detection; Robot kinematics; Robotic manufacturing cells; Semantic modelling; Trajectory Planning; Virtual reality",Article,"Final","",Scopus,2-s2.0-84929628929
"Bates M., Saridaki M., Kolovou E., Mourlas C., Brown D., Burton A., Battersby S., Parsonage S., Yarnall T.","55936218900;48761881400;57070393300;35088636400;55553726573;56346819500;16067987600;57070245400;57070186900;","Designing location-based gaming applications with teenagers to address early school leaving",2015,"Proceedings of the European Conference on Games-based Learning","2015-January",,,"50","57",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955162412&partnerID=40&md5=8b5af1036e754d1607f5d10232f8ac3e","Interactive Systems Research Group, Nottingham Trent University, United Kingdom; Department of Communication and Media, School of Economic and Political Sciences, University of Athens, Ariu, Gaetana, Greece; Greenhat Interactive Ltd, Birmingham, United Kingdom","Bates, M., Interactive Systems Research Group, Nottingham Trent University, United Kingdom; Saridaki, M., Department of Communication and Media, School of Economic and Political Sciences, University of Athens, Ariu, Gaetana, Greece; Kolovou, E., Department of Communication and Media, School of Economic and Political Sciences, University of Athens, Ariu, Gaetana, Greece; Mourlas, C., Department of Communication and Media, School of Economic and Political Sciences, University of Athens, Ariu, Gaetana, Greece; Brown, D., Interactive Systems Research Group, Nottingham Trent University, United Kingdom; Burton, A., Interactive Systems Research Group, Nottingham Trent University, United Kingdom; Battersby, S., Interactive Systems Research Group, Nottingham Trent University, United Kingdom; Parsonage, S., Greenhat Interactive Ltd, Birmingham, United Kingdom; Yarnall, T., Interactive Systems Research Group, Nottingham Trent University, United Kingdom","Early school leaving (ESL) is an urgent and serious problem, both for individuals and society as a whole. Factors such as learning difficulties, social problems or a lack of motivation, guidance or support all contribute to ESL, although the situation varies across EU countries. High rates of ESL are detrimental to making lifelong learning a reality and increase the risk of unemployment, poverty and social exclusion. Since normally there is not a unique reason for leaving education or vocational training, answers are no easy. In response to these concerns, the Code RED project (http://www.coderedproject. eu) has been created to address the high proportion of drop out from Initial Vocational Education and Training (IVET) and ESL in the UK, Greece, Italy and Cyprus via the development of new games-based learning applications (both desktop and mobile) to inform young adults (aged 16+) of the issues surrounding ESL. Location-based gaming (LBG) applications represent a form of play that is designed to be undertaken on a device in motion which changes the game experience based on the location. The design of these products presents many challenges to developers surrounding user interfaces, processing power and the availability of space. The ARIS platform (Augmented Reality and Interactive Storytelling) covers a broad field of LBG design components such as geo-location data, location-sensitive informational objects, interactive dialogues and QR code input. As such, ARIS has been selected by Code RED researchers to teach LBG and mobile augmented reality design concepts and prototype new design ideas with young adults. This paper will discuss the issues which are contributing to ESL within the EU and report upon the results of a short term participatory design initiative within Code RED to co-design new location-based gaming applications with participating IVET students (aged 14+) to address these issues. In the UK, participating students were successful in formulating a game concept suitable for transfer into LBG surrounding lifestyle choices such as alcohol and drug abuse which may contribute to ESL. In Greece, participating students with learning disabilities were successful in creating a fictional 'solve the mystery' LBG using the ARIS platform. Students decided to focus the game's narrative on the issue of exclusion from school and jumping into fast conclusions during schooling years. In Italy, participating children were successful in designing an orienteering-based LBG to promote cultural heritage via exploration of an ancient castle. This process also enabled participants to research and learn more about this local landmark. The paper will discuss the application of the participatory design methodology between project partners and will document the LBG output from this process. Finally, the paper will identify how these products will be positioned as part of future work to address ESL.","ARIS; Early school leaving; Employability; Location-based games; Participatory design","Augmented reality; Availability; Codes (symbols); Design; Education; Location; Students; User interfaces; ARIS; Early school leaving; Employability; Location based games; Participatory design; Product design",Conference Paper,"Final","",Scopus,2-s2.0-84955162412
"Wener R., Panindre P., Kumar S., Feygina I., Smith E., Dalton J., Seal U.","6505968187;36344085700;27169014900;35726731200;57212645518;36139254200;57213974369;","Assessment of web-based interactive game system methodology for dissemination and diffusion to improve firefighter safety and wellness",2015,"Fire Safety Journal","72",,,"59","67",,3,"10.1016/j.firesaf.2015.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922694890&doi=10.1016%2fj.firesaf.2015.02.005&partnerID=40&md5=97610669ca7dcb9162c53d781594eb36","Department of Technology, Culture and Society, New York University, Brooklyn, NY  11201, United States; Department of Mechanical Engineering, New York University, Brooklyn, NY  11201, United States; Engineering Division, New York University - Abu Dhabi, PO Box 129188, United Arab Emirates; Fire Depart. of New York City, Brooklyn, NY  11201, United States; Chicago Fire Department (CFD), Chicago, IL  60607, United States; Bloomington Fire Department (BFD), Bloomington, MN  55420, United States","Wener, R., Department of Technology, Culture and Society, New York University, Brooklyn, NY  11201, United States; Panindre, P., Department of Mechanical Engineering, New York University, Brooklyn, NY  11201, United States; Kumar, S., Department of Mechanical Engineering, New York University, Brooklyn, NY  11201, United States, Engineering Division, New York University - Abu Dhabi, PO Box 129188, United Arab Emirates; Feygina, I., Department of Technology, Culture and Society, New York University, Brooklyn, NY  11201, United States; Smith, E., Fire Depart. of New York City, Brooklyn, NY  11201, United States; Dalton, J., Chicago Fire Department (CFD), Chicago, IL  60607, United States; Seal, U., Bloomington Fire Department (BFD), Bloomington, MN  55420, United States","This paper describes research to assess the efficacy of web-based interactive game-like training in comparison to traditional classroom training for improving firefighters' knowledge, to advance the dissemination, diffusion, implementation and adoption of new information stemming from research interventions in fire-service. A web-based, interactive multimedia training tool called ALIVE (Advanced Learning in Integrated Visual Environments) was developed, that simulates the critical decision-making aspects of firefighting, and imparts knowledge and training though interactive game playing. Field experiments were conducted in three cities with career and volunteer firefighters to assess the efficacy of ALIVE and its adaptability for use in different fire-service related topics. Analyses of the results showed that knowledge transfer and retention using ALIVE was better compared to traditional classroom training in almost every case. Firefighters trained with ALIVE performed significantly better post-training and on a long-term retention test than those who were trained in a classroom. They also rated their own acquisition of knowledge higher than did those who had participated in the classroom training. These results hold true for the participant population as a whole, as well as with career and volunteer firefighters, and across participants of different levels of firefighting experience. These findings suggest that ALIVE training may be a particularly efficacious tool for use as part of firefighter training programs. ©2015 Elsevier Ltd. All rights reserved.","Dissemination and diffusion; Firefighter safety and health; Firefighter training; Implementation and adoption of innovation; Interactive training; Training field experiment","Decision making; Diffusion; Emergency services; Fire extinguishers; Fires; Interactive computer systems; Knowledge management; Multimedia systems; Social networking (online); Websites; Adoption of innovations; Classroom training; Field experiment; Interactive multimedia; Interactive training; Knowledge transfer; Long-term retention; Safety and healths; Fire fighting equipment",Article,"Final","",Scopus,2-s2.0-84922694890
"Seo S.H., Geiskkovitch D., Nakane M., King C., Young J.E.","55954218200;56896186200;55953658500;56895536000;20434970900;","Poor Thing! Would You Feel Sorry for a Simulated Robot?: A comparison of empathy toward a physical and a simulated robot",2015,"ACM/IEEE International Conference on Human-Robot Interaction","2015-March",,,"125","132",,52,"10.1145/2696454.2696471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943526431&doi=10.1145%2f2696454.2696471&partnerID=40&md5=5ac18b53dc70b062ef9d8f67cfea37d1","University of Manitoba, Winnipeg, MB, Canada; ZenFri Inc., Winnipeg, MB, Canada","Seo, S.H., University of Manitoba, Winnipeg, MB, Canada; Geiskkovitch, D., University of Manitoba, Winnipeg, MB, Canada; Nakane, M., University of Manitoba, Winnipeg, MB, Canada; King, C., ZenFri Inc., Winnipeg, MB, Canada; Young, J.E., University of Manitoba, Winnipeg, MB, Canada","In designing and evaluating human-robot interactions and interfaces, researchers often use a simulated robot due to the high cost of robots and time required to program them. However, it is important to consider how interaction with a simulated robot differs from a real robot; that is, do simulated robots provide authentic interaction? We contribute to a growing body of work that explores this question and maps out simulated-versus-real differences, by explicitly investigating empathy: how people empathize with a physical or simulated robot when something bad happens to it. Our results suggest that people may empathize more with a physical robot than a simulated one, a finding that has important implications on the generalizability and applicability of simulated HRI work. Empathy is particularly relevant to social HRI and is integral to, for example, companion and care robots. Our contribution additionally includes an original and reproducible HRI experimental design to induce empathy toward robots in laboratory settings, and an experimentally validated empathy-measuring instrument from psychology for use with HRI. © 2015 ACM.","empathy; human-robot interaction; robot embodiment; simulated interaction","Human computer interaction; Machine design; Man machine systems; Robots; empathy; Growing bodies; High costs; Measuring instruments; Physical robots; Real robot; Simulated interactions; Simulated robot; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-84943526431
"Vélaz Y., Arce J.R., Gutiérrez T., Lozano-Rodero A., Suescun A.","36662969400;57194191682;18036923100;35090084900;24282105300;","The influence of interaction technology on the learning of assembly tasks using virtual reality",2014,"Journal of Computing and Information Science in Engineering","14","4", 041007,"","",,24,"10.1115/1.4028588","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907830996&doi=10.1115%2f1.4028588&partnerID=40&md5=5bff1b3e24e502112ce9273c861e09e8","Department of Mechanical Engineering, CEIT and TECNUN, University of Navarra, Manuel Lardizabal 15, San-Sebastián, 20018, Spain; Industry and Transport Division, C/Geldo-Parque Tecnologico de Bizkaia, Edificio 700, Derio, 48160, Spain; TECNALIA, San-Sebastián, 20009, Spain","Vélaz, Y., Department of Mechanical Engineering, CEIT and TECNUN, University of Navarra, Manuel Lardizabal 15, San-Sebastián, 20018, Spain; Arce, J.R., Department of Mechanical Engineering, CEIT and TECNUN, University of Navarra, Manuel Lardizabal 15, San-Sebastián, 20018, Spain; Gutiérrez, T., Industry and Transport Division, C/Geldo-Parque Tecnologico de Bizkaia, Edificio 700, Derio, 48160, Spain, TECNALIA, San-Sebastián, 20009, Spain; Lozano-Rodero, A., Department of Mechanical Engineering, CEIT and TECNUN, University of Navarra, Manuel Lardizabal 15, San-Sebastián, 20018, Spain; Suescun, A., Department of Mechanical Engineering, CEIT and TECNUN, University of Navarra, Manuel Lardizabal 15, San-Sebastián, 20018, Spain","This paper focuses on the use of virtual reality (VR) systems for teaching industrial assembly tasks and studies the influence of the interaction technology on the learning process. The experiment conducted follows a between-subjects design with 60 participants distributed in five groups. Four groups were trained on the target assembly task with a VR system, but each group used a different interaction technology: mouse-based, Phantom Omni® haptic, and two configurations of the Markerless Motion Capture (Mmocap) system (with 2D or 3D tracking of hands). The fifth group was trained with a video tutorial. A post-training test carried out the day after evaluated performance in the real task. The experiment studies the efficiency and effectiveness of each interaction technology for learning the task, taking in consideration both quantitative measures (such as training time, real task performance, evolution from the virtual task to real one), and qualitative data (user feedback from a questionnaire). Results show that there were no significant differences in the final performance among the five groups. However, users trained under mouse and 2D-tracking Mmocap systems took significantly less training time than the rest of the virtual modalities. This brings out two main outcomes: (1) the perception of collisions using haptics does not increase the learning transfer of procedural tasks demanding low motor skills and (2) Mmocap-based interactions can be valid for training this kind of tasks. Copyright © 2014 by ASME.","assembly training; haptic devices; knowledge transfer; learning; motion capture; virtual reality","Assembly trainings; Haptic devices; Knowledge transfer; learning; Motion capture; Virtual reality",Article,"Final","",Scopus,2-s2.0-84907830996
"Itoh Y., Klinker G.","56154865900;6603530980;","Performance and sensitivity analysis of INDICA: INteraction-Free DIsplay CAlibration for Optical See-Through Head-Mounted Displays",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948424,"171","176",,20,"10.1109/ISMAR.2014.6948424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945128507&doi=10.1109%2fISMAR.2014.6948424&partnerID=40&md5=a5f7e94ab15349a3b47d9c354073238e","Technische Universität München, Germany","Itoh, Y., Technische Universität München, Germany; Klinker, G., Technische Universität München, Germany","An issue in AR applications with Optical See-Through Head-Mounted Display (OST-HMD) is to correctly project 3D information to the current viewpoint of the user. Manual calibration methods give the projection as a black box which explains observed 2D-3D relationships well (Fig. 1). Recently, we have proposed an INteraction-free DIsplay CAlibration method (INDICA) for OST-HMD, utilizing camera-based eye tracking [7]. It reformulates the projection in two ways: a black box with an actual eye model (Recycle Setup), and a combination of an explicit display model and an eye model (Full Setup). Although we have shown the former performs more stably than a repeated SPAAM calibration, we could not yet prove whether the same holds for the Full Setup. More importantly, it is still unclear how the error in the calibration parameters affects the final results. Thus, the users can not know how accurately they need to estimate each parameter in practice. We provide: (1) the fact that the Full Setup performs as accurately as the Recycle Setup under a marker-based display calibration, (2) an error sensitivity analysis for both SPAAM and INDICA over the on-/offline parameters, and (3) an investigation of the theoretical sensitivity on an OST-HMD justified by the real measurements. © 2014 IEEE.","augmented; H.5.1 [Information Interfaces and Presentation]; Multimedia Information Systems - Artificial; virtual realities","Augmented reality; Calibration; Recycling; Sensitivity analysis; Sensory perception; Street traffic control; Technology transfer; Virtual reality; augmented; Calibration parameters; Display calibrations; Error sensitivity analysis; H.5.1 [Information interfaces and presentation]; Manual calibration; Multimedia information systems-artificial; Optical see-through head-mounted displays; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-84945128507
"Kurz D.","57197559989;","Thermal touch: Thermography-enabled everywhere touch interfaces for mobile augmented reality applications",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948403,"9","16",,8,"10.1109/ISMAR.2014.6948403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945143898&doi=10.1109%2fISMAR.2014.6948403&partnerID=40&md5=a116f7a8c70547675604edd4ef89e20f","Metaio GmbH, South Korea","Kurz, D., Metaio GmbH, South Korea","We present an approach that makes any real object a true touch interface for mobile Augmented Reality applications. Using infrared thermography, we detect residual heat resulting from a warm fingertip touching the colder surface of an object. This approach can clearly distinguish if a surface has actually been touched, or if a finger only approached it without any physical contact, and hence significantly less heat transfer. Once a touch has been detected in the thermal image, we determine the corresponding 3D position on the touched object based on visual object tracking using a visible light camera. Finally the 3D position of the touch is used by human machine interfaces for Augmented Reality providing natural means to interact with real and virtual objects. The emergence of wearable computers and head-mounted displays desires for alternatives to a touch screen, which is the primary user interface in handheld Augmented Reality applications. Voice control and touchpads provide a useful alternative to interact with wearables for certain tasks, but particularly common interaction tasks in Augmented Reality require to accurately select or define 3D points on real surfaces. We propose to enable this kind of interaction by simply touching the respective surface with a fingertip. Based on tests with a variety of different materials and different users, we show that our method enables intuitive interaction for mobile Augmented Reality with most common objects. © 2014 IEEE.","Artificial, augmented, virtual realities - Evaluation/methodology; H.5.1 [Multimedia Information Systems]; H.5.2 [User Interfaces]: Input devices and strategies - Graphical user interfaces","Graphical user interfaces; Heat transfer; Helmet mounted displays; Thermography (imaging); Touch screens; User interfaces; Virtual reality; Wearable computers; Wearable technology; Evaluation/methodology; H.5.1 [multimedia information systems]; Handheld augmented realities; Human Machine Interface; Input devices and strategies; Intuitive interaction; Mobile augmented reality; Visual object tracking; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84945143898
"Knorr S.B., Kurz D.","57214298094;57197559989;","Real-time illumination estimation from faces for coherent rendering",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings",,, 6948483,"349","350",,18,"10.1109/ISMAR.2014.6948483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945120597&doi=10.1109%2fISMAR.2014.6948483&partnerID=40&md5=4915b51fc0c30e4b5ac5eb1fcb85b029","Metaio GmbH, South Korea","Knorr, S.B., Metaio GmbH, South Korea; Kurz, D., Metaio GmbH, South Korea","We showcase a method for estimating the real-world lighting conditions within a scene based on the visual appearance of the user's face captured in a single image of a monocular user-facing RGB camera. The implementation is based on our ISMAR 2014 paper [8]. The light reflected from the face towards the camera is measured and the most plausible real-world lighting condition explaining the measurement is estimated in real-time based on knowledge acquired in a one-time pre-processing of a set of images of different faces under known illumination. The estimated illumination is instantly used for the rendering of the virtual objects. © 2014 IEEE.","Augmented Reality; Coherent Rendering; Computer Graphics; Computer Vision; Demo; Illumination Estimation; Inverse Lighting; ISMAR 2014; Machine Learning; Monocular; Precomputed Radiance Transfer; Real-Time; Spherical Harmonics","Artificial intelligence; Augmented reality; Cameras; Computer graphics; Computer vision; Face recognition; Learning systems; Lighting; Coherent Rendering; Demo; Illumination estimation; ISMAR 2014; Monocular; Precomputed radiance transfer; Real time; Spherical harmonics; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-84945120597
"Gasco J., Patel A., Ortega-Barnett J., Branch D., Desai S., Kuo Y.F., Luciano C., Rizzi S., Kania P., Matuyauskas M., Banerjee P., Roitberg B.Z.","25651346500;55872445100;55872799500;56450305700;55329860700;7403230584;7004699651;16310730600;55872987900;56451176800;35580367600;33968027300;","Virtual reality spine surgery simulation: An empirical study of its usefulness",2014,"Neurological Research","36","11",,"968","973",,39,"10.1179/1743132814Y.0000000388","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918773938&doi=10.1179%2f1743132814Y.0000000388&partnerID=40&md5=5fc079e5fc03200e369e9c4f47872f0f","Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Division of Epidemiology and Biostatistics, Preventive Medicine Department, University of Texas Medical Branch, Galveston, TX, United States; Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Section of Neurosurgery, University of ChicagoIL, United States","Gasco, J., Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Patel, A., Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Ortega-Barnett, J., Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Branch, D., Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Desai, S., Division of Neurosurgery, Department of Surgery, University of Texas Medical Branch, Galveston, TX, United States; Kuo, Y.F., Division of Epidemiology and Biostatistics, Preventive Medicine Department, University of Texas Medical Branch, Galveston, TX, United States; Luciano, C., Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Rizzi, S., Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Kania, P., Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Matuyauskas, M., Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Banerjee, P., Department of Mechanical and Industrial Engineering, University of Illinois, Chicago, IL, United States; Roitberg, B.Z., Section of Neurosurgery, University of ChicagoIL, United States","Objective: This study explores the usefulness of virtual simulation training for learning to place pedicle screws in the lumbar spine.Methods: Twenty-six senior medical students anonymously participated and were randomized into two groups (A=no simulation; B=simulation). Both groups were given 15 minutes to place two pedicle screws in a sawbones model. Students in Group A underwent traditional visual/verbal instruction whereas students in Group B underwent training on pedicle screw placement in the ImmersiveTouch® simulator. The students in both groups then placed two pedicle screws each in a lumbar sawbones models that underwent triplanar thin slice computerized tomography and subsequent analysis based on coronal entry point, axial and sagittal deviations, length error, and pedicle breach. The average number of errors per screw was calculated for each group. Semi-parametric regression analysis for clustered data was used with generalized estimating equations accommodating a negative binomial distribution to determine any statistical difference of significance.Results: A total of 52 pedicle screws were analyzed. The reduction in the average number of errors per screw after a single session of simulation training was 53.7% (P = 0.0067). The average number of errors per screw in the simulation group was 0.96 versus 2.08 in the non-simulation group. The simulation group outperformed the non-simulation group in all variables measured. The three most benefited measured variables were length error (86.7%), coronal error (71.4%), and pedicle breach (66.7%).Conclusions: Computer-based simulation appears to be a valuable teaching tool for non-experts in a highly technical procedural task such as pedicle screw placement that involves sequential learning, depth perception, and understanding triplanar anatomy. © W. S. Maney & Son Ltd 2014.","Neurosurgery; Pedicle; Screw; Simulation; Virtual reality","Article; axial error; biomechanics; bone screw; computer assisted tomography; computer simulation; coronal error; human; length error; lumbar spine; pedicle breach error; pedicle screw; randomized controlled trial; sagittal error; spine stabilization; spine surgery; surgical error; three dimensional imaging; training; virtual reality; computer interface; education; evaluation study; neurosurgery; spinal cord; surgery; task performance; Humans; Neurosurgery; Spinal Cord; Task Performance and Analysis; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84918773938
"Chellali A., Zhang L., Sankaranarayanan G., Arikatla V.S., Ahn W., Derevianko A., Schwaitzberg S.D., Jones D.B., DeMoya M., Cao C.G.L.","26767578800;37076447700;15623319200;26654027600;9334513400;36893032600;7007036892;55387240300;57208993340;25957557800;","Validation of the VBLaST peg transfer task: a first step toward an alternate training standard",2014,"Surgical Endoscopy","28","10",,"2856","2862",,12,"10.1007/s00464-014-3538-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936111555&doi=10.1007%2fs00464-014-3538-2&partnerID=40&md5=275bd2100968f7dc0ddd99297a4eeaff","Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States; Department of Computer Engineering, University of Evry, IBISC Laboratory, Evry, France; Department of Mechanical Engineering, Tufts University, Medford, MA, United States; Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, NY, United States; Department of Surgery, Massachusetts General Hospital, Boston, MA, United States; Department of Surgery, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States; Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, 207 Russ Engineering Center, 3640 Colonel Glenn Hwy, Dayton, OH  45435, United States","Chellali, A., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States, Department of Computer Engineering, University of Evry, IBISC Laboratory, Evry, France; Zhang, L., Department of Mechanical Engineering, Tufts University, Medford, MA, United States; Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, NY, United States; Arikatla, V.S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, NY, United States; Ahn, W., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, NY, United States; Derevianko, A., Department of Surgery, Massachusetts General Hospital, Boston, MA, United States; Schwaitzberg, S.D., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Cambridge, MA, United States; Jones, D.B., Department of Surgery, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States; DeMoya, M., Department of Surgery, Massachusetts General Hospital, Boston, MA, United States; Cao, C.G.L., Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, 207 Russ Engineering Center, 3640 Colonel Glenn Hwy, Dayton, OH  45435, United States","Background: The FLS trainer lacks objective and automated assessments of laparoscopic performance and requires a large supply of relatively expensive consumables. Virtual reality simulation has a great potential as a training and assessment tool of laparoscopic skills and can overcome some limitations of the FLS trainer. This study was carried out to assess the value of our Virtual Basic Laparoscopic Surgical Trainer (VBLaST©) in the peg transfer task compared to the FLS trainer and its ability to differentiate performance between novice, intermediate, and expert groups. Methods: Thirty subjects were divided into three groups: novices (PGY1-2, n = 10), intermediates (PGY3-4, n = 10), and experts (PGY5, surgical fellows and attendings, n = 10). All subjects performed ten trials of the peg transfer task on each simulator. Assessment of laparoscopic performance was based on FLS scoring while a questionnaire was used for subjective evaluation. Results: The performance scores in the two simulators were correlated, though subjects performed significantly better in the FLS trainer. Experts performed better than novices only on the FLS trainer while no significant differences were observed between the other groups. Moreover, a significant learning effect was found on both trainers, with a greater improvement of performance on the VBLaST©. Finally, 82.6 % of the subjects preferred the FLS over the VBLaST© for surgical training which could be attributed to the novelty of the VR technology and existing deficiencies of the user interface for the VBLaST©. Conclusion: This study demonstrated that the VBLaST© reproduced faithfully some aspects of the FLS peg transfer task (such as color, size, and shape of the peg board, etc.) while other aspects require additional development. Future improvement of the user interface and haptic feedback will enhance the value of the system as an alternative to the FLS as the standard training tool for laparoscopic surgery skills. © 2014, Springer Science+Business Media New York.","Force feedback; Fundamentals of Laparoscopic Skills (FLS); Surgical training; Virtual Basic Laparoscopic Surgical Trainer (VBLaST); Virtual reality (VR)","adult; Article; clinical article; controlled study; eye hand coordination; feedback system; female; health care personnel; human; laparoscopic surgery; male; outcome assessment; priority journal; psychomotor performance; questionnaire; surgical training; task performance; virtual basic laparoscopic surgical trainer; virtual reality; clinical competence; computer interface; computer simulation; education; laparoscopy; middle aged; validation study; Adult; Clinical Competence; Computer Simulation; Feedback; Female; Humans; Laparoscopy; Male; Middle Aged; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84936111555
"Bouwsema H., Van Der Sluis C.K., Bongers R.M.","24605186000;6701309500;6602524481;","Effect of feedback during virtual training of grip force control with a myoelectric prosthesis",2014,"PLoS ONE","9","5", e98301,"","",,19,"10.1371/journal.pone.0098301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901410443&doi=10.1371%2fjournal.pone.0098301&partnerID=40&md5=7fd369ec7e575eb7529d7ced6ecd3750","University of Groningen, University Medical Center Groningen, Center for Human Movement Sciences, Groningen, Netherlands; University Medical Center Groningen, Center for Rehabilitation, Groningen, Netherlands","Bouwsema, H., University of Groningen, University Medical Center Groningen, Center for Human Movement Sciences, Groningen, Netherlands; Van Der Sluis, C.K., University Medical Center Groningen, Center for Rehabilitation, Groningen, Netherlands; Bongers, R.M., University of Groningen, University Medical Center Groningen, Center for Human Movement Sciences, Groningen, Netherlands","The aim of this study was to determine whether virtual training improves grip force control in prosthesis use, and to examine which type of augmented feedback facilitates its learning most. Thirty-two able-bodied participants trained grip force with a virtual ball-throwing game for five sessions in a two-week period, using a myoelectric simulator. They received either feedback on movement outcome or on movement execution. Sixteen controls received training that did not focus on force control. Variability over learning was examined with the Tolerance-Noise-Covariation approach, and the transfer of grip force control was assessed in five test-tasks that assessed different aspects of force control in a pretest, a posttest and a retention test. During training performance increased while the variability in performance was decreased, mainly by reduction in noise. Grip force control only improved in the test-tasks that provided information on performance. Starting the training with a task that required low force production showed no transfer of the learned grip force. Feedback on movement execution was detrimental to grip force control, whereas feedback on movement outcome enhanced transfer of grip force control to tasks other than trained. Clinical implications of these results regarding virtual training of grip force control are discussed. © 2014 Bouwsema et al.",,"adult; article; controlled study; feedback system; female; functional status; human; male; matching test task; muscle stimulator; myoelectric control; object test task; outcome assessment; percentage test task; physical capacity; picture test task; pinch strength; plyometrics; task performance; tracking test task; virtual reality; arm; biomechanics; hand strength; limb prosthesis; motor performance; physiology; young adult; Adult; Arm; Artificial Limbs; Biomechanical Phenomena; Feedback; Female; Hand Strength; Humans; Male; Motor Skills; Young Adult",Article,"Final","",Scopus,2-s2.0-84901410443
"Covaci A., Olivier A.-H., Multon F.","56419559200;15761037000;6602128151;","Third person view and guidance for more natural motor behaviour in immersive basketball playing",2014,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"55","64",,19,"10.1145/2671015.2671023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911165254&doi=10.1145%2f2671015.2671023&partnerID=40&md5=96d2c2541f6beaa265768ac7208a6e8b","Middlesex University London, United Kingdom; Inria, France; M2S Lab, University Rennes2, Inria, France","Covaci, A., Middlesex University London, United Kingdom; Olivier, A.-H., Inria, France; Multon, F., M2S Lab, University Rennes2, Inria, France","The use of Virtual Reality (VR) in sports training is now widely studied with the perspective to transfer motor skills learned in virtual environments (VEs) to real practice. However precision motor tasks that require high accuracy have been rarely studied in the context of VE, especially in Large Screen Image Display (LSID) platforms. An example of such a motor task is the basketball free throw, where the player has to throw a ball in a 46cm wide basket placed at 4.2m away from her. In order to determine the best VE training conditions for this type of skill, we proposed and compared three training paradigms. These training conditions were used to compare the combinations of different user perspectives: first (1PP) and third-person (3PP) perspectives, and the effectiveness of visual guidance. We analysed the performance of eleven amateur subjects who performed series of free throws in a real and immersive 1:1 scale environment under the proposed conditions. The results show that ball speed at the moment of the release in 1PP was significantly lower compared to real world, supporting the hypothesis that distance is underestimated in large screen VEs. However ball speed in 3PP condition was more similar to the real condition, especially if combined with guidance feedback. Moreover, when guidance information was proposed, the subjects released the ball at higher - and closer to optimal - position (5-7% higher compared to no-guidance conditions). This type of information contributes to better understand the impact of visual feedback on the motor performance of users who wish to train motor skills using immersive environments. Moreover, this information can be used by exergames designers who wish to develop coaching systems to transfer motor skills learned in VEs to real practice. Copyright © 2014 by the Association for Computing Machinery, Inc (ACM).","Basketball training; Immersive room; Perception of distance in VR; Performance; Visual feedback","Virtual reality; Visual communication; Guidance feedbacks; Guidance information; Immersive; Immersive environment; Motor performance; Performance; Training conditions; Visual feedback; Sports",Conference Paper,"Final","",Scopus,2-s2.0-84911165254
"Gruber L., Langlotz T., Sen P., Hoherer T., Schmalstieg D.","57197371956;8250843500;7402920249;56159602900;55101019100;","Efficient and robust radiance transfer for probeless photorealistic augmented reality",2014,"Proceedings - IEEE Virtual Reality",,, 6802044,"15","20",,22,"10.1109/VR.2014.6802044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900535157&doi=10.1109%2fVR.2014.6802044&partnerID=40&md5=ac44806520e5403714f1820be9b679e1","Graz University of Technology, Austria; University of California, Santa Barbara, United States","Gruber, L., Graz University of Technology, Austria; Langlotz, T., Graz University of Technology, Austria; Sen, P., University of California, Santa Barbara, United States; Hoherer, T., University of California, Santa Barbara, United States; Schmalstieg, D., Graz University of Technology, Austria","Photorealistic Augmented Reality (AR) requires knowledge of the scene geometry and environment lighting to compute photometric registration. Recent work has introduced probeless photometric registration, where environment lighting is estimated directly from observations of reflections in the scene rather than through an invasive probe such as a reflective ball. However, computing the dense radiance transfer of a dynamically changing scene is computationally challenging. In this work, we present an improved radiance transfer sampling approach, which combines adaptive sampling in image and visibility space with robust caching of radiance transfer to yield real time framerates for photorealistic AR scenes with dynamically changing scene geometry and environment lighting. © 2014 IEEE.","Augmented reality; photometric registration; radiance transfer","Computational geometry; Lighting; Photometry; Virtual reality; Adaptive sampling; Environment lighting; Photo-realistic; Photometric registration; Radiance transfer; Real time; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84900535157
"Shim J., Kong M., Yang Y., Seo J., Han T.-D.","37562105900;56115823700;56115920500;35119473900;7202802357;","Interactive features based augmented reality authoring tool",2014,"Digest of Technical Papers - IEEE International Conference on Consumer Electronics",,, 6775902,"47","50",,5,"10.1109/ICCE.2014.6775902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898681293&doi=10.1109%2fICCE.2014.6775902&partnerID=40&md5=a6eb440ffc53aab54881ad53cc5ba9d1","Yonsei University, Seoul, South Korea","Shim, J., Yonsei University, Seoul, South Korea; Kong, M., Yonsei University, Seoul, South Korea; Yang, Y., Yonsei University, Seoul, South Korea; Seo, J., Yonsei University, Seoul, South Korea; Han, T.-D., Yonsei University, Seoul, South Korea","This paper intends to propose an authoring tool system that allows users to easily create augmented reality content with the application of marker based and gesture interactions. It is possible to generate a simplified form of marker based augmented reality content with the user interface by using Kinect that enables gesture interaction. We seek to provide methods for the user to actively interact with augmented reality content. The interaction method we put forth in this study is a gesture interaction approach capable of Transfer, Rotate, Enlarge, and Shrink an object by recognizing and tracking the user's bare hand. As for marker based interaction methods, there is one employing two markers and another using marker occlusion. We ascertain the positive results of this study from the user evaluation of an authoring tool system and marker based and gesture interactions. © 2014 IEEE.",,"Consumer electronics; Human computer interaction; User interfaces; Augmented reality authoring; Augmented reality content; Authoring tool; Bare-hand; Gesture interaction; Interaction methods; Interactive features; User evaluations; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84898681293
"Kelly J.W., Hammel W.W., Siegel Z.D., Sjolund L.A.","55346243800;56095556100;56094810300;55547706500;","Recalibration of perceived distance in virtual environments occurs rapidly and transfers asymmetrically across scale",2014,"IEEE Transactions on Visualization and Computer Graphics","20","4", 6777445,"588","595",,35,"10.1109/TVCG.2014.36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897409643&doi=10.1109%2fTVCG.2014.36&partnerID=40&md5=20a9acf9c3a6fc0511fd9c4b60755273","Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Department of Psychology, Iowa State University, Ames, IA 50011, United States","Kelly, J.W., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Hammel, W.W., Department of Psychology, Iowa State University, Ames, IA 50011, United States; Siegel, Z.D., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States; Sjolund, L.A., Department of Psychology, Virtual Reality Application Center, Iowa State University, Ames, IA 50011, United States","Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance. © 2014 IEEE.","Distance perception; recalibration; virtual reality","Depth perception; Experiments; Visual communication; Current projects; Distance perception; Immersive virtual reality; Improved distance; Perceived distances; Recalibrations; Visual feedback; Walking through; Virtual reality",Article,"Final","",Scopus,2-s2.0-84897409643
"Trojan J., Diers M., Fuchs X., Bach F., Bekrater-Bodmann R., Foell J., Kamping S., Rance M., Maaß H., Flor H.","56262402900;6603232814;56154824700;55695210800;42260980200;44760992500;6507437083;36791108000;7005301066;7006743137;","An augmented reality home-training system based on the mirror training and imagery approach",2014,"Behavior Research Methods","46","3",,"634","640",,28,"10.3758/s13428-013-0412-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904767056&doi=10.3758%2fs13428-013-0412-4&partnerID=40&md5=fe577624a9748b6fbfbe2bdb194b15b8","Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Department of Psychology, University of Koblenz-Landau, Landau, Germany; Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Psychology, Florida State University, Tallahassee, FL, United States","Trojan, J., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany, Department of Psychology, University of Koblenz-Landau, Landau, Germany; Diers, M., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Fuchs, X., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Bach, F., Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Bekrater-Bodmann, R., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Foell, J., Department of Psychology, Florida State University, Tallahassee, FL, United States; Kamping, S., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Rance, M., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany; Maaß, H., Institute of Applied Computer Science, Karlsruhe Institute of Technology, Karlsruhe, Germany; Flor, H., Department of Cognitive und Clinical Neuroscience, Central Institute of Mental Health, Heidelberg University, J 5, 68159 Mannheim, Germany","Mirror training and movement imagery have been demonstrated to be effective in treating several clinical conditions, such as phantom limb pain, stroke-induced hemiparesis, and complex regional pain syndrome. This article presents an augmented reality home-training system based on the mirror and imagery treatment approaches for hand training. A head-mounted display equipped with cameras captures one hand held in front of the body, mirrors this hand, and displays it in real time in a set of four different training tasks: (1) flexing fingers in a predefined sequence, (2) moving the hand into a posture fitting into a silhouette template, (3) driving a ""Snake"" video game with the index finger, and (4) grasping and moving a virtual ball. The system records task performance and transfers these data to a central server via the Internet, allowing monitoring of training progress. We evaluated the system by having 7 healthy participants train with it over the course of ten sessions of 15-min duration. No technical problems emerged during this time. Performance indicators showed that the system achieves a good balance between relatively easy and more challenging tasks and that participants improved significantly over the training sessions. This suggests that the system is well suited to maintain motivation in patients, especially when it is used for a prolonged period of time. © 2013 The Author(s).","Augmented reality; Complex regional pain syndrome; Imagery; Mirror training; Phantom limb pain; Rehabilitation; Stroke; Virtual reality","adult; agnosia; article; cerebrovascular accident; complex regional pain syndrome; equipment design; female; finger; hand; hand strength; human; male; middle aged; movement (physiology); paresis; physiology; psychotherapy; recreation; reproducibility; young adult; Adult; Complex Regional Pain Syndromes; Equipment Design; Female; Fingers; Hand; Hand Strength; Humans; Imagery (Psychotherapy); Male; Middle Aged; Movement; Paresis; Phantom Limb; Reproducibility of Results; Stroke; Video Games; Young Adult",Article,"Final","",Scopus,2-s2.0-84904767056
"Lu W., Nguyen L.-C., Chuah T.L., Do E.Y.-L.","24468740000;56156976500;55141347900;14824449600;","Effects of mobile AR-enabled interactions on retention and transfer for learning in art museum contexts",2014,"ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Media, Arts, Social Science, Humanities and Design 2014, Proceedings",,, 6935432,"3","11",,14,"10.1109/ISMAR-AMH.2014.6935432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912530498&doi=10.1109%2fISMAR-AMH.2014.6935432&partnerID=40&md5=79288fde10975377b0ec3f35c18dfaef","Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore, Singapore","Lu, W., Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore, Singapore; Nguyen, L.-C., Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore, Singapore; Chuah, T.L., Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore, Singapore; Do, E.Y.-L., Keio-NUS CUTE Center, Interactive and Digital Media Institute, National University of Singapore, Singapore, Singapore","In this paper, we describe an experiment to study the effect of mobile Augmented Reality (AR) on learning in art museum contexts. We created six original paintings and placed them in a mini art museum. We then created an AR application on the iPad to enable the artist to visually augment each painting by introducing animation. We then measured the ability of the visitors to remember the appearance of the paintings after 24 hours, as well as their ability to objectify the paintings. Experiment results show that while AR does improve retention and transfer of such art information, the benefits of AR are mediated by other factors such as interference from other elements of the exhibition, as well as subjects' own prior art experience and training. The use of AR may also produce unexpected benefits, such as providing users with a new perspective of the artwork, as well as increasing their curiosity and encouraging them to experiment with the technology. Such benefits may potentially improve the chances for learning and analytical activities to take place. © 2014 IEEE.","Evaluation; Learning; Museums; Software","Animation; Augmented reality; Computer software; Experiments; Museums; AR application; Art museums; Evaluation; Learning; Mobile Ar; Mobile augmented reality; Prior arts; Exhibitions",Conference Paper,"Final","",Scopus,2-s2.0-84912530498
"Ganier F., Hoareau C., Tisseau J.","6507178310;55975572000;6603486416;","Evaluation of procedural learning transfer from a virtual environment to a real situation: A case study on tank maintenance training",2014,"Ergonomics","57","6",,"828","843",,36,"10.1080/00140139.2014.899628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901692802&doi=10.1080%2f00140139.2014.899628&partnerID=40&md5=d3e64acf6619d1cb4b87e64de8cc64fe","Lab-STICC, UMR 6285 CNRS, Université Européenne de Bretagne, Université de Bretagne Occidentale, European Center for Virtual Reality, Plouzané, France; Lab-STICC, UMR 6285 CNRS, Université Européenne de Bretagne, École Nationale d'Ingénieurs de Brest, European Center for Virtual Reality, Plouzané, France","Ganier, F., Lab-STICC, UMR 6285 CNRS, Université Européenne de Bretagne, Université de Bretagne Occidentale, European Center for Virtual Reality, Plouzané, France; Hoareau, C., Lab-STICC, UMR 6285 CNRS, Université Européenne de Bretagne, Université de Bretagne Occidentale, European Center for Virtual Reality, Plouzané, France; Tisseau, J., Lab-STICC, UMR 6285 CNRS, Université Européenne de Bretagne, École Nationale d'Ingénieurs de Brest, European Center for Virtual Reality, Plouzané, France","Virtual reality opens new opportunities for operator training in complex tasks. It lowers costs and has fewer constraints than traditional training. The ultimate goal of virtual training is to transfer knowledge gained in a virtual environment to an actual real-world setting. This study tested whether a maintenance procedure could be learnt equally well by virtual-environment and conventional training. Forty-two adults were divided into three equally sized groups: virtual training (GVT® [generic virtual training]), conventional training (using a real tank suspension and preparation station) and control (no training). Participants then performed the procedure individually in the real environment. Both training types (conventional and virtual) produced similar levels of performance when the procedure was carried out in real conditions. Performance level for the two trained groups was better in terms of success and time taken to complete the task, time spent consulting job instructions and number of times the instructor provided guidance. Practitioner Summary: A key issue for virtual environments for training (VETs) is the transfer of skills to real situations. An experiment investigated whether skills acquired in a VET could be applied in a real situation. Results suggest that a procedure can be successfully transferred from the virtual to the real. © 2014 © 2014 Taylor & Francis.","knowledge transfer; maintenance; procedural learning; virtual environments for training (VET); virtual reality","E-learning; Knowledge management; Maintenance; Tanks (containers); Virtual reality; Knowledge transfer; Maintenance procedures; Maintenance training; Operator training; Performance level; Procedural learning; Real environments; Virtual training; Personnel training; adult; comparative study; computer simulation; computer-assisted instruction; Female; Humans; Inservice Training; Maintenance; Male; Manufacturing Industry; methods; Teaching; Transfer (Psychology); User-Computer Interface; Weapons; Young Adult; Adult; Computer Simulation; Computer-Assisted Instruction; Female; Humans; Inservice Training; Maintenance; Male; Manufacturing Industry; Teaching; Transfer (Psychology); User-Computer Interface; Weapons; Young Adult",Article,"Final","",Scopus,2-s2.0-84901692802
"Falah J., Khan S., Alfalah T., Alfalah S.F.M., Chan W., Harrison D.K., Charissis V.","55350011200;55869080000;56410895700;55583479900;55471377200;7403545546;22733653900;","Virtual Reality medical training system for anatomy education",2014,"Proceedings of 2014 Science and Information Conference, SAI 2014",,, 6918271,"752","758",,28,"10.1109/SAI.2014.6918271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909594509&doi=10.1109%2fSAI.2014.6918271&partnerID=40&md5=48368b5ac00dfd3e6a7d06f299067683","School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom; School of Business, Applied Science University, Amman, Jordan; King Abdullah II School for Information Technology, University of Jordan, Amman, Jordan","Falah, J., School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom; Khan, S., School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom; Alfalah, T., School of Business, Applied Science University, Amman, Jordan; Alfalah, S.F.M., King Abdullah II School for Information Technology, University of Jordan, Amman, Jordan; Chan, W., School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom; Harrison, D.K., School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom; Charissis, V., School of Engineering and Built Environment, Glasgow Caledonian University, Cowcaddens Road, Glasgow, G4 0BA, United Kingdom","Medical education is a dynamic field that witnesses continuous evolution and development. The employment of Virtual Reality (VR) based visualization and training environments in the delivery of anatomy teaching transfers the learning experience from one that involves memorising the structures without a true understanding of the 3-Dimensional (3D) relations, to a process that involves a thorough understanding of the structure based on visualisation rather than memorising, which makes the learning process more efficient and enjoyable, and less time consuming. This paper describes the development of a Virtual Reality and 3D visualisation system for anatomy teaching. The developed system offers a real-time 3D representation of the heart in an interactive VR environment that provides self-directed learning and assessment tools through a variety of interfaces and functionalities. To ensure the accuracy and precision of the developed system it was evaluated by a group of medical professionals. © 2014 The Science and Information (SAI) Organization.","3D; Anatomy; Heart; Medical Education; Virtual Reality","Heart; Medical education; Three dimensional computer graphics; Virtual reality; Visualization; 3D; 3d representations; Accuracy and precision; Anatomy; Learning experiences; Medical professionals; Medical training system; Self-directed learning; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84909594509
"Zollmann S., Hoppe C., Langlotz T., Reitmayr G.","8250843400;53163755900;8250843500;6602277577;","FlyAR: Augmented reality supported micro aerial vehicle navigation",2014,"IEEE Transactions on Visualization and Computer Graphics","20","4", 6777462,"560","568",,31,"10.1109/TVCG.2014.24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897370709&doi=10.1109%2fTVCG.2014.24&partnerID=40&md5=f983e7194988d8162c44ef8f383a2e7d","Graz University of Technology, Austria; Graz University of Technology, University of Otago, Austria","Zollmann, S., Graz University of Technology, Austria; Hoppe, C., Graz University of Technology, Austria; Langlotz, T., Graz University of Technology, University of Otago, Austria; Reitmayr, G., Graz University of Technology, Austria","Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle&amp;#146;s position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user's view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding. © 2014 IEEE.","Augmented reality; Micro aerial vehicles; Visualization","Augmented reality; Depth perception; Flow visualization; Micro air vehicle (MAV); Motion planning; Planning; Area of interest; Automatic flight; High resolution camera; Human-in-the-loop; Micro aerial vehicle; Physical environments; Spatial relationships; Visualization technique; Physical addresses; aircraft; computer interface; depth perception; devices; human; man machine interaction; miniaturization; orientation; physiology; procedures; three dimensional imaging; Aircraft; Depth Perception; Humans; Imaging, Three-Dimensional; Man-Machine Systems; Miniaturization; Orientation; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84897370709
"Araujo S.E.A., Delaney C.P., Seid V.E., Imperiale A.R., Bertoncini A.B., Nahas S.C., Cecconello I.","55788278900;7103378404;6506919015;6604032220;54681643900;7004706439;54979488500;","Short-duration virtual reality simulation training positively impacts performance during laparoscopic colectomy in animal model: Results of a single-blinded randomized trial - VR warm-up for laparoscopic colectomy",2014,"Surgical Endoscopy","28","9",,"2547","2554",,11,"10.1007/s00464-014-3500-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906937962&doi=10.1007%2fs00464-014-3500-3&partnerID=40&md5=90d44c151ec09287a2901d4632bc9429","Department of Gastroenterology, University of Sao Paulo Medical School, São Paulo, Brazil; Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil; Av. Dr. Eneas de Carvalho Aguiar, 255, São Paulo, SP 05403-000, Brazil; Case Western Reserve University Center for Skills and Simulation, Cleveland, OH, United States; Digestive Health Institute University Hospitals Case Medical Center, Case Western Reserve University, Cleveland, OH, United States; Digestive Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil","Araujo, S.E.A., Department of Gastroenterology, University of Sao Paulo Medical School, São Paulo, Brazil, Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil, Av. Dr. Eneas de Carvalho Aguiar, 255, São Paulo, SP 05403-000, Brazil; Delaney, C.P., Case Western Reserve University Center for Skills and Simulation, Cleveland, OH, United States, Digestive Health Institute University Hospitals Case Medical Center, Case Western Reserve University, Cleveland, OH, United States; Seid, V.E., Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil; Imperiale, A.R., Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil; Bertoncini, A.B., Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil; Nahas, S.C., Department of Gastroenterology, University of Sao Paulo Medical School, São Paulo, Brazil, Colorectal Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil; Cecconello, I., Department of Gastroenterology, University of Sao Paulo Medical School, São Paulo, Brazil, Digestive Surgery Division, University of Sao Paulo Medical Center, São Paulo, SP, Brazil","Background: Several studies have demonstrated skills transfer after virtual reality (VR) simulation training in laparoscopic surgery. However, the impact of VR simulation training on transfer of skills related to laparoscopic colectomy remains not investigated. The present study aimed at determining the impact of VR simulation warm-up on performance during laparoscopic colectomy in the porcine model. Methods: Fourteen residents naive to laparoscopic colectomy as surgeons were randomly assigned in block to two groups. Seven trainees completed a 2-h VR simulator training in the laparoscopic sigmoid colectomy module (study group). The remaining seven surgeons (control group) underwent no intervention. On the same day, all participants performed a sigmoid colectomy with anastomosis on a pig. All operations were video recorded. Two board-certified expert colorectal surgeons independently assessed performance during the colectomy on the swine. Examiners were blinded to group assignment. The two examiners used a previously validated clinical instrument specific to laparoscopic colectomy. The primary outcome was the generic and specific skills score values. Results: Surgeons undergoing short-duration training on the VR simulator performed significantly better during laparoscopic colectomy on the pig regarding general and specific technical skills evaluation. The average score of generic skills was 17.2 (16.5-18) for the control group and 20.1 (16.5-22) for the study group (p = 0.002). The specific skills average score for the control group was 20.2 (19-21.5) and 24.2 (21-27.5) for the study group (p = 0.001). There was acceptable concordance (Kendall's W) regarding the video assessment of generic (W = 0.78) and specific skills (W = 0.84) between the two examiners. Conclusions: A single short-duration VR simulator practice positively impacted surgeons' generic and specific skills performance required to accomplish laparoscopic colectomy in the swine model. © 2014 Springer Science+Business Media.","Clinical skills; Colectomy; Colonic neoplasms; Credentialing; Education; Laparoscopy","article; clinical assessment tool; colon resection; control group; controlled study; human; laparoscopic colectomy; laparoscopic surgery; medical student; nonhuman; performance; porcine model; priority journal; randomized controlled trial; resident; simulation; simulator; single blind procedure; skill; surgeon; surgical training; teaching; videorecording; virtual reality; warm up; animal; animal model; clinical competence; colon resection; computer interface; computer simulation; education; female; laparoscopy; male; procedures; swine; time; Animals; Clinical Competence; Colectomy; Computer Simulation; Female; Humans; Laparoscopy; Male; Models, Animal; Single-Blind Method; Swine; Time Factors; User-Computer Interface; Video Recording",Article,"Final","",Scopus,2-s2.0-84906937962
"Li S., Chen Y., Whittinghill D.M., Vorvoreanu M.","56301468400;56301553700;43462227400;15053834400;","Exploring the potential for augmented reality to motivate English vocabulary learning in Chinese college students",2014,"ASEE Annual Conference and Exposition, Conference Proceedings",,,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905166159&partnerID=40&md5=ccacf60a42c5c97267777a8c06ab2fa7","Purdue University, West Lafayette, United States; Dept. of Computer Graphics Technology and Computer and Information Technology, Purdue University, West Lafayette, United States","Li, S., Purdue University, West Lafayette, United States; Chen, Y., Purdue University, West Lafayette, United States; Whittinghill, D.M., Dept. of Computer Graphics Technology and Computer and Information Technology, Purdue University, West Lafayette, United States; Vorvoreanu, M., Purdue University, West Lafayette, United States","With the advent and accelerated development of augmented reality (AR), an increasing number of studies have been conducted to test the effectiveness of this technique in education. Few, however, have investigated how AR might influence students' motivation toward learning of a second language. To address this gap in the literature, we used a combination of convenience sampling and criterion sampling to select five Chinese college students to evaluate an English vocabulary learning application built upon augmented reality technology. To assess student motivation, the ARCS motivational model was adopted. A semi-structured interview with openended questions was used to collect data. Participants indicated that though they were attracted by this tool at the beginning, their motivation level decreased toward the end of the study session. An interpretation of our observations in the context of the ARCS model suggests three motivational issues. First, predefined AR materials failed to establish relevance to subjects' personal interests and previous experiences. Secondly, subjects' confidence seemed to have been negatively influenced due to their difficulty in achieving the stated learning objectives. Lastly, technical issues delayed the computer quickly identifying the triggering image and thus resulted in a noticeable lack of system responsiveness. It seems this delay decreased subjects' satisfaction and distracted their attention from the learning task. These factors seemed most determinative in compromising AR's effectiveness as a tool to increase student motivation toward English vocabulary learning. It must be stressed that this study is a low subject N exploratory pilot not intended to produce binding generalizations. Nonetheless, these findings should provide useful insights toward the successful application of AR in the educational realm and identify potential causal factors that could form the foundation of future experimental research. The authors recommend further study with a larger number of subjects with a wider range of vocabulary sample and a more powerful viewing device capable of more quickly identifying the trigger images. © American Society for Engineering Education, 2014.","ARCS model; Augmented reality; English vocabulary learning; Learning motivation","Augmented reality; Computer aided instruction; Engineering education; Motivation; ARCS model; Augmented reality technology; Experimental research; Learning motivation; Learning objectives; Open-ended questions; Semi structured interviews; Vocabulary learning; Students",Conference Paper,"Final","",Scopus,2-s2.0-84905166159
"Custurǎ-Crǎciun D., Cochior D.","56006705600;19634420000;","Surgical virtual reality: Highlights in developing a surgical haptic device [Realitatea virtualǎ chirurgicalǎ: Repere în dezvoltarea unui dispozitiv haptic chirurgical]",2014,"EEA - Electrotehnica, Electronica, Automatica","62","1",,"105","112",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902662382&partnerID=40&md5=1d0ba23daa0dabf6ae3c9e46cba4c1ca","Universitatea Politehnicǎ Bucureşti, Spl. Independenţei, 313, Bucureşti, Romania; Universitatea Titu Maiorescu din Bucureşti Romania, Str. Dâmbovnicului 22, Bucureşti, Romania","Custurǎ-Crǎciun, D., Universitatea Politehnicǎ Bucureşti, Spl. Independenţei, 313, Bucureşti, Romania; Cochior, D., Universitatea Titu Maiorescu din Bucureşti Romania, Str. Dâmbovnicului 22, Bucureşti, Romania","Just like simulators are a standard in aviation and aerospace sciences, we expect for surgical simulators to soon become a standard in medical applications. These will correctly instruct future doctors in surgical techniques without there being a need for hands on patient instruction. Using virtual reality by digitally transposing surgical procedures changes surgery in a revolutionary manner by offering possibilities for implementing new, much more efficient, learning methods, by allowing the practice of new surgical techniques and by improving surgeon abilities and skills. Perfecting haptic devices has opened the door to a series of opportunities in the fields of research, industry, nuclear science and medicine. Concepts purely theoretical at first, such as telerobotics, telepresence or telerepresentation, have become a practical reality as calculus techniques, telecommunications and haptic devices evolved, virtual reality taking a new leap. In the field of surgery barriers and controversies still remain, regarding implementation and generalization of surgical virtual simulators. These obstacles remain connected to the high costs of this yet fully sufficiently developed technology, especially in the domain of haptic devices.","Haptic device; Surgical simulator; Surgical virtual reality; Virtual surgical training","Aerospace engineering; E-learning; Medical applications; Transplantation (surgical); Virtual reality; Visual communication; Haptic devices; Learning methods; Surgical procedures; Surgical simulators; Surgical techniques; Surgical training; Tele-robotics; Virtual simulators; Surgical equipment",Article,"Final","",Scopus,2-s2.0-84902662382
"Jalink M.B., Goris J., Heineman E., Pierie J.P.E.N., Hoedemaker H.O.T.C.","56107478500;55841755100;7005529702;8656937000;55470561000;","Construct and concurrent validity of a Nintendo Wii video game made for training basic laparoscopic skills",2014,"Surgical Endoscopy","28","2",,"537","542",,22,"10.1007/s00464-013-3199-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899478909&doi=10.1007%2fs00464-013-3199-6&partnerID=40&md5=cad21a9dbdee0a93fac5828d9fd05461","Department of Surgery, University Medical Center Groningen, University of Groningen, De Brug, P.O. Box 30 001, 9700 RB Groningen, Netherlands; Wenckebach Institute, University Medical Center Groningen, University of Groningen, Groningen, Netherlands; Department of Surgery, University Medical Center Groningen, University of Groningen, Groningen, Netherlands; Postgraduate School of Medicine, University Medical Center Groningen, University of Groningen, Groningen, Netherlands; Department of Surgery, Medical Center Leeuwarden, Leeuwarden, Netherlands; Leeuwarden Institute for Minimally Invasive Surgery, Medical Center Leeuwarden, Leeuwarden, Netherlands","Jalink, M.B., Department of Surgery, University Medical Center Groningen, University of Groningen, De Brug, P.O. Box 30 001, 9700 RB Groningen, Netherlands; Goris, J., Wenckebach Institute, University Medical Center Groningen, University of Groningen, Groningen, Netherlands; Heineman, E., Department of Surgery, University Medical Center Groningen, University of Groningen, Groningen, Netherlands; Pierie, J.P.E.N., Postgraduate School of Medicine, University Medical Center Groningen, University of Groningen, Groningen, Netherlands, Department of Surgery, Medical Center Leeuwarden, Leeuwarden, Netherlands, Leeuwarden Institute for Minimally Invasive Surgery, Medical Center Leeuwarden, Leeuwarden, Netherlands; Hoedemaker, H.O.T.C., Wenckebach Institute, University Medical Center Groningen, University of Groningen, Groningen, Netherlands, Department of Surgery, University Medical Center Groningen, University of Groningen, Groningen, Netherlands","Background: Virtual reality (VR) laparoscopic simulators have been around for more than 10 years and have proven to be cost- and time-effective in laparoscopic skills training. However, most simulators are, in our experience, considered less interesting by residents and are often poorly accessible. Consequently, these devices are rarely used in actual training. In an effort to make a low-cost and more attractive simulator, a custom-made Nintendo Wii game was developed. This game could ultimately be used to train the same basic skills as VR laparoscopic simulators ought to. Before such a video game can be implemented into a surgical training program, it has to be validated according to international standards. Methods: The main goal of this study was to test construct and concurrent validity of the controls of a prototype of the game. In this study, the basic laparoscopic skills of experts (surgeons, urologists, and gynecologists, n = 15) were compared to those of complete novices (internists, n = 15) using the Wii Laparoscopy (construct validity). Scores were also compared to the Fundamentals of Laparoscopy (FLS) Peg Transfer test, an already established assessment method for measuring basic laparoscopic skills (concurrent validity). Results: Results showed that experts were 111% faster (P = 0.001) on the Wii Laparoscopy task than novices. Also, scores of the FLS Peg Transfer test and the Wii Laparoscopy showed a significant, high correlation (r = 0.812, P < 0.001). Conclusions: The prototype setup of the Wii Laparoscopy possesses solid construct and concurrent validity. © Springer Science+Business Media 2013.","Education; Laparoscopy; Simulator; Video games","adult; article; concurrent validity; construct validity; controlled study; education program; female; gynecologist; human; laparoscopic surgery; laparoscopic surgical instrument; male; motor performance; priority journal; recreation; simulator; skill; surgeon; surgical training; task performance; urologist; virtual reality; clinical competence; computer interface; computer simulation; education; laparoscopy; medical education; physician; procedures; standards; validation study; Clinical Competence; Computer Simulation; Education, Medical, Continuing; Humans; Laparoscopy; Physicians; User-Computer Interface; Video Games",Article,"Final","",Scopus,2-s2.0-84899478909
"Zhang Z., Zhang M., Chang Y., Esche S.K., Chassapis C.","55858936500;55859014100;55501373900;6603892215;7004391468;","An efficient method for creating virtual spaces for virtual reality",2014,"ASME International Mechanical Engineering Congress and Exposition, Proceedings (IMECE)","5",,,"","",,8,"10.1115/IMECE2014-37149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926381647&doi=10.1115%2fIMECE2014-37149&partnerID=40&md5=b5aa8fe03dd6ab9d308e4587187e4c7c","Stevens Institute of Technology, Hoboken, IN, United States","Zhang, Z., Stevens Institute of Technology, Hoboken, IN, United States; Zhang, M., Stevens Institute of Technology, Hoboken, IN, United States; Chang, Y., Stevens Institute of Technology, Hoboken, IN, United States; Esche, S.K., Stevens Institute of Technology, Hoboken, IN, United States; Chassapis, C., Stevens Institute of Technology, Hoboken, IN, United States","A virtual space (VS) is an indispensable component of a virtual environment (VE) in virtual reality (VR). Usually, it is created using general tools and skills that are independent of the users' specific applications and intents. Creating a VS by surveying the real world with traditional measuring tools or creating virtual features with CAD software involves many steps and thus is time consuming and complicated. This renders the construction of VEs difficult, impairs their flexibility and hampers their widespread usage. In this paper, an efficient method for creating VSs with a handheld camera is introduced. In this approach, the camera is used as a measuring tool that scans the real scene and obtains the corresponding surface information. This information is then used to generate a virtual 3D model through a series of data processing procedures. Firstly, the camera's pose is traced in order to locate the points of the scene's surface, whereby these surface points form a point cloud. Then, this point cloud is meshed and the mesh elements are textured automatically one by one. Unfortunately, the virtual 3D model resulting from this procedure represents an impenetrable solid and thus collision detection would prevent the avatars from entering into this VS. Therefore, an approach for eliminating this restriction is proposed here. Finally, a game-based virtual laboratory (GBVL) for an undergraduate mechanical engineering class was developed to demonstrate the feasibility of the proposed methodology. The model format used in Garry's Mod (GMod) is also found in other VEs, and therefore the method proposed here can be straightforwardly generalized to other VE implementations. Copyright © 2014 by ASME.","GBVL; GMod; Kinect; VR","Cameras; Data handling; Three dimensional computer graphics; Virtual reality; Collision detection; GBVL; GMod; Kinect; Processing procedures; Surface information; Virtual laboratories; VR; Computer aided design",Conference Paper,"Final","",Scopus,2-s2.0-84926381647
"Wong Sarver N., Beidel D.C., Spitalnick J.S.","55897023000;7006749171;23480308700;","The Feasibility and Acceptability of Virtual Environments in the Treatment of Childhood Social Anxiety Disorder",2014,"Journal of Clinical Child and Adolescent Psychology","43","1",,"63","73",,30,"10.1080/15374416.2013.843461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892480783&doi=10.1080%2f15374416.2013.843461&partnerID=40&md5=25af832fdacd7b2a6c0a7f6a8144bc08","Department of Psychology, University of Central Florida, United States; Virtually Better, Inc., United States","Wong Sarver, N., Department of Psychology, University of Central Florida, United States; Beidel, D.C., Department of Psychology, University of Central Florida, United States; Spitalnick, J.S., Virtually Better, Inc., United States","Two significant challenges for the dissemination of social skills training programs are the need to assure generalizability and provide sufficient practice opportunities. In the case of social anxiety disorder, virtual environments may provide one strategy to address these issues. This study evaluated the utility of an interactive virtual school environment for the treatment of social anxiety disorder in preadolescent children. Eleven children with a primary diagnosis of social anxiety disorder between 8 to 12 years old participated in this initial feasibility trial. All children were treated with Social Effectiveness Therapy for Children, an empirically supported treatment for children with social anxiety disorder. However, the in vivo peer generalization sessions and standard parent-assisted homework assignments were substituted by practice in a virtual environment. Overall, the virtual environment programs were acceptable, feasible, and credible treatment components. Both children and clinicians were satisfied with using the virtual environment technology, and children believed it was a high-quality program overall. In addition, parents were satisfied with the virtual environment augmented treatment and indicated that they would recommend the program to family and friends. Findings indicate that the virtual environments are viewed as acceptable and credible by potential recipients. Furthermore, they are easy to implement by even novice users and appear to be useful adjunctive elements for the treatment of childhood social anxiety disorder. © 2014 Copyright Taylor and Francis Group, LLC.",,"article; behavior disorder; child; feasibility study; female; human; male; parent; patient satisfaction; phobia; program evaluation; psychological aspect; treatment outcome; virtual reality exposure therapy; Child; Child Behavior Disorders; Feasibility Studies; Female; Humans; Male; Parents; Patient Satisfaction; Phobic Disorders; Program Evaluation; Treatment Outcome; Virtual Reality Exposure Therapy",Article,"Final","",Scopus,2-s2.0-84892480783
"Martínez-Moreno J.M., Sánchez-González P., García A., González S., Cáceres C., Sánchez-Carrión R., Roig T., Tormos J.M., Gómez E.J.","55978248900;24512442400;57199310894;55977824900;7101909848;15074469100;56814733300;55156701400;7201729604;","A graphical tool for designing interactive video cognitive rehabilitation therapies",2014,"IFMBE Proceedings","41",,,"1803","1806",,1,"10.1007/978-3-319-00846-2_445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891330713&doi=10.1007%2f978-3-319-00846-2_445&partnerID=40&md5=6ae55a045af6cdcd28731542c37156a7","Biomedical Engineering and Telemedicine Centre, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain; Biomedical Research Networking Center in Bioengineering, Biomaterials and Nanomedicine, Madrid, Spain; Institut Guttmann Neurorehabilitation Hospital, Badalona, Spain; Lavinia Interactiva, Barcelona, Spain","Martínez-Moreno, J.M., Biomedical Engineering and Telemedicine Centre, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain, Biomedical Research Networking Center in Bioengineering, Biomaterials and Nanomedicine, Madrid, Spain; Sánchez-González, P., Biomedical Engineering and Telemedicine Centre, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain, Biomedical Research Networking Center in Bioengineering, Biomaterials and Nanomedicine, Madrid, Spain; García, A., Institut Guttmann Neurorehabilitation Hospital, Badalona, Spain; González, S., Lavinia Interactiva, Barcelona, Spain; Cáceres, C., Lavinia Interactiva, Barcelona, Spain; Sánchez-Carrión, R., Institut Guttmann Neurorehabilitation Hospital, Badalona, Spain; Roig, T., Institut Guttmann Neurorehabilitation Hospital, Badalona, Spain; Tormos, J.M., Institut Guttmann Neurorehabilitation Hospital, Badalona, Spain; Gómez, E.J., Biomedical Engineering and Telemedicine Centre, ETSI Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain, Biomedical Research Networking Center in Bioengineering, Biomaterials and Nanomedicine, Madrid, Spain","Acquired Brain Injury (ABI) has become one of the most common causes of neurological disability in developed countries. Cognitive disorders result in a loss of independence and therefore patients' quality of life. Cognitive rehabilitation aims to promote patients' skills to achieve their highest degree of personal autonomy. New technologies such as interactive video, whereby real situations of daily living are reproduced within a controlled virtual environment, enable the design of personalized therapies with a high level of generalization and a great ecological validity. This paper presents a graphical tool that allows neuropsychologists to design, modify, and configure interactive video therapeutic activities, through the combination of graphic and natural language. The tool has been validated creating several Activities of Daily Living and a preliminary usability evaluation has been performed showing a good clinical acceptance in the definition of complex interactive video therapies for cognitive rehabilitation. © Springer International Publishing Switzerland 2014.","Acquired brain injury; Activities of daily living; Cognitive rehabilitation; Graphical tool; Interactive video","Biochemical engineering; Medical computing; Virtual reality; Acquired brain injuries; Activities of Daily Living; Cognitive rehabilitation; Graphical tools; Interactive video; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-84891330713
"Reiner M., Gelfeld T.M.","7005454900;35797166900;","Estimating mental workload through event-related fluctuations of pupil area during a task in a virtual world",2014,"International Journal of Psychophysiology","93","1",,"38","44",,28,"10.1016/j.ijpsycho.2013.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904747464&doi=10.1016%2fj.ijpsycho.2013.11.002&partnerID=40&md5=c3f8772295e9b21d05e844f238c4d117","Israel Institute of Technology, Israel","Reiner, M., Israel Institute of Technology, Israel; Gelfeld, T.M., Israel Institute of Technology, Israel","Monitoring mental load for optimal performance has become increasingly central with the recently evolving need to cope with exponentially increasing amounts of data. This paper describes a non-intrusive, objective method to estimate mental workload in an immersive virtual reality system, through analysis of frequencies of pupil fluctuations. We tested changes in mental workload with a number of task-repetitions, level of predictability of the task and the effect of prior experience in predictable task performance, on mental workload of unpredictable task performance. Two measures were used to calculate mental workload: the ratio of Low Frequency to High Frequency components of pupil fluctuations, and the High Frequency alone, all extracted from the Power Spectrum Density of pupil fluctuations. Results show that mental workload decreases with a number of repetitions, creating a mode in which the brain acts as an automatic controller. Automaticity during training occurs only after a minimal number of repetitions, which once achieved, resulted in further improvements in the performance of unpredictable motor tasks, following training in a predictable task. These results indicate that automaticity is a central component in the transfer of skills from highly predictable to low predictable motor tasks. Our results suggest a potentially applicable method to brain-computer-interface systems that adapt to human mental workload, and provide intelligent automated support for enhanced performance. © 2013 Elsevier B.V.","High/Low Frequency components; Mental workload; Power Spectral Density; Virtual word; Virtual-hand-illusion","adult; article; brain computer interface; comparative study; computer; computer program; controlled study; ecological validity; eye tracker; female; fundus camera; human; human experiment; male; medical device; mental load; normal human; power spectrum; pupil fluctuation; right handedness; stereoscopic vision; task performance; virtual reality; virtual reality modeling language; visual illusion; computer interface; evoked response; eye movement; illusion; learning; physiology; psychology; psychomotor performance; pupil; workload; young adult; Adult; Evoked Potentials; Eye Movements; Female; Humans; Illusions; Learning; Male; Psychomotor Performance; Pupil; User-Computer Interface; Workload; Young Adult",Article,"Final","",Scopus,2-s2.0-84904747464
"Lin Y., Wang X., Wu F., Chen X., Wang C., Shen G.","55714532600;7501852131;55986005400;55739098400;8147957500;55612194500;","Development and validation of a surgical training simulator with haptic feedback for learning bone-sawing skill",2014,"Journal of Biomedical Informatics","48",,,"122","129",,52,"10.1016/j.jbi.2013.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899473640&doi=10.1016%2fj.jbi.2013.12.010&partnerID=40&md5=6716666e72c4783e04a615941d6de5bf","School of Mechanical Engineering, State Key Laboratory of Mechanical System and Vibration, Institute of Biomedical Manufacturing and Life Quality Engineering, Shanghai 200240, China; Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University, School of Medicine, Shanghai 200011, China","Lin, Y., School of Mechanical Engineering, State Key Laboratory of Mechanical System and Vibration, Institute of Biomedical Manufacturing and Life Quality Engineering, Shanghai 200240, China; Wang, X., Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University, School of Medicine, Shanghai 200011, China; Wu, F., School of Mechanical Engineering, State Key Laboratory of Mechanical System and Vibration, Institute of Biomedical Manufacturing and Life Quality Engineering, Shanghai 200240, China; Chen, X., School of Mechanical Engineering, State Key Laboratory of Mechanical System and Vibration, Institute of Biomedical Manufacturing and Life Quality Engineering, Shanghai 200240, China; Wang, C., School of Mechanical Engineering, State Key Laboratory of Mechanical System and Vibration, Institute of Biomedical Manufacturing and Life Quality Engineering, Shanghai 200240, China; Shen, G., Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University, School of Medicine, Shanghai 200011, China","Objective: Bone sawing or cutting is widely used for bone removal processes in bone surgery. It is an essential skill that surgeons should execute with a high level of experience and sensitive force perception. Surgical training simulators, with virtual and haptic feedback functions, can offer a safe, repeatable and cost-effective alternative to traditional surgeries. In this research, we developed a surgical training simulator with virtual and haptic force feedback for maxillofacial surgery, and we validated the effects on the learning of bone-sawing skills through empirical evaluation. Methods: Omega.6 from Force Dimension was employed as the haptic device, and Display300 from SenseGraphices was used as the 3D stereo display. The voxel-based model was constructed using computed tomography (CT) images, and the virtual tools were built through reverse engineering. The multi-point collision detection method was applied for haptic rendering to test the 3D relationship between the virtual tool and the bone voxels. Bone-sawing procedures in maxillofacial surgery were simulated with a virtual environment and real-time haptic feedback. A total of 25 participants (16 novices and 9 experienced surgeons) were included in 2 groups to perform the bone-sawing simulation for assessing the construct validity. Each of the participants completed the same bone-sawing procedure at the predefined maxillary region six times. For each trial, the sawing operative time, the maximal acceleration, and the percentage of the haptic force exceeding the threshold were recorded and analysed to evaluate the validity. After six trials, all of the participants scored the simulator in terms of safe force learning, stable hand control and overall performance to confirm the face validity. Moreover, 10 novices in 2 groups indentified the transfer validity on rapid prototype skull models by comparing the operative time and the maximal acceleration. Results: The analysed results of construct validity showed that the two groups significantly reduced their sawing operative times after six trials. Regarding maximal acceleration, the curve significantly descended and reached a plateau after the fifth repetition (novices) or third repetition (surgeons). Regarding safe haptic force, the novices obviously reduced the percentage of the haptic force exceeding the threshold, with statistical significance after four trials, but the surgeons did not show a significant difference. Moreover, the subjectively scored results demonstrated that the proposed simulator was more helpful for the novices than for the experienced surgeons, with scores of 8.31 and 7.22, respectively, for their overall performance. The experimental results on skill transference showed that the experimental group performed bone-sawing operation in lower maximal acceleration than control group with a significant difference (p<. 0.05). These findings suggested that the simulator training had positive effects on real sawing. Conclusions: The evaluation results proved the construct validity, face validity and the transfer validity of the simulator. These results indicated that this simulator was able to produce the effect of learning bone-sawing skill, and it could provide a training alternative for novices. © 2013 Elsevier Inc.","Bone sawing; Haptic; Simulator validation; Skill learning; Surgical training simulator","Bone; Computerized tomography; Haptic interfaces; Personnel training; Reverse engineering; Sawing; Simulators; Three dimensional; Three dimensional computer graphics; Tools; Transplantation (surgical); Virtual reality; Bone; Computerized tomography; Cost effectiveness; Display devices; Haptic interfaces; Personnel training; Reverse engineering; Sawing; Simulators; Stereo image processing; Surgery; Three dimensional computer graphics; Transplantation (surgical); Virtual reality; Bone sawing; Haptic; Simulator validation; Skill learning; Surgical training; Surgical equipment; Surgical equipment; adult; article; bone saw; computer assisted tomography; construct validity; female; human; male; maxillofacial surgery; medical education; normal human; operation duration; priority journal; simulator; skill; surgeon; surgical technique; surgical training; tactile feedback; task performance; validation study; Article; bone sawing skill; computer interface; computer simulation; construct validity; controlled study; empirical research; face validity; maxillofacial surgery; oral surgery; three dimensional imaging; transfer validity; validation study; virtual reality; bone; computer graphics; computer program; computer simulation; education; feedback system; learning; middle aged; orthopedics; procedures; reproducibility; surgery; young adult; Adult; Bone and Bones; Computer Graphics; Computer Simulation; Feedback; Female; Humans; Imaging, Three-Dimensional; Learning; Male; Middle Aged; Orthopedics; Reproducibility of Results; Software; Tomography, X-Ray Computed; User-Computer Interface; Young Adult",Article,"Final","",Scopus,2-s2.0-84899473640
"Bowyer S.A., Davies B.L., Rodriguez Y Baena F.","48261104600;56255206500;15132536100;","Active constraints/virtual fixtures: A survey",2014,"IEEE Transactions on Robotics","30","1", 6634270,"138","157",,145,"10.1109/TRO.2013.2283410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894502691&doi=10.1109%2fTRO.2013.2283410&partnerID=40&md5=d751d2a1d6ea25a7350b543ab349fcb5","Mechatronics in Medicine Laboratory, Department of Mechanical Engineering, Imperial College London, London, SW7 2AZ, United Kingdom; Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genova, 16163, Italy","Bowyer, S.A., Mechatronics in Medicine Laboratory, Department of Mechanical Engineering, Imperial College London, London, SW7 2AZ, United Kingdom; Davies, B.L., Mechatronics in Medicine Laboratory, Department of Mechanical Engineering, Imperial College London, London, SW7 2AZ, United Kingdom, Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genova, 16163, Italy; Rodriguez Y Baena, F., Mechatronics in Medicine Laboratory, Department of Mechanical Engineering, Imperial College London, London, SW7 2AZ, United Kingdom","Active constraints, also known as virtual fixtures, are high-level control algorithms which can be used to assist a human in man-machine collaborative manipulation tasks. The active constraint controller monitors the robotic manipulator with respect to the environment and task, and anisotropically regulates the motion to provide assistance. The type of assistance offered by active constraints can vary, but they are typically used to either guide the user along a task-specific pathway or limit the user to within a ""safe"" region. There are several diverse methods described within the literature for applying active constraints, and these are surveyed within this paper. The active constraint research is described and compared using a simple generalized framework, which consists of three primary processes: 1) constraint definition, 2) constraint evaluation, and 3) constraint enforcement. All relevant research approaches for each of these processes, found using search terms associated to ""virtual fixture, "" ""active constraint"" and 'motion constraint,' are presented. © 2013 IEEE.","Haptics and haptic interfaces; Impedance/admittance control; Medical robots and systems; Physical human-robot interaction; Telerobotics","Algorithms; Fixtures (tooling); Surveys; Virtual reality; Active constraints; Haptics and haptic interfaces; Medical robots and systems; Motion constraints; Physical human-robot interactions; Research approach; Robotic manipulators; Tele-robotics; Haptic interfaces",Article,"Final","",Scopus,2-s2.0-84894502691
"Williams C.K., McKee P., Stefanovich A., Carnahan H.","36196045100;55907574200;55484577900;7004489168;","Using model hands for learning orthotic fabrication eric hagemann",2014,"American Journal of Occupational Therapy","68","1",,"86","94",,7,"10.5014/ajot.2014.009001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892613484&doi=10.5014%2fajot.2014.009001&partnerID=40&md5=c0b2beaa114cc47b4dcb24bfd6e4c76c","Department of Rehabilitation Science, University of Toronto, Toronto, ON, Canada; Graduate Department of Rehabilitation Science, Canada; Wilson Centre for Research in Education, University of Toronto, Toronto, ON, Canada; Department of Occupational Science and Occupational Therapy, University of Toronto, Toronto, ON, Canada; Memorial University of Newfoundland, St. Johns, NL A1C 5S7, Canada; Department of Occupational Science and Occupational Therapy, Canada; Wilson Centre for Research in Education, University of Toronto, ON, Canada","Williams, C.K., Department of Rehabilitation Science, University of Toronto, Toronto, ON, Canada, Graduate Department of Rehabilitation Science, Canada, Wilson Centre for Research in Education, University of Toronto, Toronto, ON, Canada; McKee, P., Department of Occupational Science and Occupational Therapy, University of Toronto, Toronto, ON, Canada; Stefanovich, A., Department of Occupational Science and Occupational Therapy, University of Toronto, Toronto, ON, Canada, ; Carnahan, H., Memorial University of Newfoundland, St. Johns, NL A1C 5S7, Canada, Department of Occupational Science and Occupational Therapy, Canada, Wilson Centre for Research in Education, University of Toronto, ON, Canada","Trainees could benefit from practicing orthotic fabrication on simulated hands with joint deformities. As a first step toward such training, we explored the use of a nonpathological model hand. Twenty-one participants were randomized into one of two groups that practiced using a persons right hand or a model right hand. One week later, all participants returned for a transfer test in which they made one orthosis on a persons left hand. All participants performance and orthoses were evaluated using a validated checklist and a global rating scale (GRS). Fabrication time for each orthosis also was recorded. The GRS score and fabrication time changed significantly over the course of practice. Trainees who practiced with the model hand made better orthoses during practice and on the transfer test, as measured with the checklists final product subscore. Instructional and contextual factors that may affect trainees performance and learning are discussed.",,"article; audiovisual equipment; checklist; controlled clinical trial; controlled study; education; equipment design; female; hand; human; learning; male; methodology; orthosis; paramedical personnel; professional competence; randomized controlled trial; time; vocational education; young adult; Allied Health Personnel; Checklist; Education, Professional; Educational Measurement; Equipment Design; Female; Hand; Humans; Male; Manikins; Models, Anatomic; Orthotic Devices; Practice (Psychology); Professional Competence; Time Factors; Young Adult",Article,"Final","",Scopus,2-s2.0-84892613484
"Lü X., Lu T., Kibert C.J., Viljanen M.","7404839744;7402684563;6602508604;56214697700;","A novel dynamic modeling approach for predicting building energy performance",2014,"Applied Energy","114",,,"91","103",,37,"10.1016/j.apenergy.2013.08.093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885933555&doi=10.1016%2fj.apenergy.2013.08.093&partnerID=40&md5=08c63065b6798d13f6ff10d27a525d8d","Department of Civil and Structural Engineering, School of Engineering, Aalto University, P.O. Box 12100, FIN-02015 Espoo, Finland; Powell Center for Construction and oEnvironment, University of Florida, P.O. Box 115703, Gainesville, FL 32611-5703, United States; College of Construction Engineering, Jilin University, Changchun, China","Lü, X., Department of Civil and Structural Engineering, School of Engineering, Aalto University, P.O. Box 12100, FIN-02015 Espoo, Finland, College of Construction Engineering, Jilin University, Changchun, China; Lu, T., Department of Civil and Structural Engineering, School of Engineering, Aalto University, P.O. Box 12100, FIN-02015 Espoo, Finland; Kibert, C.J., Powell Center for Construction and oEnvironment, University of Florida, P.O. Box 115703, Gainesville, FL 32611-5703, United States; Viljanen, M., Department of Civil and Structural Engineering, School of Engineering, Aalto University, P.O. Box 12100, FIN-02015 Espoo, Finland","This paper presents a new methodology for modeling building energy performance that addresses some important limitations of building simulation. This new methodology develops a physical model for accurately predicting indoor environmental conditions and energy consumption by selecting best match parameters and variables. The innovative aspect of the proposed methodology is the introduction of open and closed loop system approaches to dynamically model the complex interaction of factors that contribute to building thermal performance and their uncertainties. This allows simultaneous tracking of both the lead and lag times between heating excitations and indoor thermal responses to account for their mutually excitatory interaction. The model system is solved using a Laplace transform technique, with an explicit solution that includes physical and generalized parameters calibrated by measurements. Singular value decomposition techniques are applied to further determine the model variables for the best approximation using lower dimensions. As a result the model complexity and the model parameters and variables are minimized while still preserving the physical meaning of the model. A careful, detailed validation and assessment of the model performance is conducted using a case study of a dance hall at a swim center (R2&gt;0.9). A further validation of the model is also undertaken by assessing its forecasting capability against benchmark persistence models. The proposed model outperforms the benchmarks especially over longer time horizons. The methodology is useful in developing a minimal but comprehensive and accurate energy performance physical model which can reliably capture the dynamics of building thermal and energy performance. The proposed method can serve the needs of prediction and control applications in a wide variety of building types and can be incorporated into the most commonly used simulation models. © 2013 Elsevier Ltd.","Buildings; Energy consumption; Modeling method; Physical model","Closed loop systems; Computer simulation; Electric load forecasting; Energy efficiency; Energy utilization; Forecasting; Laplace transforms; Building energy performance; Building thermal performance; Dynamic modeling approach; Environmental conditions; Laplace transform techniques; Model method; Physical model; Singular value decomposition technique; Buildings; building construction; energy use; environmental conditions; numerical model; optimization; parameterization; simulation; tracking; uncertainty analysis",Article,"Final","",Scopus,2-s2.0-84885933555
"Rauter G., Sigrist R., Koch C., Crivelli F., Van Raai M., Riener R., Wolf P.","26423348100;37073446800;56024522400;55633747400;16305180100;7003404145;55627877036;","Transfer of complex skill learning from virtual to real rowing",2013,"PLoS ONE","8","12", e82145,"","",,24,"10.1371/journal.pone.0082145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893381578&doi=10.1371%2fjournal.pone.0082145&partnerID=40&md5=9f70db43e46ed8e315c51becb818a422","Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland; Medical Faculty, University of Zurich, Zurich, Switzerland","Rauter, G., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Sigrist, R., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Koch, C., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Crivelli, F., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Van Raai, M., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Riener, R., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland; Wolf, P., Sensory-Motor Systems (SMS) Lab, Institute of Robotics and Intelligent Systems (IRIS), ETH Zurich, Zurich, Switzerland, Medical Faculty, University of Zurich, Zurich, Switzerland","Simulators are commonly used to train complex tasks. In particular, simulators are applied to train dangerous tasks, to save costs, and to investigate the impact of different factors on task performance. However, in most cases, the transfer of simulator training to the real task has not been investigated. Without a proof for successful skill transfer, simulators might not be helpful at all or even counter-productive for learning the real task. In this paper, the skill transfer of complex technical aspects trained on a scull rowing simulator to sculling on water was investigated. We assume if a simulator provides high fidelity rendering of the interactions with the environment even without augmented feedback, training on such a realistic simulator would allow similar skill gains as training in the real environment. These learned skills were expected to transfer to the real environment. Two groups of four recreational rowers participated. One group trained on water, the other group trained on a simulator. Within two weeks, both groups performed four training sessions with the same licensed rowing trainer. The development in performance was assessed by quantitative biomechanical performance measures and by a qualitative video evaluation of an independent, blinded trainer. In general, both groups could improve their performance on water. The used biomechanical measures seem to allow only a limited insight into the rowers' development, while the independent trainer could also rate the rowers' overall impression. The simulator quality and naturalism was confirmed by the participants in a questionnaire. In conclusion, realistic simulator training fostered skill gains to a similar extent as training in the real environment and enabled skill transfer to the real environment. In combination with augmented feedback, simulator training can be further exploited to foster motor learning even to a higher extent, which is subject to future work. © 2013 Rauter et al.",,"adult; article; biomechanics; controlled study; feedback system; human; human experiment; learning; motor performance; normal human; rowing; simulator; skill transfer; virtual reality; computer interface; computer simulation; female; male; middle aged; questionnaire; ship; task performance; videorecording; Adult; Biomechanical Phenomena; Computer Simulation; Female; Humans; Learning; Male; Middle Aged; Questionnaires; Ships; Task Performance and Analysis; User-Computer Interface; Video Recording",Article,"Final","",Scopus,2-s2.0-84893381578
"Cowan B., Kapralos B.","55411698800;55946696900;","Spatial sound rendering for dynamic virtual environments",2013,"2013 18th International Conference on Digital Signal Processing, DSP 2013",,, 6622815,"","",,2,"10.1109/ICDSP.2013.6622815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888863809&doi=10.1109%2fICDSP.2013.6622815&partnerID=40&md5=b08e4c7a4470dd3a1dc65146ef7e12b7","Faculty of Business and Information Technology, University of Ontario, Institute of Technology, Oshawa, ON, Canada","Cowan, B., Faculty of Business and Information Technology, University of Ontario, Institute of Technology, Oshawa, ON, Canada; Kapralos, B., Faculty of Business and Information Technology, University of Ontario, Institute of Technology, Oshawa, ON, Canada","We present the details of a virtual sound rendering engine (VSRE) that is being developed for virtual environments and serious games. The VSRE incorporates innovative graphics processing unit - based methods to allow for the approximation of acoustical occlusion/diffraction and reverberation effects at interactive rates. In addition, the VSRE includes a GPU-based method that performs the one-dimensional convolution allowing for the incorporation of head-related transfer functions also at interactive rates. The VSRE is being developed as a research tool for examining multi-modal (audio-visual) interactions through the simple manipulation of the acoustic environment and audio parameters (sound quality), that will, through a series of human-based experiments, allow for the testing of the effect of varying these parameters may have on immersion, engagement, and visual fidelity perception within a virtual environment. Finally, we also provide a running time comparison of several one-dimensional convolution implementations. © 2013 IEEE.","3D sound; Graphics processing unit (GPU); Serious games; Spatial sound; Virtual environment","3D sound; Acoustic environment; Dynamic virtual environment; Graphics Processing Unit; Head related transfer function; Reverberation effects; Serious games; Spatial sound; Convolution; Digital signal processing; Program processors; Sound reproduction; Three dimensional computer graphics; Virtual reality; Computer graphics equipment",Conference Paper,"Final","",Scopus,2-s2.0-84888863809
"Varlet M., Filippeschi A., Ben-sadoun G., Ratto M., Marin L., Ruffaldi E., Bardy B.G.","36961417300;26666003000;55920844000;56026663600;7102404415;15023001900;7004216646;","Virtual reality as a tool to learn interpersonal coordination: Example of team rowing",2013,"Presence: Teleoperators and Virtual Environments","22","3",,"202","215",,8,"10.1162/PRES_a_00151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893508256&doi=10.1162%2fPRES_a_00151&partnerID=40&md5=5c8cc2e1122c12c9c389128237de63a2","Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpellier, France; PERCRO Lab, Scuola Superiore S. Anna, Pisa, Italy; Institut Universitaire de France, France","Varlet, M., Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpellier, France; Filippeschi, A., PERCRO Lab, Scuola Superiore S. Anna, Pisa, Italy; Ben-sadoun, G., Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpellier, France; Ratto, M., Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpellier, France; Marin, L., Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpellier, France; Ruffaldi, E., PERCRO Lab, Scuola Superiore S. Anna, Pisa, Italy; Bardy, B.G., PERCRO Lab, Scuola Superiore S. Anna, Pisa, Italy, Institut Universitaire de France, France","The success of interpersonal activities strongly depends on the coordination between our movements and those of others. Learning to coordinate with other people requires a long training time and is often limited by the difficulty of having people available at the same time and of giving them accurate and real-time feedback about their coordination. The goal of the present study was to determine in an indoor team rowing situation whether virtual-reality and motion-capture technologies can help the acquisition of interpersonal coordination. More specifically, we investigated the possibility for participants to (1) learn the skill of interpersonal coordination when training with a virtual teammate, (2) accelerate learning with real-time visual feedback, and (3) transfer this skill to synchronizing situations with a real teammate. Our results show that participants improved their coordination with both virtual and real teammates, and that this improvement was better for participants who received the feedback. Generally, our results demonstrate the interest of virtual reality for learning the coordination with other people; further, our results open promising training perspectives for team rowing but also for several other interpersonal activities. © 2014 by the Massachusetts Institute of Technology.",,"Interpersonal coordinations; Motion capture; Real-time feedback; Training time; Visual feedback; Personnel training; SportS; Virtual reality; Visual communication; Human resource management",Article,"Final","",Scopus,2-s2.0-84893508256
"Marty J., Carron T., Talbot S., Houzet G., Pernelle P.","7202982666;14631830600;7102399009;16030821000;36138922300;","Integrating non-virtual electronic activities in game-based learning environments",2013,"7th European Conference on Games Based Learning, ECGBL 2013","1",,,"378","385",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893649325&partnerID=40&md5=b0227794ffea9136bb365467ba38551d","LIRIS, UMR5205, F-69621, France; LIP6 Lab, UMR CNRS 7606, Université Pierre and Marie Curie, France; Université de Savoie, France; Imep-Lahc Lab, Université de Savoie, Campus scientifique, 73376 Le Bourget Du Lac, France; DISP Lab, Université de Lyon, France","Marty, J., LIRIS, UMR5205, F-69621, France, Université de Savoie, France; Carron, T., LIP6 Lab, UMR CNRS 7606, Université Pierre and Marie Curie, France, Université de Savoie, France; Talbot, S., Université de Savoie, France; Houzet, G., Imep-Lahc Lab, Université de Savoie, Campus scientifique, 73376 Le Bourget Du Lac, France; Pernelle, P., DISP Lab, Université de Lyon, France","Our past experiments with Game-Based Learning multi-players environments, have shown some weaknesses in specific learning activities. Learners seem to acquire a skill in the game, but they are not able to apply it easily in the real world. This is particularly the case for learning skills that require concrete manipulation with real objects. In fact, Game Based Learning Environments (GBLE) lack of means to learn know-how aspects. Some learning processes involving real world objects are very difficult to reproduce in the GBLE and there is an essential technological issue in mixing virtual and real aspects in GBLE. In this article, we describe these problems through an example in the electronic domain. We explain how to consider activities taking place outside the numeric environment. We have set up an experiment, where students needed to design ""electronic circuits"" with concrete electronic elements before being allowed to continue a quest in a virtual world. A complete scenario aiming at learning this kind of knowledge thus swaps from activities in the virtual world to activities in the real world. New issues linked to this transition are explained.","Game-based learning environment; Know-how activity evaluation; Mixing numeric and face-to-face learning tasks; Online multiplayer game; User model","Activity evaluations; Face-to-face learning; Game-based learning environments; Online multiplayer games; User Modeling; Concretes; Experiments; Mixing; Technology transfer; Virtual reality; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-84893649325
"Chen S.-C., Hsu C.-W., Huang D.-Y., Lin S.-Y., Hung Y.-P.","54781137900;57198783917;55536530700;57026557500;26643286300;","TelePort: Virtual touring of Dun-Huang with a mobile device",2013,"Electronic Proceedings of the 2013 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2013",,, 6618406,"","",,3,"10.1109/ICMEW.2013.6618406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888225852&doi=10.1109%2fICMEW.2013.6618406&partnerID=40&md5=fdee4b6bdc06c3299e013445ad291016","Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","Chen, S.-C., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Hsu, C.-W., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Huang, D.-Y., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Lin, S.-Y., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Hung, Y.-P., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan, Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","Some of our daily encountering is the objects of pictures, antiques, sculptures or wall-paintings captured or moved from the source space to another media platform. For example, the Direct Mail advertising of house sale, the historical exhibit in the museum. In fact, few figures or text descriptions will never be able to provide us the rewarding experience unless people can actually visit the source space in person. Inspired by the Boo's doors in the Disney's cartoon of Monsters Inc. which are used as a pathway to different spaces, we propose the 'TelePort' as a gateway providing a virtual touring to the source space and achieve the prototype using a mobile device. To begin the TelePort, users first aim the object onto the mobile's screen. Subsequently, the screen view will immediately transfer to the view of the source space seamlessly as if users look into the space from the entrance gate. After transferring to the space, users can explore the source space and interact with the contents belonged to the space in reality through panorama visualization. To support the first person navigation, the viewpoint direction within the space can be detected by the mobile pose. This novel navigation interface can deliver to users an interesting and realistic experience as teleportation. Our contribution is to demonstrate an innovative experience for a virtual tour of the Mogao Caves using a mobile device. The 'Teleport' featuring the virtual gateway which brings users to the Cave not only attracts attention but also enhances the experience of space exploring. © 2013 IEEE.","HCI; interaction design; mobile; navigation; Touring","Direct mail advertising; First-person-navigation; Interaction design; Media platforms; mobile; Navigation interface; Touring; Virtual gateways; Exhibitions; Human computer interaction; Navigation; Mobile devices",Conference Paper,"Final","",Scopus,2-s2.0-84888225852
"Umakatsu A., Mashita T., Kiyokawa K., Takernura H.","45261534100;14319405900;7005065755;55869055000;","Pinch-n-Paste: Direct texture transfer interaction in augmented reality",2013,"Proceedings - IEEE Virtual Reality",,, 6549369,"73","74",,,"10.1109/VR.2013.6549369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884877577&doi=10.1109%2fVR.2013.6549369&partnerID=40&md5=7d7963e7b9ed03ab4f4ef35a2b738a1f","Osaka University, Japan","Umakatsu, A., Osaka University, Japan; Mashita, T., Osaka University, Japan; Kiyokawa, K., Osaka University, Japan; Takernura, H., Osaka University, Japan","Our Pinch-n-Paste allows a user to touch or pinch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object. In this poster, we will describe the basic idea, implementation details, and example interaction results and a preliminary user study. © 2013 IEEE.","3D physics interaction; Augmented Reality; texture transfer","3D physics interaction; Conformal map; Least Square; Moving least squares; Target object; Texture image; Texture transfer; User study; Augmented reality; Conformal mapping; Textures; Virtual reality; Image texture",Conference Paper,"Final","",Scopus,2-s2.0-84884877577
"Al-Atabany W., McGovern B., Mehran K., Berlinguer-Palmini R., Degenaar P.","36242946600;34979110700;26029478200;6506886997;9636467400;","A processing platform for optoelectronic/optogenetic retinal prosthesis",2013,"IEEE Transactions on Biomedical Engineering","60","3",,"781","791",,21,"10.1109/TBME.2011.2177498","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884661204&doi=10.1109%2fTBME.2011.2177498&partnerID=40&md5=c00aff29bb35e8ef492f22b38021e370","Department of Biomedical Engineering, Helwan University, Helwan 11421, Egypt; School of Electrical, Electronic and Computer Engineering, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom; Department of Physics, Imperial College London, London, SW7 2AZ, United Kingdom; Institute of Neuroscience, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom","Al-Atabany, W., Department of Biomedical Engineering, Helwan University, Helwan 11421, Egypt, School of Electrical, Electronic and Computer Engineering, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom; McGovern, B., Department of Physics, Imperial College London, London, SW7 2AZ, United Kingdom; Mehran, K., School of Electrical, Electronic and Computer Engineering, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom; Berlinguer-Palmini, R., Institute of Neuroscience, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom; Degenaar, P., School of Electrical, Electronic and Computer Engineering, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom, Institute of Neuroscience, Newcastle University, Newcastle upon Tyne, NE1 8QB, United Kingdom","The field of retinal prosthesis has been steadily developing over the last two decades. Despite the many obstacles, clinical trials for electronic approaches are in progress and already demonstrating some success. Optogenetic/optoelectronic retinal prosthesis may prove to have even greater capabilities. Although resolutions are now moving beyond recognition of simple shapes, it will nevertheless be poor compared to normal vision. If we define the aim to be to return mobility and natural scene recognition to the patient, it is important to maximize the useful visual information we attempt to transfer. In this paper, we highlight a method to simplify the scene, perform spatial image compression, and then apply spike coding. We then show the potential for translation on standard consumer processors. The algorithms are applicable to all forms of visual prosthesis, but we particularly focus on optogenetic approaches. © 2013 IEEE.","Augmented vision; Channelrhodopsin; Optoelectronics; Optogenetics; Retinal prosthesis; Scene enhancement; Visual prosthesis","Channelrhodopsin; Clinical trial; Consumer processors; Optogenetics; Processing platform; Retinal prosthesis; Visual information; Visual prosthesis; Digital signal processing; Ophthalmology; Optoelectronic devices; Prosthetics; Aldehydes; rhodopsin; algorithm; anisotropy; article; coding; consumer; contrast enhancement; electronics; human; image analysis; image enhancement; image processing; nerve cell; optic nerve; optoelectronics; photoreceptor; photosensitization; processing; recognition; retina; retina macula age related degeneration; retinal implant; spike wave; vision; visual acuity; visual information; visual memory; visual prosthesis; Action Potentials; Algorithms; Animals; Biomedical Engineering; Computer Simulation; Humans; Image Processing, Computer-Assisted; Macaca mulatta; Models, Biological; Optogenetics; Prosthesis Design; Signal Processing, Computer-Assisted; Visual Prosthesis",Article,"Final","",Scopus,2-s2.0-84884661204
"Zmuda M.A., Wonser J.L., Bachmann E.R., Hodgson E.","55905032400;35729733600;7005745121;14053915200;","Optimizing constrained-environment redirected walking instructions using search techniques",2013,"IEEE Transactions on Visualization and Computer Graphics","19","11", 6520845,"1872","1884",,37,"10.1109/TVCG.2013.88","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884554083&doi=10.1109%2fTVCG.2013.88&partnerID=40&md5=091fedd90ee3852d2e41955d41a5eb1f","Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Department of Psychology, Miami University, Oxford, OH 45056, United States","Zmuda, M.A., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Wonser, J.L., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Bachmann, E.R., Department of Computer Science and Software Engineering, Miami University, 201D Benton Hall, Oxford, OH 45056, United States; Hodgson, E., Department of Psychology, Miami University, Oxford, OH 45056, United States","A goal of redirected walking (RDW) is to allow large virtual worlds to be explored within small tracking areas. Generalized steering algorithms, such as steer-to-center, simply move the user toward locations that are considered to be collision free in most cases. The algorithm developed here, FORCE, identifies collision-free paths by using a map of the tracking area's shape and obstacles, in addition to a multistep, probabilistic prediction of the user's virtual path through a known virtual environment. In the present implementation, the path predictions describe a user's possible movements through a virtual store with aisles. Based on both the user's physical and virtual location/orientation, a search-based optimization technique identifies the optimal steering instruction given the possible user paths. Path prediction uses the map of the virtual world; consequently, the search may propose steering instructions that put the user close to walls if the user's future actions eventually lead away from the wall. Results from both simulated and real users are presented. FORCE identifies collision-free paths in 55.0 percent of the starting conditions compared to 46.1 percent for generalized methods. When considering only the conditions that result in different outcomes, redirection based on FORCE produces collision-free path 94.5 percent of the time. © 2013 IEEE.","Backtracking; motion compression; redirected walking; virtual reality","Backtracking; Collision-free paths; Generalized method; Motion compression; Optimization techniques; Probabilistic prediction; Redirected walkings; Steering algorithms; Algorithms; Interactive computer graphics; Virtual reality; Optimization; adult; algorithm; computer graphics; computer interface; computer simulation; environment; human; male; orientation; physiology; procedures; three dimensional imaging; walking; article; methodology; three dimensional imaging; walking; Adult; Algorithms; Computer Graphics; Computer Simulation; Environment; Humans; Imaging, Three-Dimensional; Male; Orientation; User-Computer Interface; Walking; Adult; Algorithms; Computer Graphics; Computer Simulation; Environment; Humans; Imaging, Three-Dimensional; Male; Orientation; User-Computer Interface; Walking",Article,"Final","",Scopus,2-s2.0-84884554083
"Deutsch J.E., Myslinski M.J., Kafri M., Ranky R., Sivak M., Mavroidis C., Lewis J.A.","7201985389;12769194300;55914359900;36081179400;36106349000;7005291566;8693257800;","Feasibility of virtual reality augmented cycling for health promotion of people poststroke",2013,"Journal of Neurologic Physical Therapy","37","3",,"118","124",,13,"10.1097/NPT.0b013e3182a0a078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883449764&doi=10.1097%2fNPT.0b013e3182a0a078&partnerID=40&md5=0e04fa87326d124dffc295478d0a4240","Research in Virtual Environments and Rehabilitation Sciences Laboratory, Department of Rehabilitation and Movement Sciences, Rutgers University 65, Bergen Street, Newark, NJ 07103, United States; Biomedical Mechatronics Laboratory, Northeastern University, Boston, MA, United States; VRehab LLC, Jersey City, NJ, United States","Deutsch, J.E., Research in Virtual Environments and Rehabilitation Sciences Laboratory, Department of Rehabilitation and Movement Sciences, Rutgers University 65, Bergen Street, Newark, NJ 07103, United States, VRehab LLC, Jersey City, NJ, United States; Myslinski, M.J., Research in Virtual Environments and Rehabilitation Sciences Laboratory, Department of Rehabilitation and Movement Sciences, Rutgers University 65, Bergen Street, Newark, NJ 07103, United States; Kafri, M., Research in Virtual Environments and Rehabilitation Sciences Laboratory, Department of Rehabilitation and Movement Sciences, Rutgers University 65, Bergen Street, Newark, NJ 07103, United States; Ranky, R., Biomedical Mechatronics Laboratory, Northeastern University, Boston, MA, United States; Sivak, M., Biomedical Mechatronics Laboratory, Northeastern University, Boston, MA, United States; Mavroidis, C., Biomedical Mechatronics Laboratory, Northeastern University, Boston, MA, United States; Lewis, J.A., VRehab LLC, Jersey City, NJ, United States","BACKGROUND AND PURPOSE:: A virtual reality (VR) augmented cycling kit (VRACK) was developed to address motor control and fitness deficits of individuals with chronic stroke. In this article, we report on the safety, feasibility, and efficacy of using the VR augmented cycling kit to improve cardiorespiratory (CR) fitness of individuals in the chronic phase poststroke. METHODS:: Four individuals with chronic stroke (47-65 years old and ≥3 years poststroke), with residual lower extremity impairments (Fugl-Meyer 24-26/34), who were limited community ambulators (gait speed range 0.56-1.1 m/s) participated in this study. Safety was defined as the absence of adverse events. Feasibility was measured using attendance, total exercise time, and ""involvement"" measured with the presence questionnaire (PQ). Efficacy of CR fitness was evaluated using a submaximal bicycle ergometer test before and after an 8-week training program. RESULTS:: The intervention was safe and feasible with participants having 1 adverse event, 100% adherence, achieving between 90 and 125 minutes of cycling each week, and a mean PQ score of 39 (SD 3.3). There was a statistically significant (13%; P = 0.035) improvement in peak VO2, with a range of 6% to 24.5%. DISCUSSION AND CONCLUSION:: For these individuals, poststroke, VR augmented cycling, using their heart rate to set their avatarÊs speed, fostered training of sufficient duration and intensity to promote CR fitness. In addition, there was a transfer of training from the bicycle to walking endurance. VR augmented cycling may be an addition to the therapistÊs tools for concurrent training of mobility and health promotion of individuals poststroke. Copyright © 2013 Neurology Section, APTA.","fitness; health promotion; mobility; stroke; virtual reality","aged; article; bicycle; cerebrovascular accident; computer interface; feasibility study; female; fitness; health promotion; human; male; middle aged; pathophysiology; physiology; treatment outcome; Aged; Bicycling; Feasibility Studies; Female; Health Promotion; Humans; Male; Middle Aged; Physical Fitness; Stroke; Treatment Outcome; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84883449764
"Duarte R.J., Cury J., Oliveira L.C.N., Srougi M.","7005370698;36338774200;35574498800;7006308114;","Establishing the minimal number of virtual reality simulator training sessions necessary to develop basic laparoscopic skills competence: Evaluation of the learning curve",2013,"International Braz J Urol","39","5",,"712","719",,8,"10.1590/S1677-5538.IBJU.2013.05.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891641710&doi=10.1590%2fS1677-5538.IBJU.2013.05.14&partnerID=40&md5=889f58bf7a18bcb605bf31713db3925d","Sao Paulo University Medical School, CEPEC Vicky Safra, Department of Urology, Sao Paulo, Brazil","Duarte, R.J., Sao Paulo University Medical School, CEPEC Vicky Safra, Department of Urology, Sao Paulo, Brazil; Cury, J., Sao Paulo University Medical School, CEPEC Vicky Safra, Department of Urology, Sao Paulo, Brazil; Oliveira, L.C.N., Sao Paulo University Medical School, CEPEC Vicky Safra, Department of Urology, Sao Paulo, Brazil; Srougi, M., Sao Paulo University Medical School, CEPEC Vicky Safra, Department of Urology, Sao Paulo, Brazil","Introduction: Medical literature is scarce on information to define a basic skills training program for laparoscopic surgery (peg and transferring, cutting, clipping). The aim of this study was to determine the minimal number of simulator sessions of basic laparoscopic tasks necessary to elaborate an optimal virtual reality training curriculum. Materials and Methods: Eleven medical students with no previous laparoscopic experience were spontaneously enrolled. They were submitted to simulator training sessions starting at level 1 (Immersion Lap VR, San Jose, CA), including sequentially camera handling, peg and transfer, clipping and cutting. Each student trained twice a week until 10 sessions were completed. The score indexes were registered and analyzed. The total of errors of the evaluation sequences (camera, peg and transfer, clipping and cutting) were computed and thereafter, they were correlated to the total of items evaluated in each step, resulting in a success percent ratio for each student for each set of each completed session. Thereafter, we computed the cumulative success rate in 10 sessions, obtaining an analysis of the learning process. By non-linear regression the learning curve was analyzed. Results: By the non-linear regression method the learning curve was analyzed and a r2 = 0.73 (p &lt; 0.001) was obtained, being necessary 4.26 (~five sessions) to reach the plateau of 80% of the estimated acquired knowledge, being that 100% of the students have reached this level of skills. From the fifth session till the 10th, the gain of knowledge was not significant, although some students reached 96% of the expected improvement. Conclusions: This study revealed that after five simulator training sequential sessions the students' learning curve reaches a plateau. The forward sessions in the same difficult level do not promote any improvement in laparoscopic basic surgical skills, and the students should be introduced to a more difficult training tasks level.","Aptitude; General surgery; Laparoscopy; Methods [subheading]; Teaching","article; chi square distribution; clinical competence; computer interface; computer simulation; education; evaluation study; female; human; laparoscopy; learning curve; male; medical student; reference value; reproducibility; task performance; time; utilization review; Chi-Square Distribution; Clinical Competence; Computer Simulation; Female; Humans; Laparoscopy; Learning Curve; Male; Reference Values; Reproducibility of Results; Students, Medical; Task Performance and Analysis; Time Factors; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84891641710
"Jenkins J., Szu H.","57169484400;55696168800;","Augmented reality for biomedical wellness sensor systems",2013,"Proceedings of SPIE - The International Society for Optical Engineering","8750",, 875017,"","",,,"10.1117/12.2018409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881111963&doi=10.1117%2f12.2018409&partnerID=40&md5=b40e728b41594166b16966ea4e6919ad","Computer Science Department, George Mason University, Fairfax VA, United States; Catholic University of America, Washington DC, United States","Jenkins, J., Computer Science Department, George Mason University, Fairfax VA, United States; Szu, H., Catholic University of America, Washington DC, United States","Due to the commercial move and gaming industries, Augmented Reality (AR) technology has matured. By definition of AR, both artificial and real humans can be simultaneously present and realistically interact among one another. With the help of physics and physiology, we can build in the AR tool together with real human day-night webcam inputs through a simple interaction of heat transfer -getting hot, action and reaction -walking or falling, as well as the physiology -sweating due to activity. Knowing the person age, weight and 3D coordinates of joints in the body, we deduce the force, the torque, and the energy expenditure during real human movements and apply to an AR human model. We wish to support the physics-physiology AR version, PP-AR, as a BMW surveillance tool for senior home alone (SHA). The functionality is to record senior walking and hand movements inside a home environment. Besides the fringe benefit of enabling more visits from grand children through AR video games, the PP-AR surveillance tool may serve as a means to screen patients in the home for potential falls at points around in house. Moreover, we anticipate PP-AR may help analyze the behavior history of SHA, e.g. enhancing the Smartphone SHA Ubiquitous Care Program, by discovering early symptoms of candidate Alzheimer-like midnight excursions, or Parkinson-like trembling motion for when performing challenging muscular joint movements. Using a set of coordinates corresponding to a set of 3D positions representing human joint locations, we compute the Kinetic Energy (KE) generated by each body segment over time. The Work is then calculated, and converted into calories. Using common graphics rendering pipelines, one could invoke AR technology to provide more information about patients to caretakers. Alerts to caretakers can be prompted by a patient's departure from their personal baseline, and the patient's time ordered joint information can be loaded to a graphics viewer allowing for high-definition digital reconstruction. Then an entire scene can be viewed from any position in virtual space, and AR can display certain measurements values which either constituted an alert, or otherwise indicate signs of the transition from wellness to illness. © 2013 SPIE.","Augmented Reality; Biomedical wellness; Smart sensor networks; Thermodynamics","Biomedical wellness; Digital reconstruction; Energy expenditure; Graphics rendering; High definition; Home environment; Joint information; Surveillance tools; Biological systems; Kinetics; Neural networks; Physiology; Sensor networks; Smart sensors; Thermodynamics; Tools; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84881111963
"Luboz V., Zhang Y., Johnson S., Song Y., Kilkenny C., Hunt C., Woolnough H., Guediri S., Zhai J., Odetoyinbo T., Littler P., Fisher A., Hughes C., Chalmers N., Kessel D., Clough P.J., Ward J., Phillips R., How T., Bulpitt A., John N.W., Bello F., Gould D.","22433444000;57203832077;55510533300;55762550800;49961557500;35113279400;6508339573;54405446400;35779486100;35778717900;22955734500;57193014467;57199279937;7007102058;7102689322;7004534518;36119465400;7404240954;7005647444;6603305265;7005876140;24329025000;7201621787;","ImaGiNe Seldinger: First simulator for Seldinger technique and angiography training",2013,"Computer Methods and Programs in Biomedicine","111","2",,"419","434",,19,"10.1016/j.cmpb.2013.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880035681&doi=10.1016%2fj.cmpb.2013.05.014&partnerID=40&md5=b9ca99554c0ed727c390b810bb6e42d1","Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom","Luboz, V., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Zhang, Y., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Johnson, S., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Song, Y., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Kilkenny, C., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Hunt, C., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Woolnough, H., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Guediri, S., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Zhai, J., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Odetoyinbo, T., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Littler, P., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Fisher, A., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Hughes, C., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Chalmers, N., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Kessel, D., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Clough, P.J., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Ward, J., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Phillips, R., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; How, T., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Bulpitt, A., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; John, N.W., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Bello, F., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom; Gould, D., Collaborators in Radiological Interventional Virtual Environments (CRaIVE), United Kingdom","In vascular interventional radiology, procedures generally start with the Seldinger technique to access the vasculature, using a needle through which a guidewire is inserted, followed by navigation of catheters within the vessels. Visual and tactile skills are learnt in a patient apprenticeship which is expensive and risky for patients. We propose a training alternative through a new virtual simulator supporting the Seldinger technique: ImaGiNe (imaging guided interventional needle) Seldinger. It is composed of two workstations: (1) a simulated pulse is palpated, in an immersive environment, to guide needle puncture and (2) two haptic devices provide a novel interface where a needle can direct a guidewire and catheter within the vessel lumen, using virtual fluoroscopy. Different complexities are provided by 28 real patient datasets. The feel of the simulation is enhanced by replicating, with the haptics, real force and flexibility measurements. A preliminary validation study has demonstrated training effectiveness for skills transfer. © 2013.","Haptics devices; Interventional radiology; Seldinger technique; Training; Vascular surgery; Virtual simulation","Haptics; Interventional radiology; Seldinger technique; Vascular surgery; Virtual simulations; Catheters; Haptic interfaces; Personnel training; Radiation; Radiology; Needles; angiography; article; catheter; catheterization; devices; needle; simulation; simulator; training; Haptics devices; Interventional radiology; Seldinger technique; Training; Vascular surgery; Virtual simulation; Algorithms; Angiography; Animals; Catheterization; Computer Simulation; Elasticity; Equipment Design; Fluoroscopy; Friction; Humans; Image Processing, Computer-Assisted; Models, Theoretical; Needles; Radiology, Interventional; Software; Swine; Task Performance and Analysis; User-Computer Interface; Vascular Diseases",Article,"Final","",Scopus,2-s2.0-84880035681
"Sharma M., Macafee D., Horgan A.F.","7403268837;26643338900;7004192567;","Basic laparoscopic skills training using fresh frozen cadaver: A randomized controlled trial",2013,"American Journal of Surgery","206","1",,"23","31",,22,"10.1016/j.amjsurg.2012.10.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879245782&doi=10.1016%2fj.amjsurg.2012.10.037&partnerID=40&md5=e9ce6f8f3c883ac5114e12a46d2b99e2","Newcastle Surgical Training Centre, Department of General Surgery, Freeman Hospital NHS Trust, Newcastle Upon Tyne NE7 7DN, United Kingdom; Department of General Surgery, James Cook University Hospital, Middlesbrough, United Kingdom","Sharma, M., Newcastle Surgical Training Centre, Department of General Surgery, Freeman Hospital NHS Trust, Newcastle Upon Tyne NE7 7DN, United Kingdom; Macafee, D., Department of General Surgery, James Cook University Hospital, Middlesbrough, United Kingdom; Horgan, A.F., Newcastle Surgical Training Centre, Department of General Surgery, Freeman Hospital NHS Trust, Newcastle Upon Tyne NE7 7DN, United Kingdom","Background: The purpose of this study was to determine whether training on fresh cadavers improves the laparoscopic skills performance of novices. Methods: Junior surgical trainees, novices (<3 laparoscopic procedure performed) in laparoscopic surgery, were randomized into control (group A) and practice groups (group B). Group B performed 10 repetitions of a set of structured laparoscopic tasks on fresh frozen cadavers (FFCs) improvised from fundamentals of laparoscopic skills technical curriculum. Performance on cadavers was scored using a validated, objective Global Operative Assessment of Laparoscopic Skills scale. The baseline technical ability of the 2 groups and any transfer of skills from FFCs was measured using a full procedural laparoscopic cholecystectomy task on a virtual reality simulator before and after practice on FFCs, respectively. Nonparametric tests were used for analysis of the results. Results: Twenty candidates were randomized; 1 withdrew before the study commenced, and 19 were analyzed (group A, n = 9; group B; n = 10). Four of 5 tasks (nondominant to dominant hand transfer, simulated appendectomy, intracorporeal, and extracorporeal knot tying) on FFCs showed significant improvement on learning curve analysis. After training, significant improvement was shown for safety of cautery (P =.040) and the left arm path length (P =.047) on the virtual reality simulator by the practice group. Conclusions: Training on FFCs significantly improves basic laparoscopic skills and can improve full procedural performance. © 2013 Elsevier Inc. All rights reserved.","Cadaver training; LAP Mentor; Laparoscopic skills training; Virtual reality simulator","adult; appendectomy; article; cadaver; cauterization; cholecystectomy; controlled study; curriculum; device safety; female; frozen section; Global Operative Assessment of Laparoscopic Skills scale; human; human experiment; human tissue; learning curve; male; normal human; priority journal; randomized controlled trial (topic); rating scale; resident; simulator; surgical training; task performance; validity; virtual reality; Adult; Anatomy; Cadaver; Clinical Competence; Computer Simulation; Female; Great Britain; Humans; Internship and Residency; Laparoscopy; Learning Curve; Male; Operative Time; Reproducibility of Results; Task Performance and Analysis; Teaching; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84879245782
"Sodhi R., Poupyrev I., Glisson M., Israr A.","26424844100;6603553340;55347042000;15022614400;","AIREAL: Interactive tactile experiences in free air",2013,"ACM Transactions on Graphics","32","4", 134,"","",,172,"10.1145/2461912.2462007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880776379&doi=10.1145%2f2461912.2462007&partnerID=40&md5=7130f7d61e77b4ba44170aedef5f0b43","Disney Research, Pittsburgh, United States; University of Illinois, United States","Sodhi, R., Disney Research, Pittsburgh, United States, University of Illinois, United States; Poupyrev, I., Disney Research, Pittsburgh, United States; Glisson, M., Disney Research, Pittsburgh, United States; Israr, A., Disney Research, Pittsburgh, United States","AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation. Copyright © ACM. Copyright © ACM 2013.","3D interfaces; Augmented reality; Augmented surfaces; Haptics; Tactile displays; Tangible interfaces; Touch interaction.","3D interface; Haptics; Tactile display; Tangible interfaces; Touch interaction; Augmented reality; Display devices; Haptic interfaces; Human computer interaction; Vortex flow",Article,"Final","",Scopus,2-s2.0-84880776379
"Stefanidis D., Yonce T.C., Korndorffer Jr. J.R., Phillips R., Coker A.","8543232800;55615022000;7801480028;35771403400;55441359600;","Does the incorporation of motion metrics into the existing FLS metrics lead to improved skill acquisition on simulators?: A single blinded, randomized controlled trial",2013,"Annals of Surgery","258","1",,"46","52",,16,"10.1097/SLA.0b013e318285f531","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879117340&doi=10.1097%2fSLA.0b013e318285f531&partnerID=40&md5=f9574debfd9606217074315cc44ebcba","Carolinas Simulation Center, Division of Gastrointestinal and Minimally Invasive Surgery, Department of Surgery, Carolinas HealthCare System, 1025 Morehead Medical Dr., Charlotte, NC 28204, United States; Department of Surgery, Tulane University, School of Medicine, New Orleans, LA, United States","Stefanidis, D., Carolinas Simulation Center, Division of Gastrointestinal and Minimally Invasive Surgery, Department of Surgery, Carolinas HealthCare System, 1025 Morehead Medical Dr., Charlotte, NC 28204, United States; Yonce, T.C., Carolinas Simulation Center, Division of Gastrointestinal and Minimally Invasive Surgery, Department of Surgery, Carolinas HealthCare System, 1025 Morehead Medical Dr., Charlotte, NC 28204, United States; Korndorffer Jr., J.R., Department of Surgery, Tulane University, School of Medicine, New Orleans, LA, United States; Phillips, R., Carolinas Simulation Center, Division of Gastrointestinal and Minimally Invasive Surgery, Department of Surgery, Carolinas HealthCare System, 1025 Morehead Medical Dr., Charlotte, NC 28204, United States; Coker, A., Carolinas Simulation Center, Division of Gastrointestinal and Minimally Invasive Surgery, Department of Surgery, Carolinas HealthCare System, 1025 Morehead Medical Dr., Charlotte, NC 28204, United States","Objective: We hypothesized that training to expert-derived levels of speed and motion will lead to improved learning and will translate to better operating room (OR) performance of novices than training to goals of speed or motion alone. Background: Motion tracking has been suggested to be a more sensitive performance metric than time and errors for the assessment of surgical performance. Methods: An institutional review board-approved, single blinded, randomized controlled trial was conducted at our level-I American College of Surgeons accredited Education Institute. Forty-two novices trained to proficiency in laparoscopic suturing after being randomized into 3 groups: The speed group (n = 14) had to achieve expert levels of speed, the motion group (n = 15) expert levels of motion (path length and smoothness), and the speed and motion group (n = 13) both levels. To achieve proficiency, all groups also had to demonstrate error-free performance. The FLS suture module (task 5) was used for training inside the ProMIS simulator that tracks instrument motion. All groups participated in transfer and retention tests in the OR. OR performance was assessed by a blinded expert rater using Global Operative Assessment of Laparoscopic Skills, speed, accuracy, and inadvertent injuries. Results: Thirty (71%) participants achieved proficiency and participated in the transfer and retention tests. The speed group achieved simulator proficiency significantly faster than the other groups (P < 0.001). With the exception of a higher injury rate during the transfer test for the speed group (that reversed during the retention test), there were no significant performance differences among the groups on all assessed parameters. Conclusions: The incorporation of motion metrics into the time/accuracy goals of the FLS laparoscopic suturing curriculum had limited impact on participant skill transfer to the OR. Given the increased training requirements for such a curriculum, further study is needed before the addition of motion metrics to the current FLS metrics can be recommended. Copyright © 2013 by Lippincott Williams and Wilkins.","FLS program; Motion metrics; ProMIS simulator; Simulation; Skills training","accuracy; adult; article; body movement; controlled study; female; good laboratory practice; human; job performance; male; medical education; metric system; operating room; priority journal; randomized controlled trial; simulator; single blind procedure; skill; student; suturing method; velocity; Animals; Chi-Square Distribution; Clinical Competence; Computer Simulation; Female; Humans; Laparoscopy; Male; Motion; Psychomotor Performance; Single-Blind Method; Statistics, Nonparametric; Stomach; Suture Techniques; Swine; Young Adult",Article,"Final","",Scopus,2-s2.0-84879117340
"Hayes A.T., Straub C.L., Dieker L.A., Hughes C.E., Hynes M.C.","55904744300;55925439900;8352122200;7401857048;36728914300;","Ludic learning: Exploration of TLE TeachLivE™ and effective teacher training",2013,"International Journal of Gaming and Computer-Mediated Simulations","5","2",,"20","33",,18,"10.4018/jgcms.2013040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887453745&doi=10.4018%2fjgcms.2013040102&partnerID=40&md5=4921bfb4e153c1c8e6fed8cbe71588c3","University of Central Florida, Orlando, FL, United States","Hayes, A.T., University of Central Florida, Orlando, FL, United States; Straub, C.L., University of Central Florida, Orlando, FL, United States; Dieker, L.A., University of Central Florida, Orlando, FL, United States; Hughes, C.E., University of Central Florida, Orlando, FL, United States; Hynes, M.C., University of Central Florida, Orlando, FL, United States","New and emerging technology in the field of virtual environments has permitted a certain malleability of learning milieus. These emerging environments allow learning and transfer through interactions that have been intentionally designed to be pleasurable experiences. TLE TeachLivE™ is just such an emerging environment that engages teachers in practice on pedagogical and content aspects of teaching in a simulator. The sense of presence, engagement, and ludus of TLE TeachLivE™ are derived from the compelling Mixed Reality that includes components of off-the shelf and emerging technologies. Some of the noted features that have been identified relevant to the ludic nature of TeachLivE include the flow, fidelity, unpredicability, suspension of disbelief, social presence, and gamelike elements. This article explores TLE TeachLivE™ in terms of the ludology, paideic user experience, the source of the ludus, and outcomes of the ludic nature of the experience. Copyright © 2013, IGI Global.","Engagement; Fidelity; Ludic Simulation; Mixed Reality; Serious Games; Suspension of Disbelief; Teacher Education; Virtual Environment","Engagement; Fidelity; Ludic Simulation; Mixed reality; Serious games; Teacher education; Personnel training; Suspensions (fluids); Virtual reality; Teaching",Article,"Final","",Scopus,2-s2.0-84887453745
"Hodgson E., Bachmann E.","14053915200;7005745121;","Comparing four approaches to generalized redirected walking: Simulation and live user data",2013,"IEEE Transactions on Visualization and Computer Graphics","19","4", 6479192,"634","643",,50,"10.1109/TVCG.2013.28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883482085&doi=10.1109%2fTVCG.2013.28&partnerID=40&md5=bf61425914579fff616f4a290bdfc967","Smale Interactive Visualization Center, Miami University, Ohio, United States; Computer Science and Software Engineering, HIVE, Miami University, Ohio, United States","Hodgson, E., Smale Interactive Visualization Center, Miami University, Ohio, United States; Bachmann, E., Computer Science and Software Engineering, HIVE, Miami University, Ohio, United States","Redirected walking algorithms imperceptibly rotate a virtual scene and scale movements to guide users of immersive virtual environment systems away from tracking area boundaries. These distortions ideally permit users to explore large and potentially unbounded virtual worlds while walking naturally through a physically limited space. Estimates of the physical space required to perform effective redirected walking have been based largely on the ability of humans to perceive the distortions introduced by redirected walking and have not examined the impact the overall steering strategy used. This work compares four generalized redirected walking algorithms, including Steer-to-Center, Steer-to-Orbit, Steer-to-Multiple-Targets and Steer-to- Multiple+Center. Two experiments are presented based on simulated navigation as well as live-user navigation carried out in a large immersive virtual environment facility. Simulations were conducted with both synthetic paths and previously-logged user data. Primary comparison metrics include mean and maximum distances from the tracking area center for each algorithm, number of wall contacts, and mean rates of redirection. Results indicated that Steer-to-Center out-performed all other algorithms relative to these metrics. Steer-to-Orbit also performed well in some circumstances. © 2013 IEEE.","Human computer interaction; Live users; Navigation; Redirected walking; Simulation; Virtual environments","Comparison metrics; Immersive virtual environments; Live users; Maximum distance; Redirected walkings; Simulation; Tracking areas; Virtual scenes; Human computer interaction; Navigation; Virtual reality; Algorithms; algorithm; article; association; comparative study; computer graphics; computer interface; evaluation; human; methodology; physiology; psychophysiology; three dimensional imaging; vision; walking; Algorithms; Biofeedback, Psychology; Computer Graphics; Cues; Humans; Imaging, Three-Dimensional; User-Computer Interface; Visual Perception; Walking",Article,"Final","",Scopus,2-s2.0-84883482085
"Yudkowsky R., Luciano C., Banerjee P., Schwartz A., Alaraj A., Lemole G.M., Charbel F., Smith K., Rizzi S., Byrne R., Bendok B., Frim D.","6601987761;7004699651;35580367600;57217731418;15838807800;55296736700;16945837000;57198916195;16310730600;35324055100;35427674600;35475527700;","Practice on an augmented reality/haptic simulator and library of virtual brains improves residents' ability to perform a ventriculostomy",2013,"Simulation in Healthcare","8","1",,"25","31",,45,"10.1097/SIH.0b013e3182662c69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873568123&doi=10.1097%2fSIH.0b013e3182662c69&partnerID=40&md5=dd5b67ff63bce5530f728ec4db273855","Department of Medical Education, College of Medicine, University of Illinois at Chicago, 808 S Wood St, Chicago, IL 60612, United States; Department of Neurosurgery, College of Medicine, United States; Department of Medicine, College of Medicine, United States; Department of Mechanical and Industrial Engineering, College of Engineering, University of Illinois, Chicago, United States; Department of Neurosurgery, Rush University, United States; Department of Neurological Surgery, Northwestern University Feinberg School of Medicine, United States; Section of Neurosurgery, University of Chicago, Chicago, IL, United States; Division of Neurosurgery, University of Arizona, Tucson, AZ, United States","Yudkowsky, R., Department of Medical Education, College of Medicine, University of Illinois at Chicago, 808 S Wood St, Chicago, IL 60612, United States; Luciano, C., Department of Mechanical and Industrial Engineering, College of Engineering, University of Illinois, Chicago, United States; Banerjee, P., Department of Mechanical and Industrial Engineering, College of Engineering, University of Illinois, Chicago, United States; Schwartz, A., Department of Medical Education, College of Medicine, University of Illinois at Chicago, 808 S Wood St, Chicago, IL 60612, United States; Alaraj, A., Department of Neurosurgery, College of Medicine, United States; Lemole, G.M., Division of Neurosurgery, University of Arizona, Tucson, AZ, United States; Charbel, F., Department of Neurosurgery, College of Medicine, United States; Smith, K., Department of Medicine, College of Medicine, United States; Rizzi, S., Department of Mechanical and Industrial Engineering, College of Engineering, University of Illinois, Chicago, United States; Byrne, R., Department of Neurosurgery, Rush University, United States; Bendok, B., Department of Neurological Surgery, Northwestern University Feinberg School of Medicine, United States; Frim, D., Section of Neurosurgery, University of Chicago, Chicago, IL, United States","INTRODUCTION: Ventriculostomy is a neurosurgical procedure for providing therapeutic cerebrospinal fluid drainage. Complications may arise during repeated attempts at placing the catheter in the ventricle. We studied the impact of simulation-based practice with a library of virtual brains on neurosurgery residents' performance in simulated and live surgical ventriculostomies. METHODS: Using computed tomographic scans of actual patients, we developed a library of 15 virtual brains for the ImmersiveTouch system, a head- and hand-tracked augmented reality and haptic simulator. The virtual brains represent a range of anatomies including normal, shifted, and compressed ventricles. Neurosurgery residents participated in individual simulator practice on the library of brains including visualizing the 3-dimensional location of the catheter within the brain immediately after each insertion. Performance of participants on novel brains in the simulator and during actual surgery before and after intervention was analyzed using generalized linear mixed models. RESULTS: Simulator cannulation success rates increased after intervention, and live procedure outcomes showed improvement in the rate of successful cannulation on the first pass. However, the incidence of deeper, contralateral (simulator) and third-ventricle (live) placements increased after intervention. Residents reported that simulations were realistic and helpful in improving procedural skills such as aiming the probe, sensing the pressure change when entering the ventricle, and estimating how far the catheter should be advanced within the ventricle. CONCLUSIONS: Simulator practice with a library of virtual brains representing a range of anatomies and difficulty levels may improve performance, potentially decreasing complications due to inexpert technique. Copyright © 2013 by the Society for Simulation in Healthcare. Unauthorized reproduction of this article is prohibited.","Augmented reality; Haptics; Neurosurgery; Simulation; Virtual reality","anastomosis; article; brain; computer interface; computer simulation; education; evaluation; histology; human; medical staff; neurosurgery; questionnaire; touch; United States; Brain; Chicago; Computer Simulation; Humans; Medical Staff, Hospital; Neurosurgery; Questionnaires; Touch Perception; User-Computer Interface; Ventriculostomy",Article,"Final","",Scopus,2-s2.0-84873568123
"Rankouhi S., Waugh L.M.","56743224400;14629800800;","A literature review on the comparison role of virtual reality and augmented reality technologies in the AEC industry",2013,"Proceedings, Annual Conference - Canadian Society for Civil Engineering","2","January",,"1359","1368",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938271063&partnerID=40&md5=c5308f2cb214207b60bb14aee5f09c9e","Department of Civil Engineering, University of New Brunswick, Canada","Rankouhi, S., Department of Civil Engineering, University of New Brunswick, Canada; Waugh, L.M., Department of Civil Engineering, University of New Brunswick, Canada","The application of Virtual Reality (VR) and Augmented Reality (AR) technologies in the Architecture, Engineering, and Construction industry has tremendously increased. These technologies play various roles in different stages of the construction projects, such as simulating construction performance, comparing as-built and as-planned statuses of projects, pre-empting schedule disputes, improving collaboration opportunities, and training for similar projects. This article provides an extended foundation for future research by presenting a review of the comparison role of virtual reality and augmented reality technologies. The review is based on articles found within four well-known journals in the AEC industry for the period 2000 to 2011 inclusive. The selected journal articles are classified in different comparison categories e.g., comparison modes, comparison by project phase, comparison purpose, comparison performer, comparison tools and techniques. The number of articles found to match each of these dimensions is used to identify emerging trends in the literature as well as to synthesize the current state-of-the-art of comparison role of VR and AR research in the construction industry. In summary, the literature has focused on the comparison role and its prominent purpose such as progress monitoring in the construction phases of a project; in parallel, the literature addresses issues faced by on-site individual audiences such as project managers and workers/technicians.",,"Augmented reality; Construction industry; Human resource management; Technology transfer; Virtual reality; Architecture , engineering , and constructions; Augmented reality technology; Construction performance; Construction phasis; Construction projects; Literature reviews; State of the art; Tools and techniques; Project management",Conference Paper,"Final","",Scopus,2-s2.0-84938271063
"Blümel E.","23966267600;","Global challenges and innovative technologies geared toward new markets: Prospects for virtual and augmented reality",2013,"Procedia Computer Science","25",,,"4","13",,5,"10.1016/j.procs.2013.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890780803&doi=10.1016%2fj.procs.2013.11.002&partnerID=40&md5=41fb0acaa1f0a94f344b080ef8315d5e","Fraunhofer Institute for Factory Operation and Automation IFF, Sandtorstrassse 22, 39106 Magdeburg, Germany","Blümel, E., Fraunhofer Institute for Factory Operation and Automation IFF, Sandtorstrassse 22, 39106 Magdeburg, Germany","Effective applied research is based on close collaboration between research and industry, which, taking the findings of basic research on customer demands as its starting point, creates new means to develop and market innovative products. What is more, growing demands for innovative and sustainable results of research and development are prompting the examination of global trends such as demographic change, growing megacities, rising energy consumption and increasing traffic and the resultant social challenges. These trends and increasing traffic in particular are giving rise to new fields of work, especially for digital technologies, as a social responsibility, e.g. on driver assistance and traffic control systems that increase safety. The social challenges are increasingly affecting markets and requiring new innovative products, efficient production processes and integrative forms of human resource development and training and qualification. The virtualization and digitization of objects and processes is becoming an enabler of the development of new strategies and concepts such as smart cities, green energy, electric vehicle networks, smart manufacturing and smart logistics. This paper examines means by which digital engineering and virtual and augmented reality technologies can support the creation of sustainable smart manufacturing and smart logistics processes as well as on-the-job training and qualification and knowledge transfer. © 2013 The Authors. Published by Elsevier B.V.","Augmented reality; Digital engineering; Global trends; Logistics; Production; Social challenges; Virtual reality",,Conference Paper,"Final","",Scopus,2-s2.0-84890780803
"Gorecky D., Mura K., Arlt F.","36095668000;55523862900;19333243300;","A vision on training and knowledge sharing applications in future factories",2013,"IFAC Proceedings Volumes (IFAC-PapersOnline)","12","PART 1",,"90","97",,8,"10.3182/20130811-5-US-2037.00019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885696182&doi=10.3182%2f20130811-5-US-2037.00019&partnerID=40&md5=c23f01bc863003996c38ff3ed364ed09","Innovative Factory Systems (IFS), German Research Center for Artificial Intelligence (DFKI), Trippstadter Strasse 122, 67663 Kaiserslautern, Germany; Adam Opel AG, Germany","Gorecky, D., Innovative Factory Systems (IFS), German Research Center for Artificial Intelligence (DFKI), Trippstadter Strasse 122, 67663 Kaiserslautern, Germany; Mura, K., Innovative Factory Systems (IFS), German Research Center for Artificial Intelligence (DFKI), Trippstadter Strasse 122, 67663 Kaiserslautern, Germany; Arlt, F., Adam Opel AG, Germany","Under the pressure of global technological, social and economic changes, manufacturing enterprises require to invest in the advanced training of their employees. The technological basis for new forms of training and knowledge sharing concepts is created by advances in information and communication technology (ICT) and human-computer interaction (HMI). The paper provides an overview about the vision on human-centered production from the perspective of the EU-FP7 project VISTRA. The relevant state-of-the-art for training and knowledge sharing systems is described and the future trends and requirements are anticipated. On the example of the vehicle industry the qualification requirements are stated and transferred into two visionary systems for knowledge-delivery and skill transfer. Copyright © 2013 IFAC.","Advanced Interaction; Augmented Reality; Human-Centered Production; Knowledge Sharing Systems; Mobile Interaction; Training Systems; Virtual Reality","Augmented reality; Knowledge based systems; Knowledge management; Man machine systems; Personnel training; Virtual reality; Advanced Interaction; Information and Communication Technologies; Knowledge delivery; Knowledge sharing systems; Knowledge-sharing; Manufacturing enterprise; Mobile interaction; Training Systems; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84885696182
"Abhyankar K., Ganapathy S.","55750622200;7005639257;","Augmented reality based learning system for effective knowledge transfer - Engineering education",2013,"IIE Annual Conference and Expo 2013",,,,"463","472",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900344865&partnerID=40&md5=af08069e19463285a15986eb9902e6b9","Wright State University, Dayton OH, United States","Abhyankar, K., Wright State University, Dayton OH, United States; Ganapathy, S., Wright State University, Dayton OH, United States","The Gen 'Y' interactions have changed drastically over a decade with the use of handheld computing and communicating devices. With the adoption of this technology, comes a responsibility and opportunity to tailor the technology affordances to the educational requirements. The widespread use of small form factor devices and a novel integration of augmented reality (AR) approach can help address some of the concerns in the education domain especially in engineering. This paper will present conclusive pseudo-ethnographic data conducted with the help of 1:1 interviews and web polls understand the barriers of engineering concepts. This paper will also present the design methodology used towards developing multiple educational tools e.g. animations, videos, graphers and so on. which will be available at their fingertips to address the problem of knowledge acquisition. One way to answer this problem is to make the students understand the problems better with the help of the AR tool prototype provides instant and easy access to these educational tools via a rich interactive AR platform over small form factor. Our hypothesis of testing the effectiveness includes assumptions such as better learner engagement, improved focus, motivation and better learner control.","Augmented reality; Ethnographic research; Inductive learning; Small form factor devices","Approximation theory; Augmented reality; Exhibitions; Knowledge management; Learning systems; Tools; Design Methodology; Educational tools; Engineering concepts; Handheld computing; Inductive learning; Knowledge transfer; Small form factors; Technology affordances; Education",Conference Paper,"Final","",Scopus,2-s2.0-84900344865
"Alaraj A., Charbel F.T., Birk D., Tobin M., Luciano C., Banerjee P.P., Rizzi S., Sorenson J., Foley K., Slavin K., Roitberg B.","15838807800;16945837000;14120930700;55550172100;7004699651;35580367600;16310730600;8690246600;7102856392;7006065355;33968027300;","Role of cranial and spinal virtual and augmented reality simulation using immersive touch modules in neurosurgical training",2013,"Neurosurgery","72","SUPPL. 1",,"A115","A123",,64,"10.1227/NEU.0b013e3182753093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872064481&doi=10.1227%2fNEU.0b013e3182753093&partnerID=40&md5=b9581c8c6bde510488311acc4dda173d","Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, College of Engineering, Chicago, IL, United States; Section of Neurosurgery, University of Chicago, Chicago, IL, United States; Medical Education and Research Institute, Memphis, TN, United States","Alaraj, A., Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Charbel, F.T., Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Birk, D., Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Tobin, M., Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Luciano, C., Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, College of Engineering, Chicago, IL, United States; Banerjee, P.P., Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, College of Engineering, Chicago, IL, United States, Section of Neurosurgery, University of Chicago, Chicago, IL, United States; Rizzi, S., Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, College of Engineering, Chicago, IL, United States; Sorenson, J., Medical Education and Research Institute, Memphis, TN, United States; Foley, K., Medical Education and Research Institute, Memphis, TN, United States; Slavin, K., Department of Neurosurgery, College of Medicine, University of Illinois at Chicago, 912 S Wood St, Chicago, IL 60612-7329, United States; Roitberg, B., Section of Neurosurgery, University of Chicago, Chicago, IL, United States","Recent studies have shown that mental script-based rehearsal and simulation-based training improve the transfer of surgical skills in various medical disciplines. Despite significant advances in technology and intraoperative techniques over the last several decades, surgical skills training on neurosurgical operations still carries significant risk of serious morbidity or mortality. Potentially avoidable technical errors are well recognized as contributing to poor surgical outcome. Surgical education is undergoing overwhelming change, as a result of the reduction of work hours and current trends focusing on patient safety and linking reimbursement with clinical outcomes. Thus, there is a need for adjunctive means for neurosurgical training, which is a recent advancement in simulation technology. ImmersiveTouch is an augmented reality system that integrates a haptic device and a high-resolution stereoscopic display. This simulation platform uses multiple sensory modalities, re-creating many of the environmental cues experienced during an actual procedure. Modules available include ventriculostomy, bone drilling, percutaneous trigeminal rhizotomy, and simulated spinal modules such as pedicle screw placement, vertebroplasty, and lumbar puncture. We present our experience with the development of such augmented reality neurosurgical modules and the feedback from neurosurgical residents. Copyright © 2012 by the Congress of Neurological Surgeons.","Neurosurgical training; Spinal instrumentation; Surgical Simulation; Ventriculostomy; Virtual reality","anastomosis; article; bone drilling; human; lumbar puncture; neurosurgery; pedicle screw; percutaneous trigeminal rhizotomy; percutaneous vertebroplasty; priority journal; rhizotomy; skull surgery; surgical training; virtual reality; Central Nervous System Diseases; Competency-Based Education; Computer Simulation; Craniotomy; Education, Medical, Graduate; Feedback; Humans; Imaging, Three-Dimensional; Internship and Residency; Medical Errors; Neurosurgical Procedures; Rhizotomy; Spinal Fusion; Spinal Puncture; Touch; Trigeminal Neuralgia; User-Computer Interface; Ventriculostomy; Vertebroplasty",Article,"Final","",Scopus,2-s2.0-84872064481
"Gao K., Wiederhold B.K., Kong L., Wiederhold M.D.","56041506200;7003634518;56041612100;7005352040;","Methodology case study of the application of haptics to combat medic training programs",2013,"Annual Review of CyberTherapy and Telemedicine","11",,,"53","57",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019710653&partnerID=40&md5=eea47f53a1fcac165da1336784e1fef1","Virtual Reality Medical Center; Interactive Media Institute","Gao, K., Virtual Reality Medical Center; Wiederhold, B.K., Interactive Media Institute; Kong, L., Virtual Reality Medical Center; Wiederhold, M.D., Virtual Reality Medical Center","Of the available training methods for emergency responders, including other methods based on computer technology, virtual reality video game training with haptics (tactile) features will be shown to provide the most effective transfer of skills to real-world emergency situations, providing a model for the development of new training products for combat medics and civilian first responders. This paper aims to provide a methodological case study of haptics use in medical training programs and highlight achievements in terms of performance. Review of these cases show that the addition of haptics to an existing simulationbased training program increases user performance in terms of completion time, error rates, and learning rate. With this case study, haptics can be further incorporated into training programs designed for military combat medics. © 2013 Interactive Media Institute.","Combat medic; Haptics; Kinesthetic learning style; Virtual reality training","achievement; human; learning style; medical education; training; virtual reality",Article,"Final","",Scopus,2-s2.0-85019710653
"Gao K., Wiederhold B.K., Kong L., Wiederhold M.D.","56041506200;7003634518;56041612100;7005352040;","Methodology case study of the application of haptics to combat medic training programs",2013,"Studies in Health Technology and Informatics","191",,,"53","57",,4,"10.3233/978-1-61499-282-0-53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894261638&doi=10.3233%2f978-1-61499-282-0-53&partnerID=40&md5=da3eb89824cbf032f4a9a264f8fd588c","Virtual Reality Medical Center, United States; Interactive Media Institute, United States","Gao, K., Virtual Reality Medical Center, United States; Wiederhold, B.K., Interactive Media Institute, United States; Kong, L., Virtual Reality Medical Center, United States; Wiederhold, M.D., Virtual Reality Medical Center, United States","Of the available training methods for emergency responders, including other methods based on computer technology, virtual reality video game training with haptics (tactile) features will be shown to provide the most effective transfer of skills to real-world emergency situations, providing a model for the development of new training products for combat medics and civilian first responders. This paper aims to provide a methodological case study of haptics use in medical training programs and highlight achievements in terms of performance. Review of these cases show that the addition of haptics to an existing simulation-based training program increases user performance in terms of completion time, error rates, and learning rate. With this case study, haptics can be further incorporated into training programs designed for military combat medics. © 2013 Interactive Media Institute and IOS Press.","Combat Medic; Haptics; Kinesthetic Learning Style; Virtual Reality Training","Application programs; Computer games; Virtual reality; Combat Medic; Computer technology; Emergency responders; Emergency situation; Haptics; Learning Style; Simulation-based training; Virtual reality training; E-learning; article; computer assisted diagnosis; computer assisted therapy; computer interface; education; human; methodology; military medicine; psychophysiology; rescue personnel; touch; Biofeedback, Psychology; Diagnosis, Computer-Assisted; Emergency Medical Technicians; Humans; Military Medicine; Therapy, Computer-Assisted; Touch; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84894261638
"Custurã-Crãciun D., Cochior D., Constantinoiu S., Neagu C.","56006705600;19634420000;6601929228;7004612496;","Surgical virtual reality - Highlights in developing a high performance surgical haptic device",2013,"Chirurgia (Romania)","108","6",,"757","763",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892607831&partnerID=40&md5=3fa2ed8ef08dc6c6648309f662481041","Gh. Popescu Street, 050222, Bucharest, Romania; Titu Maiorescu University, Department of Surgical Disciplines, Faculty of Medicine, Bucharest, Romania; UMF Carol Davila, Department for General and Oesophageal Surgery St. Mary Hospital, Bucharest, Romania; Department of Manufacturing Technology, Polytechnic University of Bucharest, Romania","Custurã-Crãciun, D., Gh. Popescu Street, 050222, Bucharest, Romania; Cochior, D., Titu Maiorescu University, Department of Surgical Disciplines, Faculty of Medicine, Bucharest, Romania; Constantinoiu, S., UMF Carol Davila, Department for General and Oesophageal Surgery St. Mary Hospital, Bucharest, Romania; Neagu, C., Department of Manufacturing Technology, Polytechnic University of Bucharest, Romania","Just like simulators are a standard in aviation and aerospace sciences, we expect for surgical simulators to soon become a standard in medical applications. These will correctly instruct future doctors in surgical techniques without there being a need for hands on patient instruction. Using virtual reality by digitally transposing surgical procedures changes surgery in a revolutionary manner by offering possibilities for implementing new, much more efficient, learning methods, by allowing the practice of new surgical techniques and by improving surgeon abilities and skills. Perfecting haptic devices has opened the door to a series of opportunities in the fields of research, industry, nuclear science and medicine. Concepts purely theoretical at first, such as telerobotics, telepresence or telerepresentation, have become a practical reality as calculus techniques, telecommunications and haptic devices evolved, virtual reality taking a new leap. In the field of surgery barriers and controversies still remain, regarding implementation and generalization of surgical virtual simulators. These obstacles remain connected to the high costs of this yet fully sufficiently developed technology, especially in the domain of haptic devices.","Haptic Device; Surgical Virtual Reality; Virtual Surgical Training","article; clinical competence; computer interface; computer simulation; education; human; methodology; robotics; Romania; standard; surgery; teaching; Clinical Competence; Computer Simulation; Computer-Assisted Instruction; Humans; Robotics; Romania; Surgical Procedures, Operative; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84892607831
"Medeiros D., Teixeira L., Carvalho F., Santos I., Raposo A.","55428540900;57212518247;23396260500;7102671754;6603721426;","A tablet-based 3D interaction tool for virtual engineering environments",2013,"Proceedings - VRCAI 2013: 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry",,,,"211","218",,14,"10.1145/2534329.2534349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891519598&doi=10.1145%2f2534329.2534349&partnerID=40&md5=fac54922eb295c032efdb5293a9c8806","Tecgraf/PUC-Rio, Brazil; Petrobras, Brazil","Medeiros, D., Tecgraf/PUC-Rio, Brazil; Teixeira, L., Tecgraf/PUC-Rio, Brazil; Carvalho, F., Tecgraf/PUC-Rio, Brazil; Santos, I., Petrobras, Brazil; Raposo, A., Tecgraf/PUC-Rio, Brazil","Three-dimensional computer-aided design (3D CAD) modeling and reviewing is one of the most common engineering project tools. Interaction in these environments is characterized by the need for a high precision level to execute specific tasks. Generally this kind of task uses specific interaction devices with 4 or more degrees of freedom, such as 3D mice. Currently applications involving 3D interaction use interaction devices for object modeling or for the implementation of navigation, selection and manipulation techniques in a virtual environment. A related problem is the need to control naturally non-immersive tasks, such as symbolic input (e.g., text, photos). In addition, the steep learning curve to handle such non-conventional devices is a recurring problem. The addition of sensors and the popularization of smart-phones and tablets, allowed the use of such devices in virtual engineering environments. These devices, differs to other devices by the possibility of including additional information and performing naturally non-immersive tasks. This work presents a 3D interaction tablet-based tool, which allows the aggregation of all major 3D interaction topics, such as navigation, selection, manipulation, system control and symbolic input. To validate the proposed tool, the SimUEP-Ambsim application was chosen, an oil and gas simulator that has the complexity needed and which allows the use of all techniques implemented. Then, the tool was tested in another application, a photo-voltaic solar plant simulator, in order to evaluate the generality of this work concept. © 2013 ACM.","3D interaction; mobile devices; virtual engineering environments; virtual reality","Computer aided design; Interactive computer graphics; Mobile devices; Tools; Virtual reality; 3D interactions; Engineering project; Interaction devices; Manipulation techniques; Specific interaction; Specific tasks; Steep learning curve; Virtual engineering; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84891519598
"Ellis S.R., Adelstein B.D., Yeom K.","7402787942;6701481469;13610509300;","Misalignment effect function for oblique rotation of a teleoperations viewpoint: Counterintuitive predictions and implications for Fitts' law",2013,"IFAC Proceedings Volumes (IFAC-PapersOnline)","12","PART 1",,"543","548",,,"10.3182/20130811-5-US-2037.00027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885807397&doi=10.3182%2f20130811-5-US-2037.00027&partnerID=40&md5=59e3ecce3fc4f8ec3d14024e3523469e","NASA Ames Research Center, Moffett Field, CA 94035, United States; San Jose State University Foundation, Moffett Field, CA 94035, United States","Ellis, S.R., NASA Ames Research Center, Moffett Field, CA 94035, United States; Adelstein, B.D., NASA Ames Research Center, Moffett Field, CA 94035, United States; Yeom, K., San Jose State University Foundation, Moffett Field, CA 94035, United States","The Misalignment Effect Function (MEF) describes the decrement in manual performance due to rotation between the users' visual display frame of reference and that of their manual control. A targeting process, called the Secant Rule, is proposed to explain this function for rotation angles less than 65°. This rule describes data from three previous experiments without a need for hypothetical user mental rotation. Analysis of its three-dimensional generalization leads to an unexpected conclusion that difficulties due to rotational misalignment should get harder as the required movement is shorter. This prediction is confirmed, an approach for generalizing the analysis to larger rotations is considered, and a connection with Crossman and Goodeve's explanation of Fitts's Law is examined. Copyright © 2013 IFAC.","Human-machine systems; Motor control; Teleoperations; Virtual environments","Alignment; Function evaluation; Man machine systems; Remote control; Virtual reality; Fitts's law; Frame of reference; Mental rotation; Misalignment effects; Motor control; Rotation angles; Rotational misalignments; Visual display; Rotation",Conference Paper,"Final","",Scopus,2-s2.0-84885807397
"Arikatla V.S., Sankaranarayanan G., Ahn W., Chellali A., De S., Caroline G.L., Hwabejire J., Demoya M., Schwaitzberg S., Jones D.B.","26654027600;15623319200;9334513400;26767578800;7202304567;55922469100;55209924200;57208993340;7007036892;55387240300;","Face and construct validation of a virtual peg transfer simulator",2013,"Surgical Endoscopy","27","5",,"1721","1729",,21,"10.1007/s00464-012-2664-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876298731&doi=10.1007%2fs00464-012-2664-y&partnerID=40&md5=2c402e060ca951af0b145264970d8a31","Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, 12180 NY, United States; Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Boston, MA, United States; Division of Trauma, Massachusetts General Hospital, Harvard Medical School, Boston, MA, United States; Department of Surgery, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States; Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, Fairborn, OH, United States; Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110, 8th Street, Troy, NY 12180, United States","Arikatla, V.S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, 12180 NY, United States; Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, 12180 NY, United States; Ahn, W., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, 12180 NY, United States; Chellali, A., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Boston, MA, United States; De, S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, Troy, 12180 NY, United States, Department of Mechanical, Aerospace and Nuclear Engineering, Rensselaer Polytechnic Institute, 110, 8th Street, Troy, NY 12180, United States; Caroline, G.L., Department of Biomedical, Industrial and Human Factors Engineering, Wright State University, Fairborn, OH, United States; Hwabejire, J., Division of Trauma, Massachusetts General Hospital, Harvard Medical School, Boston, MA, United States; Demoya, M., Division of Trauma, Massachusetts General Hospital, Harvard Medical School, Boston, MA, United States; Schwaitzberg, S., Department of Surgery, Cambridge Health Alliance, Harvard Medical School, Boston, MA, United States; Jones, D.B., Department of Surgery, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, United States","Background: The Fundamentals of Laparoscopic Surgery (FLS) trainer box is now established as a standard for evaluating minimally invasive surgical skills. A particularly simple task in this trainer box is the peg transfer task which is aimed at testing the surgeon's bimanual dexterity, hand-eye coordination, speed, and precision. The Virtual Basic Laparoscopic Skill Trainer (VBLaST©) is a virtual version of the FLS tasks which allows automatic scoring and real-time, subjective quantification of performance without the need of a human proctor. In this article we report validation studies of the VBLaST© peg transfer (VBLaST-PT©) simulator. Methods: Thirty-five subjects with medical background were divided into two groups: experts (PGY 4-5, fellows, and practicing surgeons) and novices (PGY 1-3). The subjects were asked to perform the peg transfer task on both the FLS trainer box and the VBLaST-PT© simulator; their performance was evaluated based on established metrics of error and time. A new length of trajectory (LOT) metric has also been introduced for offline analysis. A questionnaire was used to rate the realism of the virtual system on a 5-point Likert scale. Results: Preliminary face validation of the VBLaST-PT© with 34 subjects rated on a 5-point Likert scale questionnaire revealed high scores for all aspects of simulation, with 3.53 being the lowest mean score across all questions. A two-tailed Mann-Whitney test performed on the total scores showed significant (p = 0.001) difference between the groups. A similar test performed on the task time (p = 0.002) and the LOT (p = 0.004) separately showed statistically significant differences between the experts and the novices (p < 0.05). The experts appear to be traversing shorter overall trajectories in less time than the novices. Conclusion: VBLaST-PT© showed both face and construct validity and has promise as a substitute for the FLS for training peg transfer skills. © 2012 Springer Science+Business Media New York.","Construct; Face validity; Laparoscopy; Length of trajectory; Surgical training; Virtual reality","accuracy; article; computer program; construct validity; eye hand coordination; face validity; handedness; human; laparoscopic surgery; medical expert; operation duration; postgraduate education; priority journal; professional competence; quantitative analysis; rank sum test; scoring system; simulator; surgeon; surgical error; task performance; velocity; virtual peg transfer simulator; virtual reality; Adult; Clinical Competence; Computer Simulation; Computer Systems; Educational Measurement; Educational Technology; Feedback, Sensory; Humans; Laparoscopy; Models, Theoretical; Practice (Psychology); Psychomotor Performance; Questionnaires; Software; Touch; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84876298731
"Carron T., Pernelle P., Talbot S.","14631830600;36138922300;7102399009;","Issues of learning games: From virtual to real",2013,"IADIS International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2013",,,,"133","140",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898785647&partnerID=40&md5=2e511772ce76492ab9aa49c227d16d46","Lip6 Lab. Paris 6, Université de Savoie, France; Disp Lab., Université de Lyon, France; Université de Savoie, France","Carron, T., Lip6 Lab. Paris 6, Université de Savoie, France; Pernelle, P., Disp Lab., Université de Lyon, France; Talbot, S., Université de Savoie, France","Our research work deals with the development of new learning environments, and we are particularly interested in studying the different aspects linked to users' collaboration in these environments. We believe that Game-based Learning can significantly enhance learning. That is why we have developed learning environments grounded on graphical representations of a course. These environments allow us to set up experiments with students in our university. The emergence of online multiplayer games led us to apply the metaphor of exploring a virtual 3D world, where each student embarks on a quest in order to collect knowledge related to a learning activity. In the environment, each part of the world represents a place, sometimes a collaborative place, where students are supposed to acquire a particular concept. Learning objects, artefacts or collaborative tools may be present in each location and a correct answer to a specific exercise gives a key to the students, allowing them to access other activities. Although the students appreciate this approach, there is a lack of assessment of know-how-type skills, especially for the teacher. Indeed, certain domains present the particularity of exhibiting both theoretical knowledge and practical knowhow (operations in manufacturing or medicine, for example). For such contexts, the current Learning Games are not efficient concerning this second point: a unique and full digitalisation of these objects alone is not sufficient to guarantee both effective learning and assessment of the techniques. We consider this as a gamification problem. As a matter of fact, in industrial domains, the learning processes are often based on the use of certain objects that are difficult to include in a learning game. Moreover, although some games are collaborative, an effective collaborative activity is more effective in a real context. Our objective is then to facilitate the transfer by integrating a new object present both in the game and in the classroom. In this paper, we focus on the way to integrate the use of real objects into the learning game environments. In our approach, we develop a digital copy of the object that will help to exhibit the specific know-how that must be evaluated. It is then easier during the gamification process to include in the learning scenario certain parts that are relevant to the use of such an object and that are achieved in a real context. First, we describe briefly the ""Learning Adventure"" environment: a generic game-based platform. Then, we explain what the new issues of such contexts are, and how we are able to setup a collaborative learning session with both virtual and real objects. The third part concerns the application to a concrete problem: to identify and manage Non Conformities thanks to a Product Lifecycle Management tool. A real experiment has thus been carried out at our university in the PLM domain with the help of a multi-touch tabletop, to validate the feasibility of the approach and illustrate our point.","Collaborative activity; Mixed reality; Nonconformity; Product Lifecycle Management; Serious Game","Computer aided instruction; Experiments; Life cycle; Students; Teaching; Technology transfer; Tools; Virtual reality; Collaborative activities; Mixed reality; Nonconformity; Product life cycle management; Serious games; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84898785647
"Antonya C.","22133383000;","Hybrid dynamic model for haptic systems with planar mechanisms",2013,"IEEE Conference on Robotics, Automation and Mechatronics, RAM - Proceedings",,, 6758579,"174","178",,1,"10.1109/RAM.2013.6758579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898021585&doi=10.1109%2fRAM.2013.6758579&partnerID=40&md5=f0c7588fb03be44335bba2c68b26b86c","Automotive and Transportation Department, Faculty of Mechanical Engineering, Transilvania University of Brasov, 29 Eroilor Blvd, Brasov, 500036, Romania","Antonya, C., Automotive and Transportation Department, Faculty of Mechanical Engineering, Transilvania University of Brasov, 29 Eroilor Blvd, Brasov, 500036, Romania","The aim of this paper is to present a new partitioning method for solving the equation of motion of a planar mechanism with more than one degree of freedom, using a hybrid dynamic model. The procedure is according to the Newton-Euler formalism with Lagrange equations and can be used in the case that some of the applied forces acting on the elements of the mechanism are unknown, but the time history of generalized coordinate variation (acceleration) of the same elements are imposed. The resulting equations can be used for real time simulations whenever some of the external forces are unknown (but the motion of the same elements are tracked or imposed) and when a high rate of update is expected. For example virtual prototyping with haptic systems needs a very fast haptic rendering loop and forces developed by the user are unknown. Fast simulation makes possible the expansion of the frequency of aliased harmonics of the generated forces in the haptic system, so that it can better mechanically filter them, allowing a wider bandwidth of force. Haptic interaction with an accurate and fast dynamic simulation provides unique insights into the behaviors of the virtual prototype. The proposed partitioning method can be used only in these special circumstances and enforce dynamic consistency over time. © 2013 IEEE.",,"Computer simulation; Dynamic models; Equations of motion; Haptic interfaces; Dynamic consistencies; Equation of motion; Generalized coordinates; Haptic interactions; Hybrid dynamic modeling; Partitioning methods; Real time simulations; Virtual prototyping; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84898021585
"Umakatsu A., Mashita T., Kiyokawa K., Takemura H.","45261534100;14319405900;7005065755;7201655004;","Pinch-n-Paste: Direct texture transfer interaction in augmented reality",2012,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"185","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871944806&partnerID=40&md5=3da0909373ff493069409c254f148a45","Osaka University, Japan","Umakatsu, A., Osaka University, Japan; Mashita, T., Osaka University, Japan; Kiyokawa, K., Osaka University, Japan; Takemura, H., Osaka University, Japan","Our Pinch-n-Paste allows a user to touch or pinch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object. Copyright 2012 ACM.","3D Interaction; Augmented Reality","3D interactions; Conformal map; Least Square; Moving least squares; Target object; Texture image; Texture transfer; Augmented reality; Conformal mapping; Textures; Virtual reality; Image texture",Conference Paper,"Final","",Scopus,2-s2.0-84871944806
"Yoshinaga T., Arita D., Masuda K.","35735607100;55943147000;7401446825;","Development of Augmented Reality Body-Mark system to support echography",2012,"2012 IEEE Biomedical Circuits and Systems Conference: Intelligent Biomedical Electronics and Systems for Better Life and Better Environment, BioCAS 2012 - Conference Publications",,, 6418425,"348","351",,,"10.1109/BioCAS.2012.6418425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874132953&doi=10.1109%2fBioCAS.2012.6418425&partnerID=40&md5=6479729009b849104b65c4b05f468b25","Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Graduate School of Bio-Applications and Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan","Yoshinaga, T., Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Arita, D., Institute of Systems, Information Technologies and Nanotechnologies, Fukuoka, Japan; Masuda, K., Graduate School of Bio-Applications and Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan","We propose visualization system of 3D shape of internal organs using Augmented Reality and Virtual Reality technology. Echography has been used in every field of medical diagnosis because of its safety and cost-effectiveness. Therefore, portable echography device so called 'Ubiquitous Echo' was released by many manufacturers. However, the skill to recognize relationship between ultrasound probe and internal organ is required to acquire echogram. Therefore we have developed tracking system of probe orientation to transfer 2D position of echogram into 3D space. Then we have applied Radial Basis Function Interpolation to reconstruct the shape of internal organs. Furthermore, GUI interface to visualize the internal organ was developed by OpenGL. As a result of evaluation experiments, visualization of internal organ was satisfied to show the echogram for diagnosis. © 2012 IEEE.",,"3-D shape; 3-D space; Echography; Evaluation experiments; Internal organs; Radial basis function interpolation; Tracking system; Ultrasound probes; Virtual reality technology; Visualization system; Application programming interfaces (API); Augmented reality; Diagnosis; Probes; Radial basis function networks; Ultrasonic imaging; Virtual reality; Visualization; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84874132953
"Umakatsu A., Mashita T., Kiyokawa K., Takemura H.","45261534100;14319405900;7005065755;7201655004;","Touch-n-Paste: Direct texture transfer interaction in AR environments",2012,"ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers",,, 6402596,"325","326",,2,"10.1109/ISMAR.2012.6402596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873536172&doi=10.1109%2fISMAR.2012.6402596&partnerID=40&md5=1b7c4d64b9ff65ac2a7399302a37bcd8","Osaka University, Japan","Umakatsu, A., Osaka University, Japan; Mashita, T., Osaka University, Japan; Kiyokawa, K., Osaka University, Japan; Takemura, H., Osaka University, Japan","Our Touch-n-Paste allows a user to touch one part of an object, copy and move its texture, and paste it onto another object, directly with his or her hand, in an augmented reality environment. To transfer texture appropriately from one part of an object to another, two texture images are generated by the Least Square Conformal Map (LSCM) technique. Two regions in the texture images corresponding to source and target areas of interest are then obtained using cross-boundary brushes. Target texel values are sampled from corresponding source texels by Moving Least Squares (MLS), and are finally mapped onto the target object. © 2012 IEEE.","and virtual realities I.3.7 [Computer Graphics]: Three-dimensional Graphics and Realism - Color; augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; shading; shadowing; texture","augmented; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial; I.3.7 [computer graphics]: three-dimensional graphics and realism; shading; shadowing; Augmented reality; Conformal mapping; Textures; Virtual reality; Image texture",Conference Paper,"Final","",Scopus,2-s2.0-84873536172
"Hoang T.N., Thomas B.H.","14819594500;55467685600;","Distance-based modeling and manipulation techniques using ultrasonic gloves",2012,"ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers",,, 6402577,"287","288",,5,"10.1109/ISMAR.2012.6402577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560024&doi=10.1109%2fISMAR.2012.6402577&partnerID=40&md5=46ed206868e0dba0c80160c16cade270","Wearable Computer Lab., University of South Australia, Australia","Hoang, T.N., Wearable Computer Lab., University of South Australia, Australia; Thomas, B.H., Wearable Computer Lab., University of South Australia, Australia","We present a set of distance-based interaction techniques for modeling and manipulation, enabled by a new input device called the ultrasonic gloves. The ultrasonic gloves are built upon the original design of the pinch glove device for virtual reality systems with a tilt sensor and a pair of ultrasonic transducers in the palms of the gloves. The transducers are distance-ranging sensors that allow the user to specify a range of distances by natural gestures such as facing the palms towards each other or towards other surfaces. The user is able to create virtual models of physical objects by specifying their dimensions with hand gestures. We combine the reported distance with the tilt orientation data to construct virtual models. We also map the distance data to create a set of affine transformation techniques, including relative and fixed scaling, translation, and rotation. Our techniques can be generalized to different sensor technologies. © 2012 IEEE.","distance-based techniques; manipulation; modeling; ultrasonic gloves","Affine transformations; Distance datum; Distance-based; Hand gesture; Input devices; Interaction techniques; manipulation; Manipulation techniques; Orientation data; Original design; Physical objects; Sensor technologies; Tilt sensor; Virtual models; Virtual reality system; Augmented reality; Models; Sensors; Transducers; Ultrasonic transducers; Virtual reality; Ultrasonic testing",Conference Paper,"Final","",Scopus,2-s2.0-84873560024
"Stork A., Sevilmis N., Weber D., Gorecky D., Stahl C., Loskyll M., Michel F.","56234965900;8879416000;57135495000;36095668000;57213242480;42761907200;57196560681;","Enabling virtual assembly training in and beyond the automotive industry",2012,"Proceedings of the 2012 18th International Conference on Virtual Systems and Multimedia, VSMM 2012: Virtual Systems in the Information Society",,, 6365944,"347","352",,30,"10.1109/VSMM.2012.6365944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872014955&doi=10.1109%2fVSMM.2012.6365944&partnerID=40&md5=8a75643145301603c789009ddcdd5c1d","Interactive Engineering Technologies, Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany; Innovative Factory Systems, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany; Augmented Vision, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany","Stork, A., Interactive Engineering Technologies, Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany; Sevilmis, N., Interactive Engineering Technologies, Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany; Weber, D., Interactive Engineering Technologies, Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany; Gorecky, D., Innovative Factory Systems, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany; Stahl, C., Innovative Factory Systems, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany; Loskyll, M., Innovative Factory Systems, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany; Michel, F., Augmented Vision, German Research Center for Artificial Intelligence, DFKI, Kaiserslautern, Germany","Virtual assembly training systems show a high potential to complement or even replace physical setups for training of assembly processes in and beyond the automotive industry. The precondition for the breakthrough of virtual training is that it overcomes the problems of former approaches. The paper presents the design approach taken during the development of a game-based, virtual training system for procedural assembly knowledge in the EU-FP7 project VISTRA. One key challenge to address when developing virtual assembly training is the extensive authoring effort for setting up virtual environments. Although knowledge from the product and manufacturing design is available and could be used for virtual training, a concept for integration of this data is still missing. This paper presents the design of a platform which transfers available enterprise data into a unified model for virtual training and thus enables virtual training of workers at the assembly line before the physical prototypes exist. The data requirements and constraints stemming from industrial partners involved in the project will be discussed. A second hurdle for virtual training is the insufficient user integration and acceptance. In this context, the paper introduces an innovative hardware set-up for game-based user interaction, which has been chosen to enhance user involvement and acceptance of virtual training. © 2012 IEEE.","Data Integration; Industrial Training; PLM; User-Interaction; Virtual Reality","Assembly line; Assembly process; Data integration; Data requirements; Design approaches; Enterprise data; High potential; Industrial partners; Industrial training; Manufacturing design; PLM; Still missing; Training Systems; Unified model; User integration; User involvement; User-Interaction; Virtual assembly; Virtual training; Assembly; Automotive industry; Information science; Product design; Virtual reality; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84872014955
"Zellmann S., Aumüller M., Lang U.","55658645800;57221322452;7006119097;","Image-based remote real-Time volume rendering â Decoupling rendering from view point updates",2012,"Proceedings of the ASME Design Engineering Technical Conference","2","PARTS A AND B",,"1385","1394",,5,"10.1115/DETC2012-70811","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884604894&doi=10.1115%2fDETC2012-70811&partnerID=40&md5=c78a66607df7fe82794084cb5634eca5","Department of Computer Science, University of Cologne, Cologne, 50931, Germany","Zellmann, S., Department of Computer Science, University of Cologne, Cologne, 50931, Germany; Aumüller, M.; Lang, U., Department of Computer Science, University of Cologne, Cologne, 50931, Germany","Remote rendering is employed when the visualization task is too challenging for the hardware used to display a dataset or when it is too time consuming to transfer the complete dataset. Volume visualization with its dataset sizes growing with the 3rd power of their spatial resolution is such a task. Since remote rendering introduces additional sources of latency, its applicability to virtual environments is limited because of the required low delays from user action to displayed image. We counter these latencies with image-based rendering techniques: color image data along with additional depth information is warped, while new data has not been completely received. Using these approximate images, it is possible to decouple the cheap display phase from rendering. While depth values are trivially deduced for polygons, we contribute heuristics for volumetric datasets with varying transparency. Copyright © 2012 by ASME.",,"Color images; Data set size; Depth information; Image-Based Rendering; Remote rendering; Spatial resolution; Volume visualization; Volumetric data sets; Image reconstruction; Virtual reality; Visualization; Volume rendering; Design",Conference Paper,"Final","",Scopus,2-s2.0-84884604894
"Knoll A., Mayer H., Staub C., Bauernschmitt R.","7102250639;56247876800;35312391800;7004080519;","Selective automation and skill transfer in medical robotics: A demonstration on surgical knot-tying",2012,"International Journal of Medical Robotics and Computer Assisted Surgery","8","4",,"384","397",,11,"10.1002/rcs.1419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870398309&doi=10.1002%2frcs.1419&partnerID=40&md5=1a19027fba7a480621792efe07732dfb","Technische Universität München, Garching, Germany; Isar Kliniken München, Germany","Knoll, A., Technische Universität München, Garching, Germany; Mayer, H., Technische Universität München, Garching, Germany; Staub, C., Technische Universität München, Garching, Germany; Bauernschmitt, R., Isar Kliniken München, Germany","Background: Transferring non-trivial human manipulation skills to robot systems is a challenging task. There have been a number of attempts to design research systems for skill transfer, but the level of the complexity of the actual skills transferable to the robot was rather limited, and delicate operations requiring a high dexterity and long action sequences with many sub-operations were impossible to transfer. Methods: A novel approach to human-machine skill transfer for multi-arm robot systems is presented. The methodology capitalizes on the metaphor of 'scaffolded learning', which has gained widespread acceptance in psychology. The main idea is to formalize the superior knowledge of a teacher in a certain way to generate support for a trainee. In our case, the scaffolding is constituted by abstract patterns, which facilitate the structuring and segmentation of information during 'learning by demonstration'. The actual skill generalization is then based on simulating fluid dynamics. Results: The approach has been successfully evaluated in the medical domain for the delicate task of automated knot-tying for suturing with standard surgical instruments and a realistic minimally invasive robotic surgery system. © 2012 John Wiley & Sons, Ltd.","Automated surgery; Imitation; Learning by demonstration; Medical robotics; Scaffolding; Skill transfer","article; automation; computer interface; human; robotics; skill transfer; suturing method; task performance; Automation; Humans; Learning; Robotics; Surgery, Computer-Assisted; Surgical Procedures, Minimally Invasive; Suture Techniques",Article,"Final","",Scopus,2-s2.0-84870398309
"Aleotti J., Caselli S.","23026542300;7004694361;","Learning manipulation tasks from human demonstration and 3D shape segmentation",2012,"Advanced Robotics","26","16",,"1863","1884",,4,"10.1080/01691864.2012.703167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866405210&doi=10.1080%2f01691864.2012.703167&partnerID=40&md5=6019d98d0d7bd46365fa195b9b64089d","Robotics and Intelligent Machines Laboratory (RIMLab), Department of Information Engineering, University of Parma, 43124, Parma, Italy","Aleotti, J., Robotics and Intelligent Machines Laboratory (RIMLab), Department of Information Engineering, University of Parma, 43124, Parma, Italy; Caselli, S., Robotics and Intelligent Machines Laboratory (RIMLab), Department of Information Engineering, University of Parma, 43124, Parma, Italy","According to neuro-psychology studies, 3D shape segmentation plays an important role in human perception of objects because when an object is perceived for grasping it is first parsed in its constituent parts. This capability is missing in current robot planning systems, which are therefore hindered in their ability to plan part-specific grasps suitable for the current task. In this paper, a novel approach for part-based grasping is presented that combines 3D shape segmentation, programing by human demonstration and manipulation planning. The central advantage over previous approaches is the use of a topological method for shape segmentation enabling both object categorization and robot grasping according to the affordances of an object. Manipulation tasks are demonstrated in a virtual reality environment using a data glove and a motion tracker, and the specific parts of the objects where grasping occurs are learned and encoded in the task description. Tasks are then planned and executed in a robot environment targeting semantically relevant parts for grasping. Planning in the robot environment can be generalized to objects that are similar to the ones used for task demonstration, i.e. objects that belong to the same category. Results obtained in 3D simulation confirm that the proposed approach finds with less effort grasps appropriate for the requested task. © 2012 Taylor & Francis and The Robotics Society of Japan.","grasping; manipulation planning; programing by demonstration; shape segmentation; virtual reality","3-D shape; 3D simulations; Affordances; Data glove; grasping; Human perception; Manipulation planning; Manipulation task; Motion tracker; Object categorization; Programing by demonstrations; Robot environment; Robot grasping; Robot planning; Shape segmentation; Task description; Topological methods; Virtual-reality environment; Demonstrations; Three dimensional computer graphics; Virtual reality; Three dimensional",Article,"Final","",Scopus,2-s2.0-84866405210
"Mendonça C., Campos G., Dias P., Vieira J., Ferreira J.P., Santos J.A.","37030015300;8603962700;22333370800;57194062298;57190223504;26656923700;","On the improvement of localization accuracy with non-individualized HRTF-based sounds",2012,"AES: Journal of the Audio Engineering Society","60","10",,"821","830",,25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870349596&partnerID=40&md5=a5ee2375ecd2a37b4de4598005a92f86","University of Minho: School of Psychology, Centro Algoritmi, Centro de Computação Gráfica, Guimarães, Portugal; University of Aveiro, Department of Electronics Telecomunications and Informatics, Aveiro, Portugal","Mendonça, C., University of Minho: School of Psychology, Centro Algoritmi, Centro de Computação Gráfica, Guimarães, Portugal; Campos, G., University of Aveiro, Department of Electronics Telecomunications and Informatics, Aveiro, Portugal; Dias, P., University of Aveiro, Department of Electronics Telecomunications and Informatics, Aveiro, Portugal; Vieira, J., University of Aveiro, Department of Electronics Telecomunications and Informatics, Aveiro, Portugal; Ferreira, J.P., University of Minho: School of Psychology, Centro Algoritmi, Centro de Computação Gráfica, Guimarães, Portugal; Santos, J.A., University of Minho: School of Psychology, Centro Algoritmi, Centro de Computação Gráfica, Guimarães, Portugal","Auralization is a powerful tool to increase the realism and sense of immersion in Virtual Reality environments. The Head Related Transfer Function (HRTF) filters commonly used for auralization are non-individualized, as obtaining individualized HRTFs poses very serious practical difficulties. It is therefore extremely important to understand to what extent this hinders sound perception. In this paper we address this issue from a learning perspective. In a set of experiments, we observed that mere exposure to virtual sounds processed with generic HRTF did not improve the subjects' performance in sound source localization, but short training periods involving active learning and feedback led to significantly better results. We propose that using auralization with non-individualized HRTF should always be preceded by a learning period.",,"Active Learning; Auralizations; Head related transfer function; Localization accuracy; Sound perception; Sound source localization; Virtual sound; Virtual-reality environment; Virtual reality; Sound reproduction",Article,"Final","",Scopus,2-s2.0-84870349596
"Heuer H., Klimmer F., Luttmann A., Bolbach U.","24518497100;6603939446;7004148891;55383706700;","Specificity of motor learning in simulator training of endoscopic-surgery skills",2012,"Ergonomics","55","10",,"1157","1165",,11,"10.1080/00140139.2012.703697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867241236&doi=10.1080%2f00140139.2012.703697&partnerID=40&md5=6fb33eeadf761d0b551685436c2a8f61","IfADo - Leibniz Research Centre for Working Environment and Human Factors, Dortmund, Germany; Department of Urology at Klinikum Dortmund, Dortmund, Germany","Heuer, H., IfADo - Leibniz Research Centre for Working Environment and Human Factors, Dortmund, Germany; Klimmer, F., IfADo - Leibniz Research Centre for Working Environment and Human Factors, Dortmund, Germany; Luttmann, A., IfADo - Leibniz Research Centre for Working Environment and Human Factors, Dortmund, Germany; Bolbach, U., Department of Urology at Klinikum Dortmund, Dortmund, Germany","The introduction of simulators for the practice of endoscopic-surgery sensori-motor skills opens a wide range of design options. An obvious one is augmented visual information early in practice, in particular a direct view of the site instead of the endoscopic view. We studied the effects of such augmented visual information on the simulated ablation of tissue with straight, horizontal and parallel cuts. Direct view had an immediate beneficial effect on performance as compared with endoscopic-view practice. However, in subsequent tests with endoscopic view the benefits disappeared and turned into costs for some aspects of performance, e.g., duration. This finding highlights for a simulated surgical task that optimisation of practice by a performance criterion may not result in optimisation by a transfer criterion.Practitioner Summary: Endoscopic surgery represents a challenge for human sensori-motor skills, but new simulator-based training methods give leeway for optimisation. A candidate is augmented visual feedback, in particular a direct rather than endoscopic view of the site. However, performance becomes dependent on the augmented feedback so that the costs outweigh the benefits. © 2012 Copyright Taylor and Francis Group, LLC.","augmented feedback; resection; simulation; transfer","Augmented feedback; Beneficial effects; Design option; Endoscopic surgery; Motor learning; Optimisations; Parallel cuts; Performance criterion; resection; simulation; Simulator training; Surgical tasks; Training methods; transfer; Visual feedback; Visual information; Endoscopy; Optimization; Personnel training; Simulators; Surgery; Tissue; Visual communication; Surgical equipment; adaptive behavior; adult; algorithm; analysis of variance; article; clinical competence; computer interface; computer simulation; education; endoscopy; female; Germany; human; learning; male; methodology; motor performance; task performance; Adult; Algorithms; Analysis of Variance; Clinical Competence; Computer Simulation; Endoscopy; Feedback, Psychological; Female; Germany; Humans; Learning; Male; Motor Skills; Task Performance and Analysis; User-Computer Interface; Young Adult",Article,"Final","",Scopus,2-s2.0-84867241236
"Mendes F.A.D.S., Pompeu J.E., Lobo A.M., da Silva K.G., Oliveira T.D.P., Zomignani A.P., Piemonte M.E.P.","36139012800;35763916700;55292795300;55293039900;55340229200;55293704000;24081312100;","Motor learning, retention and transfer after virtual-reality-based training in Parkinson's disease - effect of motor and cognitive demands of games: A longitudinal, controlled clinical study",2012,"Physiotherapy (United Kingdom)","98","3",,"217","223",,119,"10.1016/j.physio.2012.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865179939&doi=10.1016%2fj.physio.2012.06.001&partnerID=40&md5=52ec3cdf35a6f5e71a446fa93d6eaad0","Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil; Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil; Faculty of Physical Therapy, Padre Anchieta University, Jundiai, Brazil","Mendes, F.A.D.S., Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil, Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil, Faculty of Physical Therapy, Padre Anchieta University, Jundiai, Brazil; Pompeu, J.E., Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil, Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil; Lobo, A.M., Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil, Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil; da Silva, K.G., Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil, Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil; Oliveira, T.D.P., Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil; Zomignani, A.P., Faculty of Physical Therapy, Padre Anchieta University, Jundiai, Brazil; Piemonte, M.E.P., Department of Neuroscience and Behaviour, Institute of Psychology, University of São Paulo, São Paulo, Brazil, Department of Physical Therapy, Faculty of Medical Science, University of São Paulo, São Paulo, Brazil","Objectives: To evaluate the learning, retention and transfer of performance improvements after Nintendo Wii Fit™ training in patients with Parkinson's disease and healthy elderly people. Design: Longitudinal, controlled clinical study. Participants: Sixteen patients with early-stage Parkinson's disease and 11 healthy elderly people. Interventions: Warm-up exercises and Wii Fit training that involved training motor (shifts centre of gravity and step alternation) and cognitive skills. A follow-up evaluative Wii Fit session was held 60 days after the end of training. Participants performed a functional reach test before and after training as a measure of learning transfer. Main outcome measures: Learning and retention were determined based on the scores of 10 Wii Fit games over eight sessions. Transfer of learning was assessed after training using the functional reach test. Results: Patients with Parkinson's disease showed no deficit in learning or retention on seven of the 10 games, despite showing poorer performance on five games compared with the healthy elderly group. Patients with Parkinson's disease showed marked learning deficits on three other games, independent of poorer initial performance. This deficit appears to be associated with cognitive demands of the games which require decision-making, response inhibition, divided attention and working memory. Finally, patients with Parkinson's disease were able to transfer motor ability trained on the games to a similar untrained task. Conclusions: The ability of patients with Parkinson's disease to learn, retain and transfer performance improvements after training on the Nintendo Wii Fit depends largely on the demands, particularly cognitive demands, of the games involved, reiterating the importance of game selection for rehabilitation purposes. © 2012 Chartered Society of Physiotherapy.","Executive function; Motor learning; Parkinson's disease; Rehabilitation; Transfer; Virtual reality","levodopa; aged; article; attention; clinical article; computer program; controlled study; decision making; exercise; follow up; functional training; human; learning; longitudinal study; motor performance; Parkinson disease; physiotherapy; priority journal; recreation; skill; virtual reality; working memory; Aged; Aging; Cognition; Exercise Therapy; Follow-Up Studies; Humans; Longitudinal Studies; Middle Aged; Motor Skills; Parkinson Disease; Physical Therapy Modalities; Postural Balance; Treatment Outcome; Video Games; Virtual Reality Exposure Therapy",Article,"Final","",Scopus,2-s2.0-84865179939
"Richards D., Szilas N.","57193711592;14042573800;","Challenging reality using techniques from interactive drama to support social simulations in virtual worlds",2012,"ACM International Conference Proceeding Series",,,,"","",,3,"10.1145/2336727.2336739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865323267&doi=10.1145%2f2336727.2336739&partnerID=40&md5=49f1a1909e18960e74b88d2d4deb6b86","Department of Computing, Macquarie University, NSW 2109, Australia; TECFA - FPSE, Université de Genève, CH 1211 Genève 4, Switzerland","Richards, D., Department of Computing, Macquarie University, NSW 2109, Australia; Szilas, N., TECFA - FPSE, Université de Genève, CH 1211 Genève 4, Switzerland","Simulations of social situations have great potential to be applied to many of the social problems that we find in society and organisations. Social simulations can do more than provide experience and transfer current best practice; they may be used to transform current social realities. As many educationalists, organizations and researchers are finding, Virtual Worlds (VWs) provide an environment for conducting person to person social simulations. In this paper we consider a more challenging form of social simulation in VWs involving intelligent social interactions between humans and computer-based non-player characters in VWs, known as intelligent virtual agents (IVAs). However, using IVAs to simulate social behavior requires some reconsideration of the role that reality plays and challenges the definition of a simulation as a representation of reality. By bringing in the element of fiction (non-reality) often associated with drama, narrative and storytelling together with virtual worlds, we can relax some of the constraints associated with reality and go beyond reality. In beyond reality simulations, we actually use simulations to exaggerate aspects of the real world in order to emphasize a particular learning concept or even to break the rules, strategies, roles and operators which apply in the real world. © 2012 ACM.","agents; beyond reality; interactive drama; narrative; reality; simulation; storytelling; virtual worlds","beyond reality; Interactive drama; narrative; reality; simulation; storytelling; Virtual worlds; Agents; Computer applications; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-84865323267
"Frikha T., Ben Amor N., Loukil K., Abid M.","55332242000;8412251500;24343842800;35742803600;","Virtual reality system on embedded platforms",2012,"7th International Conference on Design and Technology of Integrated Systems in Nanoscale Era, DTIS 2012",,, 6232969,"","",,2,"10.1109/DTIS.2012.6232969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864838542&doi=10.1109%2fDTIS.2012.6232969&partnerID=40&md5=1979325a0bd8012c85c6207d77c6f26f","CES-Laboratory, Sfax University, National Engineering School of Sfax, Sfax, Tunisia","Frikha, T., CES-Laboratory, Sfax University, National Engineering School of Sfax, Sfax, Tunisia; Ben Amor, N., CES-Laboratory, Sfax University, National Engineering School of Sfax, Sfax, Tunisia; Loukil, K., CES-Laboratory, Sfax University, National Engineering School of Sfax, Sfax, Tunisia; Abid, M., CES-Laboratory, Sfax University, National Engineering School of Sfax, Sfax, Tunisia","Virtual reality applications grow up nowadays. These applications are not only developed for the PCs but also for embedded systems such as game console, Smartphone, touchpad The limited touchpad resources and network communication between touchpad need an adaptation to environment noises. To solve this type of problem, we'll test a 3D application adaptation on FPGA platforms. These platforms will be simulated to touchpad. This application is assimilated to a virtual reality application. In this paper we describe the OS implementation results and the adaptation of the 3D application data to the network transfer [1]. © 2012 IEEE.",,"3D application; Embedded platforms; Game consoles; Network communications; Network transfers; Touchpad; Virtual reality system; Integrated control; Nanotechnology; Three dimensional; Virtual reality; Field programmable gate arrays (FPGA)",Conference Paper,"Final","",Scopus,2-s2.0-84864838542
"Aleotti J., Caselli S.","23026542300;7004694361;","Grasp programming by demonstration in virtual reality with automatic environment reconstruction",2012,"Virtual Reality","16","2",,"87","104",,10,"10.1007/s10055-010-0172-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861707584&doi=10.1007%2fs10055-010-0172-8&partnerID=40&md5=13bb20d3fa723fae654d5a100ec4a9e1","Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy","Aleotti, J., Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy; Caselli, S., Dipartimento di Ingegneria dell'Informazione, University of Parma, Parma, Italy","A virtual reality system enabling high-level programming of robot grasps is described. The system is designed to support programming by demonstration (PbD), an approach aimed at simplifying robot programming and empowering even unexperienced users with the ability to easily transfer knowledge to a robotic system. Programming robot grasps from human demonstrations requires an analysis phase, comprising learning and classification of human grasps, as well as a synthesis phase, where an appropriate human-demonstrated grasp is imitated and adapted to a specific robotic device and object to be grasped. The virtual reality system described in this paper supports both phases, thereby enabling end-to-end imitation-based programming of robot grasps. Moreover, as in the PbD approach robot environment interactions are no longer explicitly programmed, the system includes a method for automatic environment reconstruction that relieves the designer from manually editing the pose of the objects in the scene and enables intelligent manipulation. A workspace modeling technique based on monocular vision and computation of edge-face graphs is proposed. The modeling algorithm works in real time and supports registration of multiple views. Object recognition and workspace reconstruction features, along with grasp analysis and synthesis, have been tested in simulated tasks involving 3D user interaction and programming of assembly operations. Experiments reported in the paper assess the capabilities of the three main components of the system: the grasp recognizer, the vision-based environment modeling system, and the grasp synthesizer. © 2010 Springer-Verlag London Limited.","Environment modeling; Glove interaction; Grasp programming; Virtual reality","Assembly operations; Environment modeling; Environment modeling systems; Glove interaction; Grasp analysis; High-level programming; Intelligent manipulation; Modeling technique; Monocular vision; Multiple views; Programming by demonstration; Programming robots; Real time; Robot-environment interaction; Robotic devices; Robotic systems; Synthesis phase; User interaction; Virtual reality system; Vision based; Object recognition; Robot learning; Robotics; Three dimensional; Virtual reality; Robot programming",Article,"Final","",Scopus,2-s2.0-84861707584
"Gjerald S.U., Brekken R., Hergum T., D'Hooge J.","35769012100;22333318300;8626998400;7006639761;","Real-time ultrasound simulation using the GPU",2012,"IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","59","5", 6202412,"885","892",,13,"10.1109/TUFFC.2012.2273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861508893&doi=10.1109%2fTUFFC.2012.2273&partnerID=40&md5=eb45695a5397d6960d852e11f9e32867","Medical Imaging Lab, Department of Circulation and Medical Imaging, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Medical Technology, SINTEF, Trondheim, Norway; Laboratory on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Sciences, Katholieke Universiteit Leuven, Leuven, Belgium","Gjerald, S.U., Medical Imaging Lab, Department of Circulation and Medical Imaging, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Brekken, R., Medical Imaging Lab, Department of Circulation and Medical Imaging, Norwegian University of Science and Technology (NTNU), Trondheim, Norway, Department of Medical Technology, SINTEF, Trondheim, Norway; Hergum, T., Medical Imaging Lab, Department of Circulation and Medical Imaging, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; D'Hooge, J., Medical Imaging Lab, Department of Circulation and Medical Imaging, Norwegian University of Science and Technology (NTNU), Trondheim, Norway, Laboratory on Cardiovascular Imaging and Dynamics, Department of Cardiovascular Sciences, Katholieke Universiteit Leuven, Leuven, Belgium","Ultrasound simulators can be used for training ultrasound image acquisition and interpretation. In such simulators, synthetic ultrasound images must be generated in real time. Anatomy can be modeled by computed tomography (CT). Shadows can be calculated by combining reflection coefficients and depth dependent, exponential attenuation. To include speckle, a pre-calculated texture map is typically added. Dynamic objects must be simulated separately. We propose to increase the speckle realism and allow for dynamic objects by using a physical model of the underlying scattering process. The model is based on convolution of the point spread function (PSF) of the ultrasound scanner with a scatterer distribution. The challenge is that the typical field-of-view contains millions of scatterers which must be selected by a virtual probe from an even larger body of scatterers. The main idea of this paper is to select and sample scatterers in parallel on the graphic processing unit (GPU). The method was used to image a cyst phantom and a movable needle. Speckle images were produced in real time (more than 10 frames per second) on a standard GPU. The ultrasound images were visually similar to images calculated by a reference method. © 2012 IEEE.",,"Computed Tomography; Depth dependents; Dynamic objects; Exponential attenuation; Field of views; Frames per seconds; Graphic processing units; Physical model; Point-spread functions; Real time; Reference method; Scattering process; Speckle images; Texture maps; Ultrasound images; Ultrasound scanners; Ultrasound simulation; Virtual probes; Computerized tomography; Optical transfer function; Scattering; Speckle; Ultrasonics; algorithm; article; biological model; computer graphics; computer simulation; computer system; echography; human; image processing; image quality; methodology; teaching; Algorithms; Computer Graphics; Computer Simulation; Computer Systems; Computer-Assisted Instruction; Humans; Image Processing, Computer-Assisted; Models, Biological; Phantoms, Imaging; Ultrasonography",Article,"Final","",Scopus,2-s2.0-84861508893
"Kotranza A., Lind D.S., Lok B.","15061513400;7004808046;57203616548;","Real-time evaluation and visualization of learner performance in a mixed-reality environment for clinical breast examination",2012,"IEEE Transactions on Visualization and Computer Graphics","18","7", 5963665,"1101","1114",,10,"10.1109/TVCG.2011.132","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861462426&doi=10.1109%2fTVCG.2011.132&partnerID=40&md5=1e7979649708e7d7bc9698d84ce512b8","Department of Computer and Information Science and Engineering, University of Florida, E301 CSE Building, PO Box 116120, Gainesville, FL 32611, United States; Department of Surgery, Medical College of Georgia, BB 4514, Augusta, GA 30912, United States","Kotranza, A., Department of Computer and Information Science and Engineering, University of Florida, E301 CSE Building, PO Box 116120, Gainesville, FL 32611, United States; Lind, D.S., Department of Surgery, Medical College of Georgia, BB 4514, Augusta, GA 30912, United States; Lok, B., Department of Computer and Information Science and Engineering, University of Florida, E301 CSE Building, PO Box 116120, Gainesville, FL 32611, United States","We investigate the efficacy of incorporating real-time feedback of user performance within mixed-reality environments (MREs) for training real-world tasks with tightly coupled cognitive and psychomotor components. This paper presents an approach to providing real-time evaluation and visual feedback of learner performance in an MRE for training clinical breast examination (CBE). In a user study of experienced and novice CBE practitioners (n = 69), novices receiving real-time feedback performed equivalently or better than more experienced practitioners in the completeness and correctness of the exam. A second user study (n = 8) followed novices through repeated practice of CBE in the MRE. Results indicate that skills improvement in the MRE transfers to the real-world task of CBE of human patients. This initial case study demonstrates the efficacy of MREs incorporating real-time feedback for training real-world cognitive-psychomotor tasks. © 2012 IEEE.","information visualization; life and medical sciences; Mixed and augmented reality","Human patients; Information visualization; Life and medical science; Mixed-reality environment; Real-time feedback; Real-world task; Tightly-coupled; User performance; User study; Visual feedback; Augmented reality; Information systems; Visual communication; Visualization; article; audiovisual equipment; breast; computer graphics; computer interface; female; human; medical informatics; methodology; palpation; physiology; pressure; sensory feedback; teaching; Breast; Computer Graphics; Computer-Assisted Instruction; Feedback, Sensory; Female; Humans; Medical Informatics Applications; Models, Anatomic; Palpation; Pressure; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84861462426
"Jardón A., Victores J.G., Martínez S., Balaguer C.","14627718800;36716896800;56250315700;6701864168;","Experience acquisition simulator for operating microtuneling boring machines",2012,"Automation in Construction","23",,,"33","46",,6,"10.1016/j.autcon.2011.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858058964&doi=10.1016%2fj.autcon.2011.12.002&partnerID=40&md5=7ba0e580a14a766b7e9d7c72142d1728","Robotics Lab, Engineering Systems and Automation Department, Universidad Carlos III de Madrid, 28911 Leganés, Spain","Jardón, A., Robotics Lab, Engineering Systems and Automation Department, Universidad Carlos III de Madrid, 28911 Leganés, Spain; Victores, J.G., Robotics Lab, Engineering Systems and Automation Department, Universidad Carlos III de Madrid, 28911 Leganés, Spain; Martínez, S., Robotics Lab, Engineering Systems and Automation Department, Universidad Carlos III de Madrid, 28911 Leganés, Spain; Balaguer, C., Robotics Lab, Engineering Systems and Automation Department, Universidad Carlos III de Madrid, 28911 Leganés, Spain","This paper describes an innovative modeling and training framework and an simulator application for micro tunneling machines under heterogeneous gravel and sand soils. From the selective collection of skilled pilots' know-how of a pipe jacking microtunnelling machine in operation, to generate a rule-based system based on grouped rules and states that replicates machine performance. The adjustment of these states and associated rules allows creation, setup and analysis of a realistic functional model for tunneling machines. The system integrates a friendly human machine interface (HMI) that closely replicates real machine's pilot cabinet and allows natural interaction with the implemented inference engine through the simulated control panels. Additionally, the framework allows the training of tunneling machine's operators by simulation and subsequent gathered data analysis. The virtual pilot's desk is the first implementation of a jack piping microtunneling machine simulator by means of pilot's steering know-how capture methodology. © 2011 Published by Elsevier B.V. All rights reserved.","Pipe jacking training; Robotic automation; Simulator; Tunnel execution modeling; Tunneling machine","Control panels; Functional model; Human Machine Interface; Know-how; Machine performance; Micro-tunneling; Microtunnelling; Natural interactions; Pipe jacking; Robotic automation; Sand soils; Training framework; Virtual pilots; Jacks; Personnel training; Simulators; Technology transfer; Tunneling machines; Construction equipment",Article,"Final","",Scopus,2-s2.0-84858058964
"Zhang L., Grosdemouge C., Arikatla V.S., Ahn W., Sankaranarayanan G., De S., Jones D., Schwaitzberg S., Cao C.G.L.","37076447700;54580819000;26654027600;9334513400;15623319200;7202304567;55387240300;7007036892;25957557800;","The added value of virtual reality technology and force feedback for surgical training simulators",2012,"Work","41","SUPPL.1",,"2288","2292",,17,"10.3233/WOR-2012-0453-2288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859820585&doi=10.3233%2fWOR-2012-0453-2288&partnerID=40&md5=02f7bd8976824eac796a5655c5f32b8a","Department of Mechanical Engineering, Tufts University, 200 College Avenue, Medford, MA, United States; Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Department of Surgery, Beth Israel Deaconess Medical Center, TCC 140, 330 Brookline Avenue, Boston, MA, United States; Department of Surgery, Cambridge Health Alliance Hospital, 1493 Cambridge Street, Cambridge, MA, United States","Zhang, L., Department of Mechanical Engineering, Tufts University, 200 College Avenue, Medford, MA, United States; Grosdemouge, C., Department of Mechanical Engineering, Tufts University, 200 College Avenue, Medford, MA, United States; Arikatla, V.S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Ahn, W., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Sankaranarayanan, G., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; De, S., Center for Modeling, Simulation and Imaging in Medicine, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Jones, D., Department of Surgery, Beth Israel Deaconess Medical Center, TCC 140, 330 Brookline Avenue, Boston, MA, United States; Schwaitzberg, S., Department of Surgery, Cambridge Health Alliance Hospital, 1493 Cambridge Street, Cambridge, MA, United States; Cao, C.G.L., Department of Mechanical Engineering, Tufts University, 200 College Avenue, Medford, MA, United States","Laparoscopic surgery requires more specialized training of the surgeons than traditional open surgery. The Virtual Basic Laparoscopic Surgical Trainer (VBLaST) is being developed as a virtual version of the Fundamentals of Laparoscopic Skills (FLS) trainer. This study assessed the current haptic and virtual reality (VR) technology of a virtual peg transfer task of the VBLaST, based on the subjective preference of surgeons and their objective task performance measures. Twenty-one surgical residents, fellows and attendings performed a peg-transfer task in the FLS and the VBLaST. Each subject performed 10 trials on each simulator. Results showed that subjects performed significantly better on the FLS than on the VBLaST. Subjects showed a significant learning effect on both simulators, but with an accelerated improvement on the VBLaST. Even so, 81% of the subjects preferred the FLS over the VBLaST for surgical training which could be attributed to the novelty of the VR technology and existing deficiencies of the haptic interface. Despite the subjective preference for the physical simulator, the performance results indicate an added value of VR and haptics in surgical training, which is expected to be demonstrated in more surgically relevant tasks such as suturing and knot-tying. © 2012 - IOS Press and the authors. All rights reserved.","force feedback; fundamentals of laparoscopic skills (FLS); surgical training; virtual basic laparoscopic surgical trainer (VBLaST); virtual reality (VR)","article; computer interface; computer simulation; education; evaluation study; feedback system; human; laparoscopy; touch; United States; Boston; Computer Simulation; Feedback; Humans; Laparoscopy; Touch; User-Computer Interface",Conference Paper,"Final","",Scopus,2-s2.0-84859820585
"Edmunds T., Pai D.K.","36494232300;7102473274;","Perceptually augmented simulator design",2012,"IEEE Transactions on Haptics","5","1", 5975145,"66","76",,2,"10.1109/TOH.2011.42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858377683&doi=10.1109%2fTOH.2011.42&partnerID=40&md5=41a93c00224665e46eca47311716cd62","Sensorimotor Systems Lab, Department of Computer Science, University of British Columbia, Vancouver, BC V6T 1Z4, Canada","Edmunds, T., Sensorimotor Systems Lab, Department of Computer Science, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Pai, D.K., Sensorimotor Systems Lab, Department of Computer Science, University of British Columbia, Vancouver, BC V6T 1Z4, Canada","Training simulators have proven their worth in a variety of fields, from piloting to air-traffic control to nuclear power station monitoring. Designing surgical simulators, however, poses the challenge of creating trainers that effectively instill not only high-level understanding of the steps to be taken in a given situation, but also the low-level muscle-memory needed to perform delicate surgical procedures. It is often impossible to build an ideal simulator that perfectly mimics the haptic experience of a surgical procedure, but by focussing on the aspects of the experience that are perceptually salient we can build simulators that effectively instill learning. We propose a general method for the design of surgical simulators that augment the perceptually salient aspects of an interaction. Using this method, we can increase skill-transfer rates without requiring expensive improvements in the capability of the rendering hardware or the computational complexity of the simulation. In this paper, we present our decomposition-based method for surgical simulator design, and describe a user-study comparing the training effectiveness of a haptic-search-task simulator designed using our method versus an unaugmented simulator. The results show that perception-based task decomposition can be used to improve the design of surgical simulators that effectively impart skill by targeting perceptually significant aspects of the interaction. © 2012 IEEE.","and virtual realities; artificial; augmented; Haptic I/O; life and medical sciences; surgical simulation","artificial; augmented; Haptic I/O; Life and medical science; Surgical simulation; Air traffic control; Design; Surgical equipment; Virtual reality; Simulators",Article,"Final","",Scopus,2-s2.0-84858377683
"de Tommaso D., Calinon S., Caldwell D.G.","36727098400;6507248533;7202685497;","A Tangible Interface for Transferring Skills: Using Perception and Projection Capabilities in Human-Robot Collaboration Tasks",2012,"International Journal of Social Robotics","4","4",,"397","408",,4,"10.1007/s12369-012-0154-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868019236&doi=10.1007%2fs12369-012-0154-y&partnerID=40&md5=ba94ddad59e26339bf0ef3aa7306c248","Department of Advanced Robotics, Istituto Italiano di Tecnologia, via Morego, 30, 16163 Genova, Italy","de Tommaso, D., Department of Advanced Robotics, Istituto Italiano di Tecnologia, via Morego, 30, 16163 Genova, Italy; Calinon, S., Department of Advanced Robotics, Istituto Italiano di Tecnologia, via Morego, 30, 16163 Genova, Italy; Caldwell, D.G., Department of Advanced Robotics, Istituto Italiano di Tecnologia, via Morego, 30, 16163 Genova, Italy","Our research focuses on exploring new modalities to make robots acquire skills in a fast and user-friendly manner. In this work we present a novel active interface with perception and projection capabilities for simplifying the skill transfer process. The interface allows humans and robots to interact with each other in the same environment, with respect to visual feedback. During the learning process, the real workspace is used as a tangible interface for helping the user to better understand what the robot has learned up to then, to display information about the task or to get feedback and guidance. Thus, the user is able to incrementally visualize and assess the learner's state and, at the same time, focus on the skill transfer without disrupting the continuity of the teaching interaction. We also propose a proof-of-concept, as a core element of the architecture, based on an experimental setting where a pico-projector and an rgb-depth sensor are mounted onto the end-effector of a 7-DOF robotic arm. © 2012 Springer Science & Business Media BV.","Augmented reality; Human-Robot interaction; Learning from Demonstration; Tangible interfaces","Augmented reality; End effectors; Feedback; Human computer interaction; Interfaces (computer); Visual communication; Visual servoing; Active interface; Experimental settings; Human-robot collaboration; Learning from demonstration; Learning process; Pico projectors; Proof of concept; Tangible interfaces; Human robot interaction",Article,"Final","",Scopus,2-s2.0-84868019236
"Knecht M., Traxler C., Purgathofer W., Wimmer M.","57202623079;36740557200;6603751465;7103054284;","Adaptive camera-based color mapping for mixed-reality applications",2011,"2011 10th IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2011",,, 6092382,"165","168",,6,"10.1109/ISMAR.2011.6092382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84055184251&doi=10.1109%2fISMAR.2011.6092382&partnerID=40&md5=170bdac7b5d660ffd7881767d9f7acc4","Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria; VRVis, Center for Virtual Reality and Visualization Research, Ltd., Austria","Knecht, M., Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria; Traxler, C., VRVis, Center for Virtual Reality and Visualization Research, Ltd., Austria; Purgathofer, W., Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria; Wimmer, M., Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria","We present a novel adaptive color mapping method for virtual objects in mixed-reality environments. In several mixed-reality applications, added virtual objects should be visually indistinguishable from real objects. Recent mixed-reality methods use global-illumination algorithms to approach this goal. However, simulating the light distribution is not enough for visually plausible images. Since the observing camera has its very own transfer function from real-world radiance values to RGB colors, virtual objects look artificial just because their rendered colors do not match with those of the camera. Our approach combines an on-line camera characterization method with a heuristic to map colors of virtual objects to colors as they would be seen by the observing camera. Previous tone-mapping functions were not designed for use in mixed-reality systems and thus did not take the camera-specific behavior into account. In contrast, our method takes the camera into account and thus can also handle changes of its parameters during runtime. The results show that virtual objects look visually more plausible than by just applying tone-mapping operators. © 2011 IEEE.","Color Matching; Differential Rendering; Mixed Reality; Tone Mapping","Camera characterization; Color mapping; Differential Rendering; Light distribution; Mixed reality; Mixed-reality environment; Radiance values; Real objects; Runtimes; Tone mapping; Virtual objects; Augmented reality; Cameras; Color; Color printing; Heuristic methods; Image matching; Mapping; Virtual reality; Color matching",Conference Paper,"Final","",Scopus,2-s2.0-84055184251
"Stadon J., Grasset R.","35318731100;11240216000;","A syncretic approach to artistic research in mixed reality data transfer",2011,"2011 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities, ISMAR-AMH 2011",,, 6093659,"67","72",,2,"10.1109/ISMAR-AMH.2011.6093659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755174264&doi=10.1109%2fISMAR-AMH.2011.6093659&partnerID=40&md5=a7bada76286ef15538e4b911051116d1","Curtin University of Technology, Australia; Graz University of Technology, Austria","Stadon, J., Curtin University of Technology, Australia; Grasset, R., Graz University of Technology, Austria","This paper offers a contribution to an emerging culturally orientated discourse regarding mixed reality interaction. It seeks to define syncretic, or hybridized agency, particularly in social mixed reality data transfer artworks. Recent developments in bridging relationships with digital representation of identity through mixed reality interfacing, have brought about the need for further analysis of these new post-biological, hybridized states of being that traverse traditional paradigms of biophysics. time and space, and artistic practice. Roy Ascott's interpretation of syncretism within digital networks may facilitate further understanding of multi-layered consciousness, both material and metaphysical, that are emerging from our engagement with such pervasive computational technologies and post-biological systems. Syncretism has traditionally been regarded as an attempt to harmonise and analogise [1] Citing recent examples of practical research outcomes, this paper will cite what Deleuze and Guattari have called deterritorialisation of the human body and its dispersion throughout multiple reality manifestations and how mixed reality data transfer might constitute a reterritorialising effect that creates a syncretic post-biological digital identity for the user [2]. © 2011 IEEE.","Art; Cross-reality; Dual Reality; Media Art; Metaverse; Performing Arts; Post-humanism","Art; Cross-reality; Dual Reality; Media arts; Metaverse; Performing arts; Post-humanism; Augmented reality; Biological materials; Data transfer; Electronic document identification systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-83755174264
"Stadon J.","35318731100;","Post-biological hypersurfacing: Embodied mixed reality data transfer",2011,"Proceedings - 2011 International Conference on Cyberworlds, Cyberworlds 2011",,, 6079377,"264","268",,,"10.1109/CW.2011.41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83355174214&doi=10.1109%2fCW.2011.41&partnerID=40&md5=861fdbb7cfe18afdbda3ffa6eb48e9c6","Department of Art, Department of Internet Studies, Curtin University, Perth, Australia","Stadon, J., Department of Art, Department of Internet Studies, Curtin University, Perth, Australia","This paper focuses on the (d)evolving interface between cyber worlds and the real world, what Giannachi has called the hyper surface. This fusion of real and representation, linking cyber and real worlds constitutes mixed reality interaction as experienced by humans in the physical world, their avatars, agents, and virtual humans. Current mixed reality XML RPC (Remote Procedure Call) interfaces and real-time data transfer enhance the experience of the hyper surface for the audience beyond any previous virtual media types, such as, hypertext, HTML, VRML, virtual reality, etc. Previous research in mixed reality and interactive workspaces that use the concept of a bridge for data transfer have largely inspired this research and I aim to continue the development of new knowledge in this field by critically applying cultural discourse in order to develop a theory regarding the impact of such systems on the notion of post biological identity. © 2011 IEEE.","cyberworlds; human-computer interaction; networked collaborations; social networking; virtual humans and avatars","Cyberworlds; Hyper-surfaces; Interactive workspace; Media types; Mixed reality; Networked collaboration; Physical world; Real-time data; Remote Procedure Call; Virtual humans; XML-RPC; Data transfer; Human computer interaction; Hypertext systems; Real time systems; Social networking (online); Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-83355174214
"Iwata T., Yamabe T., Nakajima T.","35791972100;7102734073;35249011400;","Augmented reality go: Extending traditional game play with interactive self-learning support",2011,"Proceedings - 17th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications, RTCSA 2011","1",, 6029834,"105","114",,17,"10.1109/RTCSA.2011.43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855545940&doi=10.1109%2fRTCSA.2011.43&partnerID=40&md5=2247f77cb040907285a91115b43fb446","Department of Computer Science and Engineering, Waseda University, Japan","Iwata, T., Department of Computer Science and Engineering, Waseda University, Japan; Yamabe, T., Department of Computer Science and Engineering, Waseda University, Japan; Nakajima, T., Department of Computer Science and Engineering, Waseda University, Japan","The augmented reality (AR)-based learning support has several advantages over virtual reality or PC applications. AR enables to maintain the physical interaction that an activity originally offers, thus the skills and knowledge acquired in an augmented learning process can be intuitively applied to practice use. Whereas lots of AR-based self-learning support systems have been developed in previous studies, it has not been sufficiently evaluated how it influences a learner's mindset and the efficiency of training. In this paper, we investigate the user experience brought by AR technologies in a self-learning process. We chose the game of Go as a study program, and developed the Augmented Reality Go (ARGo) system to compare the AR and conventional PC-based learning assistance. We found that the physical interaction with the original game apparatus enhanced the subjects' intrinsic motivation towards self-learning. Moreover, the original look-and-feel induced deeper concentration and higher elaboration on problem solving. Design issues are also discussed to generalize the concept of AR self-learning support towards broader application domains. © 2011 IEEE.",,"Application domains; Design issues; Intrinsic motivation; Learning process; Learning support; PC applications; PC-based; Physical interactions; Self-learning; Support systems; User experience; Augmented reality; Virtual reality; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-84855545940
"Stadon J., Grasset R.","35318731100;11240216000;","Syncretic post-biological digital identity: Hybridizing mixed reality data transfer systems",2011,"Proceedings - IEEE International Symposium on Distributed Simulation and Real-Time Applications",,, 6051788,"120","125",,,"10.1109/DS-RT.2011.28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255179686&doi=10.1109%2fDS-RT.2011.28&partnerID=40&md5=ba37a1d1dbb1c5c9dd356eddef3dee93","Curtin University, 18 Ethel Street, North Perth, WA 6050, Australia; Graz University of Technology, Austria","Stadon, J., Curtin University, 18 Ethel Street, North Perth, WA 6050, Australia; Grasset, R., Graz University of Technology, Austria","This paper offers a contribution to an emerging culturally orientated discourse regarding mixed reality interaction. It seeks to analyse syncretic, hybridized agency, particularly in mixed reality data transfer systems. Recent developments in bridging autonomous relationships with digital representation through mixed reality interfacing, have brought about the need for further analysis of these new 'post-biological', hybridized states of being that traverse traditional paradigms of time and space. Roy Ascott's concept of syncretism may facilitate further understanding of multi-layered world views, both material and metaphysical, that are emerging from our engagement with such pervasive computational technologies and post-biological systems. Syncretism has traditionally been regarded as an attempt to harmonise and analogise [1] Citing recent examples of practical research outcomes, this paper will analyse what Gilles Deleuze and Fèlix Guattari have called 'deterritorialisation' of the human body through its dispersion throughout multiple reality manifestations and how mixed reality data transfer might constitute a 'reterritorialising' effect on syncretic post-biological digital identity construction [2]. © 2011 IEEE.","Art; cross-reality; dual reality; Media art; Metaverse; Performing Arts; post-humanism; transhumanism","Art; cross-reality; dual reality; Media arts; Metaverse; Performing arts; post-humanism; Transhumanism; Biological materials; Data transfer; Electronic document identification systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-82255179686
"Hoppen M., Rossmann J., Schluse M., Waspe R., Rast M.","35310501100;56251698200;6507542372;35312849100;36161393400;","Combining 3D simulation technology with object-oriented databases: A database oriented approach to virtual reality systems",2011,"Proceedings of the ASME Design Engineering Technical Conference","2","PARTS A AND B",,"1545","1554",,4,"10.1115/DETC2011-48230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863572265&doi=10.1115%2fDETC2011-48230&partnerID=40&md5=73fc3d9faadd94d079fddbed51781099","Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany","Hoppen, M., Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany; Rossmann, J., Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany; Schluse, M., Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany; Waspe, R., Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany; Rast, M., Institute for Man-Machine Interaction, RWTH Aachen University, Aachen, 52074, Germany","Using object-oriented databases as the primary data source in VR applications has a variety of advantages, but requires the development of new techniques concerning data modeling, data handling and data transfer from a Virtual Reality system's point of view. The many advantages are outlined in the first part of this paper. We first introduce versioning and collaboration techniques as our main motivation. These can also be used in the traditional file based approach, but are much more powerful when realized with a database on an object and attribute level. Using an object-oriented approach to data modeling, objects of the real world can be modeled more intuitively by defining appropriate classes with their relevant attributes. Furthermore, databases can function as central communication hubs for consistent multi user interaction. Besides, the use of databases with open interface standards allows to easily cooperate with other applications such as modeling tools and other data generators. The second part of this paper focuses on our approach to seamlessly integrate such databases in Virtual Reality systems. For this we developed an object-oriented internal graph database and linked it to object-oriented external databases for central storage and collaboration. Object classes defined by XML data schemata allow to easily integrate new data models in VR applications at run-time. A fully transparent database layer in the simulation system makes it easy to interchange the external database. We present the basic structure of our simulation graph database, as well as the mechanisms which are used to transparently map data and meta-data from the external database to the simulation database. To show the validity and flexibility of our approach selected applications realized with our simulation system so far e. g. applications based on geoinformation databases such as forest inventory systems and city models, applications in the field of distributed control and simulation of assembly lines or database-driven virtual testbeds applications for automatic map generation in planetary landing missions are introduced. © 2011 by ASME.",,"3D simulations; Assembly line; Attribute levels; Basic structure; City model; Communication hub; Database layer; Distributed control; Forest inventory; Geo-information; Graph database; Map data; Map generation; Modeling tool; Multi-user interaction; Object class; Object oriented; Object oriented approach; Object oriented database; Open Interface; Planetary landing; Primary data source; Runtimes; Simulation graphs; Simulation systems; Versioning; Virtual reality system; Virtual test beds; VR applications; XML data; Data handling; Data transfer; Design; Distributed parameter control systems; Three dimensional computer graphics; Virtual reality; Database systems",Conference Paper,"Final","",Scopus,2-s2.0-84863572265
"Yang S., Hwang W.-H., Tsai Y.-C., Liu F.-K., Hsieh L.-F., Chern J.-S.","7408515375;54581055700;54581748500;54581103000;7101988949;36819947700;","Improving balance skills in patients who had stroke through virtual reality treadmill training",2011,"American Journal of Physical Medicine and Rehabilitation","90","12",,"969","978",,69,"10.1097/PHM.0b013e3182389fae","https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855176112&doi=10.1097%2fPHM.0b013e3182389fae&partnerID=40&md5=cdf3f4b9c54f117dd3a1d988db6321ce","Institute of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan; Rehabilitation Medicine, Cheng-Hsin Rehabilitation Medical Center, Taipei, Taiwan; Department of Rehabilitation, Shin Kong Wu Ho-Su Memorial Hospital, Taipei, Taiwan; Department of Occupational Therapy, Chang Gung University, No. 259, Wen-Hwa 1st Rd, Kwei-Shan Township, Taoyuan County, 333, Taiwan","Yang, S., Institute of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan; Hwang, W.-H., Institute of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan; Tsai, Y.-C., Institute of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan; Liu, F.-K., Rehabilitation Medicine, Cheng-Hsin Rehabilitation Medical Center, Taipei, Taiwan; Hsieh, L.-F., Department of Rehabilitation, Shin Kong Wu Ho-Su Memorial Hospital, Taipei, Taiwan; Chern, J.-S., Department of Occupational Therapy, Chang Gung University, No. 259, Wen-Hwa 1st Rd, Kwei-Shan Township, Taoyuan County, 333, Taiwan","Objective: The aim of this study was to evaluate the effects of virtual reality (VR) treadmill training on the balance skills of patients who have had a stroke. Design: A total of 14 patients with strokes were recruited and randomly assigned to receive VR treadmill or traditional treadmill training. The outcome measures that were included for the study were center of pressure (COP) sway excursion, COP maximum sway in anterior-posterior direction, COP maximum sway in medial-lateral direction, COP sway area, bilateral limb-loading symmetric index, the sway excursion values for the paretic foot (sway excursion/P), paretic limb stance time (stance time/P), number of steps of the paretic limb (number of steps/P), and contact area of the paretic foot (contact A/P) during quiet stance, sit-to-stand transfer, and level walking. Results: There were no significant improvements in COP-related measures and symmetric index during the quiet stance, either in the VR treadmill or traditional treadmill training group (P > 0.05). However, the difference between groups after training in COP maximum sway in medial-lateral direction during the quiet stance was significant (P = 0.038). Traditional treadmill training failed to improve sit-to-stand performance, whereas VR treadmill training improved symmetric index (P = 0.028) and sway excursion (P = 0.046) significantly during sit-to-stand transfer. The changes of symmetric index between groups were markedly different (P = 0.045). Finally, both groups improved significantly in stance time/P, but only VR treadmill training increased contact A/P (P = 0.034) after training during level walking. The difference between groups during level walking was not significant. Conclusions: Neither traditional treadmill nor VR treadmill training had any effect on balance skill during quiet stance, but VR treadmill training improved balance skill in the medial-lateral direction better than traditional training did. VR treadmill training also improved balance skill during sit-to-stand transfers and the involvement of paretic limb in level walking more than the traditional one did. Copyright © 2011 by Lippincott Williams & Wilkins.","Stroke; Treadmill Exercise; Virtual Reality; Weight Shifting","aged; article; body equilibrium; clinical trial; comparative study; computer interface; computer simulation; controlled clinical trial; controlled study; convalescence; exercise test; female; hospitalization; human; male; middle aged; nonparametric test; physiology; randomized controlled trial; reference value; stroke; walking; Aged; Computer Simulation; Exercise Test; Female; Humans; Male; Middle Aged; Postural Balance; Recovery of Function; Reference Values; Severity of Illness Index; Statistics, Nonparametric; Stroke; User-Computer Interface; Walking",Article,"Final","",Scopus,2-s2.0-81855176112
"Aleotti J., Caselli S.","23026542300;7004694361;","Part-based robot grasp planning from human demonstration",2011,"Proceedings - IEEE International Conference on Robotics and Automation",,, 5979632,"4554","4560",,33,"10.1109/ICRA.2011.5979632","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866370534&doi=10.1109%2fICRA.2011.5979632&partnerID=40&md5=95ea888d6b1e6e6fac562cec2a62cdd6","RIMLab. - Robotics and Intelligent Machines Laboratory, Dipartimento di Ingegneria dell'Informazione, University of Parma, Italy","Aleotti, J., RIMLab. - Robotics and Intelligent Machines Laboratory, Dipartimento di Ingegneria dell'Informazione, University of Parma, Italy; Caselli, S., RIMLab. - Robotics and Intelligent Machines Laboratory, Dipartimento di Ingegneria dell'Informazione, University of Parma, Italy","In this work we introduce a novel approach for robot grasp planning. The proposed method combines the benefits of programming by human demonstration for teaching appropriate grasps with those of automatic 3D shape segmentation for object recognition and semantic modeling. The work is motivated by important studies on human manipulation suggesting that when an object is perceived for grasping it is first parsed in its constituent parts. Following these findings we present a manipulation planning system capable of grasping objects by their parts which learns new tasks from human demonstration. The central advantage over previous approaches is the use of a topological method for shape segmentation enabling both object retrieval and part-based grasp planning according to the affordances of an object. Manipulation tasks are demonstrated in a virtual reality environment using a data glove. After the learning phase, each task is planned and executed in a robot environment that is able to generalize to similar, but previously unknown, objects. © 2011 IEEE.",,"3-D shape; Affordances; Data glove; Grasp planning; Grasping objects; Human manipulation; Learning phase; Manipulation planning; Manipulation task; Object retrieval; Robot environment; Semantic modeling; Shape segmentation; Topological methods; Virtual-reality environment; Demonstrations; Object recognition; Robot learning; Semantics; Virtual reality; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84866370534
"Ha T., Woo W.","15020792200;35575439600;","ARWand: Phone-based 3D object manipulation in augmented reality environment",2011,"Proceedings - 2011 International Symposium on Ubiquitous Virtual Reality, ISUVR 2011",,, 6068304,"44","47",,13,"10.1109/ISUVR.2011.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82055162736&doi=10.1109%2fISUVR.2011.14&partnerID=40&md5=2f03f1e5d872add11cc4b5a1988ebc6e","GIST U-VR Lab., 500-712, South Korea","Ha, T., GIST U-VR Lab., 500-712, South Korea; Woo, W., GIST U-VR Lab., 500-712, South Korea","In this paper, we suggest a mobile phone-based indirect 3D object manipulation method that uses sensor information in an augmented reality environment. Specifically, we propose 1) a method that exploits a 2D touch screen, a 3DOF accelerometer, and compass sensors information to manipulate 3D objects in 3D space, 2) design transfer functions to map the control space of mobile phones to an augmented reality (AR) display space, and 3) confirm the feasibility of the transfer functions by implementation. Our work could be applicable to the design and implementation of a future mobile phone-based 3D user interface for AR application in normal indoor and outdoor environments without any special tracking installations. © 2011 IEEE.","3D object manipulation; Augmented reality; HMD-based wearable computing; mobile phone; sensor based interaction","3-D space; 3D object; 3D user interface; AR application; Control spaces; Outdoor environment; Sensor informations; Touch screen; Wearable computing; Accelerometers; Augmented reality; Mobile phones; Sensors; Telephone sets; Transfer functions; User interfaces; Virtual reality; Wearable computers; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-82055162736
"Skinner A.D., Tan V., Berka C.","26658704300;36919261100;57204279614;","Use of neurophysiological metrics within a real and virtual perceptual skills task to assess virtual training environment fidelity requirements",2011,"Proceedings of the Human Factors and Ergonomics Society",,,,"2188","2192",,,"10.1177/1071181311551456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855183860&doi=10.1177%2f1071181311551456&partnerID=40&md5=49b6ad958073ccc3fe60e09267684c43","AnthroTronix, Inc., United States; Advanced Brain Monitoring, Inc., United States","Skinner, A.D., AnthroTronix, Inc., United States; Tan, V., Advanced Brain Monitoring, Inc., United States; Berka, C., Advanced Brain Monitoring, Inc., United States","Both military and civilian virtual environment (VE) developers are increasingly looking to cognitive scientists to provide virtual training platforms to support optimal training effectiveness within significant time and cost constraints. A need exists for objective and efficient methods of identifying optimal fidelity characteristics to support transfer of training. Previous research has demonstrated that brain eventrelated potentials (ERP' s) are sensitive to slight variations in virtual task environment fidelity, even in cases in which task performance does not significantly differ. The current experiment compared physiological and performance data for subjects completing a real-world perceptual discrimination task, as well as a similarly-structured VE training task under systematically varied fidelity conditions, in an effort to characterize the impact of the various conditions. Significant differences were found for task condition (real world versus virtual, as well as visual stimulus parameters within each condition), within both the performance and physiological data. Copyright 2011 by Human Factors and Ergonomics Society, Inc. All rights reserved.",,"Cost constraints; Event related potentials; Optimal training; Perceptual skills; Performance data; Physiological data; Task environment; Task performance; Virtual environments; Virtual training; Visual stimulus; Ergonomics; Human computer interaction; Optimization; Physiological models; Physiology; Virtual reality; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-81855183860
"Bernhard M., Grosse K., Wimmer M.","36633791800;54880797000;7103054284;","Bimodal task-facilitation in a virtual traffic scenario through spatialized sound rendering",2011,"ACM Transactions on Applied Perception","8","4", 24,"","",,4,"10.1145/2043603.2043606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855213891&doi=10.1145%2f2043603.2043606&partnerID=40&md5=d0c12e5e5510b519150a7ac34b38c028","Vienna University of Technology, Austria","Bernhard, M., Vienna University of Technology, Austria; Grosse, K., Vienna University of Technology, Austria; Wimmer, M., Vienna University of Technology, Austria","Audio rendering is generally used to increase the realism of virtual environments (VE). In addition, audio rendering may also improve the performance in specific tasks carried out in interactive applications such as games or simulators. In this article we investigate the effect of the quality of sound rendering on task performance in a task which is inherently vision-dominated. The task is a virtual traffic gap-crossing scenario with two elements: first, to discriminate crossable and uncrossable gaps in oncoming traffic, and second, to find the right timing to start crossing the street without an accident. A study was carried out with 48 participants in an immersive virtual environment setup with a large screen and headphones. Participants were grouped into three different scenarios. In the first one, spatialized audio rendering with head-related transfer function (HRTF) filtering was used. The second group was tested with conventional stereo rendering, and the remaining group ran the experiment in a mute condition. Our results give a clear evidence that spatialized audio improves task performance compared to the unimodal mute condition. Since all task-relevant information was in the participants' field-of-view, we conclude that an enhancement of task performance results from a bimodal advantage due to the integration of visual and auditory spatial cues. © 2011 ACM.","Audio-visual perception; Bimodal; Human-computer interaction; Pedestrian safety; Pedestrian simulator; Spatialized audio rendering; Task facilitation; Virtual environments","Audio rendering; Bimodal; Field of views; Head related transfer function; Immersive virtual environments; Interactive applications; Large screen; Quality of sounds; Second group; Spatial cues; Spatialized audio; Spatialized sound; Specific tasks; Task facilitation; Task performance; Unimodal; Virtual environments; Human computer interaction; Pedestrian safety; Sensory perception; Sound reproduction; Virtual reality; Audio acoustics",Article,"Final","",Scopus,2-s2.0-84855213891
"Chen Y.-C., Chi H.-L., Hung W.-H., Kang S.-C.","56942712900;35096047900;7201803676;23090862400;","Use of tangible and augmented reality models in engineering graphics courses",2011,"Journal of Professional Issues in Engineering Education and Practice","137","4",,"267","276",,71,"10.1061/(ASCE)EI.1943-5541.0000078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855195990&doi=10.1061%2f%28ASCE%29EI.1943-5541.0000078&partnerID=40&md5=1332e9db88a220d97b02900adf49460a","National Taiwan Univ., Dept. of Civil Engineering, Taipei City 10617, Taiwan","Chen, Y.-C., National Taiwan Univ., Dept. of Civil Engineering, Taipei City 10617, Taiwan; Chi, H.-L., National Taiwan Univ., Dept. of Civil Engineering, Taipei City 10617, Taiwan; Hung, W.-H., National Taiwan Univ., Dept. of Civil Engineering, Taipei City 10617, Taiwan; Kang, S.-C., National Taiwan Univ., Dept. of Civil Engineering, Taipei City 10617, Taiwan","Engineering graphics courses are typically a requirement for engineering students around the world. Besides understanding and depicting graphic representation of engineering objects, the goal of these courses is to provide students with an understanding of the relationship between three-dimensional (3D) objects and their projections. However, in the classroom, where time is limited, it is very difficult to explain 3D geometry using only drawings on paper or at the blackboard. The research presented herein aims to develop two teaching aids; a tangible model and an augmented reality (AR) model, to help students better understand the relationship between 3D objects and their projections. Tangible models refer to the physical objects which are comprised of a set of differently shaped pieces. The tangible model we developed includes eight wooden blocks that include all the main geometrical features with respect to their 3D projections. The AR models are the virtual models which can superimpose 3D graphics of typical geometries on real-time video and dynamically vary view perspective in real-time to be seen as real objects. The AR model was developed using the ARToolKitPlus library and includes all the geometrical features generally taught in engineering graphics courses or technical drawing courses. To verify the effectiveness and applicability of the models we developed, we conducted a user test on 35 engineering-major students. The statistical results indicated that the tangible model significantly increased the learning performance of students in their abilities to transfer 3D objects onto two-dimensional (2D) projections. Students also demonstrated higher engagement with the AR model during the learning process. Compared to using the screen-based orthogonal and pictorial images, the tangible model and augmented reality model were evaluated to be more effective teaching aids for engineering graphics courses. © 2011 American Society of Civil Engineers.","Augmented reality; Engineering education; Engineering graphics; Graphic methods; Models; Tangible model","3D geometry; 3D graphics; 3D object; AR models; Effective teaching; Engineering graphics; Engineering objects; Geometrical features; Graphic representation; Learning performance; Learning process; Physical objects; Pictorial images; Real objects; Real time videos; Tangible models; Teaching aids; Technical drawing; Three-dimensional (3D) objects; Two-dimensional (2D) projection; User tests; Virtual models; Wooden blocks; Augmented reality; Content based retrieval; Drawing (graphics); Education computing; Engineering education; Graphic methods; Graphical user interfaces; Models; Professional aspects; Students; Teaching; Three dimensional",Article,"Final","",Scopus,2-s2.0-81855195990
"Maçães G., Pimenta W., Carvalho E.","50961471900;50961440700;57197985379;","Using augmented reality virtual assistants to teach the traditional leather tanning process",2011,"Proceedings of the 6th Iberian Conference on Information Systems and Technologies, CISTI 2011",,, 5974179,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052431137&partnerID=40&md5=b7e103a263b06c0c6ccc1bba5f39ec6b","CCG - Centro de Computação Gráfica, Guimarães, Portugal","Maçães, G., CCG - Centro de Computação Gráfica, Guimarães, Portugal; Pimenta, W., CCG - Centro de Computação Gráfica, Guimarães, Portugal; Carvalho, E., CCG - Centro de Computação Gráfica, Guimarães, Portugal","The leather tanning process carries a high cultural value in Guimarães city. In some areas of Portugal, most of the society and its history are somehow interlaced with the history of the leather and all the associated processes of its production. There is a great lack of a ""vivid"" memory about the leather tanning process and its social implications. References to it in Guimarães city can be reduced to text and poor quality pictures. On the other hand, it is important to transmit to future generations the valuable role of the leather tanning in the local culture. At this point, the use of augmented reality techniques can help to illustrate interactively, expressively and with a compelling degree of immersion how this process used to be done, and help to preserve its memory. This project consisted in the development and implementation of an augmented reality application to young audiences that illustrates the ancient leather tanning process step-by-step. The resulting visualizations offer an impressive education experience that help to transfer a significant culture value of the Guimarães city. © 2011 AISTI.","Augmented reality; cultural heritage; multimedia learning","Augmented reality applications; Cultural heritages; Cultural value; Future generations; Leather tanning; Multimedia learning; Portugal; Social implication; Step-by-step; Augmented reality; Information systems; Leather; Virtual reality; Visualization; Tanning",Conference Paper,"Final","",Scopus,2-s2.0-80052431137
"Shepherd I.D.H., Bleasdale-Shepherd I.D.","7006459882;36162886400;","The design-by-adaptation approach to universal access: Learning from videogame technology",2011,"Universal Access in the Information Society","10","3",,"319","336",,2,"10.1007/s10209-010-0204-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960646197&doi=10.1007%2fs10209-010-0204-x&partnerID=40&md5=ce01bbecaea5ced46fb9791b6301f5dd","Department of Marketing and Enterprise, Middlesex University, The Burroughs, London NW4 4BT, United Kingdom; Valve Software, 10900 NE 4th St., Suite 500, Bellevue, WA 98004, United States","Shepherd, I.D.H., Department of Marketing and Enterprise, Middlesex University, The Burroughs, London NW4 4BT, United Kingdom; Bleasdale-Shepherd, I.D., Valve Software, 10900 NE 4th St., Suite 500, Bellevue, WA 98004, United States","This paper proposes an alternative approach to the design of universally accessible interfaces to that provided by formal design frameworks applied ab initio to the development of new software. This approach, design-byadaptation, involves the transfer of interface technology and/or design principles from one application domain to another, in situations where the recipient domain is similar to the host domain in terms of modelled systems, tasks and users. Using the example of interaction in 3D virtual environments, the paper explores how principles underlying the design of videogame interfaces may be applied to a broad family of visualization and analysis software which handles geographical data (virtual geographic environments, or VGEs). One of the motivations behind the current study is that VGE technology lags some way behind videogame technology in the modelling of 3D environments, and has a less-developed track record in providing the variety of interaction methods needed to undertake varied tasks in 3D virtual worlds by users with varied levels of experience. The current analysis extracted a set of interaction principles from videogames which were used to devise a set of 3D task interfaces that have been implemented in a prototype VGE for formal evaluation. © 2010 Springer-Verlag.","Adaptation; GIS; Transfer; User interfaces; Videogames; Virtual geographical environments","3-D environments; 3-D virtual environment; Ab initio; Adaptation; Alternative approach; Application domains; Current analysis; Design Principles; Formal design; Geographical data; Interaction methods; Interface technology; Task interface; Track record; Transfer; Universal access; Video game; Virtual geographic environments; Virtual geographical environments; Virtual worlds; Visualization and analysis; Data visualization; Formal methods; Geographic information systems; Interactive computer graphics; Technology; Three dimensional; User interfaces; Virtual reality; Visualization; Design",Article,"Final","",Scopus,2-s2.0-79960646197
"Rohs M., Oulasvirta A., Suomalainen T.","17346497700;13006124600;35254183100;","Interaction with magic lenses: Real-world validation of a Fitts' law model",2011,"Conference on Human Factors in Computing Systems - Proceedings",,,,"2725","2728",,26,"10.1145/1978942.1979343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958152378&doi=10.1145%2f1978942.1979343&partnerID=40&md5=5282decce28aef43d212e86e0843862d","Deutsche Telekom Laboratories, TU Berlin, Germany; Ludwig-Maximilians-Universität München, Munich, Germany; Helsinki Institute for Information Technology, HIIT, Aalto University, Finland; Helsinki Institute for Information Technology, HIIT, University of Helsinki, Finland","Rohs, M., Deutsche Telekom Laboratories, TU Berlin, Germany, Ludwig-Maximilians-Universität München, Munich, Germany; Oulasvirta, A., Helsinki Institute for Information Technology, HIIT, Aalto University, Finland, Helsinki Institute for Information Technology, HIIT, University of Helsinki, Finland; Suomalainen, T., Helsinki Institute for Information Technology, HIIT, Aalto University, Finland, Helsinki Institute for Information Technology, HIIT, University of Helsinki, Finland","Rohs and Oulasvirta (2008) proposed a two-component Fitts' law model for target acquisition with magic lenses in mobile augmented reality (AR) with 1) a physical pointing phase, in which the target can be directly observed on the background surface, and 2) a virtual pointing phase, in which the target can only be observed through the device display. The model provides a good fit (R2=0.88) with laboratory data, but it is not known if it generalizes to real-world AR tasks. In the present outdoor study, subjects (N=12) did building-selection tasks in an urban area. The differences in task characteristics to the laboratory study are drastic: targets are three-dimensional and they vary in shape, size, z-distance, and visual context. Nevertheless, the model yielded an R2 of 0.80, and when using effective target width an R2 of 0.88 was achieved. Copyright 2011 ACM.","Augmented reality; Field experiment; Fitts' law; Humanperformance modeling; Magic lens pointing; Target acquisition","Field experiment; Fitts' law; Human-performance; Magiclenses; Target acquisition; Augmented reality; Display devices; Human computer interaction; Human engineering; Virtual reality; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-79958152378
"Hamon L., Lucidarme P., Richard E., Richard P.","26767792700;6603318317;14036205600;57210457077;","Virtual reality and programming by demonstration:Teaching a robot to grasp a dynamic object by the generalization of human demonstrations",2011,"Presence: Teleoperators and Virtual Environments","20","3",,"241","253",,2,"10.1162/PRES_a_00047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052150278&doi=10.1162%2fPRES_a_00047&partnerID=40&md5=bba09c2f06a80cf07fae935808f49a3b","LISA Laboratory, University of Angers, 49000 Angers, France","Hamon, L., LISA Laboratory, University of Angers, 49000 Angers, France; Lucidarme, P., LISA Laboratory, University of Angers, 49000 Angers, France; Richard, E., LISA Laboratory, University of Angers, 49000 Angers, France; Richard, P., LISA Laboratory, University of Angers, 49000 Angers, France","Humans possess the ability to perform complex manipulations without the need to consciously perceive detailed motion plans. When a large number of trials and tests are required for techniques such as learning by imitation and programming by demonstration, the virtual reality approach provides an effective method. Indeed, virtual environments can be built economically and quickly, and can be automatically reinitialized. In the fields of robotics and virtual reality, this has now become commonplace. Rather than imitating human actions, our focus is to develop an intuitive and interactive method based on user demonstrations to create humanlike, autonomous behavior for a virtual character or robot. Initially, a virtual character is built via real-time virtual simulation in which the user demonstrates the task by controlling the virtual agent. The necessary data (position, speed, etc.) to accomplish the task are acquired in a Cartesian space during the demonstration session. These data are then generalized off-line by using a neural network with a back-propagation algorithm. The objective is to model a function that represents the studied task, and by so doing, to adapt the agent to deal with new cases. In this study, the virtual agent is a 6-DOF arm manipulator, Kuka Kr6, and the task is to grasp a ball thrown into its workspace. Our approach is to find a minimum number of necessary demonstrations while maintaining adequate task efficiency. Moreover, the relationship between the number of dimensions of the estimated function and the number of human trials is studied, depending on the evolution of the learning system. © 2011 by the Massachusetts Institute of Technology.",,"Autonomous behaviors; Cartesian Space; Dynamic objects; Human actions; Interactive methods; Learning by imitation; Programming by demonstration; Virtual agent; Virtual character; Virtual environments; Virtual simulations; Backpropagation algorithms; Demonstrations; Neural networks; Robot programming; Robots; User interfaces; Virtual reality",Article,"Final","",Scopus,2-s2.0-80052150278
"Rezazadeh I.M., Wang X., Firoozabadi M., Hashemi Golpayegani M.R.","15844046500;8945580300;6506647252;15843404600;","Using affective human-machine interface to increase the operation performance in virtual construction crane training system: A novel approach",2011,"Automation in Construction","20","3",,"289","298",,59,"10.1016/j.autcon.2010.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952624201&doi=10.1016%2fj.autcon.2010.10.005&partnerID=40&md5=3b3261e268baa37c5a20c027201e52ae","Department of Biomedical Eng., Science and Research Branch, Islamic Azad University, Tehran, Iran; Faculty of Built Environment, University of New South Wales, Australia; Department of Housing and Interior Design, Kyung Hee University, Seoul, South Korea; School of Medical Sciences, Tarbiat Modares University, Tehran, Iran; Department of Biomedical Eng., Amir Kabir University of Technology, Tehran, Iran","Rezazadeh, I.M., Department of Biomedical Eng., Science and Research Branch, Islamic Azad University, Tehran, Iran; Wang, X., Faculty of Built Environment, University of New South Wales, Australia, Department of Housing and Interior Design, Kyung Hee University, Seoul, South Korea; Firoozabadi, M., Department of Biomedical Eng., Science and Research Branch, Islamic Azad University, Tehran, Iran, School of Medical Sciences, Tarbiat Modares University, Tehran, Iran; Hashemi Golpayegani, M.R., Department of Biomedical Eng., Science and Research Branch, Islamic Azad University, Tehran, Iran, Department of Biomedical Eng., Amir Kabir University of Technology, Tehran, Iran","In the construction industry, some progress have been achieved by researchers to design and implement environments for task training using VR technology and its derivatives such as Augmented and Mixed Reality. Although, these developments have been well recognized at the application level, however crucial to the virtual training system is the effective and reliable measurement of training performance of the particular skill and handling the experiment for long-run. It is known that motor skills cannot be measured directly, but only inferred by observing behaviour or performance measures. The typical way of measuring performance is through measuring task completion time and accuracy, but can be supported by indirect measurement of some other factors. In this paper, a virtual crane training system has been developed which can be controlled using control commands extracted from facial gestures and is capable to lift up loads/materials in the virtual construction sites. Then, we integrate affective computing concept into the conventional VR training platform for measuring the cognitive load and level of satisfaction during performance using human's forehead bioelectric-signals. By employing the affective measures and our novel control scheme, the designed interface could be adapted to user's affective status during the performance in real-time. This adaptable user interface approach helps the trainee to cope with the training for long-run performance, leads to gaining more expertise and provides more effective transfer of learning to other operation environments. The detailed methodology of the affective control is presented in the paper. The results and future applications of the proposed method for disabled users, especially from neck down are discussed. © 2010 Elsevier B.V. All rights reserved.","Affective computing; Affective measures; Construction training; Facial bioelectric-signals; Virtual Reality","Adaptable user interfaces; Affective Computing; Affective measures; Application level; Cognitive loads; Construction training; Control command; Facial bioelectric-signals; Facial gestures; Future applications; Human Machine Interface; Indirect measurements; Level of satisfaction; Long-run performance; Measuring performance; Mixed reality; Motor skills; Novel control scheme; Operation performance; Performance measure; Reliable measurement; Task completion time; Training platform; Training Systems; Transfer of learning; Virtual construction; Virtual construction site; Virtual training; VR technology; Construction industry; Cranes; Human computer interaction; Man machine systems; User interfaces; Virtual reality; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-79952624201
"Omerčević D., Leonardis A.","23397688500;7003317327;","Hyperlinking reality via camera phones",2011,"Machine Vision and Applications","22","3",,"521","534",,3,"10.1007/s00138-010-0285-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958793923&doi=10.1007%2fs00138-010-0285-9&partnerID=40&md5=4be667dafb35e87449a226acc1332238","Faculty of Computer and Information Science, University of Ljubljana, Tržaška cesta 25, Ljubljana, Slovenia","Omerčević, D., Faculty of Computer and Information Science, University of Ljubljana, Tržaška cesta 25, Ljubljana, Slovenia; Leonardis, A., Faculty of Computer and Information Science, University of Ljubljana, Tržaška cesta 25, Ljubljana, Slovenia","A novel user interface concept for camera phones, called ""Hyperlinking Reality via Camera Phones"", that we present in this article, provides a solution to one of the main challenges facing mobile user interfaces, that is, the problem of selection and visualization of actions that are relevant to the user in her current context. Instead of typing keywords on a small and inconvenient keypad of a mobile device, a user of our system just snaps a photo of her surroundings and objects in the image become hyperlinks to information. Our method commences by matching a query image to reference panoramas depicting the same scene that were collected and annotated with information beforehand. Once the query image is related to the reference panoramas, we transfer the relevant information from the reference panoramas to the query image. By visualizing the information on the query image and displaying it on the camera phone's (multi-)touch screen, the query image augmented with hyperlinks allows the user intuitive access to information. © Springer-Verlag 2010.","Augmented reality; Image matching using local invariant features; Image-based localization; Wide baseline stereo matching","Camera Phone; Hyperlinking; Hyperlinks; Image-based; Mobile user interface; Query images; Touch screen; Wide baseline stereo matching; Augmented reality; Global system for mobile communications; Hypertext systems; Image matching; Mobile devices; Mobile phones; Telephone sets; User interfaces; Virtual reality; Visualization; Cameras",Article,"Final","",Scopus,2-s2.0-79958793923
"Mouawad M.R., Doust C.G., Max M.D., McNulty P.A.","41461659800;41461314300;41461634900;7103088810;","Wii-based movement therapy to promote improved upper extremity function post-stroke: A pilot study",2011,"Journal of Rehabilitation Medicine","43","6",,"527","533",,141,"10.2340/16501977-0816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959497136&doi=10.2340%2f16501977-0816&partnerID=40&md5=dd0fa8b228bbe1dcffd373778688dbe7","Neuroscience Research Australia, University of New South Wales, Sydney, Australia; Neuroscience Research Australia, Barker Street, Randwick, NSW 2031, Australia","Mouawad, M.R., Neuroscience Research Australia, University of New South Wales, Sydney, Australia; Doust, C.G., Neuroscience Research Australia, University of New South Wales, Sydney, Australia; Max, M.D., Neuroscience Research Australia, University of New South Wales, Sydney, Australia; McNulty, P.A., Neuroscience Research Australia, University of New South Wales, Sydney, Australia, Neuroscience Research Australia, Barker Street, Randwick, NSW 2031, Australia","Background: Virtual-reality is increasingly used to improve rehabilitation outcomes. The Nintendo Wii offers an inexpensive alternative to more complex systems. Objective: To investigate the efficacy of Wii-based therapy for post-stroke rehabilitation. Methods: Seven patients (5 men, 2 women, aged 42-83 years; 1-38 months post-stroke, mean 15.3 months) and 5 healthy controls (3 men, 2 women, aged 41-71 years) undertook 1 h of therapy on 10 consecutive weekdays. Patients progressively increased home practice to 3 h per day. Results: Functional ability improved for every patient. The mean performance time significantly decreased per Wolf Motor Function Test task, from 3.2 to 2.8 s, and Fugl-Meyer Assessment scores increased from 42.3 to 47.3. Upper extremity range-of-motion increased by 20.1° and 14.33° for passive and active movements, respectively. Mean Motor Activity Log (Quality of Movement scale) scores increased from 63.2 to 87.5, reflecting a transfer of functional recovery to everyday activities. Balance and dexterity did not improve significantly. No significant change was seen in any of these measures for healthy controls, despite improved skill levels for Wii games. Conclusion: An intensive 2-week protocol resulted in significant and clinically relevant improvements in functional motor ability post-stroke. These gains translated to improvement in activities of daily living. © 2011 Foundation of Rehabilitation Information.","Functional ability; Hemiplegia; Motor control; Stroke rehabilitation; Upper limb; Virtual reality","adult; aged; arm; article; computer interface; daily life activity; female; hemiplegia; human; joint characteristics and functions; male; middle aged; outcome assessment; pathophysiology; physiology; pilot study; psychomotor performance; recreation; stroke; time; treatment outcome; Activities of Daily Living; Adult; Aged; Aged, 80 and over; Female; Hemiplegia; Humans; Male; Middle Aged; Outcome Assessment (Health Care); Pilot Projects; Psychomotor Performance; Range of Motion, Articular; Stroke; Time Factors; Treatment Outcome; Upper Extremity; User-Computer Interface; Video Games",Article,"Final","",Scopus,2-s2.0-79959497136
"White S.A., Prachyabrued M., Chambers T.L., Borst C.W., Reiners D.","55452526400;18936813700;7202454867;9736479200;6507129285;","Low-cost simulated MIG welding for advancement in technical training",2011,"Virtual Reality","15","1",,"69","81",,9,"10.1007/s10055-010-0162-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951950718&doi=10.1007%2fs10055-010-0162-x&partnerID=40&md5=4949d7a5a2faa7342746e263e141d8e2","University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States","White, S.A., University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States; Prachyabrued, M., University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States; Chambers, T.L., University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States; Borst, C.W., University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States; Reiners, D., University of Louisiana at Lafayette, 537 Cajundome Boulevard, Lafayette, LA 70506, United States","The simulated MIG lab (sMIG) is a training simulator for Metal Inert Gas (MIG) welding. It is based on commercial off the shelf (COTS) components and targeted at familiarizing beginning students with the MIG equipment and best practices to follow to become competent and effective MIG welders. To do this, it simulates the welding process as realistically as possible using standard welding hardware components (helmet, gun) for input and by using head-tracking and a 3D-capable low-cost monitor and standard speakers for output. We developed a simulation to generate realistic audio and visuals based on numerical heat transfer methods and verified the accuracy against real welds. sMIG runs in real time producing a realistic, interactive, and immersive welding experience while maintaining a low installation cost. In addition to being realistic, the system provides instant feedback beyond what is possible in a traditional lab. This help students avoid learning (and unlearning) incorrect movement patterns. © 2010 Springer-Verlag London Limited.","Acoustics; Finite difference; Simulation; Virtual reality; Welding","Best-practices; Commercial off-the-shelf components; Finite difference; Hardware components; Immersive; Installation costs; Metal inert gas welding; MIG welding; Movement pattern; Numerical heat transfer; Real time; Simulation; Technical training; Training simulator; Welding process; Electric welding; Heat transfer; Inert gas welding; Inert gases; Numerical methods; Virtual reality",Article,"Final","",Scopus,2-s2.0-79951950718
"Dunston P.S., Wang X.","6602079727;8945580300;","An iterative methodology for mapping mixed reality technologies to AEC operations",2011,"Electronic Journal of Information Technology in Construction","16",,,"509","528",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955676092&partnerID=40&md5=1ede6b41610c1588946eff5c359ea4b4","School of Civil Engineering, Purdue University, United States; Construction Management and Property Program, Faculty of the Built Environment, University of New South Wales, Australia","Dunston, P.S., School of Civil Engineering, Purdue University, United States; Wang, X., Construction Management and Property Program, Faculty of the Built Environment, University of New South Wales, Australia","Mixed Reality (MR) technology can create an environment where objects from the digital and real worlds can be combined together in a real time manner. The practical contribution of this paper is a structured and iterative methodology developed for mapping appropriate Mixed Reality technology to a specific architecture, engineering, and construction (AEC) task. To increase the likelihood of success in technology transfer, this methodology for developing user-centered, performance enhancing MR-based systems is formulated, where AEC tasks are generically analyzed and categorized according to common functional features, which could be mapped to a collection of suitable or required MR-related technology strategies. Also, a technology selection process is identified to choose appropriate technology characteristics including information representations, interaction methods and, tracking technology for a specific task category. Such a thorough mapping methodology can be used to guide new MR-based system design as well as to help evaluate existing systems. © 2011 The authors.","AEC; Augmented Reality; Mixed Reality; Specification; Technology-task mapping","AEC; Appropriate technologies; Architecture , engineering , and constructions; Existing systems; Functional features; Information representation; Interaction methods; Iterative methodology; Mapping methodology; Mixed Reality; Mixed reality technologies; Performance enhancing; Real time; Specific tasks; System design; Technology selection; Technology strategies; Tracking technology; User-centered; Augmented reality; Mapping; Specifications; Systems analysis; Technology transfer; Virtual reality",Article,"Final","",Scopus,2-s2.0-79955676092
"Schreuder H.W.R., Oei G., Maas M., Borleffs J.C.C., Schijven M.P.","7004444996;7005217897;7103204541;35902591300;6602492995;","Implementation of simulation in surgical practice: Minimally invasive surgery has taken the lead: The Dutch experience",2011,"Medical Teacher","33","2",,"105","115",,38,"10.3109/0142159X.2011.550967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551555014&doi=10.3109%2f0142159X.2011.550967&partnerID=40&md5=082cf8ac167a6886a1a7191e429c43ff","Division of Women and Baby, Department of Gynecologic Surgery and Oncology, University Medical Centre Utrecht, P.O. Box 85500, 3508 GA, Utrecht, Netherlands; Máxima Medical Centre, Veldhoven, Netherlands; Academic Medical Centre, Amsterdam, Netherlands; University of Groningen, Netherlands; Department of Surgery, Academic Medical Centre, Amsterdam, Netherlands","Schreuder, H.W.R., Division of Women and Baby, Department of Gynecologic Surgery and Oncology, University Medical Centre Utrecht, P.O. Box 85500, 3508 GA, Utrecht, Netherlands; Oei, G., Máxima Medical Centre, Veldhoven, Netherlands; Maas, M., Academic Medical Centre, Amsterdam, Netherlands; Borleffs, J.C.C., University of Groningen, Netherlands; Schijven, M.P., Department of Surgery, Academic Medical Centre, Amsterdam, Netherlands","Minimal invasive techniques are rapidly becoming standard surgical techniques for many surgical procedures. To develop the skills necessary to apply these techniques, box trainers and/or inanimate models may be used, but these trainers lack the possibility of inherent objective classification of results. In the past decade, virtual reality (VR) trainers were introduced for training minimal invasive techniques. Minimally invasive surgery (MIS) is, by nature, very suitable for this type of training. The specific psychomotor skills and eye-hand coordination needed for MIS can be mastered largely using VR simulation techniques. It is also possible to transfer skills learned on a simulator to real operations, resulting in error reduction and shortening of procedural operating time. The authors aim to enlighten the process of gaining acceptance in the Netherlands for novel training techniques. The Dutch Societies of Surgery, Obstetrics and Gynecology, and Urology each developed individual training curricula for MIS using simulation techniques, to be implemented in daily practice. The ultimate goal is to improve patient safety. The authors outline the opinions of actors involved, such as different simulators, surgical trainees, surgeons, surgical societies, hospital boards, government, and the public. The actual implementation of nationwide training curricula for MIS is, however, a challenging step. © 2011 Informa UK Ltd All rights reserved.",,"article; clinical competence; computer interface; computer simulation; education; human; minimally invasive surgery; Netherlands; patient care; psychomotor performance; Clinical Competence; Computer Simulation; Humans; Netherlands; Patient Care Team; Psychomotor Performance; Surgical Procedures, Minimally Invasive; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-79551555014
"Deutsch J.E.","7201985389;","Using virtual reality to improve walking post-stroke: Translation to individuals with diabetes",2011,"Journal of Diabetes Science and Technology","5","2",,"309","314",,11,"10.1177/193229681100500216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053193651&doi=10.1177%2f193229681100500216&partnerID=40&md5=3d4cbed9fd801c234bf41c42920ccbfb","Department of Rehabilitation and Movement Sciences, School of Health Related Professions, University of Medicine and Dentistry, New Jersey, Newark, NJ, United States","Deutsch, J.E., Department of Rehabilitation and Movement Sciences, School of Health Related Professions, University of Medicine and Dentistry, New Jersey, Newark, NJ, United States","Use of virtual reality (VR) technology to improve walking for people post-stroke has been studied for its clinical application since 2004. The hardware and software used to create these systems has varied but has predominantly been constituted by projected environments with users walking on treadmills. Transfer of training from the virtual environment to real-world walking has modest but positive research support. Translation of the research findings to clinical practice has been hampered by commercial availability and costs of the VR systems. Suggestions for how the work for individuals post-stroke might be applied and adapted for individuals with diabetes and other impaired ambulatory conditions include involvement of the target user groups (both practitioners and clients) early in the design and integration of activity and education into the systems. © Diabetes Technology Society.","Fitness; Interactive video gaming; Mobility; Motor control; Stroke; Virtual reality","cerebrovascular accident; clinical practice; conference paper; diabetes mellitus; education; financial management; human; lifestyle modification; mobilization; physical activity; treadmill; virtual reality; walking; biomechanics; computer interface; computer simulation; diabetes mellitus; exercise; health promotion; kinesiotherapy; methodology; review; stroke; treatment outcome; walking; Biomechanics; Computer Simulation; Diabetes Mellitus; Exercise; Exercise Therapy; Health Promotion; Humans; Stroke; Treatment Outcome; User-Computer Interface; Walking",Conference Paper,"Final","",Scopus,2-s2.0-80053193651
"Moyano-Cuevas J.L., Sánchez-Margallo F.M., Sánchez-Peralta L.F., Pagador J.B., Enciso S., Sánchez-González P., Gómez-Aguilera E.J., Usón-Gargallo J.","35811111400;6507750669;35811197200;7801485529;37101357400;24512442400;7201729604;6603379138;","Validation of SINERGIA as training tool: A randomized study to test the transfer of acquired basic psychomotor skills to LapMentor",2011,"International Journal of Computer Assisted Radiology and Surgery","6","6",,"839","846",,5,"10.1007/s11548-011-0561-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027954549&doi=10.1007%2fs11548-011-0561-9&partnerID=40&md5=ec0184e7bf9f76df97e55ce29b496299","Bioengineering and Health Technology Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Laparoscopy Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Bioengineering and Telemedicine Centre, Univesidad Politécnica de Madrid, Madrid, Spain","Moyano-Cuevas, J.L., Bioengineering and Health Technology Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Sánchez-Margallo, F.M., Laparoscopy Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Sánchez-Peralta, L.F., Bioengineering and Health Technology Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Pagador, J.B., Bioengineering and Health Technology Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Enciso, S., Laparoscopy Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain; Sánchez-González, P., Bioengineering and Telemedicine Centre, Univesidad Politécnica de Madrid, Madrid, Spain; Gómez-Aguilera, E.J., Bioengineering and Telemedicine Centre, Univesidad Politécnica de Madrid, Madrid, Spain; Usón-Gargallo, J., Laparoscopy Unit, Jesús Usón Minimally Invasive Surgery Centre, Cáceres, Spain","Purpose: Laparoscopic surgery is commonly used in many surgical procedures but requires a learning process to develop the necessary skills. Virtual reality simulators play an essential role within the training curricula. This paper aims to determine whether training in SINERGIA VR simulator allows novice surgeons to improve their basic psychomotor laparoscopic skills. Methods: Forty-two people participated in this study, including 28 unexperience medical students and 14 expert surgeons who developed previously more than 100 laparoscopic procedures. Medical students made a pre-training test in LapMentor II; then, they trained in SINERGIA and they finally accomplished a post-training test in LapMentor II. Experts just made one trial in LapMentor II. A statistical analysis was carried out and results of pre- and post-training tests of novices were compared with Wilcoxon signed-rank test. Pre- and post-training tests of novices were also compared with results of experts with Mann-Whitney U test. Results: Most metrics provided by LapMentor II and included in this study show significant differences when comparing pre- and post-training tests of novices. Analysis of pre-training test of novices and experts results show significant differences in all analyzed metrics for all studied tasks. On the other hand, LapMentor was not able to distinguish between experts and novices after training in SINERGIA for any metric in the camera manipulation task and for some metrics of the other tasks. Conclusions: Training in SINERGIA VR simulator allows improvement of basic psychomotor laparoscpic skills and transferring them to another virtual simulator. Therefore, it could be used in laparoscopic surgery training programs. © 2011 CARS.","Laparoscopy; Simulator; Training; Validation; Virtual reality","adult; article; camera; controlled study; female; human; laparoscopic surgery; male; medical student; priority journal; psychomotor performance; simulator; surgeon; surgical training; validation study; virtual reality",Article,"Final","",Scopus,2-s2.0-85027954549
"Gutiérrez T., Rodríguez J., Vélaz Y., Casado S., Suescun A., Sánchez. E.J.","18036923100;57194191682;36662969400;7005494293;24282105300;57196876065;","IMA-VR: A multimodal virtual training system for skills transfer in Industrial Maintenance and Assembly tasks",2010,"Proceedings - IEEE International Workshop on Robot and Human Interactive Communication",,, 5598643,"428","433",,30,"10.1109/ROMAN.2010.5598643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649887580&doi=10.1109%2fROMAN.2010.5598643&partnerID=40&md5=e056988459082e88e90c396a90a89540","Department of Applied Mechanics, CEIT, TECNUN, Spain; LABEIN-TECNALIA, Spain","Gutiérrez, T., LABEIN-TECNALIA, Spain; Rodríguez, J., Department of Applied Mechanics, CEIT, TECNUN, Spain; Vélaz, Y., Department of Applied Mechanics, CEIT, TECNUN, Spain; Casado, S., LABEIN-TECNALIA, Spain; Suescun, A., Department of Applied Mechanics, CEIT, TECNUN, Spain; Sánchez., E.J., Department of Applied Mechanics, CEIT, TECNUN, Spain","Industrial Maintenance and Assembly is a very complex task involving both cognitive skills (procedural skills) and motor skills (fine motor control and bi-manual coordination skills). This paper presents a controlled multimodal training system, for transferring the motor and cognitive skills involved in these tasks. The new platform provides different multimodal aids and learning strategies that help and guide the users during their training process. One of the main features of this system is its flexibility to adapt itself to the task demands and to the users' preferences and needs supporting different configurations. To address bi-manual operations the platform offers different alternatives, one of them is a set-up composed of a haptic device to track the motion of the operator's dominant hand and simulate the physical interaction within the virtual environment, together with a marker-less motion capture system to track the motion of the other hand in real time. © 2010 IEEE.",,"Assembly tasks; Bi manuals; Co-ordination skills; Cognitive skill; Complex task; Fine motor control; Haptic devices; Industrial maintenance; Learning strategy; Motion capture system; Motor skills; Multi-modal; Physical interactions; Real time; Task demand; Training process; Training Systems; Virtual environments; Virtual training; Maintenance; Virtual reality; Coordination reactions",Conference Paper,"Final","",Scopus,2-s2.0-78649887580
"Hulin T., Schmirgel V., Yechiam E., Zimmermann U.E., Preusche C., Pöhler G.","23008567500;57199627082;55892873800;7201909731;6602947104;57220760440;","Evaluating exemplary training accelerators for programming-by-demonstration",2010,"Proceedings - IEEE International Workshop on Robot and Human Interactive Communication",,, 5598611,"440","445",,8,"10.1109/ROMAN.2010.5598611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649872096&doi=10.1109%2fROMAN.2010.5598611&partnerID=40&md5=855a0146c91be5af1131171d337aab81","KUKA Roboter GmbH, Germany; Faculty of Industrial Engineering and Management, Technion, Israel; Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Otto-von-Guericke University, Magdeburg, Germany","Hulin, T., Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Schmirgel, V., KUKA Roboter GmbH, Germany; Yechiam, E., Faculty of Industrial Engineering and Management, Technion, Israel; Zimmermann, U.E., KUKA Roboter GmbH, Germany; Preusche, C., Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany; Pöhler, G., Otto-von-Guericke University, Magdeburg, Germany","Robot Programming by Demonstration requires comprehending the usage of a robotic system. This article is about accelerating the training of these skills, using the example of a DLR/KUKA light-weight robot. An augmented reality and a virtual reality setup are presented that aim to demonstrate and evaluate skills transfer of two different sub-tasks of this system: Avoiding robot singularities and setting correct compliance parameters. For this purpose training accelerators are introduced for visualising robot singularities, exploring robot singularities and feeling compliance parameters. An evaluation procedure for all three accelerators is suggested and has been performed on the first two. As interesting evaluation result a contrast to the Cognitive Theory of Multimedia Learning hypothesis could be observed: Additional visual information on the robot singularities impairs the participants' performance. © 2010 IEEE.",,"Cognitive theory of multimedia learning; Evaluation results; Light weight; Robot programming by demonstration; Robotic systems; Subtasks; Visual information; Augmented reality; Human computer interaction; Programmable robots; Virtual reality; Visual communication; Robot programming",Conference Paper,"Final","",Scopus,2-s2.0-78649872096
"Wilson A.D., Benko H.","57199229209;9737287100;","Combining multiple depth cameras and projectors for interactions on, above, and between surfaces",2010,"UIST 2010 - 23rd ACM Symposium on User Interface Software and Technology",,,,"273","282",,231,"10.1145/1866029.1866073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649585148&doi=10.1145%2f1866029.1866073&partnerID=40&md5=5bceaf938460583857db498370f28297","Microsoft Research, One Microsoft Way, Redmond, WA, United States","Wilson, A.D., Microsoft Research, One Microsoft Way, Redmond, WA, United States; Benko, H., Microsoft Research, One Microsoft Way, Redmond, WA, United States","Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may ""pick up"" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and ""drop"" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.","Augmented reality; Depth cameras; Interactive spaces; Surface computing; Ubiquitous computing","Computational strategy; Depth camera; Future directions; Interactive display; Interactive spaces; Multi-touch; Small rooms; Surface computing; Virtual objects; World coordinates; Augmented reality; Cameras; User interfaces; Virtual reality; Ubiquitous computing",Conference Paper,"Final","",Scopus,2-s2.0-78649585148
"Anderson E.F., McLoughlin L., Liarokapis F., Peters C., Petridis P., de Freitas S.","34976300400;26656832500;7801416785;26425154400;36128917500;57203045925;","Developing serious games for cultural heritage: A state-of-the-art Review",2010,"Virtual Reality","14","4",,"255","275",,173,"10.1007/s10055-010-0177-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649514858&doi=10.1007%2fs10055-010-0177-3&partnerID=40&md5=74e34a16c358fff2a58b9ec302a3ab56","Interactive Worlds Applied Research Group (iWARG), Coventry University, Coventry, United Kingdom; The National Centre for Computer Animation (NCCA), Bournemouth University, Bournemouth, United Kingdom; Serious Games Institute (SGI), Coventry University, Coventry, United Kingdom","Anderson, E.F., Interactive Worlds Applied Research Group (iWARG), Coventry University, Coventry, United Kingdom; McLoughlin, L., The National Centre for Computer Animation (NCCA), Bournemouth University, Bournemouth, United Kingdom; Liarokapis, F., Interactive Worlds Applied Research Group (iWARG), Coventry University, Coventry, United Kingdom; Peters, C., Interactive Worlds Applied Research Group (iWARG), Coventry University, Coventry, United Kingdom; Petridis, P., Serious Games Institute (SGI), Coventry University, Coventry, United Kingdom; de Freitas, S., Serious Games Institute (SGI), Coventry University, Coventry, United Kingdom","Although the widespread use of gaming for leisure purposes has been well documented, the use of games to support cultural heritage purposes, such as historical teaching and learning, or for enhancing museum visits, has been less well considered. The state-of-the-art in serious game technology is identical to that of the state-of-the-art in entertainment games technology. As a result, the field of serious heritage games concerns itself with recent advances in computer games, real-time computer graphics, virtual and augmented reality and artificial intelligence. On the other hand, the main strengths of serious gaming applications may be generalised as being in the areas of communication, visual expression of information, collaboration mechanisms, interactivity and entertainment. In this report, we will focus on the state-of-the-art with respect to the theories, methods and technologies used in serious heritage games. We provide an overview of existing literature of relevance to the domain, discuss the strengths and weaknesses of the described methods and point out unsolved problems and challenges. In addition, several case studies illustrating the application of methods and technologies used in cultural heritage are presented. © 2010 Springer-Verlag London Limited.","Computer games technology; Cultural heritage; Serious games","Collaboration mechanisms; Computer game; Computer games technology; Cultural heritage; Cultural heritages; Interactivity; Real time computer graphics; Serious games; Serious gaming; State-of-the-art reviews; Teaching and learning; Unsolved problems; Virtual and augmented reality; Artificial intelligence; Augmented reality; Computer software; Human computer interaction; Virtual reality; Technology",Article,"Final","",Scopus,2-s2.0-78649514858
"Tung T., Matsuyama T.","8649992500;7401770410;","3D video performance segmentation",2010,"Proceedings - International Conference on Image Processing, ICIP",,, 5652541,"25","28",,1,"10.1109/ICIP.2010.5652541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651098975&doi=10.1109%2fICIP.2010.5652541&partnerID=40&md5=146ff7e977e8f7e1a5108f28aee2b1a5","Graduate School of Informatics, Kyoto University, Japan","Tung, T., Graduate School of Informatics, Kyoto University, Japan; Matsuyama, T., Graduate School of Informatics, Kyoto University, Japan","We present a novel approach that achieves segmentation of subject body parts in 3D videos. 3D video consists in a free-viewpoint video of real-world subjects in motion immersed in a virtual world. Each 3D video frame is composed of one or several 3D models. A topology dictionary is used to cluster 3D video sequences with respect to the model topology and shape. The topology is characterized using Reeb graph-based descriptors and no prior explicit model on the subject shape is necessary to perform the clustering process. In this framework, the dictionary consists in a set of training input poses with a priori segmentation and labels. As a consequence, all identified frames of 3D video sequences can be automatically segmented. Finally, motion flows computed between consecutive frames are used to transfer segmented region labels to unidentified frames. Our method allows us to perform robust body part segmentation and tracking in 3D cinema sequences. © 2010 IEEE.","3D video; Body segmentation; Shape matching; Topology dictionary","3D models; 3D video; Body parts; Body segmentation; Clustering process; D-Cinema; Descriptors; Explicit models; Free-viewpoint video; Real-world; Reeb graph; Segmentation and tracking; Segmented regions; Shape matching; Virtual worlds; Clustering algorithms; Image processing; Imaging systems; Topology; Video recording; Virtual reality; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-78651098975
"Mazzoni S., Badiali G., Lancellotti L., Babbi L., Bianchi A., Marchetti C.","14420064000;6507698745;24399388600;36149141600;55523544900;7101710582;","Simulation-guided navigation: A new approach to improve intraoperative three-dimensional reproducibility during orthognathic surgery",2010,"Journal of Craniofacial Surgery","21","6",,"1698","1705",,94,"10.1097/SCS.0b013e3181f3c6a8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650531352&doi=10.1097%2fSCS.0b013e3181f3c6a8&partnerID=40&md5=b0cfba39670eb8c25a5fa45d05f514cc","Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy","Mazzoni, S., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy; Badiali, G., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy; Lancellotti, L., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy; Babbi, L., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy; Bianchi, A., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy; Marchetti, C., Oral and Maxillofacial Surgery Unit, S. Orsola-Malpighi University Hospital, via Massarenti 9, 40138, Bologna, Italy","Because of the recent development of three-dimensional technology, computer software is increasingly being used for diagnosis, analysis, data documentation, and surgical planning for orthognathic surgery.Currently, the typical method to reposition jaws in the correct and planned location is based on the use of surgical splints, which have a quite high level of imprecision. The most important differences between planned and achieved maxillary movements are in the vertical and rotational positioning. Several methods have been described for intraoperative maxillary control, but none of these procedures is satisfactory.We present a new method to transfer individualized three-dimensional virtual planning of the patient using a navigation system in the operating room to improve reproducibility of the simulation. We enrolled 10 patients with dentofacial deformities from November 2008 to May 2009. All patients were studied and treated according to the following steps: cone-beam computed tomography data acquisition, virtual simulation of the surgical procedure, surgery with intraoperative navigation, and validation through reproducibility evaluation.We found 86.5% mean preoperative surgical plan reproducibility with the assistance of simulation-guided navigation compared with 80% mean reproducibility obtained in our previous group, in which no intraoperative navigation was performed.According to these results, we can assume that simulation-guided navigation would be a helpful procedure during orthognathic surgery to improve reproducibility of the preoperative virtual surgical planning. Copyright © 2010 by Mutaz B. Habal, MD.","computer-aided surgery; Orthognathic surgery; simulation-guided navigation; virtual surgery simulation","adolescent; adult; comparative study; computer assisted surgery; computer interface; computer simulation; cone beam computed tomography; conference paper; congenital malformation; female; human; image processing; instrumentation; male; malocclusion; mandible; maxilla; methodology; middle aged; orthognathic surgery; osteotomy; patient care planning; reproducibility; statistics; stereotactic procedure; three dimensional imaging; treatment outcome; validation study; Adolescent; Adult; Computer Simulation; Cone-Beam Computed Tomography; Female; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Male; Malocclusion; Mandible; Maxilla; Middle Aged; Orthognathic Surgical Procedures; Osteotomy; Patient Care Planning; Reproducibility of Results; Stereotaxic Techniques; Surgery, Computer-Assisted; Treatment Outcome; User-Computer Interface",Conference Paper,"Final","",Scopus,2-s2.0-78650531352
"Vidarte J.D.T., Rinderknecht C., Kim J.-I., Kim H.S.","36554831000;56012247900;24483676600;35109382900;","A Tangible interface for learning recursion and functional programming",2010,"Proceedings - 2010 International Symposium on Ubiquitous Virtual Reality, ISUVR 2010",,, 5557937,"32","35",,3,"10.1109/ISUVR.2010.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957961923&doi=10.1109%2fISUVR.2010.18&partnerID=40&md5=3918d411cf4300067b56ca19a31b0887","Department of Advanced Technology Fusion, Konkuk University, Seoul, South Korea; Department of Internet and Multimedia Engineering, Konkuk University, Seoul, South Korea","Vidarte, J.D.T., Department of Advanced Technology Fusion, Konkuk University, Seoul, South Korea; Rinderknecht, C., Department of Internet and Multimedia Engineering, Konkuk University, Seoul, South Korea; Kim, J.-I., Department of Internet and Multimedia Engineering, Konkuk University, Seoul, South Korea; Kim, H.S., Department of Internet and Multimedia Engineering, Konkuk University, Seoul, South Korea","Recursion is a powerful programming technique which is notoriously difficult to master, especially in functional languages because they prominently feature structural recursion as the main control-flow mechanism.We propose several hypotheses to understand the issue and put some to the test by designing an open-source interactive interface based on a tangible blockworld with augmented reality and software feedback. Stacks of blocks are used as an analogy for the list data structure, which enables the simplest form of structural recursion. After using this application, students are expected to transfer their training to directly write recursive programs in sequential Erlang, a purely functional language. © 2010 IEEE.","Augmented reality; Block world; Functional programming; Software feedback; Tangible user interface","Block world; Control-flow; Functional languages; Interactive interfaces; Open-source; Programming technique; Recursions; Recursive programs; Software feedback; Structural recursion; Tangible interfaces; Tangible user interfaces; Augmented reality; Data structures; User interfaces; Virtual reality; Functional programming",Conference Paper,"Final","",Scopus,2-s2.0-77957961923
"Cameirão M.S., Badia S.B.I., Oller E.D., Verschure P.F.M.J.","21740694600;6506360007;25927398500;7006315557;","Neurorehabilitation using the virtual reality based Rehabilitation Gaming System: Methodology, design, psychometrics, usability and validation",2010,"Journal of NeuroEngineering and Rehabilitation","7","1", 48,"","",,200,"10.1186/1743-0003-7-48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956799027&doi=10.1186%2f1743-0003-7-48&partnerID=40&md5=79115b93e16570166a80d961567a0d6e","Department of Technology, Laboratory of Synthetic Perceptive Emotive and Cognitive Systems (SPECS), Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain; Servei de Medicina Física i Rehabilitaciá, Hospital de l'Esperança, Barcelona, Spain; Instituciá Catalana de Recerca i Estudis Avançats (ICREA), Barcelona, Spain","Cameirão, M.S., Department of Technology, Laboratory of Synthetic Perceptive Emotive and Cognitive Systems (SPECS), Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain; Badia, S.B.I., Department of Technology, Laboratory of Synthetic Perceptive Emotive and Cognitive Systems (SPECS), Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain; Oller, E.D., Servei de Medicina Física i Rehabilitaciá, Hospital de l'Esperança, Barcelona, Spain; Verschure, P.F.M.J., Department of Technology, Laboratory of Synthetic Perceptive Emotive and Cognitive Systems (SPECS), Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain, Instituciá Catalana de Recerca i Estudis Avançats (ICREA), Barcelona, Spain","Background: Stroke is a frequent cause of adult disability that can lead to enduring impairments. However, given the life-long plasticity of the brain one could assume that recovery could be facilitated by the harnessing of mechanisms underlying neuronal reorganization. Currently it is not clear how this reorganization can be mobilized. Novel technology based neurorehabilitation techniques hold promise to address this issue. Here we describe a Virtual Reality (VR) based system, the Rehabilitation Gaming System (RGS) that is based on a number of hypotheses on the neuronal mechanisms underlying recovery, the structure of training and the role of individualization. We investigate the psychometrics of the RGS in stroke patients and healthy controls. Methods: We describe the key components of the RGS and the psychometrics of one rehabilitation scenario called Spheroids. We performed trials with 21 acute/subacute stroke patients and 20 healthy controls to study the effect of the training parameters on task performance. This allowed us to develop a Personalized Training Module (PTM) for online adjustment of task difficulty. In addition, we studied task transfer between physical and virtual environments. Finally, we assessed the usability and acceptance of the RGS as a rehabilitation tool. Results: We show that the PTM implemented in RGS allows us to effectively adjust the difficulty and the parameters of the task to the user by capturing specific features of the movements of the arms. The results reported here also show a consistent transfer of movement kinematics between physical and virtual tasks. Moreover, our usability assessment shows that the RGS is highly accepted by stroke patients as a rehabilitation tool. Conclusions: We introduce a novel VR based paradigm for neurorehabilitation, RGS, which combines specific rehabilitative principles with a psychometric evaluation to provide a personalized and automated training. Our results show that the RGS effectively adjusts to the individual features of the user, allowing for an unsupervised deployment of individualized rehabilitation protocols. © 2010 Cameiro et al; licensee BioMed Central Ltd.",,"adult; aged; algorithm; article; calibration; computer graphics; computer interface; computer program; female; hemiplegia; human; male; methodology; middle aged; neurologic disease; physiology; physiotherapy; psychometry; psychomotor performance; recreation; reproducibility; statistical analysis; stroke; Adult; Aged; Algorithms; Calibration; Computer Graphics; Data Interpretation, Statistical; Female; Hemiplegia; Humans; Male; Middle Aged; Nervous System Diseases; Physical Therapy Modalities; Psychometrics; Psychomotor Performance; Reproducibility of Results; Software Design; Stroke; User-Computer Interface; Video Games",Article,"Final","",Scopus,2-s2.0-77956799027
"Hou T., Wang S., Qin H.","35172841700;57191348305;34974717300;","Active lighting learning for 3D model based vehicle tracking",2010,"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010",,, 5543911,"38","43",,1,"10.1109/CVPRW.2010.5543911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956516922&doi=10.1109%2fCVPRW.2010.5543911&partnerID=40&md5=5b3be670bca9d9da2723294d861e3c9a","Computer Science Department, Stony Brook University, United States; Kodak Research Laboratories, Eastman Kodak Company, United States","Hou, T., Computer Science Department, Stony Brook University, United States; Wang, S., Kodak Research Laboratories, Eastman Kodak Company, United States; Qin, H., Computer Science Department, Stony Brook University, United States","Varying illumination is a challenging issue in many computer vision problems (e.g., tagging, matching, and tracking), while in inverse rendering, people are interested in estimating illumination from rendered images or videos. Can these two techniques be combined together to form a unified framework for vehicle tracking and lighting learning? This paper gives probably the first thought in this joint problem, by presenting a framework to adaptively learn lighting from an image sequence while tracking the object (specifically, the vehicle) in it. We formulate the illumination model with both diffusion and specularity components using a frequency-space representation, and design a nonlinear model to estimate lighting coefficients in a low-dimensional subspace. The lighting learning and vehicle tracking are integrated in a unified Markov network, which can be solved by an iterative believe propagation (BP) method. The proposed framework can track a vehicle moving in a video, as well as transfer the learned lighting to other objects, which shows its potential in augmented reality. © 2010 IEEE.",,"3D models; Active lighting; Computer vision problems; Illumination models; Image sequence; Inverse rendering; Low-dimensional subspace; Markov networks; Non-linear model; Rendered images; Specularities; Unified framework; Vehicle tracking; Augmented reality; Computer vision; Inverse problems; Lighting; Technical presentations; Three dimensional; Tracking (position); Vehicles; Virtual reality; Target tracking",Conference Paper,"Final","",Scopus,2-s2.0-77956516922
"Tawara T., Ono K.","57196911574;35105793600;","A framework for volume segmentation and visualization using augmented reality",2010,"3DUI 2010 - IEEE Symposium on 3D User Interfaces 2010, Proceedings",,, 5444707,"121","122",,9,"10.1109/3DUI.2010.5444707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953075808&doi=10.1109%2f3DUI.2010.5444707&partnerID=40&md5=22eff03c37d56cbd0d9c527360252df2","RIKEN, Japan","Tawara, T., RIKEN, Japan; Ono, K., RIKEN, Japan","We propose a two-handed direct manipulation system to achieve complex volume segmentation of CT/MRI data in Augmented Reality with a remote controller attached to a motion tracking cube. At the same time segmented data is displayed by direct volume rendering using a programmable GPU. Our system achieves visualization of real time modification of volume data with complex shading including transparency control by changing transfer functions, displaying any cross section, and rendering multi materials using a local illumination model. Our goal is to build a system that facilitates direct manipulation of volumetric CT/MRI data for segmentation in Augmented Reality. Volume segmentation is a challenging problem and segmented data has an important role for visualization and analysis. ©2010 IEEE.","I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques; K.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented, and virtual realities; K.5.2 [Information Interfaces and Presentation]: User Interfaces-Interaction styles","I.3.6 [computer graphics]: methodology and techniques; Information interfaces; Interaction styles; Multimedia information systems-artificial; Augmented reality; Data visualization; Information systems; Three dimensional; Virtual reality; Visualization; Volume rendering; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-77953075808
"Ha T., Woo W.","15020792200;35575439600;","An empirical evaluation of virtual hand techniques for 3D object manipulation in a tangible augmented reality environment",2010,"3DUI 2010 - IEEE Symposium on 3D User Interfaces 2010, Proceedings",,, 5444713,"91","98",,25,"10.1109/3DUI.2010.5444713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953077148&doi=10.1109%2f3DUI.2010.5444713&partnerID=40&md5=bad2d4402b928dfdb6ba9ae1862ffaaa","GIST U-VR Lab., 500-712, South Korea","Ha, T., GIST U-VR Lab., 500-712, South Korea; Woo, W., GIST U-VR Lab., 500-712, South Korea","In this paper, we present a Fitts' law-based formal evaluation process and the corresponding results for 3D object manipulation techniques based on a virtual hand metaphor in a tangible augmented reality (TAR) environment. Specifically, we extend the design parameters of the 1D scale Fitts' law to 3D scale and then refine an evaluation model in order to bring generality and ease of adaptation to various TAR applications. Next, we implement and compare standard TAR manipulation techniques using a cup, a paddle, a cube, and a proposed extended paddle prop. Most manipulation techniques were well-modeled in terms of linear regression according to Fitts' law, with a correlation coefficient value of over 0.9. Notably, the throughput by ISO 9241-9 of the extended paddle technique peaked at around 1.39 to 2 times higher than in the other techniques, due to the instant 3D positioning of the 3D objects. In the discussion, we subsequently examine the characteristics of the TAR manipulation techniques in terms of stability, speed, comfort, and understanding. As a result, our evaluation process, results, and analysis can be useful in guiding the design and implementation of future TAR interfaces. ©2010 IEEE.","3D object manipulation; Augmented reality; Empirical evaluation; Fitts' law; H.5.1 [Information Interfaces and Presentation]: Multimedia information systems - Artificial, augmented, and virtual realities; H.5.2 [Information Interfaces and Presentation]: User interfaces - Input interaction styles; Tangible user interface; Virtual hand technique","3D object; 3D object manipulation; Empirical evaluations; Fitts' law; H.5.1 [information interfaces and presentation]: multimedia information systems - Artificial , augmented , and virtual realities; Interaction styles; Tangible user interfaces; Virtual hand; Augmented reality; Environmental regulations; Information systems; Tar; Three dimensional; Three dimensional computer graphics; Virtual reality; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-77953077148
"Gierlinger T., Danch D., Stork A.","23396525100;15021981500;56234965900;","Rendering techniques for mixed reality",2010,"Journal of Real-Time Image Processing","5","2",,"109","120",,8,"10.1007/s11554-009-0137-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953538148&doi=10.1007%2fs11554-009-0137-x&partnerID=40&md5=afe183eb6f9e69864d045352102c7f0a","Fraunhofer IGD, Darmstadt, Germany; TU Darmstadt, Darmstadt, Germany","Gierlinger, T., Fraunhofer IGD, Darmstadt, Germany; Danch, D., Fraunhofer IGD, Darmstadt, Germany; Stork, A., TU Darmstadt, Darmstadt, Germany","In mixed reality (MR) design review, the aesthetics of a virtual prototype is assessed by integrating a virtual model into a real-world environment and inspecting the interaction between the model and the environment (lighting, shadows and reflections) from different points of view. The visualization of the virtual model has to be as realistically as possible to provide a solid basis for this assessment and interactive rendering speed is mandatory to allow the designer to examine the scene from arbitrary positions. In this article we present a real-time rendering engine specifically tailored to the needs of MR visualization. The renderer utilizes pre-computed radiance transfer to calculate dynamic soft-shadows, high dynamic range images and image-based lighting to capture incident real-world lighting, approximate bidirectional texture functions to render materials with self-shadowing, and frame post-processing filters (bloom filter and an adaptive tone mapping operator). The proposed combination of rendering techniques provides a trade-off between rendering quality and required computing resources which enables high quality rendering in mobile MR scenarios. The resulting image fidelity is superior to radiosity-based techniques because glossy materials and dynamic environment lighting with soft-shadows are supported. Ray tracing-based techniques provide higher quality images than the proposed system, but they require a cluster of computers to achieve interactive frame rates which prevents these techniques from being used in mobile MR (especially outdoor) scenarios. The renderer was developed in the European research project IMPROVE (FP6-IST-2-004785) and is currently extended in the MAXIMUS project (FP7-ICT-1-217039) where hybrid rendering techniques which fuse PRT and ray tracing are developed. © 2009 Springer-Verlag.","Adaptive tone mapping; Approximate bidirectional texture function; High dynamic range imaging; Image-based lighting; Mixed reality rendering; Pre-computed radiance transfer; Real-time rendering","Bidirectional texture functions; High dynamic range imaging; Image-based lighting; Mixed reality; Precomputed radiance transfer; Real-time rendering; Tone mapping; Elementary particle sources; Mapping; Textures; Virtual reality; Visualization; Lighting",Article,"Final","",Scopus,2-s2.0-77953538148
"Bullinger H.-J., Bauer W., Wenzel G., Blach R.","7004916256;35483043700;23399167200;15057595400;","Towards user centred design (UCD) in architecture based on immersive virtual environments",2010,"Computers in Industry","61","4",,"372","379",,55,"10.1016/j.compind.2009.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953232146&doi=10.1016%2fj.compind.2009.12.003&partnerID=40&md5=9cd7462cf1f0f9ebfdeb8bed14e24920","Fraunhofer Headquarter, Hansastraße 27c, 80686 Munich, Germany; Fraunhofer Institute for Industrial Engineering IAO, Nobelstr. 12, 70569 Stuttgart, Germany","Bullinger, H.-J., Fraunhofer Headquarter, Hansastraße 27c, 80686 Munich, Germany; Bauer, W., Fraunhofer Institute for Industrial Engineering IAO, Nobelstr. 12, 70569 Stuttgart, Germany; Wenzel, G., Fraunhofer Institute for Industrial Engineering IAO, Nobelstr. 12, 70569 Stuttgart, Germany; Blach, R., Fraunhofer Institute for Industrial Engineering IAO, Nobelstr. 12, 70569 Stuttgart, Germany","This paper describes a generic concept of how to combine the experience of user centred design (UCD) in the field of Human Computer Interaction (HCI) with the traditional approach of participatory design (PD) in an architectural design process. Even if some basic requirements of this generic method are not available yet, this paper will also describe an approach, which enables planners even now to involve end users by using virtual environments (VE) as immersive and spatial prototype. It will be described and illustrated by the way of example using the building project Centre of Virtual Engineering of the Fraunhofer Institute for Industrial Engineering (IAO) in Stuttgart. It demonstrates that the transfer of the UCD approach to architectural planning combined with the provision of an adequate prototype can make a significant contribution towards an increase in quality and performance in building and construction projects. © 2010 Elsevier B.V. All rights reserved.","Architectural design; User centred design; Virtual environments","Building projects; End users; Fraunhofer; Generic method; Immersive; Immersive virtual environments; In-buildings; Participatory design; User centred design; Virtual engineering; Virtual environments; Architectural design; Construction industry; Structural design; Virtual reality; Human computer interaction",Article,"Final","",Scopus,2-s2.0-77953232146
"Fiorentino M., Uva A.E., Dellisanti Fabiano M., Monno G.","56350129000;6505665039;25959581000;6603171537;","Improving bi-manual 3D input in CAD modelling by part rotation optimisation",2010,"CAD Computer Aided Design","42","5",,"462","470",,13,"10.1016/j.cad.2008.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950187544&doi=10.1016%2fj.cad.2008.12.002&partnerID=40&md5=e61c41c9c14f64dc1a291aa2324fafb6","DIMEG, Politecnico di Bari, Viale Japigia 182, 70126 Bari, Italy","Fiorentino, M., DIMEG, Politecnico di Bari, Viale Japigia 182, 70126 Bari, Italy; Uva, A.E., DIMEG, Politecnico di Bari, Viale Japigia 182, 70126 Bari, Italy; Dellisanti Fabiano, M., DIMEG, Politecnico di Bari, Viale Japigia 182, 70126 Bari, Italy; Monno, G., DIMEG, Politecnico di Bari, Viale Japigia 182, 70126 Bari, Italy","Part modelling in a CAD environment requires a bi-manual 3D input interface to fully exploit its potentialities. In this research we provide extensive user tests on bi-manual modelling using different devices to control 3D model's rotation. Our results suggest that a simple trackball device is effective when the user task is mostly limited to rotation control (i.e. when modelling parts in a CAD environment). In our tests, performances are even better than those achieved with a specifically designed device. Since the task of rotating a CAD part often shows the need of flipping the controlled object, we introduce a non linear transfer function which combines the precision of a zero order control mode with the ability to recognise fast movements. This new modality shows a significant improvement in the user's performances and candidates itself for integration in next generation CAD interfaces. © 2008 Elsevier Ltd. All rights reserved.","3D input; Bimanual interaction; Human computer interface; Part modelling; VR CAD","3D models; Bi-manual interaction; CAD interfaces; Controlled objects; Fast movement; Human computer interfaces; Input interface; New modality; Non-linear; Optimisations; Rotation control; Trackballs; User tests; Zero order; Human computer interaction; Interfaces (computer); Rotation; Three dimensional",Article,"Final","",Scopus,2-s2.0-77950187544
"Watanuki K., Hou L.","7005697643;55729883700;","Augmented reality-based training system for metal casting",2010,"Journal of Mechanical Science and Technology","24","1",,"237","240",,5,"10.1007/s12206-009-1175-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949648758&doi=10.1007%2fs12206-009-1175-9&partnerID=40&md5=b4ed6b627fea183a8c41f7ead6253278","Department of Mechanical Engineering, Graduate School of Science and Engineering, Saitama University, 255 Shimo-okubo, Sakura-ku, Saitama-shi, Saitama 338-8570, Japan","Watanuki, K., Department of Mechanical Engineering, Graduate School of Science and Engineering, Saitama University, 255 Shimo-okubo, Sakura-ku, Saitama-shi, Saitama 338-8570, Japan; Hou, L., Department of Mechanical Engineering, Graduate School of Science and Engineering, Saitama University, 255 Shimo-okubo, Sakura-ku, Saitama-shi, Saitama 338-8570, Japan","Products fabricated by metal casting are affected by casting design and pouring. In order to design products efficiently, engineers must consider the shape of the pouring gate and the fluidity of metal in the runner. Engineers must envision the shape of the product and the fluidity of the metal in three dimensions. However, this task is difficult, especially for beginners. Therefore, we developed an augmented reality-based training system for metal casting that allows visualization of fluidity. The proposed training system uses simple physical operations to compute and display fluidity in real-time. Visualization of three-dimensional images as well as training on this system is simple. © KSME & Springer 2010.","Augmented reality; Interactive interface; Metal casting; Skill transfer","Casting designs; Design products; Interactive interface; Interactive interfaces; Skill transfer; Three dimensional images; Three dimensions; Training Systems; Augmented reality; Fluidity; Metal castings; Metals; Steel metallurgy; Three dimensional; Virtual reality; Visualization; Metal casting",Article,"Final","",Scopus,2-s2.0-77949648758
"De Freitas S., Rebolledo-Mendez G., Liarokapis F., Magoulas G., Poulovassilis A.","57203045925;24832441800;7801416785;26642991300;7003984218;","Learning as immersive experiences: Using the four-dimensional framework for designing and evaluating immersive learning experiences in a virtual world",2010,"British Journal of Educational Technology","41","1",,"69","85",,179,"10.1111/j.1467-8535.2009.01024.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73149108424&doi=10.1111%2fj.1467-8535.2009.01024.x&partnerID=40&md5=24e9d7ce0e30cffb0ef4fe698a2f2e33","Serious Games Institute, Coventry University, United Kingdom; London Knowledge Lab, University of London, Birkbeck, United Kingdom","De Freitas, S., Serious Games Institute, Coventry University, United Kingdom; Rebolledo-Mendez, G., Serious Games Institute, Coventry University, United Kingdom; Liarokapis, F., Serious Games Institute, Coventry University, United Kingdom; Magoulas, G., London Knowledge Lab, University of London, Birkbeck, United Kingdom; Poulovassilis, A., London Knowledge Lab, University of London, Birkbeck, United Kingdom","Traditional approaches to learning have often focused upon knowledge transfer strategies that have centred on textually-based engagements with learners, and dialogic methods of interaction with tutors. The use of virtual worlds, with text-based, voice-based and a feeling of 'presence' naturally is allowing for more complex social interactions and designed learning experiences and role plays, as well as encouraging learner empowerment through increased interactivity. To unpick these complex social interactions and more interactive designed experiences, this paper considers the use of virtual worlds in relation to structured learning activities for college and lifelong learners. This consideration necessarily has implications upon learning theories adopted and practices taken up, with real implications for tutors and learners alike. Alongside this is the notion of learning as an ongoing set of processes mediated via social interactions and experiential learning circumstances within designed virtual and hybrid spaces. This implies the need for new methodologies for evaluating the efficacy, benefits and challenges of learning in these new ways. Towards this aim, this paper proposes an evaluation methodology for supporting the development of specified learning activities in virtual worlds, based upon inductive methods and augmented by the four-dimensional framework reported in a previous study. The study undertaken aimed to test the efficacy of the proposed evaluation methodology and framework, and to evaluate the broader uses of a virtual world for supporting lifelong learners specifically in their educational choices and career decisions. The paper presents the findings of the study and considers that virtual worlds are reorganising significantly how we relate to the design and delivery of learning. This is opening up a transition in learning predicated upon the notion of learning design through the lens of 'immersive learning experiences' rather than sets of knowledge to be transferred between tutor and learner. The challenges that remain for tutors rest with the design and delivery of these activities and experiences. The approach advocated here builds upon an incremental testing and evaluation of virtual world learning experiences. © 2009 Becta.",,"Approaches to learning; Evaluation methodologies; Experiential learning; Immersive; Immersive learning; Inductive method; Interactivity; Knowledge transfer; Learning Activity; Learning designs; Learning experiences; Learning Theory; Life-long learners; Role play; Social interactions; Structured learning; Testing and evaluation; Through the lens; Virtual worlds; Complexation; Knowledge management; Virtual reality; Interactive computer graphics",Article,"Final","",Scopus,2-s2.0-73149108424
"Hoang T.N., Porter S.R., Thomas B.H.","14819594500;35180376500;55467685600;","Augmenting Image Plane AR 3D Interactions for Wearable Computers",2009,"Conferences in Research and Practice in Information Technology Series","93",,,"9","16",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873305889&partnerID=40&md5=b5f50b20260cd223906abc6ff078b773","Wearable Computer Lab, School of Computer and Information Science, University of South Australia, Australia","Hoang, T.N., Wearable Computer Lab, School of Computer and Information Science, University of South Australia, Australia; Porter, S.R., Wearable Computer Lab, School of Computer and Information Science, University of South Australia, Australia; Thomas, B.H., Wearable Computer Lab, School of Computer and Information Science, University of South Australia, Australia","This paper presents a set of large object manipulation techniques implemented in a wearable augmented reality computer system that are optimised for the outdoor setting. These techniques supplement the current image plane approach, to provide a comprehensive solution to 3D object manipulation in an augmented reality outdoor environment. The three extended manipulation techniques, Revolve, Xscale, and Ground plane translation, are focused on using what we determined to be the best coordinate system for object rotation, scaling and translation. This paper goes on to present the generalised plane technique for the constrained translation of graphical objects on arbitrary planes to enable more complex translation operations. The paper presents the techniques from both the user interface and software development perspectives. © 2009, Australian Computer Society, Inc.","3D object manipulation; And virtual environments; Outdoor augmented reality; User interfaces","3D interactions; 3D object; Co-ordinate system; Current image; Graphical objects; Ground planes; Image plane; Manipulation techniques; Object manipulation; Outdoor augmented reality; Outdoor environment; Rotation , scaling and translations; Augmented reality; Three dimensional; Three dimensional computer graphics; Virtual reality; Wearable computers; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84873305889
"Chen V.H.-H., Lin W., Haller M., Leitner J., Duh H.B.-L.","56954384700;56292242500;7102872675;23035667300;6603986391;","Communicative behaviors and flow experience in tabletop gaming",2009,"ACM International Conference Proceeding Series",,,,"281","286",,3,"10.1145/1690388.1690436","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749344428&doi=10.1145%2f1690388.1690436&partnerID=40&md5=50192c342f783f20772a65010c229ce7","Nanyang Technological University, Singapore, Singapore; Upper Austria University of Applied Sciences, Austria; National University of Singapore, Singapore","Chen, V.H.-H., Nanyang Technological University, Singapore, Singapore; Lin, W., Nanyang Technological University, Singapore, Singapore; Haller, M., Upper Austria University of Applied Sciences, Austria; Leitner, J., Upper Austria University of Applied Sciences, Austria; Duh, H.B.-L., National University of Singapore, Singapore","The tabletop interface has been touted to merge the best of traditional board gaming and cutting edge computer technology in bringing both intense social interaction and limitless virtual representation to users. This study utilizes flow theory to understand user's enjoyment of playing games. It also explores the interplay between communicative behaviors and game play. Combining observations and questionnaire, data analysis showed certain nonverbal behaviors are correlated with flow. To further explain how social interaction influence game enjoyment, three main themes were identified: through the use of space, reduced nonverbal cues and knowledge transfer. Implications for tabletop game interface design are then discussed. © ACM 2009.","Augmented reality games; Communicative behaviors; Flow theory; Social interaction; Tabletop interface","Computer technology; Cutting edges; Data analysis; Flow experience; Flow theories; Game interfaces; Knowledge transfer; Nonverbal behavior; Social interaction; Social interactions; Tabletop interfaces; Virtual representations; Augmented reality; Data flow analysis; Interfaces (computer); Knowledge management; Virtual reality; Game theory",Conference Paper,"Final","",Scopus,2-s2.0-77749344428
"Wang Z., Wu L.-X.","55933443100;55714061400;","Automated extraction of building geometric features from raw LiDAR data",2009,"International Geoscience and Remote Sensing Symposium (IGARSS)","2",, 5418108,"II436","II439",,7,"10.1109/IGARSS.2009.5418108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951110256&doi=10.1109%2fIGARSS.2009.5418108&partnerID=40&md5=e46f9493f21e69257afddad634f14a72","College of Resources and Civil Engineering, Northeastern University, Shenyang, 110004, China; Academy of Disaster Reduction and Emergency Management, Ministry of Civil Affairs, Ministry of Education (Beijing Normal University), Beijing, 100875, China","Wang, Z., College of Resources and Civil Engineering, Northeastern University, Shenyang, 110004, China, Academy of Disaster Reduction and Emergency Management, Ministry of Civil Affairs, Ministry of Education (Beijing Normal University), Beijing, 100875, China; Wu, L.-X., College of Resources and Civil Engineering, Northeastern University, Shenyang, 110004, China, Academy of Disaster Reduction and Emergency Management, Ministry of Civil Affairs, Ministry of Education (Beijing Normal University), Beijing, 100875, China","In recent years, with new services expected, such as navigation systems, location based services, and augmented reality, the need for automatically, efficient 3D building reconstruction systems becomes more urgent than ever. As a new spatial information technology, airborne LiDAR is widely used for the acquisition of 3D objects on the earth. The automatic reconstruction of 3D buildings from airborne LiDAR data has been a topic of research for decades. However the lack of 3D building reconstruction methods is still the bottleneck for the further development of airborne LiDAR. Since the 3D geometry feature is extremely important for many applications such as urban planning, car navigation, or environment monitoring etc., this paper introduces an automated method for implementing building geometric features extraction from raw LiDAR data in details. The outlines of 3D buildings are generated by using discrete curvature analysis according to the building geometry features. In the experiment, 3D building models are automatically reconstructed by using a model-driven approach according to their geometry features derived from raw LiDAR data. ©2009 IEEE.","3D building models reconstruction; Airborne LiDAR; Geometric feature","3D geometry; 3D object; Airborne LiDAR; Airborne lidar data; Automated extraction; Automated methods; Automatic reconstruction; Building geometry; Building model; Building reconstruction; Car navigation; Curvature analysis; Environment monitoring; Further development; Geometric feature; Geometry features; Location-Based Services; Model driven approach; New services; Raw lidar data; Spatial informations; Augmented reality; Buildings; Computational geometry; Feature extraction; Geology; Navigation; Optical radar; Remote sensing; Repair; Technology transfer; Urban planning; Virtual reality; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-77951110256
"Elion O., Bahat Y., Siev-Ner I., Sela I., Karni A., Weiss P.L.","6505628303;25926895000;6602071770;36950653300;57207543301;55435137100;","No transfer of gains after a single training session within a virtual environment to fundamental tests of stability",2009,"2009 Virtual Rehabilitation International Conference, VR 2009",,, 05174220,"136","139",,1,"10.1109/ICVR.2009.5174220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449112527&doi=10.1109%2fICVR.2009.5174220&partnerID=40&md5=145568b1f4d6470b97879df1bd2a60ee","CAREN VR Lab., C Sheba Medical Centre, Tel Hashomer, Israel; Dept. of Neurobiology and Ethology, Dept. of Learning Disabilities, University of Haifa, Haifa, Israel; Dept. of Occupational Therapy, University of Haifa, Haifa, Israel","Elion, O., CAREN VR Lab., C Sheba Medical Centre, Tel Hashomer, Israel; Bahat, Y., CAREN VR Lab., C Sheba Medical Centre, Tel Hashomer, Israel; Siev-Ner, I., CAREN VR Lab., C Sheba Medical Centre, Tel Hashomer, Israel; Sela, I., Dept. of Neurobiology and Ethology, Dept. of Learning Disabilities, University of Haifa, Haifa, Israel; Karni, A., Dept. of Neurobiology and Ethology, Dept. of Learning Disabilities, University of Haifa, Haifa, Israel; Weiss, P.L., Dept. of Occupational Therapy, University of Haifa, Haifa, Israel","How specific are postural and balance control skills? An important issue for the establishment of effective training and retraining (rehabilitation) programs is whether skills gained while training in laboratory settings can be transferred to performance gains in somewhat different conditions (including every-day life). While there is much evidence showing that for volitional motor tasks the gains in performance (procedural, implicit, knowledge) accrued in practice may not always be transferable to novel task conditions, it is not clear whether the (implicit) knowledge gained in learning postural adjustments can be transferred to measures of balance (reaction to external perturbations) that have not been trained. The objective of the current study was to elucidate what aspects of a postural skill learned within a virtual environment (VE) by healthy adults may be transferable to the performance of standard tests of postural adjustments. Sixteen healthy young adults, aged 20-40 years (mean ± SD = 29.8 ± 2.8 years), were pseudo-randomly assigned to either a training group (Group A) or a no-training, control group (Group B). Group A performed a single training session in a VE in which maintenance of stability on a platform, while travelling along a road scenario and reaching for visual targets (secondary task) were required. Each participant underwent 8 consecutive runs of the task (2:48 m per run). A balance assessment with a given set of perturbations was performed before and after training as well as at 24 hours and 4 weeks post-training. Group B underwent the same assessments but without VE training. The results showed that the Center of Pressure (CoP) displacement tended to decrease over successive balance assessments in both groups, however, this decrease was not statistically significant. Moreover, there was no clear advantage for Group A. Thus, the postural adjustment gains were not transferred to the balance assessment tests. Nonvolitional balance control gains are, in this respect, similar to gains attained in a volitional manual task learning. ©2009 IEEE.","Postural adjustments; Procedural knowledge; Specificity; Transfer","Balance control; Before and after; Center of pressure; Control groups; External perturbations; Motor tasks; Novel task; Performance Gain; Postural adjustments; Procedural knowledge; Specificity; Standard tests; Task learning; Training sessions; Transfer; Virtual environments; Visual targets; Motion control; Virtual reality; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-70449112527
"Thomas B.H.","55467685600;","Supporting user interfaces in ubiquitous virtual reality",2009,"Proceedings - 2009 International Symposium on Ubiquitous Virtual Reality, ISUVR 2009",,, 5232246,"15","18",,,"10.1109/ISUVR.2009.16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350562043&doi=10.1109%2fISUVR.2009.16&partnerID=40&md5=82a9fcfe2a8bcf0fbc81d4c5cb219677","Wearable Computer Lab., School of Computer and Information Science, University of South Australia, Australia","Thomas, B.H., Wearable Computer Lab., School of Computer and Information Science, University of South Australia, Australia","Ubiquitous Virtual Reality focuses on the widespread access of digital information to the user with the fusion and extension of a number of computer science disciplines. This paper will focus on ways users can conveniently and easily transfer between these different modes of interacting with digital information. Each of these domains has a particular display and interaction technologies that current support their form of information presentation. In additional these domains have software and metaphor support for their user interfaces. This paper will focus on methodologies to perform a number of tasks: 1) transitions between different presentations of information, 2) unifying technologies to better bring together these domains, and 3) articulate the important aspects of interactions within a Ubiquitous Virtual Reality system. This paper will explore a number of possible technologies, such as input devices, clipboard technologies, and software frameworks. The paper will also highlight areas that need future exploration and possible pitfalls to avoid. © 2009 IEEE.",,"Different modes; Digital information; Information presentation; Input devices; Interaction technology; Software frameworks; Virtual reality system; Computer software; Technology; User interfaces; Virtual reality; Ubiquitous computing",Conference Paper,"Final","",Scopus,2-s2.0-70350562043
"Panait L., Akkary E., Bell R.L., Roberts K.E., Dudrick S.J., Duffy A.J.","6603648726;24334261800;7404520114;8604798500;57011758000;12243581200;","The Role of Haptic Feedback in Laparoscopic Simulation Training",2009,"Journal of Surgical Research","156","2",,"312","316",,139,"10.1016/j.jss.2009.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70249107719&doi=10.1016%2fj.jss.2009.04.018&partnerID=40&md5=8e07670a8d0bfaf001876256cdfbd1a2","Saint Mary's Hospital, Waterbury, CT, United States; Yale School of Medicine, New Haven, CT, United States","Panait, L., Saint Mary's Hospital, Waterbury, CT, United States; Akkary, E., Yale School of Medicine, New Haven, CT, United States; Bell, R.L., Yale School of Medicine, New Haven, CT, United States; Roberts, K.E., Yale School of Medicine, New Haven, CT, United States; Dudrick, S.J., Saint Mary's Hospital, Waterbury, CT, United States, Yale School of Medicine, New Haven, CT, United States; Duffy, A.J., Yale School of Medicine, New Haven, CT, United States","Introduction: Laparoscopic virtual reality simulators are becoming a ubiquitous tool in resident training and assessment. These devices provide the operator with various levels of realism, including haptic (or force) feedback. However, this feature adds significantly to the cost of the devices, and limited data exist assessing the value of haptics in skill acquisition and development. Utilizing the Laparoscopy VR (Immersion Medical, Gaithersburg, MD), we hypothesized that the incorporation of force feedback in the simulated operative environment would allow superior trainee performance compared with performance of the same basic skills tasks in a non-haptic model. Methods: Ten medical students with minimal laparoscopic experience and similar baseline skill levels as proven by performance of two fundamentals of laparoscopic surgery (FLS) tasks (peg transfer and cutting drills) voluntarily participated in the study. Each performed two tasks, analogous to the FLS drills, on the Laparoscopy VR at 3 levels of difficulty, based on the established settings of the manufacturer. After achieving familiarity with the device and tasks, the students completed the drills both with and without force feedback. Data on completion time, instrument path length, right and left hand errors, and grasping tension were analyzed. The scores in the haptic-enhanced simulation environment were compared with the scores in the non-haptic model and analyzed utilizing Student's t-test. Results: The peg transfer drill showed no difference in performance between the haptic and non-haptic simulations for all metrics at all three levels of difficulty. For the more complex cutting exercise, the time to complete the tasks was significantly shorter when force feedback was provided, at all levels of difficulty (158 ± 56 versus 187 ± 51 s, 176 ± 49 versus 222 ± 68 s, and 275 ± 76 versus 422 ± 220 s, at levels 1, 2, and 3, respectively, P < 0.05). Data on instrument path length, grasping tension, and errors showed a trend toward a benefit from haptics at all difficulty levels, but this difference did not achieve statistical significance. Conclusions: In the more advanced tasks, haptics allowed superior precision, resulting in faster completion of tasks and a trend toward fewer technical errors. In the more basic tasks, haptic-enhanced simulation did not demonstrate an appreciable performance improvement among our trainees. These data suggest that the additional expense of haptic-enhanced laparoscopic simulators may be justified for advanced skill development in surgical trainees as simulator technology continues to improve. © 2009 Elsevier Inc. All rights reserved.","force feedback; haptic; surgical simulation; virtual reality","article; laparoscopic surgery; laparoscopy; medical student; priority journal; simulation; surgical instrument; surgical training; virtual reality; Clinical Competence; Computer Simulation; Computer-Assisted Instruction; Education, Medical; Educational Measurement; Feedback; Humans; Internship and Residency; Laparoscopy; Students, Medical; Surgical Procedures, Operative; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-70249107719
"Persson P.B., Höst G.E., Ynnerman A., Cooper M.D., Tibell L.A.E.","36954793600;14832738400;6602732768;57210523033;7004138753;","Improved feature detection over large force ranges using history dependent transfer functions",2009,"Proceedings - 3rd Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, World Haptics 2009",,, 4810843,"476","481",,1,"10.1109/WHC.2009.4810843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649665592&doi=10.1109%2fWHC.2009.4810843&partnerID=40&md5=636308dd6183302f6a8457c20ab39c08","Department of Science and Technology, Linköping University, Sweden; Department of Clinical and Experimental Medicine, Linköping University, Sweden","Persson, P.B., Department of Science and Technology, Linköping University, Sweden; Höst, G.E., Department of Science and Technology, Linköping University, Sweden; Ynnerman, A., Department of Science and Technology, Linköping University, Sweden; Cooper, M.D., Department of Science and Technology, Linköping University, Sweden; Tibell, L.A.E., Department of Clinical and Experimental Medicine, Linköping University, Sweden","In this paper we present a history dependent transfer function (HDTF) as a possible approach to enable improved haptic feature detection in high dynamic range (HDR) volume data. The HDTF is a multi-dimensional transfer function that uses the recent force history as a selection criterion to switch between transfer functions, thereby adapting to the explored force range. The HDTF has been evaluated using artificial test data and in a realistic application example, with the HDTF applied to haptic protein-ligand docking. Biochemistry experts performed docking tests, and expressed that the HDTF delivers the expected feedback across a large force magnitude range, conveying both weak attractive and strong repulsive protein-ligand interaction forces. Feature detection tests have been performed with positive results, indicating that the HDTF improves the ability of feature detection in HDR volume data as compared to a static transfer function covering the same range. Index Terms: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented, and virtual realities; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality; J.3 [Life and Medical Sciences]: Biology and Genetics; K.3.1 [Computers and Education]: Computer Uses in Education-Computer-assisted instruction (CAI) © 2009 IEEE.",,"Biology and genetics; Computer Assisted Instruction; Computer uses in education; Computers and education; Feature detection; Force magnitude; H.5.1 [information interfaces and presentation]: multimedia information systems - Artificial , augmented , and virtual realities; H.5.2 [information interfaces and presentation]: user interfaces- haptic I/O; High dynamic range; I.3.7 [computer graphics]: three-dimensional graphics and realism; Index terms; Medical science; Multi-dimensional transfer functions; Protein-ligand docking; Protein-ligand interactions; Realistic applications; Selection criteria; Test data; Volume data; Augmented reality; Biochemistry; Computer aided instruction; Docking; Haptic interfaces; Interfaces (computer); Ligands; Mice (computer peripherals); Multimedia systems; Remote control; Speech analysis; Surface chemistry; Virtual reality; Transfer functions",Conference Paper,"Final","",Scopus,2-s2.0-67649665592
"Edmunds T., Pai D.K.","36494232300;7102473274;","Perceptually augmented simulator design through decomposition",2009,"Proceedings - 3rd Joint EuroHaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, World Haptics 2009",,, 4810890,"505","510",,1,"10.1109/WHC.2009.4810890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649664468&doi=10.1109%2fWHC.2009.4810890&partnerID=40&md5=7ef5f7c41ec325bb78470a80ef1068c8","Rutgers University, United States; University of British Columbia, Canada","Edmunds, T., Rutgers University, United States; Pai, D.K., University of British Columbia, Canada","We approach the problem of determining a general method for augmenting haptic simulators to amplify the perceptually salient aspects of the interaction that induce effective skill transfer. Using such a method, we seek to simplify the design of haptic simulators that can improve training effectiveness without requiring expensive improvements in the capability of the rendering hardware. We present a decomposition approach to the automated design of perceptually augmented simulations, and we describe a user-study of the training effectiveness of a search-task simulator designed using our approach vs. an un-augmented simulator. The results indicate that our decomposition approach allows existing psychophysical findings to be leveraged in the design of haptic simulators that effectively impart skill by targeting perceptually significant aspects of the interaction. © 2009 IEEE.",,"Automated design; Decomposition approach; General method; Psychophysical; Rendering hardware; Simulator design; Skill transfer; Design; Haptic interfaces; Mice (computer peripherals); Remote control; Speech analysis; Virtual reality; Simulators",Conference Paper,"Final","",Scopus,2-s2.0-67649664468
"Ziemer C.J., Plumert J.M., Cremer J.F., Kearney J.K.","57203775068;6701849760;7102717840;7101792387;","Estimating distance in real and virtual environments: Does order make a difference?",2009,"Attention, Perception, and Psychophysics","71","5",,"1095","1106",,39,"10.3758/APP.71.5.1096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949170650&doi=10.3758%2fAPP.71.5.1096&partnerID=40&md5=02a4c788f67f7c620b0f58d307950d81","University of Iowa, Iowa City, Iowa, United States","Ziemer, C.J., University of Iowa, Iowa City, Iowa, United States; Plumert, J.M., University of Iowa, Iowa City, Iowa, United States; Cremer, J.F., University of Iowa, Iowa City, Iowa, United States; Kearney, J.K., University of Iowa, Iowa City, Iowa, United States","In this investigation, we examined how the order in which people experience real and virtual environments influences their distance estimates. Participants made two sets of distance estimates in one of the following conditions: (1) real environment first, virtual environment second; (2) virtual environment first, real environment second; (3) real environment first, real environment second; or (4) virtual environment first, virtual environment second. In Experiment 1, the participants imagined how long it would take to walk to targets in real and virtual environments. The participants' first estimates were significantly more accurate in the real than in the virtual environment. When the second environment was the same as the first environment (real-real and virtual- virtual), the participants' second estimates were also more accurate in the real than in the virtual environment. When the second environment differed from the first environment (real-virtual and virtual-real), however, the participants' second estimates did not differ significantly across the two environments. A second experiment, in which the participants walked blindfolded to targets in the real environment and imagined how long it would take to walk to targets in the virtual environment, replicated these results. These subtle yet persistent order effects suggest that memory can play an important role in distance perception. © 2009 The Psychonomic Society, Inc.",,"adult; article; computer interface; decision making; distance perception; ego; female; human; imagination; learning; male; orientation; pattern recognition; perceptive discrimination; sensory deprivation; short term memory; walking; Discrimination (Psychology); Distance Perception; Female; Generalization (Psychology); Humans; Imagination; Judgment; Male; Memory, Short-Term; Orientation; Pattern Recognition, Visual; Reality Testing; Reversal Learning; Sensory Deprivation; User-Computer Interface; Walking; Young Adult",Article,"Final","",Scopus,2-s2.0-68949170650
"Van Velthoven R.F., Piechaud P.T.","7004209598;14052667100;","Training centers: An essential step to developing skills in urolaparoscopy",2009,"Current Urology Reports","10","2",,"93","96",,10,"10.1007/s11934-009-0018-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149120877&doi=10.1007%2fs11934-009-0018-7&partnerID=40&md5=08a6076617a43abaff29e1d59b36b508","Chef du Service d'Urologie, Institut Jules Bordet, CHU Saint-Pierre, Heger-Bordet Street 1, Brussels 1000, Belgium","Van Velthoven, R.F., Chef du Service d'Urologie, Institut Jules Bordet, CHU Saint-Pierre, Heger-Bordet Street 1, Brussels 1000, Belgium; Piechaud, P.T.","Choosing laparoscopy is an important investment for urologists, and it must be learned from basic principles to advance skills through a steep learning curve; the caseload is made of frequent but very demanding procedures. In training centers, scholars are confronted with real-life conditions through large animal models. For about 1500 urologists, the European Institute of Tele Surgery has offered such a program for a decade. We evaluate its impact through a self-administered, Internet-hosted questionnaire. Individual data concern number and type of courses attended, skill level and type of practice before training, and expectations at registration. Personal benefit is evaluated through the delay before starting routine laparoscopy or the major procedures volume facing open counterparts. The ability of this program to meet scholar's expectations is reflected by eventual need for further training or by trainee suggestions. In spite of 85% cumulated satisfaction index, further developments must improve practical training: clear, reproducible stepwise protocols, repeated under supervision in animal models today and hopefully in augmented reality simulators tomorrow. The knowledge transfer and companionship made of theoretical and practical lessons followed by a straight supervised application represent an essential model for gaining proficiency. © 2009 Current Medicine Group, LLC References and Recommended Reading.",,"article; evaluation; experimental model; laparoscopic surgery; learning; medical education; medical student; professional development; questionnaire; simulation; skill; student satisfaction; Clinical Competence; Education, Medical, Continuing; Endoscopy; France; Humans; Laparoscopy; Models, Animal; Models, Educational; Questionnaires; Urologic Surgical Procedures; Urology",Article,"Final","",Scopus,2-s2.0-62149120877
"Mirelman A., Bonato P., Deutsch J.E.","35332484600;7006225560;7201985389;","Effects of training with a robot-virtual reality system compared with a robot alone on the gait of individuals after stroke",2009,"Stroke","40","1",,"169","174",,219,"10.1161/STROKEAHA.108.516328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-60549105219&doi=10.1161%2fSTROKEAHA.108.516328&partnerID=40&md5=f1ccd9b5cf53303383cbb893e49474a1","RiVERS Lab., Department of Rehabilitation and Movement Science, University of Medicine and Dentistry of New Jersey, NJ, United States; Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA, United States; Harvard-MIT Division of Health Sciences and Technology, Boston, MA, United States; Gait and Neurodynamics Lab., Tel Aviv Medical Center, 6 Weizmann st, Tel Aviv, 64239, Israel","Mirelman, A., RiVERS Lab., Department of Rehabilitation and Movement Science, University of Medicine and Dentistry of New Jersey, NJ, United States, Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA, United States, Gait and Neurodynamics Lab., Tel Aviv Medical Center, 6 Weizmann st, Tel Aviv, 64239, Israel; Bonato, P., Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA, United States, Harvard-MIT Division of Health Sciences and Technology, Boston, MA, United States; Deutsch, J.E., RiVERS Lab., Department of Rehabilitation and Movement Science, University of Medicine and Dentistry of New Jersey, NJ, United States","BACKGROUND AND PURPOSE: Training of the lower extremity (LE) using a robot coupled with virtual environments has shown to transfer to improved overground locomotion. The purpose of this study was to determine whether the transfer of training of LE movements to locomotion was greater using a virtual environment coupled with a robot or with the robot alone. METHODS: A single, blind, randomized clinical trial was conducted. Eighteen individuals poststroke participated in a 4-week training protocol. One group trained with the robot virtual reality (VR) system and the other group trained with the robot alone. Outcome measures were temporal features of gait measured in a laboratory setting and the community. RESULTS: Greater changes in velocity and distance walked were demonstrated for the group trained with the robotic device coupled with the VR than training with the robot alone. Similarly, significantly greater improvements in the distance walked and number of steps taken in the community were measured for the group that trained with robot coupled with the VR. These differences were maintained at 3 months' follow-up. CONCLUSIONS: The study is the first to demonstrate that LE training of individuals with chronic hemiparesis using a robotic device coupled with VR improved walking ability in the laboratory and the community better than robot training alone. Copyright © 2009 American Heart Association. All rights reserved.","Community ambulation; Gait; Robotics; Stroke; Virtual reality","adult; aged; article; clinical article; clinical trial; controlled clinical trial; controlled study; female; gait; hemiparesis; human; locomotion; male; physiotherapy; priority journal; randomized controlled trial; robotics; single blind procedure; stroke; virtual reality; walking speed; comparative study; computer interface; exercise tolerance; fitness; instrumentation; kinesiotherapy; leg; methodology; middle aged; muscle strength; neurologic disease; paresis; pathophysiology; physiology; physiotherapy; robotics; skeletal muscle; stroke; teaching; treatment outcome; Adult; Aged; Exercise Movement Techniques; Exercise Tolerance; Female; Gait Disorders, Neurologic; Humans; Leg; Locomotion; Male; Middle Aged; Muscle Strength; Muscle, Skeletal; Paresis; Physical Fitness; Physical Therapy Modalities; Robotics; Stroke; Teaching; Treatment Outcome; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-60549105219
"Swennen G.R.J., Mollemans W., Schutyser F.","6604000250;8436712700;6603476637;","Three-Dimensional Treatment Planning of Orthognathic Surgery in the Era of Virtual Imaging",2009,"Journal of Oral and Maxillofacial Surgery","67","10",,"2080","2092",,221,"10.1016/j.joms.2009.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70249136888&doi=10.1016%2fj.joms.2009.06.007&partnerID=40&md5=f0c48e13e98b4cdd1d133e25ea4987d2","Division of Maxillofacial Surgery, Department of Surgery, General Hospital St-Jan Bruges, Bruges, Belgium; Medical Image Computing, University Hospital Gasthuisberg, Faculties of Medicine and Engineering, Leuven, Belgium; Digital Dentistry of Medicim, Mechelen, Belgium","Swennen, G.R.J., Division of Maxillofacial Surgery, Department of Surgery, General Hospital St-Jan Bruges, Bruges, Belgium; Mollemans, W., Medical Image Computing, University Hospital Gasthuisberg, Faculties of Medicine and Engineering, Leuven, Belgium; Schutyser, F., Digital Dentistry of Medicim, Mechelen, Belgium","Purpose: The aim of this report was to present an integrated 3-dimensional (3D) virtual approach toward cone-beam computed tomography-based treatment planning of orthognathic surgery in the clinical routine. Materials and Methods: We have described the different stages of the workflow process for routine 3D virtual treatment planning of orthognathic surgery: 1) image acquisition for 3D virtual orthognathic surgery; 2) processing of acquired image data toward a 3D virtual augmented model of the patient's head; 3) 3D virtual diagnosis of the patient; 4) 3D virtual treatment planning of orthognathic surgery; 5) 3D virtual treatment planning communication; 6) 3D splint manufacturing; 7) 3D virtual treatment planning transfer to the operating room; and 8) 3D virtual treatment outcome evaluation. Conclusions: The potential benefits and actual limits of an integrated 3D virtual approach for the treatment of the patient with a maxillofacial deformity are discussed comprehensively from our experience using 3D virtual treatment planning clinically. © 2009 American Association of Oral and Maxillofacial Surgeons.",,"article; cone beam computed tomography; human; image analysis; orthognathic surgery; outcome assessment; patient transport; three dimensional imaging; treatment planning; virtual reality",Article,"Final","",Scopus,2-s2.0-70249136888
"Xeroulis G., Dubrowski A., Leslie K.","14061415800;6602597937;22634856500;","Simulation in laparoscopic surgery: A concurrent validity study for FLS",2009,"Surgical Endoscopy and Other Interventional Techniques","23","1",,"161","165",,23,"10.1007/s00464-008-0120-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149199771&doi=10.1007%2fs00464-008-0120-9&partnerID=40&md5=a4187ea05077666a86455a4e7b293245","Department of Surgery, Division of General Surgery, University of Western Ontario, London, ON, Canada; MIS Fellow UofT, 180 Fairglen Ave., Toronto, ON M1W 1B1, Canada; Department of Nursing Education, University of Toronto, Toronto, ON, Canada","Xeroulis, G., Department of Surgery, Division of General Surgery, University of Western Ontario, London, ON, Canada, MIS Fellow UofT, 180 Fairglen Ave., Toronto, ON M1W 1B1, Canada; Dubrowski, A., Department of Nursing Education, University of Toronto, Toronto, ON, Canada; Leslie, K., Department of Surgery, Division of General Surgery, University of Western Ontario, London, ON, Canada","Background: Current assessments using the fundamentals of laparoscopic surgery (FLS) tasks are labour intensive and depend heavily on expert raters. Hand motion analysis may offer an alternative method of objective evaluation of FLS performance. Purpose: The aim of this study was to assess whether a correlation exists between the expert rated assessments of the FLS tasks and computer-based assessment of motion efficiency using the Imperial College surgical assessment device (ICSAD). Methods: We recruited 26 volunteer subjects who were stratified into three experience groups: juniors (PGY1-3) (N = 13), seniors (PGY4,5) (N = 7) and staff surgeons (N = 6). All subjects performed four FLS tasks: (1) peg transfer, (2) pattern cut, (3) endoloop and (4) intracorporeal suturing. Performance was assessed by both standard FLS expert rating and motion analysis using ICSAD. Group differences were analyzed using the Kruskal-Wallis test, and Spearman coefficient analyses were employed to compare FLS and ICSAD scores. Results: FLS expert-derived scores discriminated effectively between experience groups for all tasks (p < 0.05). Motion efficiency scores discriminated between experience groups for tasks 1, 3 and 4 for number of movements (p < 0.05), tasks 1 and 4 for total distance (p < 0.05) and tasks 1, 2, 3 and 4 for total time (p < 0.005). There was a significant correlation between total FLS expert scores and the motion efficiency metrics of total distance, number of movements and total time (Spearman coefficient and p values of 0.81, < 0.001; 0.76; < 0.001; and 0.93, < 0.001, respectively). Conclusion: There is a high correlation between FLS standard scoring and motion efficiency metrics. The use of ICSAD for the objective assessment of FLS tasks may in the future offer an adjunctive method of evaluation. ICSAD metrics are potentially less labour intensive due to the instant and fully automated computerized scoring that it provides. © 2008 Springer Science+Business Media, LLC.","Education; Endoscopy; Training","article; computer; controlled study; correlation coefficient; human; Kruskal Wallis test; laparoscopic surgery; medical education; priority journal; simulation; Clinical Competence; Computer-Assisted Instruction; Curriculum; Hand; Humans; Laparoscopy; Motor Skills; Practice (Psychology); Reproducibility of Results; Signal Processing, Computer-Assisted; Suture Techniques",Article,"Final","",Scopus,2-s2.0-58149199771
"Gerling G.J., Rigsbee S., Childress R.M., Martin M.L.","35271881600;23980862900;23979604400;55482122100;","The design and evaluation of a computerized and physical simulator for training clinical prostate exams",2009,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans","39","2",,"388","403",,27,"10.1109/TSMCA.2008.2009769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149117616&doi=10.1109%2fTSMCA.2008.2009769&partnerID=40&md5=797ba9750fcf70c26ac03074631ae777","Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; School of Nursing, University of Virginia, Charlottesville, VA 22904, United States; School of Medicine, University of Virginia, Charlottesville, VA 22908, United States","Gerling, G.J., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; Rigsbee, S., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; Childress, R.M., School of Nursing, University of Virginia, Charlottesville, VA 22904, United States; Martin, M.L., School of Medicine, University of Virginia, Charlottesville, VA 22908, United States","The most effective screening for prostate cancer combines the prostate specific antigen blood test with the digital rectal examination (DRE). In performing a DRE, two sequential tasks are completed: (task a) palpating the prostate to identify abnormalities and (task b) linking identified abnormalities to a disease diagnosis. At present, clinicians find too few abnormalities and have variable rates of detection, due in part to the inadequacy of training simulators. The Virginia Prostate Examination Simulator (VPES) was designed, built, and tested to address the inadequacies of current simulators by incorporating the design requirements of the basic elements of accurate anatomy, multiple and reconfigurable scenarios of graded difficulty, and technique and performance feedback. We compared the training effectiveness of the VPES with two commercial simulators in an experiment of 36 medical and nurse practitioner students. Results indicate each type of training simulator-improved abilities, in general. Upon closer analysis, however, the following key patterns emerge: 1) Across all types of training, more deficiencies lie in skill-based rather than rule-based decision making, which improves only for VPES trainees; 2) only VPES training transfers both to other simulators and previously unencountered scenarios; 3) visual feedback may increase the number of abnormalities reported yet hinder the ability to discriminate; and 4) applied finger pressure did not correlate with the ability to identify abnormalities. © 2008 IEEE.","Evaluation; Haptics; Medical; Nursing; Simulation; Simulator; Tactile; Training","Diagnosis; Information dissemination; Nursing; Visual communication; Evaluation; Haptics; Medical; Simulation; Tactile; Training; Simulators",Article,"Final","",Scopus,2-s2.0-62149117616
"Cossairt O., Nayar S., Ramamoorthi R.","8620017500;35560595700;6701653981;","Light field transfer: Global illumination between real and synthetic objects",2008,"SIGGRAPH'08: International Conference on Computer Graphics and Interactive Techniques, ACM SIGGRAPH 2008 Papers 2008",,, 57,"","",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649107635&partnerID=40&md5=30c04bc4e82ecadfa0ee669fe677157f","Columbia University","Cossairt, O., Columbia University; Nayar, S., Columbia University; Ramamoorthi, R., Columbia University","We present a novel image-based method for compositing real and synthetic objects in the same scene with a high degree of visual realism. Ours is the first technique to allow global illumination and near-field lighting effects between both real and synthetic objects at interactive rates, without needing a geometric and material model of the real scene. We achieve this by using a light field interface between real and synthetic components-thus, indirect illumination can be simulated using only two 4D light fields, one captured from and one projected onto the real scene. Multiple bounces of inter-reflections are obtained simply by iterating this approach. The interactivity of our technique enables its use with time-varying scenes, including dynamic objects. This is in sharp contrast to the alternative approach of using 6D or 8D light transport functions of real objects, which are very expensive in terms of acquisition and storage and hence not suitable for real-time applications. In our method, 4D radiance fields are simultaneously captured and projected by using a lens array, video camera, and digital projector. The method supports full global illumination with restricted object placement, and accommodates moderately specular materials. We implement a complete system and show several example scene compositions that demonstrate global illumination effects between dynamic real and synthetic objects. Our implementation requires a single point light source and dark background. © 2008 ACM.","Augmented reality; Global illumination; Image-based relighting; Light field","Alternative approaches; Complete systems; Compositing; Digital projectors; Dynamic objects; Global illumination; Image-based methods; Image-based relighting; Indirect illuminations; Inter-reflections; Interactive rates; Interactivity; Lens arrays; Light field; Light transport functions; Lighting effects; Material models; Near fields; Object placements; Real objects; Real scenes; Real-time applications; Scene compositions; Sharp contrasts; Single points; Specular materials; Time-varying; Visual realisms; Augmented reality; Computer graphics; Interactive computer graphics; Light sources; Lighting; Virtual reality; Light",Conference Paper,"Final","",Scopus,2-s2.0-57649107635
"Coutrix C., Nigay L.","16229655100;6602572073;","Balancing physical and digital properties in mixed objects",2008,"Proceedings of the Workshop on Advanced Visual Interfaces AVI",,,,"305","308",,7,"10.1145/1385569.1385619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349190338&doi=10.1145%2f1385569.1385619&partnerID=40&md5=4a366c8826f38873b2f3f09526e4f6e2","Grenoble Informatics Laboratory (LIG), University of Grenoble 1, BP 53, 38041 Grenoble Cedex 9, France","Coutrix, C., Grenoble Informatics Laboratory (LIG), University of Grenoble 1, BP 53, 38041 Grenoble Cedex 9, France; Nigay, L., Grenoble Informatics Laboratory (LIG), University of Grenoble 1, BP 53, 38041 Grenoble Cedex 9, France","Mixed interactive systems seek to smoothly merge physical and digital worlds. In this paper we focus on mixed objects that take part in the interaction. Based on our Mixed Interaction Model, we introduce a new characterization space of the physical and digital properties of a mixed object from an intrinsic viewpoint without taking into account the context of use of the object. The resulting enriched Mixed Interaction Model aims at balancing physical and digital properties in the design process of mixed objects. The model extends and generalizes previous studies on the design of mixed systems and covers existing approaches of mixed systems including tangible user interfaces, augmented reality and augmented virtuality. A mixed system called ORBIS that we developed is used to illustrate the discussion: we highlight how the model informs the design alternatives of ORBIS. Copyright 2008 ACM.","Augmented reality; Design space; Mixed objects; Mixed systems; Tangible user interfaces","Design; Digital arithmetic; Flow interactions; User interfaces; Virtual reality; Augmented reality; Design space; Mixed objects; Mixed systems; Tangible user interfaces; Process engineering",Conference Paper,"Final","",Scopus,2-s2.0-57349190338
"Stone R.T., Bisantz A., Llinas J., Paquet V.","57194665670;6603831596;7004600442;7004183484;","Improving tele-robotic landmine detection through augmented reality devices",2008,"Proceedings of the Human Factors and Ergonomics Society","1",,,"206","210",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350616147&partnerID=40&md5=f8a1db73471675a4e1939e10b3b284c2","University at Buffalo, State University of New York, Amherst, NY, United States; Iowa State University, Ames, IA, United States","Stone, R.T., University at Buffalo, State University of New York, Amherst, NY, United States, Iowa State University, Ames, IA, United States; Bisantz, A., University at Buffalo, State University of New York, Amherst, NY, United States; Llinas, J., University at Buffalo, State University of New York, Amherst, NY, United States; Paquet, V., University at Buffalo, State University of New York, Amherst, NY, United States","Tele-robotic landmine/UXO (unexploded ordinance) detection and disposal is a real world example of a complex human robot interaction task. This study demonstrates that the use of augmented reality interfaces, designed based on a theoretically driven framework, can significantly improve performance, and that these same devices can be used as effective training technologies. Participants in this study were assigned to one of four experimental conditions (two uni-sensory and one multisensory augmented reality interface, and a control group). All participants were trained in tele-robotic landmine detection and the groups performed these tasks first with augmentation (session one) and then without (session two/transfer task). The results show that participants in the augmented groups significantly outperformed the control group in terms of the numbers of landmines correctly identified (in both sessions). The multisensory condition was found to result in both significantly increased landmine detection and significantly decreased task time (in both sessions).",,"Control groups; Experimental conditions; Human robot interactions; Land mine; Land mine detection; Multisensory; Unexploded ordinances; Augmented reality; Bombs (ordnance); Ergonomics; Human computer interaction; Metal detectors; Robotics; Robots; Virtual reality; Explosives",Conference Paper,"Final","",Scopus,2-s2.0-70350616147
"Burkhardt J.-M., Détienne F., Moutsingua-Mpaga L., Perron L., Safin S., Leclercq P.","7102736339;6602554663;36133610700;23393484800;26768297800;9278813000;","Multimodal collaborative activity among architectural designers using an augmented desktop at distance or in collocation",2008,"ACM International Conference Proceeding Series","369",, 1473049,"","",,4,"10.1145/1473018.1473049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953787655&doi=10.1145%2f1473018.1473049&partnerID=40&md5=a54d0ca15ded9a6591b2e8ba26166c37","Paris Descartes University, Ergonomics-Behavior-Interaction Lab., Paris, France; LTCI CNRS-Telecom. Paris Tech., Paris, France; INRIA, Rocquencourt, France; France Telecom. R and D / Orange Labs., Lannion, France; Lucid Group (Lab. for User Cognition and Innovative Design), Faculty of Applied Sciences, University of Liège, Belgium","Burkhardt, J.-M., Paris Descartes University, Ergonomics-Behavior-Interaction Lab., Paris, France, INRIA, Rocquencourt, France; Détienne, F., LTCI CNRS-Telecom. Paris Tech., Paris, France, INRIA, Rocquencourt, France; Moutsingua-Mpaga, L., INRIA, Rocquencourt, France; Perron, L., France Telecom. R and D / Orange Labs., Lannion, France; Safin, S., Lucid Group (Lab. for User Cognition and Innovative Design), Faculty of Applied Sciences, University of Liège, Belgium; Leclercq, P., Lucid Group (Lab. for User Cognition and Innovative Design), Faculty of Applied Sciences, University of Liège, Belgium","Motivation - To analyse how Augmented Reality associated to video may affect collaborative design and multimodal interactions. Research approach - An exploratory study that aims to compare 2 pairs of last year students in co-presence with 1 distant pair. Each pair had to solve an architectural design problem. Collected video has been coded with a systematic method of protocol analysis. Findings/Design - When using an AR desktop-based CAD, distance may not affect the design process itself whereas it may affect how the process is distributed across the various modalities of collaboration. Furthermore, collaborating and architectural experiences influence collaboration and/or design. Research limitations/Implications - Only 3 pairs of students participated in the study resulting in 12 h of video protocol, which limits generalisation of the findings. Originality/Value - The research makes a contribution in providing a detailed view on how external (e.g. situation, technology) and individual factors may affect the activity of collaborative design. Furthermore, we propose a coding method usable beyond design in a wide range of collaborative activities to underline how they are affected by technology and other situational constraints. Take away message - Technology constraints as well as personnal characteristics of designers result in designing with specific forms of multimodal collaboration.","collaborative design; empirical study; multimodal communication; protocol analysis method","Architectural designer; Co-presence; Coding methods; Collaborative activities; Collaborative design; Design process; Empirical studies; Exploratory studies; Generalisation; Individual factors; Multi-modal; Multi-Modal Interactions; Multimodal communications; Protocol analysis; Research approach; Situational constraints; Systematic method; Technology constraints; Architectural design; Augmented reality; Fuzzy control; Research; Virtual reality; Ergonomics",Conference Paper,"Final","",Scopus,2-s2.0-77953787655
"Chen R., Wang X.","57199982977;8945580300;","An Empirical Study on Tangible Augmented Reality Learning Space for Design Skill Transfer",2008,"Tsinghua Science and Technology","13","SUPPL. 1",,"13","18",,29,"10.1016/S1007-0214(08)70120-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62649134840&doi=10.1016%2fS1007-0214%2808%2970120-2&partnerID=40&md5=62e85e2993264269b1331ef753ca0a09","Design Lab., Faculty of Architecture, Design and Planning, Sydney, NSW 2006, Australia","Chen, R., Design Lab., Faculty of Architecture, Design and Planning, Sydney, NSW 2006, Australia; Wang, X., Design Lab., Faculty of Architecture, Design and Planning, Sydney, NSW 2006, Australia","Tangible augmented reality (TAR) technology opens a novel realm which integrates the computer-generated elements into the real word. Its applications into design education have been explored with a limitation to this entire area. TAR offers an innovative learning space by merging digital learning materials into the format of media with tools or objects which are direct parts of the physical space. It is therefore conceived that such combination opens new perspectives in teaching and learning. This paper presented and evaluated one TAR system to improve the pedagogical effectiveness of experiential and collaborative learning process in urban design education. The results from the experiments were analyzed under a previously developed theoretical framework, which show that TAR can enhance the design activities in some collaborative work. © 2008 Tsinghua University Press.","augmented reality; design learning; physicality; tangible augmented reality; tangible interface","Design; Education; Tar; Virtual reality; Collaborative learning process; Collaborative works; Design activities; Design educations; design learning; Design skills; Digital-learning; Empirical studies; Innovative learning; physicality; tangible augmented reality; tangible interface; Teaching and learning; Theoretical frameworks; Urban designs; Augmented reality",Article,"Final","",Scopus,2-s2.0-62649134840
"Arendarski B., Termath W., Mecking P.","24830119700;24831100200;24831023900;","Maintenance of complex machines in electric power systems using Virtual Reality techniques",2008,"Conference Record of IEEE International Symposium on Electrical Insulation",,, 4570378,"483","487",,22,"10.1109/ELINSL.2008.4570378","https://www.scopus.com/inward/record.uri?eid=2-s2.0-52049095031&doi=10.1109%2fELINSL.2008.4570378&partnerID=40&md5=27c62232ba194103fe3537463e347da1","Fraunhofer Institute for Factory Operation and Automation (IFF), Sandtorstrasse 22, 39106 Magdeburg, Germany; RWE Rhein-Rulir-Netzservice GmbH, Reeser Landstrasse 41, 46483 Wesel, Germany","Arendarski, B., Fraunhofer Institute for Factory Operation and Automation (IFF), Sandtorstrasse 22, 39106 Magdeburg, Germany; Termath, W., Fraunhofer Institute for Factory Operation and Automation (IFF), Sandtorstrasse 22, 39106 Magdeburg, Germany; Mecking, P., RWE Rhein-Rulir-Netzservice GmbH, Reeser Landstrasse 41, 46483 Wesel, Germany","This paper illustrates how Virtual Reality (VR) techniques can be used in electric power systems visualization and how this can increase effectiveness of vocational training in field of power industry. The content of this article is based on a project oriented to specialized staff members, dealing with maintenance, repairs and diagnostics of complex machines such transformers and generators. Introducing a scheme that integrates VR technologies to electrical machines and equipment provides an efficient tool for implementing operator training methods. Three-dimensional (3D) visualization brings a new ways of knowledge transfer and education regarding complex equipment. ©2008 IEEE.",,"Chlorine compounds; Electric generators; Electric industry; Electric power measurement; Electric power systems; Electric power transmission networks; Information management; Knowledge management; Maintainability; Maintenance; Paper coating; Power transmission; Technical presentations; Three dimensional; Virtual reality; Visualization; Complex equipment; Complex machining; Electrical insulation; Electrical machines; In-field; International symposium; Knowledge transfer; Operator training; Power industries; Staff members; Three-dimensional visualization; Virtual reality (VR); Vocational training; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-52049095031
"Quarles J., Lampotang S., Fischler I., Fishwick P., Lok B.","55868360300;35587861100;35588972600;56252883400;57203616548;","A mixed reality approach for merging abstract and concrete knowledge",2008,"Proceedings - IEEE Virtual Reality",,, 4480746,"27","34",,24,"10.1109/VR.2008.4480746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349097941&doi=10.1109%2fVR.2008.4480746&partnerID=40&md5=e541bedde2a5a643d1d30ad169d0dd8d","Dept. of CISE, University of Florida; Dept. of Anesthesiology, University of Florida; Dept. of Psychology, University of Florida","Quarles, J., Dept. of CISE, University of Florida; Lampotang, S., Dept. of Anesthesiology, University of Florida; Fischler, I., Dept. of Psychology, University of Florida; Fishwick, P., Dept. of CISE, University of Florida; Lok, B., Dept. of CISE, University of Florida","Mixed reality's (MR) ability to merge real and virtual spaces is applied to merging different knowledge types, such as abstract and concrete knowledge. To evaluate whether the merging of knowledge types can benefit learning, MR was applied to an interesting problem in anesthesia machine education. The Virtual Anesthesia Machine (VAM) is an interactive, abstract 2D transparent reality [14] simulation of the internal components and invisible gas flows of an anesthesia machine. It is widely used in anesthesia education. However when presented with an anesthesia machine, some students have difficulty transferring abstract VAM knowledge to the concrete real device. This paper presents the Augmented Anesthesia Machine (AAM). The AAM applies a magic-lens approach to combine the VAM simulation and a real anesthesia machine. The AAM allows students to interact with the real anesthesia machine while visualizing how these interactions affect the internal components and invisible gas flows in the real world context. To evaluate the AAM's learning benefits, a user study was conducted. Twenty participants were divided into either the VAM (abstract only) or AAM (concrete+abstract) conditions. The results of the study show that MR can help users bridge their abstract and concrete knowledge, thereby improving their knowledge transfer into real world domains. © 2008 IEEE.","Anesthesiology; Mixed reality; Modeling and simulation; Psychology; User studies","Gas flowing; Interesting problem; Knowledge transfer; Knowledge types; Mixed reality; Modeling and simulation; Psychology; Real-world; Real-world domains; User studies; Virtual spaces; Abstracting; Aerodynamics; Anesthesiology; Concrete construction; Education; Flow of gases; Gas dynamics; Information management; Knowledge management; Merging; Students; Virtual reality; Machine components",Conference Paper,"Final","",Scopus,2-s2.0-50349097941
"Edmunds T., Pai D.K.","36494232300;7102473274;","Perceptual rendering for learning haptic skills",2008,"Symposium on Haptics Interfaces for Virtual Environment and Teleoperator Systems 2008 - Proceedings, Haptics",,, 4479948,"225","230",,9,"10.1109/HAPTICS.2008.4479948","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49749125066&doi=10.1109%2fHAPTICS.2008.4479948&partnerID=40&md5=b28a61f5162c31a4d10f66e212927f94","Rutgers University; University of British Columbia, Rutgers University","Edmunds, T., Rutgers University; Pai, D.K., University of British Columbia, Rutgers University","We approach the problem of creating haptic simulators that effectively impart skill without requiring high-fidelity devices by identifying perceptually salient events that signal transitions in the interaction. By augmenting these events, we seek to overcome deficiencies in the fidelity of the rendering hardware. We present an extension of event-based haptic rendering to non-collision events, and we describe a user-study of die training effectiveness of passive force-field haptic simulation vs. active event-augmented simulation in a tool-manipulation task. The results indicate that active augmentation improves skill transfer without requiring an increase in the quality of the rendering device. © 2008 IEEE.",,"Collision events; Haptic rendering; Haptic simulations; Haptics; High-fidelity; Manipulation tasks; Passive forces; Perceptual rendering; Rendering hardware; Signal transitions; Skill transfer; Virtual environment; Remote control; Signal processing; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-49749125066
"Dias P., Campos G., Santos V., Casaleiro R., Seco R., Santos B.S.","22333370800;8603962700;35616433200;24477040900;24477807400;7006476948;","3D reconstruction and Auralisation of the ""painted dolmen"" of antelas",2008,"Proceedings of SPIE - The International Society for Optical Engineering","6805",, 68050Y,"","",,2,"10.1117/12.766607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949110073&doi=10.1117%2f12.766607&partnerID=40&md5=8cf6b741616e6b2cc561071a4c67c4e0","DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal","Dias, P., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Campos, G., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal; Santos, V., Departamento de Engenharia Mecânica, Univ. of Aveiro, Aveiro, Portugal; Casaleiro, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Seco, R., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal; Santos, B.S., DETI - Dep. de Electrónica, Telecomunicações e Informática, Univ. of Aveiro, Portugal, IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Portugal","This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with ""free-space nodes"", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of ""free-space nodes"" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.","3D acquisition; Augmented reality; Auralisation; Laser range finder; Virtual reality","Absorption; Acoustic wave absorption; Acoustics; Architectural acoustics; Audio acoustics; Computational efficiency; Display devices; Energy absorption; Functions; Headphones; Helmet mounted displays; Image reconstruction; Imaging systems; Impulse response; Lasers; Loudspeakers; Medical imaging; Mergers and acquisitions; Range finders; Range finding; Reflection; Restoration; Reverberation; Surfaces; Three dimensional computer graphics; Virtual reality; 3D acquisition; 3D geometries; 3D reconstructions; 3d visual models; Acoustic absorption coefficients; Archaeological sites; Audio sources; Audio visuals; Augmented reality; Auralisation; Dark rooms; Geometric acoustics; Geometric models; Head Related Transfer Functions; In-situ; Irregular surfaces; Iterative algorithms; Iterative Closest points; Laser range finder; Orientation sensors; Reconstruction softwares; Reverberation times; Software computes; Sound waves; Source localisation; Stereo headphones; Uniform grids; Visual scenes; Visualization toolkits; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-47949110073
"Reinhart G., Munzert U., Vogl W.","7101858932;7801662307;57130548200;","A programming system for robot-based remote-laser-welding with conventional optics",2008,"CIRP Annals - Manufacturing Technology","57","1",,"37","40",,74,"10.1016/j.cirp.2008.03.120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43649097125&doi=10.1016%2fj.cirp.2008.03.120&partnerID=40&md5=4f1526753380636afcb080a3b5e7c385","Institute for Machine Tools and Industrial Management (iwb), Technische Universitaet Muenchen, Munich, Germany","Reinhart, G., Institute for Machine Tools and Industrial Management (iwb), Technische Universitaet Muenchen, Munich, Germany; Munzert, U., Institute for Machine Tools and Industrial Management (iwb), Technische Universitaet Muenchen, Munich, Germany; Vogl, W., Institute for Machine Tools and Industrial Management (iwb), Technische Universitaet Muenchen, Munich, Germany","The authors present a task oriented programming system for remote-laser-welding (RLW) with conventional optics, which considers real workpiece data and process limits for the calculation of robot paths. This approach relies on an augmented-reality-based user interface for effective 3D-interaction and task definition. A powerful task and motion planning system has been developed, that transfers task-descriptions to optimized, executable robot operations. The resulting system offers fast and efficient spatial interaction methods that allow even novel users to quickly define operations on a task-level. The approach simplifies and accelerates the programming process, consequently leading to a reduction of cycle time for a RLW-task by more than 30%. © 2008.","Programming; Remote-laser-welding; Robot","Laser beam welding; Mathematical programming; User interfaces; Virtual reality; Conventional optics; Remote-laser-welding; Robot paths; Spatial interaction; Robots",Article,"Final","",Scopus,2-s2.0-43649097125
"Liu X., Scheidt R.A.","41862038700;13307318200;","Contributions of online visual feedback to the learning and generalization of novel finger coordination patterns",2008,"Journal of Neurophysiology","99","5",,"2546","2557",,29,"10.1152/jn.01044.2007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-47549104753&doi=10.1152%2fjn.01044.2007&partnerID=40&md5=d83c844ac04be99eb90ec63acee1723b","Department of Biomedical Engineering, Marquette University, Milwaukee, WI, United States; Department of Physical Medicine and Rehabilitation, Northwestern University Feinberg School of Medicine, Chicago, IL, United States; Department of Biomedical Engineering, Olin Engineering Center, 303, Marquette University, P.O. Box 1881, Milwaukee, WI 53201-1881, United States","Liu, X., Department of Biomedical Engineering, Marquette University, Milwaukee, WI, United States; Scheidt, R.A., Department of Biomedical Engineering, Marquette University, Milwaukee, WI, United States, Department of Physical Medicine and Rehabilitation, Northwestern University Feinberg School of Medicine, Chicago, IL, United States, Department of Biomedical Engineering, Olin Engineering Center, 303, Marquette University, P.O. Box 1881, Milwaukee, WI 53201-1881, United States","We explored how people learn new ways to move objects through space using neuromuscular control signals having more degrees of freedom than needed to unambiguously specify object location. Subjects wore an instrumented glove that recorded finger motions. A linear transformation matrix projected joint angle signals (a high-dimensional control vector) onto a two-dimensional cursor position on a video monitor. We assessed how visual information influences learning and generalization of novel finger coordination patterns as subjects practiced using hand gestures to manipulate cursor location. Three groups of test subjects practiced moving a visible cursor between different sets of screen targets. The hand-to-screen transformation was designed such that the different sets of targets (which we called implicit spatial cues) varied in how informative they were about the gestures to be learned. A separate control group practiced gesturing with explicit cues (pictures of desired gestures) without ongoing cursor feedback. Another control group received implicit spatial cueing and feedback only of final cursor position. We found that test subjects and subjects provided with explicit cues could learn to produce desired gestures, although training efficacy decreased as the amount of task-relevant feedback decreased. Although both control groups learned to associate screen targets with specific gestures, only subjects provided with online feedback of cursor motion learned to generalize in a manner consistent with the internal representation of an inverse hand-to-screen mapping. These findings suggest that spatial learning and generalization require dynamic feedback of object motion in response to control signal changes; static information regarding geometric relationships between controller and endpoint configurations does not suffice. Copyright © 2008 The American Physiological Society.",,"adult; article; computer program; controlled study; female; finger joint; gesture; hand movement; human; human experiment; learning; male; neuromuscular function; priority journal; visual adaptation; visual field; visual information; visuomotor coordination; algorithm; association; biomechanics; brain mapping; calibration; feedback system; finger; innervation; learning; online system; physiology; psychomotor performance; statistical analysis; Adult; Algorithms; Biomechanics; Brain Mapping; Calibration; Cues; Data Interpretation, Statistical; Feedback; Female; Fingers; Generalization (Psychology); Humans; Learning; Male; Online Systems; Psychomotor Performance",Article,"Final","",Scopus,2-s2.0-47549104753
"Jiang P., Liu Y., Zhao L.","7201470064;55649532100;55493558600;","Distributed VR-driven collaborative support for modelling 3D silicon micro components",2008,"International Journal of Internet Manufacturing and Services","1","4",,"345","365",,1,"10.1504/IJIMS.2008.022991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864407259&doi=10.1504%2fIJIMS.2008.022991&partnerID=40&md5=f439e3b6e6caa26706c35a779d5d8846","National Key Laboratory for Manufacturing Systems Engineering, School of Mechanical Engineering, Xi'an Jiaotong University, Xia'an 710049, China; CAD/CAM Institute, School of Mechanical Engineering, Xi'an Jiaotong University, Xian 710049, China","Jiang, P., National Key Laboratory for Manufacturing Systems Engineering, School of Mechanical Engineering, Xi'an Jiaotong University, Xia'an 710049, China; Liu, Y., CAD/CAM Institute, School of Mechanical Engineering, Xi'an Jiaotong University, Xian 710049, China; Zhao, L., CAD/CAM Institute, School of Mechanical Engineering, Xi'an Jiaotong University, Xian 710049, China","To making MEMS structure design in an intuitive way and to reducing design iteration, a 3D feature-based collaborative structure modelling work flow of 3D silicon micro components is proposed according to 'function to 3D shape to fabrication' design methodology. Around this collaborative structure modelling workflow, design privileges are specified to designers through design multi-views, CSG/ B-rep solid model constructs the basis of feature modelling, distributed VR devices driving model is put forward to enhance man-machine interaction and P2P communication infrastructure is used to accelerate 3D model transfer through internet. Using a micro RF switch as an example, corresponding collaborative design flow and design multi-views is scheduled and execute on the collaborative micro-components modelling prototype. Copyright © 2008, Inderscience Publishers.","3D modelling; Collaborative design; Distributed virtual reality; P2P; Peer to peer; Silicon fabricated micro-components","3D modeling; Distributed computer systems; Iterative methods; Microoptics; Silicon; Virtual reality; 3D modelling; Collaborative design; Distributed virtual reality; Micro-components; Peer to peer; Structural design",Article,"Final","",Scopus,2-s2.0-84864407259
"Takahashi C.D., Der-Yeghiaian L., Le V., Motiwala R.R., Cramer S.C.","7103131093;10043311300;36187081900;23486112900;54790581600;","Robot-based hand motor therapy after stroke",2008,"Brain","131","2",,"425","437",,413,"10.1093/brain/awm311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849191141&doi=10.1093%2fbrain%2fawm311&partnerID=40&md5=9f4e9407a8ba321ce2885af339d22333","Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States; Department of Engineering, Santa Ana College, Santa Ana, CA, United States; University of California, Irvine Medical Center, Building 53, 101 The City Drive South, Orange, CA 92868-4280, United States","Takahashi, C.D., Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States, Department of Engineering, Santa Ana College, Santa Ana, CA, United States; Der-Yeghiaian, L., Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States; Le, V., Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States; Motiwala, R.R., Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States; Cramer, S.C., Department of Neurology, Department of Anatomy and Neurobiology, University of California, Irvine, CA, United States, University of California, Irvine Medical Center, Building 53, 101 The City Drive South, Orange, CA 92868-4280, United States","Robots can improve motor status after stroke with certain advantages, but there has been less emphasis to date on robotic developments for the hand. The goal of this study was to determine whether a hand-wrist robot would improve motor function, and to evaluate the specificity of therapy effects on brain reorganization. Subjects with chronic stroke producing moderate right arm/hand weakness received 3 weeks therapy that emphasized intense active movement repetition as well as attention, speed, force, precision and timing, and included virtual reality games. Subjects initiated hand movements. If necessary, the robot completed movements, a feature available at all visits for seven of the subjects and at the latter half of visits for six of the subjects. Significant behavioural gains were found at end of treatment, for example, in Action Research Arm Test (34 ± 20 to 38 ± 19, P< 0.0005) and arm motor Fugl-Meyer score (45 ± 10 to 52 ± 10, P < 0.0001). Results suggest greater gains for subjects receiving robotic assistance in all sessions as compared to those receiving robotic assistance in half of sessions. The grasp task practiced during robotic therapy, when performed during functional MRI, showed increased sensorimotor cortex activation across the period of therapy, while a non-practiced task, supination/pronation, did not. A robot-based therapy showed improvements in hand motor function after chronic stroke. Reorganization of motor maps during the current therapy was task-specific, a finding useful when considering generalization of rehabilitation therapy. © The Author (2007). Published by Oxford University Press on behalf of the Guarantors of Brain. All rights reserved.","Functional MRI; Generalization; Motor therapy; Stroke","accuracy; action research; adult; aged; arm weakness; article; attention; behavior; body posture; cerebrovascular accident; clinical article; controlled study; electromyography; female; force; functional magnetic resonance imaging; human; male; motor performance; physiotherapy; priority journal; robotics; scoring system; sensorimotor cortex; statistical significance; stroke; task performance; velocity; virtual reality",Article,"Final","",Scopus,2-s2.0-38849191141
"Mirelman A., Patritti B.L., Bonato P., Deutsch J.E.","35332484600;10840065100;7006225560;7201985389;","Effects of robot-virtual reality compared with robot alone training on gait kinetics of individuals post stroke",2007,"2007 Virtual Rehabilitation, IWVR",,, 4362132,"65","69",,4,"10.1109/ICVR.2007.4362132","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849122343&doi=10.1109%2fICVR.2007.4362132&partnerID=40&md5=c4a6848165f872bcd03bee106d31a25c","IEEE; Department of Developmental and Rehabilitative Sciences, University of Medicine and Dentistry of New Jersey, Newark, NJ 07101, United States; Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA 02114, United States; Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston MA 02114, United States; Harvard-MIT Division of Health Sciences and Technology, Cambridge, MA 02139, United States; Department of Developmental and Rehabilitative Sciences, University of Medicine and Dentistry of New Jersey, Newark NJ 07101, United States","Mirelman, A., Department of Developmental and Rehabilitative Sciences, University of Medicine and Dentistry of New Jersey, Newark, NJ 07101, United States, Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA 02114, United States; Patritti, B.L., Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston, MA 02114, United States; Bonato, P., IEEE, Department of Physical Medicine and Rehabilitation, Harvard Medical School, Spaulding Rehabilitation Hospital, Boston MA 02114, United States, Harvard-MIT Division of Health Sciences and Technology, Cambridge, MA 02139, United States; Deutsch, J.E., Department of Developmental and Rehabilitative Sciences, University of Medicine and Dentistry of New Jersey, Newark NJ 07101, United States","Virtual reality systems have been used to deliver goal directed repetitive training to promote rehabilitation of individuals post-stroke. Lower extremity training of individuals post-stroke who used a robot coupled with virtual environments was shown to transfer to improved over-ground locomotion. To elucidate an underlying mechanism that enabled this functional change we compared the kinetic outcomes of training with the robot-virtual reality (VR) system to the robot alone. Eighteen individuals post-stroke participated in a four-week training protocol. One group trained with the robot-VR system and the other group with the robot alone. Training parameters were comparable for the two groups; however, the improvements in moments and powers generated in the ankle and hip of subjects in the robot-VR group were significantly greater than those in the robot alone group. These findings demonstrate that lower extremity training using virtual environments coupled with a robot produced greater gait related strength gains than training with robot alone. © 2007 IEEE.",,"Lower extremity; Virtual environments; Virtual rehabilitation; Computer networks; Patient rehabilitation; Virtual reality; Robots",Conference Paper,"Final","",Scopus,2-s2.0-50849122343
"Voruganti A., Mayoral R., Jacobs S., Grunert R., Moeckel H., Korb W.","23390798200;6602114429;36094574500;14628733200;23390577700;12805856900;","Surgical cartographic navigation system for endoscopic bypass grafting",2007,"Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings",,, 4352577,"1467","1470",,5,"10.1109/IEMBS.2007.4352577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649210311&doi=10.1109%2fIEMBS.2007.4352577&partnerID=40&md5=f0e2002c21e88a352d87c6e9d378d5bf","Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany; Herzzentrum, Klinik für Herzchirurgie, Leipzig, Germany","Voruganti, A., Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany; Mayoral, R., Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany; Jacobs, S., Herzzentrum, Klinik für Herzchirurgie, Leipzig, Germany; Grunert, R., Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany; Moeckel, H., Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany; Korb, W., Innovation Center Computer Assisted Surgery (ICCAS), Universität Leipzig, Philipp-Rosenthal-Str. 55, 04103 Leipzig, Germany","Endoscopic bypass grafting with the da Vinci system is still challenging and needs high level of experience and skill of the surgeon. Therefore, it is necessary to support the surgeon with enhanced vision and augmented reality. The augmentation of the patient model into the view of the endoscope is a direct approach to enhance support. The results of a preclinical study are shown in this paper. The method applied is suitable for endoscopic bypass grafting and in general applicable to minimal invasive surgery. The system was designed as an open architecture to facilitate easy transfer of the methodology into other surgical domain applications. © 2007 IEEE.",,"Biomedical engineering; Endoscopy; Stereo vision; Virtual reality; Endoscopic bypass grafting; Invasive surgery; Surgeons; Surgical cartographic navigation systems; Grafts; algorithm; article; biological model; computer assisted diagnosis; computer assisted surgery; computer interface; computer program; computer simulation; coronary artery bypass graft; coronary blood vessel; endoscopy; evaluation study; histology; human; methodology; Algorithms; Computer Simulation; Coronary Artery Bypass; Coronary Vessels; Endoscopy; Humans; Image Interpretation, Computer-Assisted; Models, Biological; Software; Software Design; Surgery, Computer-Assisted; User-Computer Interface",Conference Paper,"Final","",Scopus,2-s2.0-57649210311
"Boring S., Altendorfer M., Broll G., Hilliges O., Butz A.","18233691700;36166401100;55999882700;14041644100;55150450600;","Shoot & copy: Phonecam-based information transfer from public displays onto mobile phones",2007,"Mobility Conference 2007 - The 4th Int. Conf. Mobile Technology, Applications and Systems, Mobility 2007, Incorporating the 1st Int. Symp. Computer Human Interaction in Mobile Technology, IS-CHI 2007",,,,"24","31",,36,"10.1145/1378063.1378068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450242388&doi=10.1145%2f1378063.1378068&partnerID=40&md5=7acdaa32a6daa23971ddae2c28b40dca","University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany","Boring, S., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Altendorfer, M., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Broll, G., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Hilliges, O., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany; Butz, A., University of Munich, Media Informatics, Amalienstrasse 17, 80333 Munich, Germany","Large public displays have become pervasive in our everyday lives, but up to now, they are mostly information screens without any interaction possibilities. Users tend to forget what they saw relatively fast after leaving such a display. In this paper, we present a new interaction technique for transferring information from a public display onto a personal mobile phone with its built-in camera. Instead of having to rely on their memory, users simply take a picture of the information of interest. Instead of just storing the image, our system then retrieves the actual data represented on the screen, such as a stock quote, news text, or piece of music. The Shoot & Copy technique does not require visual codes that interfere with shown content or reduce screen real estate. Our prototype allows users to capture an arbitrary region of a standard desktop screen, containing icons, which represent pieces of data. The captured image is then analyzed and a reference to the corresponding data is sent back to the mobile phone. Once the user has time to view the information in more detail, our system allows retrieving the actual data from this reference. We present our prototype and the methods it uses for image processing, as well as an evaluation of our interaction technique illustrating its potential use and applications. Copyright 2007 ACM.","image processing; large public displays; mobile camera phones","Information transfers; Interaction techniques; Mobile camera phones; Public display; Real estate; Standard desktop; Stock quotes; Cameras; Image processing; Imaging systems; Mobile phones; Mobile telecommunication systems; Technology; Telecommunication equipment; Telephone; Telephone sets; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-70450242388
"Dekker A., Champion E.","35241961600;13006810600;","Please biofeed the zombies: Enhancing the gameplay and display of a horror game using biofeedback",2007,"3rd Digital Games Research Association International Conference: ""Situated Play"", DiGRA 2007",,,,"550","558",,58,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249118221&partnerID=40&md5=25d3a297e36c945a31b818294abd26dc","Interaction Design, School of ITEE, University of Queensland, Ipswich, Australia; Media Arts, COFA, UNSW, PO Box 259, Paddington, NSW 2021, Australia","Dekker, A., Interaction Design, School of ITEE, University of Queensland, Ipswich, Australia; Champion, E., Media Arts, COFA, UNSW, PO Box 259, Paddington, NSW 2021, Australia","This paper describes an investigation into how real-time but low-cost biometric information can be interpreted by computer games to enhance gameplay without fundamentally changing it. We adapted a cheap sensor, (the Lightstone mediation sensor device by Wild Divine), to record and transfer biometric information about the player (via sensors that clip over their fingers) into a commercial game engine, Half-Life 2. During game play, the computer game was dynamically modified by the player's biometric information to increase the cinematically augmented ""horror"" affordances. These included dynamic changes in the game shaders, screen shake, and the creation of new spawning points for the game's non-playing characters (zombies), all these features were driven by the player's biometric data. To evaluate the usefulness of this biofeedback device, we compared it against a control group of players who also had sensors clipped on their fingers, but for the second group the gameplay was not modified by the biometric information of the players. While the evaluation results indicate biometric data can improve the situated feeling of horror, there are many design issues that will need to be investigated by future research, and the judicious selection of theme and appropriate interaction is vital. © 2007 Authors & Digital Games Research Association (DiGRA).","Biofeedback; Cinematics; Gameplay; Horror; Shaders","Affordances; Biometric data; Biometric informations; Cinematics; Computer game; Control groups; Design issues; Dynamic changes; Evaluation results; Game Engine; Gameplay; Horror; Second group; Sensor device; Shaders; Biofeedback; Biometrics; Computer software; Digital storage; Human computer interaction; Research; Sensors; Interactive computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-52249118221
"Sudra G., Speidel S., Fritz D., Muller-Stich B.P., Gutt C., Dillmann R.","8722467000;22433983800;8657896200;14322034300;7005133005;35599949300;","MEDIASSIST - MEDIcal assistance for intraoperative skill transfer in minimally invasive surgery using augmented reality",2007,"Progress in Biomedical Optics and Imaging - Proceedings of SPIE","6509","PART 2", 65091O,"","",,7,"10.1117/12.709300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048892887&doi=10.1117%2f12.709300&partnerID=40&md5=440be17ab520451036dacbc9fe5ddfa7","Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany","Sudra, G., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Speidel, S., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Fritz, D., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany; Muller-Stich, B.P., Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Gutt, C., Department of General, Visceral and Accident Surgery, University of Heidelberg, Germany; Dillmann, R., Department for Computer Science, ITEC Universität Karlsruhe (TH), Germany","Minimally invasive surgery is a highly complex medical discipline with various risks for surgeon and patient, but has also numerous advantages on patient-side. The surgeon has to adapt special operation-techniques and deal with difficulties like the complex hand-eye coordination, limited field of view and restricted mobility. To alleviate with these new problems, we propose to support the surgeon's spatial cognition by using augmented reality (AR) techniques to directly visualize virtual objects in the surgical site. In order to generate an intelligent support, it is necessary to have an intraoperative assistance system that recognizes the surgical skills during the intervention and provides context-aware assistance surgeon using AR techniques. With MEDIASSIST we bundle our research activities in the field of intraoperative intelligent support and visualization. Our experimental setup consists of a stereo endoscope, an optical tracking system and a head-mounted-display for 3D visualization. The framework will be used as platform for the development and evaluation of our research in the field of skill recognition and context-aware assistance generation. This includes methods for surgical skill analysis, skill classification, context interpretation as well as assistive visualization and interaction techniques. In this paper we present the objectives of MEDIASSIST and first results in the fields of skill analysis, visualization and multi-modal interaction. In detail we present a markerless instrument tracking for surgical skill analysis as well as visualization techniques and recognition of interaction gestures in an AR environment.","Augmented reality; Calibration; Computer guided surgery; Minimally invasive surgery; Tracking; Visualization","Computer guided surgery; Interaction techniques; Minimally invasive surgery; Health risks; Intelligent control; Neurosurgery; Object recognition; Virtual reality; Medical imaging",Conference Paper,"Final","",Scopus,2-s2.0-35048892887
"Wachsmuth S., Wrede S., Hanheide M.","6506127757;6602995929;6507632075;","Coordinating interactive vision behaviors for cognitive assistance",2007,"Computer Vision and Image Understanding","108","1-2",,"135","149",,3,"10.1016/j.cviu.2006.10.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548263576&doi=10.1016%2fj.cviu.2006.10.018&partnerID=40&md5=1f89e25d56015afd6ab2ff7ef0715eae","Applied Computer Science, Bielefeld University, Faculty of Technology, P.O. Box 100131, 33501 Bielefeld, Germany","Wachsmuth, S., Applied Computer Science, Bielefeld University, Faculty of Technology, P.O. Box 100131, 33501 Bielefeld, Germany; Wrede, S., Applied Computer Science, Bielefeld University, Faculty of Technology, P.O. Box 100131, 33501 Bielefeld, Germany; Hanheide, M., Applied Computer Science, Bielefeld University, Faculty of Technology, P.O. Box 100131, 33501 Bielefeld, Germany","Most of the research conducted in human-computer interaction (HCI) focuses on a seamless interface between a user and an application that is separated from the user in terms of working space and/or control, like navigation in image databases, instruction of robots, or information retrieval systems. The interaction paradigm of cognitive assistance goes one step further in that the application consists of assisting the user performing everyday tasks in his or her own environment and in that the user and the system share the control of such tasks. This kind of tight bidirectional interaction in realistic environments demands cognitive system skills like context awareness, attention, learning, and reasoning about the external environment. Therefore, the system needs to integrate a wide variety of visual functions, like localization, object tracking and recognition, action recognition, interactive object learning, etc. In this paper we show how different kinds of system behaviors are realized using the Active Memory Infrastructure that provides the technical basis for distributed computation and a data- and event-driven integration approach. A running augmented reality system for cognitive assistance is presented that supports users in mixing beverages. The flexibility and generality of the system framework provides an ideal testbed for studying visual cues in human-computer interaction. We report about results from first user studies. © 2007 Elsevier Inc. All rights reserved.","Cognitive systems; Computer vision; HCI; System integration","Cognitive systems; Human computer interaction; Information retrieval systems; Interactive computer systems; Interfaces (computer); Robots; Cognitive assistance; Interaction paradigm; System integration; Working space; Computer vision",Article,"Final","",Scopus,2-s2.0-34548263576
"Akinladejo F.O.","18433788000;","Computer-based physical therapy: A case study on four post-acute stroke patients",2007,"Conference Proceedings - IEEE SOUTHEASTCON",,, 4147457,"370","377",,,"10.1109/SECON.2007.342927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547689155&doi=10.1109%2fSECON.2007.342927&partnerID=40&md5=b3f2b17a9ae3e6212a1c79a325fa790f","School of Computing and Information Technology, University of Technology Jamaica, 237 Old Hope Road, Kingston 6, Jamaica","Akinladejo, F.O., School of Computing and Information Technology, University of Technology Jamaica, 237 Old Hope Road, Kingston 6, Jamaica","This research investigates the outcome of using a computer-based therapy program in ambulatory training for post-acute stroke patients. Patients with stroke typically suffer dysfunctions that impair the complex set of motions involved in walking. The limited amount of therapy and resources offered by the current health care system do not provide the frequency and intensity of training needed for functional recovery of the walking skills in patients following stroke assaults. This non-traditional intervention research technique therefore sought to develop an alternative method capable of providing the frequency and intensity needed for improving the walking skills in post-acute stroke patients. The work also attempted to show how skills gained in virtual environments transfer to the real world. The work employed the case study method to report the results observed from four post-acute stroke patients who trained on the non-traditional intervention program for about half an hour per day, five days a week, for a period of four consecutive weeks at the out-patient department of the Sir John Golding Rehabilitation Center. The patients performed a computer-based painting exercise with their hemiplegic legs using a head-mounted display, and their gait variables were recorded and analyzed to determine the usefulness of the program in ambulatory training for post-acute stroke patients. A follow up examination conducted one week after the intervention sought to determine whether the patients could perform the skills learned on the computer-based intervention program in the real world. The results of the research showed that all the patients improved on their gait parameters and could walk better. An observational gait analysis conducted one week post-intervention showed that the skills gained in the virtual environment transferred to real-world conditions. The study contributes to the current effort to provide wider access to therapeutic intervention techniques using computer technology. © 2007 IEEE.",,"Diseases; Gait analysis; Patient rehabilitation; Personnel training; Virtual reality; Walking aids; Ambulatory training; Computer based physical therapy; Functional recovery; Health care systems; Patient treatment",Conference Paper,"Final","",Scopus,2-s2.0-34547689155
"Ritter E.M., Kindelan T.W., Michael C., Pimentel E.A., Bowyer M.W.","35518395300;17346093600;57196852486;8670601400;7006096986;","Concurrent validity of augmented reality metrics applied to the fundamentals of laparoscopic surgery (FLS)",2007,"Surgical Endoscopy and Other Interventional Techniques","21","8",,"1441","1445",,54,"10.1007/s00464-007-9261-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547142654&doi=10.1007%2fs00464-007-9261-5&partnerID=40&md5=be7b5efeac55d57157043ad48eb61375","NCA Medical Simulation Center, Norman M. Rich Department of Surgery, Uniformed Services University, 4301 Jones Bridge Road, Bethesda, MD 20814, United States; Division of Surgery, National Naval Medical Center, 8901 Wisconsin Avenue, Bethesda, MD 20889, United States","Ritter, E.M., NCA Medical Simulation Center, Norman M. Rich Department of Surgery, Uniformed Services University, 4301 Jones Bridge Road, Bethesda, MD 20814, United States; Kindelan, T.W., Division of Surgery, National Naval Medical Center, 8901 Wisconsin Avenue, Bethesda, MD 20889, United States; Michael, C., NCA Medical Simulation Center, Norman M. Rich Department of Surgery, Uniformed Services University, 4301 Jones Bridge Road, Bethesda, MD 20814, United States; Pimentel, E.A., NCA Medical Simulation Center, Norman M. Rich Department of Surgery, Uniformed Services University, 4301 Jones Bridge Road, Bethesda, MD 20814, United States; Bowyer, M.W., NCA Medical Simulation Center, Norman M. Rich Department of Surgery, Uniformed Services University, 4301 Jones Bridge Road, Bethesda, MD 20814, United States","Objective: Current skills assessment in the Fundamentals of Laparoscopic Surgery (FLS) program is labor intensive, requiring one proctor for every 1-2 subjects. The ProMIS Augmented Reality (AR) simulator (Haptica, Dublin IR) allows for objective assessment of physical tasks through instrument tracking technology. We hypothesized that the ProMIS metrics could differentiate between ability groups as well as standard FLS scoring with fewer personnel requirements Methods: We recruited 60 volunteer subjects. Subjects were stratified based on their laparoscopic surgical experience. Those who had performed more than 100 laparoscopic procedures were considered experienced (n = 8). Those with fewer than 10 laparoscopic procedures were considered novices (n = 44). The rest were intermediates (n = 8). All subjects performed up to five trials of the peg transfer task from FLS in the ProMIS simulator. The FLS score, instrument path length, and instrument smoothness assessment were generated for each trial. Results: For each of the five trials, experienced surgeons outperformed intermediates, who in turn outperformed novices. Statistically significant differences were seen between the groups across all trials for FLS score (p < 0.001), ProMIS path length (p < 0.001), and ProMIS smoothness (p < 0.001). When the FLS score was compared to the path length and smoothness metrics, a strong relationship between the scores was apparent for novices (r = 0.78, r = 0.94, p < 0.001) respectively), intermediates (r = 0.5, p = 0.2, r = 0.98, p < 0.001), and experienced surgeons (r = 0.86, p = 0.006, r = 0.99, p < 0.001). Conclusions: The construct that the standard scoring of the FLS peg transfer task can discriminate between experienced, intermediate, and novice surgeons is validated. The same construct is valid when the task is assessed using the metrics of the ProMIS. The high correlation between these scores establishes the concurrent validity of the ProMIS metrics. The use of AR for objective assessment of FLS tasks could reduce the personnel requirements of assessing these skills while maintaining the objectivity. © 2007 Springer Science+Business Media, LLC.","Augmented reality; Laparoscopy; Objective assessment; Simulation","adult; article; concurrent validity; controlled study; female; human; laparoscopic surgery; major clinical study; male; priority journal; skill; statistical significance; surgeon; surgical technique; Adult; Clinical Competence; Computer Simulation; Educational Measurement; Female; Humans; Laparoscopy; Male; Surgery; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-34547142654
"Grégoire M., Schömer E.","14039061200;6602331499;","Interactive simulation of one-dimensional flexible parts",2007,"CAD Computer Aided Design","39","8",,"694","707",,85,"10.1016/j.cad.2007.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34347342698&doi=10.1016%2fj.cad.2007.05.005&partnerID=40&md5=ef95f11207b171b9d0db701657969210","DaimlerChrysler Research and Technology, Germany; Johannes Gutenberg Universität Mainz, Germany","Grégoire, M., DaimlerChrysler Research and Technology, Germany, Johannes Gutenberg Universität Mainz, Germany; Schömer, E., DaimlerChrysler Research and Technology, Germany, Johannes Gutenberg Universität Mainz, Germany","In this paper, we present a system for simulating one dimensional flexible parts such as cables or hose. The modelling of bending and torsion follows the Cosserat model. For this purpose we use a generalized spring-mass system and describe its configuration by a carefully chosen set of coordinates. Gravity and contact forces as well as the forces responsible for length conservation are expressed in Cartesian coordinates. But bending and torsion effects can be dealt with more effectively by using quaternions to represent the orientation of the segments joining two neighbouring mass points. This augmented system allows an easy formulation of all interactions with the best appropriate coordinate type and yields a strongly banded Hessian matrix. An energy minimizing process accounts for a solution exempt from the oscillations that are typical of spring-mass systems. The whole system is numerically stable and can be solved at interactive frame rates. It is integrated in a virtual reality software for use in applications such as cable routing and assembly simulation. © 2007 Elsevier Ltd. All rights reserved.","Cable simulation; Cosserat model; Modelling of torsion","Cables; Interactive computer graphics; Interfaces (computer); Torsion testing; Virtual reality; Cable simulation; Contact forces; Cosserat model; Torsion modeling; Flexible structures",Article,"Final","",Scopus,2-s2.0-34347342698
"Placidi G.","7006022089;","A smart virtual glove for the hand telerehabilitation",2007,"Computers in Biology and Medicine","37","8",,"1100","1107",,40,"10.1016/j.compbiomed.2006.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250886903&doi=10.1016%2fj.compbiomed.2006.09.011&partnerID=40&md5=755f90cff915d7627f31567393364541","INFM, Department of Science and Biomedical Technologies, University of L'Aquila, Via Vetoio 10, 67100 Coppito, L'Aquila, Italy","Placidi, G., INFM, Department of Science and Biomedical Technologies, University of L'Aquila, Via Vetoio 10, 67100 Coppito, L'Aquila, Italy","Hand rehabilitation, following stroke or hand surgery, is repetitive and long duration and can be facilitated with the assistance of complex, heavy and cumbersome haptic gloves based on sensors. The present paper describes a virtual glove, software based, which tracks hand movements by using images collected from webcams and numerical analysis. Finger forces are calculated from the deformations impressed to some objects (of known elastic coefficient) grasped by the patient hand. The presented system is notable for simplicity, generality and low cost. Implementation and results of the proposed virtual glove will be the objects of a future paper. © 2006 Elsevier Ltd. All rights reserved.","Haptic gloves; Image analysis; Numerical hand model; Object recognition; Telerehabilitation; Virtual reality","Biomaterials; Haptic interfaces; Image analysis; Object recognition; Virtual reality; Numerical hand model; Smart virtual glove; Telerehabilitation; Learning aids for handicapped persons; article; computer program; glove; hand disease; hand surgery; human; imaging system; priority journal; rehabilitation care; stroke; television camera; Biomedical Engineering; Cerebrovascular Accident; Clothing; Computer Simulation; Hand; Humans; Software; Telemedicine; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-34250886903
"Wang X., Dunston P.S.","8945580300;6602079727;","Design, strategies, and issues towards an Augmented Reality-based construction training platform",2007,"Electronic Journal of Information Technology in Construction","12",,,"363","380",,60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547518881&partnerID=40&md5=be256dd99a26324e23d3ff80f33d8f84","Key Centre of Design Computing and Cognition, Faculty of Architecture, Design and Planning, University of Sydney, Australia; School of Civil Engineering, Purdue University, United States","Wang, X., Key Centre of Design Computing and Cognition, Faculty of Architecture, Design and Planning, University of Sydney, Australia; Dunston, P.S., School of Civil Engineering, Purdue University, United States","This paper provides information on Augmented Reality (AR) and their potential applications in heavy construction equipment operator training. Augmented Reality involves the use of special display and tracking technology that are capable of seamlessly merging digital (virtual) contents into real environments. Augmented Reality technology has been applied in many application domains outside construction (e.g., medical applications and surgeries, military training and warfare, manufacturing assembly and maintenance, design and modeling, precinct specific instant information, and various forms of entertainment) and the ever-increasing power of hardware rendering systems and tracking technology should motivate the creation of AR-based systems to benefit construction industry. This paper discusses the potentials of AR in construction equipment operation and operator training. A construction application for AR technology focused in this paper is an AR-hased real world Training System (ARTS) that trains the novice operators in a real worksite environment populated with virtual materials and instructions. This paper focuses on the conceptual design and development of mechanisms/strategies for the ARTS in the context of certain identified application scenarios. Discussion of limitations of Augmented Reality technology for construction applications include mature of technology, data resource, technology transfer, social attitude, etc., is also presented.","Augmented reality; System design; Training; Virtual reality; Virtual training","Construction industry; Hardware; Systems analysis; Virtual reality; AR-hased real world Training System (ARTS); Augmented Reality technology; Virtual training; Personnel training",Article,"Final","",Scopus,2-s2.0-34547518881
"Jimeno A., Puerta A.","57200400092;17435762700;","State of the art of the virtual reality applied to design and manufacturing processes",2007,"International Journal of Advanced Manufacturing Technology","33","9-10",,"866","874",,51,"10.1007/s00170-006-0534-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547241548&doi=10.1007%2fs00170-006-0534-2&partnerID=40&md5=dd83223015082c2a38dae7cb28c99106","Department of Computer Technology and Computation, Alicante University, San Vicente del Raspeig s/n, 03001 Alicante, Spain","Jimeno, A., Department of Computer Technology and Computation, Alicante University, San Vicente del Raspeig s/n, 03001 Alicante, Spain; Puerta, A., Department of Computer Technology and Computation, Alicante University, San Vicente del Raspeig s/n, 03001 Alicante, Spain","The idea that technology can transfer a person to a different environment without any physical movement and create the illusion of interaction with the artificial environment is not new. Scientists and engineers have been dedicating their efforts to its progressive development over the last fifty years. However, most of the technological advances have been made in the last ten years, undoubtedly thanks to improvements in computer efficiency and the miniaturization of sensorization devices. Nowadays, virtual reality is successfully applied in different fields, such as telemedicine, robotics or cinematography. Following on from this success, the question arises of whether we are ready to apply it to industrial design and manufacturing processes. The lack of recent reviews on this technology applied to CAD/CAM, together with its rapid evolution over the last decade, have been the primary motivations for carrying out this study. © 2006 Springer-Verlag London Limited.","Virtual environments; Virtual prototyping; Virtual reality","Computer aided design; Computer aided manufacturing; Photography; Robotics; Technology transfer; Telemedicine; Sensorization devices; Virtual prototyping; Virtual reality",Article,"Final","",Scopus,2-s2.0-34547241548
"Tijus C., Poitrenaud S., Zibetti E., Jouen F., Bui M., Pinska E.","6603707134;6506317514;16644298000;6603701102;7006594623;16643798000;","Complexity reduction: Theory, metrics and applications",2007,"2007 IEEE International Conference on Research, Innovation and Vision for the Future, RIVF 2007",,, 4223054,"65","70",,1,"10.1109/RIVF.2007.369137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250696957&doi=10.1109%2fRIVF.2007.369137&partnerID=40&md5=df62dc157f0a99a2d84a3a3d8d092748","CHArt, University Paris 8, Saint-Denis, France; CHArt - CSMC, EPHE, Paris, France; Eurocontrol Experimental Centre, Eurocontrol, Bretigny sur Orge, France","Tijus, C., CHArt, University Paris 8, Saint-Denis, France; Poitrenaud, S., CHArt, University Paris 8, Saint-Denis, France; Zibetti, E., CHArt, University Paris 8, Saint-Denis, France; Jouen, F., CHArt - CSMC, EPHE, Paris, France; Bui, M., CHArt - CSMC, EPHE, Paris, France; Pinska, E., Eurocontrol Experimental Centre, Eurocontrol, Bretigny sur Orge, France","Generalized Galois Lattices Formalism for computing contextual categorization allows metrics to evaluate complexity and efficiency as well as methods for simplifying or complicating the external object at hand. Such methods are adapted for virtual environments and augmented reality devices for which it is simple to change the distribution of features over categories. For real world objects, and human operators that operate on them, the online computation allows a survey of the complexity level and a ""simplify it first"" planning of operations. © 2007 IEEE.",,"Computational complexity; Computational efficiency; Online systems; Virtual reality; Augmented reality devices; Contextual categorization; Human operators; Online computation; Matrix algebra",Conference Paper,"Final","",Scopus,2-s2.0-34250696957
"Egges A., Papagiannakis G., Magnenat-Thalmann N.","6506510837;23051855400;24762992500;","Presence and interaction in mixed reality environments",2007,"Visual Computer","23","5",,"317","333",,12,"10.1007/s00371-007-0113-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147138907&doi=10.1007%2fs00371-007-0113-z&partnerID=40&md5=947de4fd3e826d561fe1a1ce007b8875","Center for Advanced Gaming and Simulation, Department of Information and Computing Sciences, Utrecht University, PO Box 80.089, Utrecht 3508TB, Netherlands; MIRALab., University of Geneva, Geneva, Switzerland","Egges, A., Center for Advanced Gaming and Simulation, Department of Information and Computing Sciences, Utrecht University, PO Box 80.089, Utrecht 3508TB, Netherlands; Papagiannakis, G., MIRALab., University of Geneva, Geneva, Switzerland; Magnenat-Thalmann, N., MIRALab., University of Geneva, Geneva, Switzerland","In this paper, we present a simple and robust mixed reality (MR) framework that allows for real-time interaction with virtual humans in mixed reality environments under consistent illumination. We will look at three crucial parts of this system: interaction, animation and global illumination of virtual humans for an integrated and enhanced presence. The interaction system comprises of a dialogue module, which is interfaced with a speech recognition and synthesis system. Next to speech output, the dialogue system generates face and body motions, which are in turn managed by the virtual human animation layer. Our fast animation engine can handle various types of motions, such as normal key-frame animations, or motions that are generated on-the-fly by adapting previously recorded clips. Real-time idle motions are an example of the latter category. All these different motions are generated and blended on-line, resulting in a flexible and realistic animation. Our robust rendering method operates in accordance with the previous animation layer, based on an extended for virtual humans precomputed radiance transfer (PRT) illumination model, resulting in a realistic rendition of such interactive virtual characters in mixed reality environments. Finally, we present a scenario that illustrates the interplay and application of our methods, glued under a unique framework for presence and interaction in MR. © Springer-Verlag 2007.","Animation; Interaction; Mixed reality; Presence; Real-time rendering","Animation; Human computer interaction; Motion estimation; Online systems; Real time systems; Speech recognition; Mixed reality (MR) framework; Precomputed radiance transfer (PRT); Real-time rendering; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-34147138907
"Pridmore T., Green J., Cobb S., Eastgate R., Hilton D.","6701847105;7404571640;7102309665;13204220900;15061411800;","Mixed reality environments in stroke rehabilitation: Interfaces across the real/virtual divide",2007,"International Journal on Disability and Human Development","6","1",,"87","96",,7,"10.1515/IJDHD.2007.6.1.87","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149123159&doi=10.1515%2fIJDHD.2007.6.1.87&partnerID=40&md5=5031583c1922cf6f9b8a0299c9c6a73d","University of Nottingham, School of Computer Science and IT, Jubilee Campus, Wollaton Rd, Nottingham NG8 IBB, United Kingdom; University of Nottingham, School of Mechanical Materials and Manufacturing Engineering, University Park, NG7 2RD, VIRART, Nottingham, United Kingdom; University of Nottingham, School of Nursing, Queens Medical Centre, Nottingham, NG7 2 HA, United Kingdom","Pridmore, T., University of Nottingham, School of Computer Science and IT, Jubilee Campus, Wollaton Rd, Nottingham NG8 IBB, United Kingdom; Green, J., University of Nottingham, School of Computer Science and IT, Jubilee Campus, Wollaton Rd, Nottingham NG8 IBB, United Kingdom; Cobb, S., University of Nottingham, School of Mechanical Materials and Manufacturing Engineering, University Park, NG7 2RD, VIRART, Nottingham, United Kingdom; Eastgate, R., University of Nottingham, School of Mechanical Materials and Manufacturing Engineering, University Park, NG7 2RD, VIRART, Nottingham, United Kingdom; Hilton, D., University of Nottingham, School of Nursing, Queens Medical Centre, Nottingham, NG7 2 HA, United Kingdom","Previous studies have examined the use of virtual environments (VEs) for stroke and similar rehabilitation. To be of real benefit, it is essential that skills (re-)learned within a VE transfer to corresponding real-world situations. Many tasks have been developed in VEs, but few have shown effective transfer of training. We believe that, by softening the real/virtual divide, mixed reality technology has the potential to ease the transfer of rehabilitation activities into everyday life. We present two mixed reality systems, designed to support rehabilitation of activities of daily living and providing different mixtures of digital and physical information. Functional testing of these systems is described. © 2007, by Walter de Gruyter GmbH & Co. All rights reserved.","Mixed reality; rehabilitation; tangible user interface; touch screen; training; United Kingdom","article; computer interface; daily life activity; mixed reality; rehabilitation; stroke; virtual reality",Article,"Final","",Scopus,2-s2.0-56149123159
"Duan F., Tan J.T.C., Zhang Y., Watanabe K., Pongthanya N., Sugi M., Yokoi H., Arai T.","24537140100;24537916100;55739985600;55704950200;24537397000;7102731508;7201964047;35494671300;","Analyze assembly skills using a motion simulator",2007,"2007 IEEE International Conference on Robotics and Biomimetics, ROBIO",,, 4522374,"1428","1433",,8,"10.1109/ROBIO.2007.4522374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249113360&doi=10.1109%2fROBIO.2007.4522374&partnerID=40&md5=39e2dd7e3075ae746f0272cbac4fbe7e","Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan","Duan, F., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Tan, J.T.C., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Zhang, Y., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Watanabe, K., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Pongthanya, N., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Sugi, M., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Yokoi, H., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan; Arai, T., Department of Precision Engineering, School of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan","Many researchers have focused on transferring human skills to robots by extracting those skills into the machine-understandable models. It is an important step towards creating an intelligent robot in a cooperative environment. These models not only can be employed to transfer human control strategy to robots, but also can be used to improve the new workers' performance. In this paper, we propose a method that is to extract the advantages of several operators' skills and synthesize new skills to train new workers. To realize the skill extract, synthesis and transfer system, it is absolutely necessary to reproduce and synthesize the experts' motions. In the latter case, a kinematic simulator of human body is employed to realize the synthesis of the operators' motions based on the detected motion data. To verify the proposed method, we employed it to execute a peg-in-hole assembly task. The results show that after training by the synthesized skill model, the new operator's performance is improved significantly. © 2008 IEEE.","Direct linear transformation method; Human skill transfer; Kinematic simulator of human body; Motion tracking","Automation; Biomimetics; Personnel training; Polyethylene glycols; Robots; Cooperative environment; Direct linear transformation method; Human bodies; Human control strategy; Human skill transfer; Human skills; International conferences; Kinematic simulator of human body; Motion data; Motion simulators; Motion tracking; Peg-in-hole assembly; Transfer systems; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-49249113360
"Sedlack R.E.","6701838340;","Validation of computer simulation training for esophagogastroduodenoscopy: Pilot study",2007,"Journal of Gastroenterology and Hepatology (Australia)","22","8",,"1214","1219",,30,"10.1111/j.1440-1746.2007.04841.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547653480&doi=10.1111%2fj.1440-1746.2007.04841.x&partnerID=40&md5=4e7ddc37141e8d3e9dc74203e0543a9f","Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, MN, United States; Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, MN 55905, United States","Sedlack, R.E., Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, MN, United States, Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, MN 55905, United States","Background: Little is known regarding the value of esophagogastroduodenoscopy (EGD) simulators in education. The purpose of the present paper was to validate the use of computer simulation in novice EGD training. Methods: In phase 1, expert endoscopists evaluated various aspects of simulation fidelity as compared to live endoscopy. Additionally, computer-recorded performance metrics were assessed by comparing the recorded scores from users of three different experience levels. In phase 2, the transfer of simulation-acquired skills to the clinical setting was assessed in a two-group, randomized pilot study. The setting was a large gastroenterology (GI) Fellowship training program; in phase 1, 21 subjects (seven expert, intermediate and novice endoscopist), made up the three experience groups. In phase 2, eight novice GI fellows were involved in the two-group, randomized portion of the study examining the transfer of simulation skills to the clinical setting. During the initial validation phase, each of the 21 subjects completed two standardized EDG scenarios on a computer simulator and their performance scores were recorded for seven parameters. Following this, staff participants completed a questionnaire evaluating various aspects of the simulator's fidelity. Finally, four novice GI fellows were randomly assigned to receive 6 h of simulator-augmented training (SAT group) in EGD prior to beginning 1 month of patient-based EGD training. The remaining fellows experienced 1 month of patient-based training alone (PBT group). Results of the seven measured performance parameters were compared between three groups of varying experience using a Wilcoxon ranked sum test. The staffs' simulator fidelity survey used a 7-point Likert scale (1, very unrealistic; 4, neutral; 7, very realistic) for each of the parameters examined. During the second phase of this study, supervising staff rated both SAT and PBT fellows' patient-based performance daily. Scoring in each skill was completed using a 7-point Likert scale (1, strongly disagree; 4, neutral; 7, strongly agree). Median scores were compared between groups using the Wilcoxon ranked sum test. Results: Staff evaluations of fidelity found that only two of the parameters examined (anatomy and scope maneuverability) had a significant degree of realism. The remaining areas were felt to be limited in their fidelity. Of the computer-recorded performance scores, only the novice group could be reliably identified from the other two experience groups. In the clinical application phase, the median Patient Discomfort ratings were superior in the PBT group (6; interquartile range [IQR], 5-6) as compared to the SAT group (5; IQR, 4-6; P = 0.015). PBT fellows' ratings were also superior in Sedation, Patient Discomfort, Independence and Competence during various phases of the evaluation. At no point were SAT fellows rated higher than the PBT group in any of the parameters examined. Conclusion: This EGD simulator has limitations to the degree of fidelity and can differentiate only novice endoscopists from other levels of experience. Finally, skills learned during EGD simulation training do not appear to translate well into patient-based endoscopy skills. These findings suggest against a key element of validity for the use of this computer simulator in novice EGD training. © 2007 The Author.","Computer simulator; Esophagogastroduodenoscopy; Simulation; Training; Validation","article; competence; computer simulation; controlled study; endoscopy; esophagogastroduodenoscopy; experience; gastroenterology; human; independence; medical education; performance measurement system; pilot study; priority journal; questionnaire; randomized controlled trial; rank sum test; rating scale; scoring system; simulator; skill; validation study",Article,"Final","",Scopus,2-s2.0-34547653480
"Waschbüsch M., Würmlin S., Gross M.","8881704000;7801645956;7403745074;","3D video billboard clouds",2007,"Computer Graphics Forum","26","3",,"561","569",,13,"10.1111/j.1467-8659.2007.01079.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348939665&doi=10.1111%2fj.1467-8659.2007.01079.x&partnerID=40&md5=d098bc0a5a92b57132e2a7d28541da27","Computer Graphics Laboratory, ETH Zurich, Switzerland; LiberoVision Inc., Switzerland","Waschbüsch, M., Computer Graphics Laboratory, ETH Zurich, Switzerland; Würmlin, S., LiberoVision Inc., Switzerland; Gross, M., Computer Graphics Laboratory, ETH Zurich, Switzerland","3D video billboard clouds reconstruct and represent a dynamic three-dimensional scene using displacement-mapped billboards. They consist of geometric proxy planes augmented with detailed displacement maps and combine the generality of geometry-based 3D video with the regularization properties of image-based 3D video. 3D video billboards are an image-based representation placed in the disparity space of the acquisition cameras and thus provide a regular sampling of the scene with a uniform error model. We propose a general geometry filtering framework which generates time-coherent models and removes reconstruction and quantization noise as well as calibration errors. This replaces the complex and time-consuming sub-pixel matching process in stereo reconstruction with a bilateral filter. Rendering is performed using a GPU-accelerated algorithm which generates consistent view-dependent geometry and textures for each individual frame. In addition, we present a semi-automatic approach for modeling dynamic three-dimensional scenes with a set of multiple 3D video billboards clouds. © 2007 The Eurographics Association and Blackwell Publishing Ltd.",,"Cameras; Computational geometry; Computer simulation; Error analysis; Three dimensional computer graphics; Computer graphics; Geometry; Image coding; Stereo image processing; Bilateral filter; Billboards clouds; Time-coherent models; Bilateral filters; Calibration error; Displacement maps; Filtering framework; Image-based representation; Quantization noise; Stereo reconstruction; Three-dimensional scenes; Video signal processing; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-35348939665
"Wierinck E., Puttemans V., van Steenberghe D.","8243408900;6506533201;7101895992;","Effect of tutorial input in addition to augmented feedback on manual dexterity training and its retention",2006,"European Journal of Dental Education","10","1",,"24","31",,23,"10.1111/j.1600-0579.2006.00392.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645803204&doi=10.1111%2fj.1600-0579.2006.00392.x&partnerID=40&md5=32100c14d78f52eeade41d3b31e7c11a","Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit, Leuven, Belgium; Motor Control Laboratory, Department of Kinesiology, Katholieke Universiteit, Leuven, Belgium; Department of Periodontology, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium","Wierinck, E., Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit, Leuven, Belgium; Puttemans, V., Motor Control Laboratory, Department of Kinesiology, Katholieke Universiteit, Leuven, Belgium; van Steenberghe, D., Department of Periodontology, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium","Virtual reality (VR) simulators can be used as tools in manual dexterity training. The visual feedback guides the subject towards proper performance but creates, at the same time, some dependency on this feedback. To overcome this drawback, the effect of adjunct tutorial input on motor learning behaviour was examined. Novice dental students were randomly assigned to one of two training groups or to a nontraining control group, given the task of preparing a geometrical class 1 cavity in phantom teeth. The feedback (FB) group trained under augmented visual feedback conditions, provided by the VR system (DentSimTM). The feedback-plus (FB+) group received, in addition, standardised expert input to enrich the augmented feedback information. The control group, consisting of same year students, did not participate in any training programme. All preparations were evaluated by the VR scoring system. Performance analyses revealed an overall trend towards significant improvement with practice for the training groups. Performance of the FB+ group was most accurate across training. After 1 day and 3 weeks of no practice, both training groups outperformed the control group. After 4 months, however, only the FB+ condition was significantly more accurate than the control group. The same tendency was noted for the transfer tests. Consequently, cavity preparation experience on a VR system under the condition of frequently provided feedback supplemented with expert input was most beneficial to long time learning. © Blackwell Munksgaard, 2006.","Augmented feedback; Dental training; Retention; Skill acquisition; Transfer; Virtual reality","article; audiovisual equipment; clinical trial; comparative study; computer interface; computer simulation; controlled clinical trial; controlled study; dental education; dental student; dental surgery; feedback system; follow up; human; learning; long term memory; methodology; motor performance; physiology; randomized controlled trial; teaching; Computer Simulation; Dental Cavity Preparation; Education, Dental; Feedback; Follow-Up Studies; Humans; Learning; Models, Anatomic; Motor Skills; Retention (Psychology); Students, Dental; Teaching; Transfer (Psychology); User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33645803204
"Egges A., Papagiannakis G., Magnenat-Thalmann N.","6506510837;23051855400;24762992500;","An interactive mixed reality framework for virtual humans",2006,"2006 International Conference on Cyberworlds, CW'06",,, 4030841,"165","172",,2,"10.1109/CW.2006.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949022686&doi=10.1109%2fCW.2006.15&partnerID=40&md5=603e5d0d745b7e30fa57a4622c6325d7","MIRALab., University of Geneva, 7 route de Drize, 1227 Geneva, Switzerland","Egges, A., MIRALab., University of Geneva, 7 route de Drize, 1227 Geneva, Switzerland; Papagiannakis, G., MIRALab., University of Geneva, 7 route de Drize, 1227 Geneva, Switzerland; Magnenat-Thalmann, N., MIRALab., University of Geneva, 7 route de Drize, 1227 Geneva, Switzerland","In this paper, we present a simple and robust Mixed Reality (MR) framework that allows for real-time interaction with Virtual Humans in real and virtual environments under consistent illumination. We will look at three crucial parts of this system: interaction, animation and global illumination of virtual humans for an integrated and enhanced presence. The interaction system comprises of a dialogue module, which is interfaced with a speech recognition and synthesis system. Next to speech output, the dialogue system generates face and body motions, which are in turn managed by the virtual human animation layer. Our fast animation engine can handle various types of motions, such as normal key-frame animations, or motions that are generated on-the-fly by adapting previously recorded clips. All these different motions are generated and blended on-line, resulting in a flexible and realistic animation. Our robust rendering method operates in accordance with the previous animation layer, based on an extended for virtual humans Precomputed Radiance Transfer (PRT) illumination model, resulting in a realistic display of such interactive virtual characters in mixed reality environments. Finally, we present a scenario that illustrates the interplay and application of our methods, glued under a unique framework for presence and interaction in MR. © 2006 IEEE.",,"Animation; Human computer interaction; Mathematical models; Robust control; Speech recognition; Dialogue module; Global illumination; Robust Mixed Reality (MR); Virtual humans; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-36949022686
"Orozco C., Esteban P., Trefftz H.","35875789800;15925103800;6506608333;","Collaborative and distributed augmented reality in teaching multi-variate calculus",2006,"Proceedings of the Fifth IASTED International Conference on Web-based Education","2006",,,"141","145",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847181725&partnerID=40&md5=82ec625c0ca4b2220679309836ada3c2","Computer Science and Basic Science Departments, Eafit University, AA 3300, Colombia","Orozco, C., Computer Science and Basic Science Departments, Eafit University, AA 3300, Colombia; Esteban, P., Computer Science and Basic Science Departments, Eafit University, AA 3300, Colombia; Trefftz, H., Computer Science and Basic Science Departments, Eafit University, AA 3300, Colombia","This article presents the first results of using an Augmented Reality (AR) tool, designed to support tutoring sessions in multi-variate calculus. The tool is used either in a face-to-face setting in which the instructor and the students are collocated or in a distance setting, in which the instructor and students are physically in remote places. The tool was used with two groups of students of Differential Calculus. The students had not been exposed to the concept of equations involving 3 variables and the corresponding surfaces in space. The experience explored how students generalized 2D graphics and equations with their 3D surfaces counterparts with the help of the tool.","Augmented reality; Function transformation; Surfaces; Visualization","Computer supported cooperative work; Differentiation (calculus); Distributed computer systems; Learning systems; Students; Teaching; Function transformation; Generalized graphics; Multi variate calculus; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-33847181725
"Pearson I.","7006239287;","Keynote address: A trip into the future",2006,"IET Seminar Digest","2006","11530",,"193","212",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248224852&partnerID=40&md5=17fc4396c1f9055f11e42dd276fc5c0a","BT, United Kingdom","Pearson, I., BT, United Kingdom","Technology will soon bring smart environments, with tiny chips everywhere. In our gadgets, our clothes, our food, even on and in our skin. We will finally see augmented reality giving us useful data in our field of view, but it will also bring dual architectures, with virtual appearances as important as real ones. Similarly dual fashion, with digital bathroom mirrors, digital bubbles that interact with those of other people. The world will be a much more interesting place. Meanwhile, advances in AI are taking us quickly towards smart machines, that will be conscious and smarter than people. The business world will see enormous new markets, new channels to customers, and almost every industry will be greatly affected. Only those companies that keep their focus on the customer and are swift to adapt will survive. © BT.",,"Bathroom mirrors; Digital bubbles; Competitive intelligence; Digital control systems; Research and development management; Societies and institutions; Technology transfer; Virtual reality; Microprocessor chips",Conference Paper,"Final","",Scopus,2-s2.0-34248224852
"Kobayashi H., Ohyama Y., Hashimoto H., She J.-H.","55605778107;8358930000;55454360000;7102576674;","Manipulation of human behavior by distorted dynamics vision",2006,"2006 SICE-ICASE International Joint Conference",,, 4108300,"4446","4450",,3,"10.1109/SICE.2006.314779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250768282&doi=10.1109%2fSICE.2006.314779&partnerID=40&md5=1e2a0b40381913774844609d8df95afa","Department of Biomedical Engineering, Osaka Institute of Technology, Osaka, Japan; School of Bionics, Tokyo University of Technology, Tokyo, Japan","Kobayashi, H., Department of Biomedical Engineering, Osaka Institute of Technology, Osaka, Japan; Ohyama, Y., School of Bionics, Tokyo University of Technology, Tokyo, Japan; Hashimoto, H., School of Bionics, Tokyo University of Technology, Tokyo, Japan; She, J.-H., School of Bionics, Tokyo University of Technology, Tokyo, Japan","They say that human plans its motion mostly relying on visual information. From this point of view, in this paper the authors attempt to modify transfer characteristics of human operated system by displaying false image of plant whose dynamics is intentionally distorted from the real to enhance performance. Though the proposed method is a kind of human assisting scheme, it doesn't require any physical actuator unlike others and basically only a sensor system and a display device such as LCD monitor are needed. The authors employ simulated inverted pendulum as target plant to be controlled. In previous works, the authors found that there is a relationship between performance of human who is playing inverted pendulum game and his or her gain from eye to extremity at a certain frequency. The dynamics distortion is performed by 2nd order filter which is based on this relationship. The filter is designed to educe potential performance of human thus it is called as ""performance educer"" in this paper. Experiments are made by human subjects, in which they provide higher performance than usual. Finally, the validity of proposed idea is discussed. © 2006 ICASE.","Augmented reality; Distorted dynamics; Human skill; Inverted pendulum; Performance educer; System identification; Transfer function of human","Distorted dynamics vision; Human skill; Inverted pendulum; Identification (control systems); Liquid crystal displays; Sensors; Virtual reality; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-34250768282
"Hatfield D., Shaffer D.W.","55750849400;7103303182;","Press play: Designing an epistemic game engine for journalism",2006,"ICLS 2006 - International Conference of the Learning Sciences, Proceedings","1",,,"236","242",,16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884386223&partnerID=40&md5=7081bc96d623e7adb3889acb9a604861","University of Wisconsin, 1025 W. Johnson St., Madison, WI 53706, United States","Hatfield, D., University of Wisconsin, 1025 W. Johnson St., Madison, WI 53706, United States; Shaffer, D.W., University of Wisconsin, 1025 W. Johnson St., Madison, WI 53706, United States","Epistemic games are one approach to creating educational games that give players skills that transfer beyond the game world by helping young people become fluent in valuable social practices. Epistemic games are immersive, technology-enhanced, role-playing games where players learn to become-and thus to think like-doctors, lawyers, engineers, architects, and other members of important practices and professions. In what follows we look at the design of Byline, an epistemic game engine behind science.net, an epistemic game of science journalism. In particular, we argue that rather than simply recreating the technological conditions of the profession, an epistemic game engine like Byline can encode key elements of a professional practicum and thus help young people learn through participation in simulations of the training practices of socially valued professions such as science journalism.",,"Educational game; Game Engine; Immersive; Key elements; Role-playing game; Social practices; Technological conditions; Young peoples; Interactive computer graphics; Learning systems; Personnel training; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-84884386223
"Steed A.","18435050200;","Towards a general model for selection in virtual environments",2006,"3DUI 2006: IEEE Symposium on 3D User Interfaces 2006 - Proceedings","2006",, 1647515,"103","110",,42,"10.1109/VR.2006.134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750812463&doi=10.1109%2fVR.2006.134&partnerID=40&md5=0262d76e8f21cb5424ce7ae58cdaecb5","Department of Computer Science, University College London, United Kingdom","Steed, A., Department of Computer Science, University College London, United Kingdom","Selection is one of the fundamental building blocks of all interactive virtual environment systems. Selection is the ability of the user to specify which object, or sub-part of an object in the environment, is the target for subsequent actions. Examples include selecting 3D buttons thus invoking an action or selecting a target upon which an action will occur. Selection is also an implicit or explicit part of manipulation techniques. In a virtual environment selection can be performed in many different ways. In this paper we develop a generalized model of how interaction is and could be performed in virtual environments using 3D gestures. The purpose of this model is to highlight some potential areas for development and evaluation of novel selection techniques. The model is based on an analysis of the complexity of selection. We develop a model for selection that is based on time-varying scalar fields (TVSFs) that encompasses a very broad range of existing techniques. This model will be abstract, in that a direct implementation will be prohibitively complex, but we show how some standard implementation strategies are good approximations to the formal model. © 2006 IEEE.","3D interaction; Selection; Virtual environments","Interactive virtual environment systems; Three dimensional gestures; Three dimensional interaction; Virtual environment selection; Abstracting; Mathematical models; Object recognition; Selection; Three dimensional computer graphics; User interfaces; Virtual reality; Interactive computer systems",Conference Paper,"Final","",Scopus,2-s2.0-33750812463
"Brundage S.B., Graap K., Gibbons K.F., Ferrer M., Brooks J.","14035095900;6602808254;15053039800;8962881800;8962881700;","Frequency of stuttering during challenging and supportive virtual reality job interviews",2006,"Journal of Fluency Disorders","31","4",,"325","339",,30,"10.1016/j.jfludis.2006.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750607887&doi=10.1016%2fj.jfludis.2006.08.003&partnerID=40&md5=aa6a719e377ecd93b11ae34ff34bdfb9","The George Washington University, Speech and Hearing Science Department, 1922 F Street NW, Suite 400, Washington, DC 20052, United States; Virtually Better, Inc., Decatur, GA, United States","Brundage, S.B., The George Washington University, Speech and Hearing Science Department, 1922 F Street NW, Suite 400, Washington, DC 20052, United States; Graap, K., Virtually Better, Inc., Decatur, GA, United States; Gibbons, K.F., The George Washington University, Speech and Hearing Science Department, 1922 F Street NW, Suite 400, Washington, DC 20052, United States; Ferrer, M., Virtually Better, Inc., Decatur, GA, United States; Brooks, J., Virtually Better, Inc., Decatur, GA, United States","This paper seeks to demonstrate the possibility of manipulating the frequency of stuttering using virtual reality environments (VREs). If stuttering manifests itself in VREs similarly to the way it manifests itself in real world interactions, then VREs can provide a controlled, safe, and confidential method for treatment practice and generalization. Though many researchers and clinicians recognize the need for generalization activities in the treatment of stuttering, achieving generalization in a clinical setting poses challenges to client confidentiality, safety, and the efficient use of a professionals' time. Virtual reality (VR) technology may allow professionals the opportunity to enhance and assess treatment generalization while protecting the safety and confidentiality of their clients. In this study, we developed a VR job interview environment which allowed experimental control over communication style and gender of interviewers. In this first trial, persons who stutter (PWS) experienced both challenging and supportive VR job interview conditions. The percentage of stuttered syllables was calculated for both interviews for each participant. Self-reported ratings of communication apprehension and confidence were also obtained, and were not significantly correlated with stuttering severity. Results indicated that interviewer communication style affected the amount of stuttering produced by participants, with more stuttering observed during challenging virtual interviews. Additionally, the amount of stuttering observed during the VR job interviews was significantly, positively correlated with the amount of stuttering observed during an interview with the investigator prior to VR exposure. Participants' subjective reports of the VR experience indicate reactions similar to those they report experiencing in the real world. Possible implications for the use of VR in the assessment and treatment of stuttering are discussed. Educational objectives: After reading this article, the reader will be able to-(1) list some of the challenges to treatment generalization; (2) describe how virtual reality technology can assist in alleviating some of these challenges; (3) describe how the frequency of stuttering varies across two different virtual environments. © 2006.","Adults; Software validation; Stuttering; Virtual reality","adult; article; controlled study; correlation analysis; disease severity; experience; exposure; female; human; job interview; male; self report; stuttering; verbal communication; virtual reality; Adult; Environment; Female; Humans; Interviews; Job Application; Male; Middle Aged; Stuttering; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33750607887
"Steed A.","18435050200;","Towards a general model for selection in virtual environments",2006,"Proceedings - IEEE Virtual Reality","2006",, 1624169,"131","",,9,"10.1109/VR.2006.134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750121589&doi=10.1109%2fVR.2006.134&partnerID=40&md5=5d01929678a1bcb442e6d80311923d1f","Department of Computer Science, University College London, United Kingdom","Steed, A., Department of Computer Science, University College London, United Kingdom","Selection is one of the fundamental building blocks of all interactive virtual environment systems. Selection is the ability of the user to specify which object, or sub-part of an object in the environment, is the target for subsequent actions. Examples include selecting 3D buttons thus invoking an action or selecting a target upon which an action will occur. Selection is also an implicit or explicit part of manipulation techniques. In a virtual environment selection can be performed in many different ways. In this paper we develop a generalized model of how interaction is and could be performed in virtual environments using 3D gestures. The purpose of this model is to highlight some potential areas for development and evaluation of novel selection techniques. The model is based on an analysis of the complexity of selection. We develop a model for selection that is based on time-varying scalar fields (TVSFs) that encompasses a very broad range of existing techniques. This model will be abstract, in that a direct implementation will be prohibitively complex, but we show how some standard implementation strategies are good approximations to the formal model. © 2006 IEEE.","3D interaction; Selection; Virtual environments","3D interaction; Manipulation; Virtual environment selection; Virtual environment systems; Computational complexity; Computer selection and evaluation; Graphical user interfaces; Human computer interaction; Mathematical models; Three dimensional computer graphics; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-33750121589
"Wierinck E., Puttemans V., van Steenberghe D.","8243408900;6506533201;7101895992;","Effect of reducing frequency of augmented feedback on manual dexterity training and its retention",2006,"Journal of Dentistry","34","9",,"641","647",,22,"10.1016/j.jdent.2005.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748146885&doi=10.1016%2fj.jdent.2005.12.005&partnerID=40&md5=da6341c0f1e900919dc9e3a796d070a9","Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillo-facial Surgery, Kapucijnenvoer 7, 3000 Leuven, Belgium; Motor Control Laboratory, Department of Kinesiology, Katholieke Universiteit Leuven, Leuven, Belgium; Department of Periodontology, School of Dentistry, Oral Pathology and Maxillo-facial Surgery, Leuven, Belgium","Wierinck, E., Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillo-facial Surgery, Kapucijnenvoer 7, 3000 Leuven, Belgium; Puttemans, V., Motor Control Laboratory, Department of Kinesiology, Katholieke Universiteit Leuven, Leuven, Belgium; van Steenberghe, D., Department of Periodontology, School of Dentistry, Oral Pathology and Maxillo-facial Surgery, Leuven, Belgium","Objective: The study addressed the impact of the frequency of tutorial-enriched augmented visual feedback, provided by a virtual simulation system (DentSim™), on the skill acquisition for a cavity preparation task in novice dental students. Methods: Thirty-six subjects were assigned to two training groups and a control group. The task consisted of a geometrical cross preparation on the lower left first molar. All subjects performed a pre-test to assess their basic skill level. The training groups received simulation feedback, enriched with tutorial information, across acquisition. One group trained under continuous augmented feedback, while a second group trained under an intermittent (66% of the time) feedback. At both 1-day and 4-month interval, subjects performed a retention test to explore learning specific effects. Two transfer tests were added to assess the extrapolation of the learned skills to an adjacent molar. All tests were performed in the absence of feedback. A control group performed all the tests, without preceding training. All preparations were graded by the simulation system. Results: The training groups performed similarly across acquisition and improved with practice (ANOVA, P < 0.001). After 1 day and 4 months of no practice, the training groups outperformed the control group on a retention test (ANOVA, P < 0.001) and transfer test (ANOVA, P < 0.001). Conclusions: Performance and learning of a cavity preparation task on a simulation unit was independent of the frequency of tutorial-enriched augmented visual feedback within the range tested. Training sessions on a simulation unit could be alternated with training sessions in the traditional phantom head laboratory. © 2005 Elsevier Ltd. All rights reserved.","Augmented feedback; Dental training; Manual dexterity; Retention; Skill acquisition; Transfer; Virtual reality","adolescent; adult; article; clinical trial; computer interface; computer simulation; controlled clinical trial; controlled study; dental education; dental surgery; education; feedback system; human; long term memory; methodology; motor performance; randomized controlled trial; teaching; Adolescent; Adult; Computer Simulation; Computer-Assisted Instruction; Dental Cavity Preparation; Dentistry, Operative; Education, Dental; Educational Measurement; Feedback; Humans; Motor Skills; Retention (Psychology); User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33748146885
"Yao Y.X., Xia P.J., Liu J.S., Li J.G.","55722539100;13404322900;8205876700;22953365000;","A pragmatic system to support interactive assembly planning and training in an immersive virtual environment (I-VAPTS)",2006,"International Journal of Advanced Manufacturing Technology","30","9-10",,"959","967",,28,"10.1007/s00170-005-0069-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749988080&doi=10.1007%2fs00170-005-0069-y&partnerID=40&md5=861bf3a23684c5ac29951dee2427c6fb","School of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, 150001, China","Yao, Y.X., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, 150001, China; Xia, P.J., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, 150001, China; Liu, J.S., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, 150001, China; Li, J.G., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, Harbin, 150001, China","Assembly planning for complex products is a difficult task requiring both intensive knowledge and experience. Computer aided assembly planning (CAAP) systems have been the subject of considerable research in recent years without achieving a wide application in manufacturing industry. In this paper an alternative approach to the generation of an optimal assembly planning scheme is presented based on the adoption of immersive virtual reality. A product is assembled from CAD models by providing a CAD interface to transfer assembly constraint information from the CAD system to a virtual environment. In the virtual environment an efficient dynamic recognition and management method based on surface geometry is employed, and a process-oriented assembly task model is established to support interactive assembly planning and evaluation. The system is implemented using an object oriented methodology, and has been successfully applied to train and guide the assembly workers in a pump assembly process. © Springer-Verlag London Limited 2006.","Assembly planning and training; CAD; Virtual reality","Assembly planning; Assembly planning and training; Computer aided assembly planning (CAAP); Assembly machines; Computer aided design; Interactive computer systems; Knowledge engineering; Strategic planning; Virtual reality; Industrial management",Article,"Final","",Scopus,2-s2.0-33749988080
"Brown L.D., Hua H.","56362429100;7103212541;","Magic lenses for augmented virtual environments",2006,"IEEE Computer Graphics and Applications","26","4",,"64","73",,34,"10.1109/MCG.2006.84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746172973&doi=10.1109%2fMCG.2006.84&partnerID=40&md5=6d24ed9eb3b57f1ea89b9a44acb30945","Department of Computer Science, The University of Arizona, Tucson, AZ, United States; College of Optical Sciences, Department of Computer Science, The University of Arizona, Tucson, AZ, United States","Brown, L.D., Department of Computer Science, The University of Arizona, Tucson, AZ, United States; Hua, H., College of Optical Sciences, Department of Computer Science, The University of Arizona, Tucson, AZ, United States","The authors developed a Magic Lens framework for Scape, an augmented virtual environment (AVE). They generalize the functional characteristics of Magic Lenses in terms of 3D visualization in AVEs and present two tangible Magic Lens-enabled devices with complementary interface capabilities. The authors also demonstrate their Magic Lens devices through testbed applications relevant to urban planning and medical training. © 2006 IEEE.",,"Computer graphics; Lenses; Optical devices; Three dimensional; Augmented virtual environment; Magic lenses; Testbed applications; Three dimensional visualization; Virtual reality; algorithm; article; computer assisted diagnosis; computer graphics; computer interface; computer program; computer simulation; environment; methodology; optical instrumentation; photography; theoretical model; three dimensional imaging; Algorithms; Computer Graphics; Computer Simulation; Environment; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Lenses; Models, Theoretical; Photogrammetry; Software; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33746172973
"Merians A.S., Poizner H., Boian R., Burdea G., Adamovich S.","6603018031;7005963417;6603659582;35612697900;6603916707;","Sensorimotor training in a virtual reality environment: Does it improve functional recovery poststroke?",2006,"Neurorehabilitation and Neural Repair","20","2",,"252","267",,167,"10.1177/1545968306286914","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646536087&doi=10.1177%2f1545968306286914&partnerID=40&md5=f8c1c430902d6561ec66dced59014fd4","Graduate Programs in Physical Therapy, University of Medicine and Dentistry of New Jersey, School of Health Related Professions, Newark, NJ, United States; Institute for Neural Computation, University of California, San Diego, San Diego, CA, United States; Center for Advanced Information Processing, Rutgers University, Piscataway, NJ, United States; Department of Biomedical Engineering, New Jersey Institute of Technology, Newark, NJ, United States; University of Medicine and Dentistry of New Jersey, 65 Bergen Street, Newark, NJ 07107, United States","Merians, A.S., Graduate Programs in Physical Therapy, University of Medicine and Dentistry of New Jersey, School of Health Related Professions, Newark, NJ, United States, University of Medicine and Dentistry of New Jersey, 65 Bergen Street, Newark, NJ 07107, United States; Poizner, H., Institute for Neural Computation, University of California, San Diego, San Diego, CA, United States; Boian, R., Center for Advanced Information Processing, Rutgers University, Piscataway, NJ, United States; Burdea, G., Center for Advanced Information Processing, Rutgers University, Piscataway, NJ, United States; Adamovich, S., Department of Biomedical Engineering, New Jersey Institute of Technology, Newark, NJ, United States","Objective. To investigate the effectiveness of computerized virtual reality (VR) training of the hemiparetic hand of patients poststroke using a system that provides repetitive motor reeducation and skill reacquisition. Methods. Eight subjects in the chronic phase poststroke participated in a 3-week program using their hemiparetic hand in a series of interactive computer games for 13 days of training, weekend breaks, and pretests and posttests. Each subject trained for about 2 to 2.5 h per day. Outcome measures consisted of changes in the computerized measures of thumb and finger range of motion, thumb and finger velocity, fractionation (the ability to move fingers independently), thumb and finger strength, the Jebsen Test of Hand Function, and a Kinematic reach to grasp test. Results. Subjects as a group improved in fractionation of the fingers, thumb and finger range of motion, and thumb and finger speed, retaining those gains at the 1-week retention test. Transfer of these improvements was demonstrated through changes in the Jebsen Test of Hand Function and a decrease after the therapy in the overall time from hand peak velocity to the moment when an object was lifted from the table. Conclusions. It is difficult in current service delivery models to provide the intensity of practice that appears to be needed to effect neural reorganization and functional changes poststroke. Computerized exercise systems may be a way to maximize both the patients' and the clinicians' time. The data in this study add support to the proposal to explore novel technologies for incorporation into current practice. Copyright © 2006 The American Society of Neurorehabilitation.","Haptics; Motor learning; Recovery; Rehabilitation; Stroke; Virtual reality","adult; aged; article; clinical article; female; hand function; hemiparesis; human; male; motor activity; range of motion; sensorimotor function; stroke; virtual reality; Aged; Aged, 80 and over; Cerebrovascular Accident; Exercise; Female; Hand; Humans; Male; Middle Aged; Paresis; Psychomotor Performance; Recovery of Function; Treatment Outcome; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33646536087
"Fidopiastis C.M., Stapleton C.B., Whiteside J.D., Hughes C.E., Fiore S.M., Martin G.A., Rolland J.P., Smith E.M.","6602524530;9636549800;12139131100;7401857048;35490410200;7404678724;7103051976;55479658400;","Human experience modeler: Context-driven cognitive retraining to facilitate transfer of learning",2006,"Cyberpsychology and Behavior","9","2",,"183","187",,27,"10.1089/cpb.2006.9.183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646737077&doi=10.1089%2fcpb.2006.9.183&partnerID=40&md5=2def697cf542ddd89792db3f1a95e2a1","Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States; Media Convergence Laboratory, University of Central Florida, Orlando, FL, United States; School of Film and Digital Media, University of Central Florida, Orlando, FL, United States; Communicative Disorders Clinic, University of Central Florida, Orlando, FL, United States; School of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL, United States; College of Optics and Photonics: CREOL/FPCE, University of Central Florida, Orlando, FL, United States; Cognitive Sciences Program, Department of Philosophy, University of Central Florida, Orlando, FL, United States; University of Central Florida, Institute for Simulation and Training, 3100 Technology Parkway, Orlando, FL 32826-0544, United States","Fidopiastis, C.M., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, College of Optics and Photonics: CREOL/FPCE, University of Central Florida, Orlando, FL, United States, University of Central Florida, Institute for Simulation and Training, 3100 Technology Parkway, Orlando, FL 32826-0544, United States; Stapleton, C.B., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, Media Convergence Laboratory, University of Central Florida, Orlando, FL, United States, School of Film and Digital Media, University of Central Florida, Orlando, FL, United States; Whiteside, J.D., Communicative Disorders Clinic, University of Central Florida, Orlando, FL, United States; Hughes, C.E., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, Media Convergence Laboratory, University of Central Florida, Orlando, FL, United States, School of Film and Digital Media, University of Central Florida, Orlando, FL, United States, School of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL, United States; Fiore, S.M., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, Cognitive Sciences Program, Department of Philosophy, University of Central Florida, Orlando, FL, United States; Martin, G.A., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States; Rolland, J.P., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, College of Optics and Photonics: CREOL/FPCE, University of Central Florida, Orlando, FL, United States; Smith, E.M., Institute for Simulation and Training, University of Central Florida, Orlando, FL, United States, Media Convergence Laboratory, University of Central Florida, Orlando, FL, United States","We describe a cognitive rehabilitation mixed-reality system that allows therapists to explore natural cuing, contextualization, and theoretical aspects of cognitive retraining, including transfer of training. The Human Experience Modeler (HEM) mixed-reality environment allows for a contextualized learning experience with the advantages of controlled stimuli, experience capture and feedback that would not be feasible in a traditional rehabilitation setting. A pilot study for testing the integrated components of the HEM is discussed where the participant presents with working memory impairments due to an aneurysm. © Mary Ann Liebert, Inc.",,"adult; article; association; brain artery aneurysm; camera; case report; cognitive defect; cognitive rehabilitation; computer model; computer program; experience; feedback system; frontal lobe; human; human experience modeler; learning; male; memory disorder; pilot study; stimulus response; task performance; videorecording; virtual reality; working memory; Activities of Daily Living; Brain Damage, Chronic; Computer Simulation; Environment; Frontal Lobe; Humans; Intracranial Aneurysm; Male; Memory Disorders; Middle Aged; Pilot Projects; Transfer (Psychology); User-Computer Interface",Article,"Final","",Scopus,2-s2.0-33646737077
"Mirelman A., Deutsch J.E., Bonato P.","35332484600;7201985389;7006225560;","Greater transfer to walking of lower extremity training with robotics and virtual reality than robotics training alone: Preliminary findings",2006,"Fifth International Workshop on Virtual Rehabilitation, IWVR 2006",,, 1707545,"155","159",,1,"10.1109/iwvr.2006.1707545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-41649103893&doi=10.1109%2fiwvr.2006.1707545&partnerID=40&md5=89ca3d6cf0576b14463c46cf948c1d31","School of Health Related Professions, University of Medicine and Dentistry of New Jersey, Newark, NJ 07107, United States; Motion Analysis Lab., Spaulding Rehabilitation Hospital, Boston, MA, United States; RiVERS Lab. (Research in Virtual Environments and Rehabilitation Sciences Laboratory), School of Health Related Professions, University of Medicine and Dentistry of New Jersey, Newark, NJ, United States","Mirelman, A., School of Health Related Professions, University of Medicine and Dentistry of New Jersey, Newark, NJ 07107, United States, Motion Analysis Lab., Spaulding Rehabilitation Hospital, Boston, MA, United States; Deutsch, J.E., RiVERS Lab. (Research in Virtual Environments and Rehabilitation Sciences Laboratory), School of Health Related Professions, University of Medicine and Dentistry of New Jersey, Newark, NJ, United States; Bonato, P., Motion Analysis Lab., Spaulding Rehabilitation Hospital, Boston, MA, United States","Virtual reality systems have been used to deliver goal directed repetitive training to promote rehabilitation of individuals post-stroke. Lower extremity training of individuals post-stroke who used a robot coupled with virtual environment has been shown to transfer to improved overground locomotion. To isolate the active components of training in this study we compared the outcomes of training with the robot-virtual reality (VR) system to the robot alone. Four individuals post-stroke participated in a four-week training protocol. One group trained with the robot-VR system and the other group with the robot alone. The improvement in walking speed and endurance for the robot- VR group was greater than the robot group alone. Adherence as well as the number of exercises performed in each session was comparable for the two groups. The duration of training sessions was comparable at the beginning of the study. However, subjects in the robot group reported higher fatigue and produced 16% fewer minutes of training towards the end of the study. These findings support the use of virtual environments coupled with a robot for transfer of training from the virtual to the real world environment. © 2006 IEEE.",,"Medical problems; Patient rehabilitation; Personnel training; Robotics; Training protocols; Training sessions; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-41649103893
"Figueroa P., Dachselt R., Lindt I.","7003967572;6507253418;15035603600;","A uniform specification of mixed reality interface components",2006,"Proceedings - IEEE Virtual Reality","2006",, 1624084,"289","290",,,"10.1109/VR.2006.22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750115645&doi=10.1109%2fVR.2006.22&partnerID=40&md5=ce87f5abe7b548dd04538f44f5b72b0f","Universidad de los Andes, Colombia; Dresden University of Technology, Germany; Fraunhofer FIT, St. Augustin, Germany","Figueroa, P., Universidad de los Andes, Colombia; Dachselt, R., Dresden University of Technology, Germany; Lindt, I., Fraunhofer FIT, St. Augustin, Germany","This is a short presentation of a uniform approach for specifying mixed reality user interfaces, including 3D interaction techniques and 3D widgets. Our main goal is to facilitate the process of reusing previous work, so more complex applications can be built and documented in a formal and uniform way. We describe here the conceptual model of user interface components, which allows us to generalize user interface components and to port them to different hardware settings and application contexts. An XML-based specification language implements the conceptual model and allows for automatic processing and tool support. © 2006 IEEE.","3D Interaction Techniques; 3D User Interfaces; 3D Widgets; AR; Desktop VR; Interface Description Language; Mixed Reality; VR","Computer hardware; Computer hardware description languages; Computer software reusability; Human computer interaction; Personal computers; XML; 3D Interaction Techniques; 3D User Interfaces; 3D Widgets; Desktop VR; Interface Description Languages; Mixed Reality; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-33750115645
"Butz A., Krüger A.","55150450600;35264048900;","Applying the peephole metaphor in a mixed-reality room",2006,"IEEE Computer Graphics and Applications","26","1",,"56","63",,7,"10.1109/MCG.2006.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744466388&doi=10.1109%2fMCG.2006.10&partnerID=40&md5=c4cecb5c17fda1af9b6ebaeb6927858f","University of Munich, Munich, Germany; University of Muenster, Muenster, Germany","Butz, A., University of Munich, Munich, Germany; Krüger, A., University of Muenster, Muenster, Germany","This article presents the generalized peephole metaphor, a model of interaction for ubiquitous computing and instrumented environments. The metaphor provides a way of organizing and structuring ubiquitous input and output facilities in instrumented environments consisting of several distributed but coordinated sensors and displays. Its main idea is to look at the environment as one large display and sensor continuum, in which peepholes provide localized and user-specific windows between the physical environment and a virtual information layer. The metaphor nicely matches models of human perception, for example the fact that humans make use of external representation in their environments and access information by guiding their attention to specific locations. The article presents a specific mixed-reality room and shows how a number of input and output activities can be described in terms of the peephole metaphor. It discusses how the metaphor can cope with scalability and access control and how it supports a family of interaction styles and presentation methods in instrumented environments. It analyzes the technological requirements for implementing the peephole metaphor and show that it works well with the limited hardware already available, such as projector-camera units, wall-mounted displays, and portable screens. © 2006 IEEE.",,"Display devices; Fluorescent screens; Projection screens; Sensors; User interfaces; Video cameras; Video conferencing; Mixed reality (MR) room; Peephole metaphor; Steerable projectors; Virtual camera; Virtual layer; Virtual reality; article; computer assisted diagnosis; computer graphics; computer simulation; environment; equipment; equipment design; information processing; instrumentation; methodology; risk assessment; theoretical model; three dimensional imaging; Computer Graphics; Computer Simulation; Data Display; Environment; Equipment Design; Equipment Failure Analysis; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Models, Theoretical; Risk Assessment",Article,"Final","",Scopus,2-s2.0-30744466388
"Pinska E., Tijus C., Jouen F., Poitrenaud S., Zibetti E.","16643798000;6603707134;6603701102;6506317514;16644298000;","Reducing cognitive load of air traffic controllers through the modelling of the hierarchical segmentation of inputs",2006,"5th EUROCONTROL Innovative Research Workshop and Exhibition: Disseminating ATM Innovative Research",,,,"137","144",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926385659&partnerID=40&md5=533dd311f7d036e54881202ecbf8ed2f","Eurocontrol Experimental,Centre, Bretigny su Orge cedex, France; Complex System Modelling and Cognition, European Joint Research Lab, Université Paris 8, 2 rue de la Liberté, St.Denis cedex 02, 93526, France; EPHE, Complex System Modelling and Cognition, European Joint Research Lab, 41 rue Gay Lussac, Paris, 75005, France; Laboratoire Cognition and Usages, Université Paris 8, 2 rue de la Liberté, St.Denis cedex 02, 93526, France","Pinska, E., Eurocontrol Experimental,Centre, Bretigny su Orge cedex, France; Tijus, C., Complex System Modelling and Cognition, European Joint Research Lab, Université Paris 8, 2 rue de la Liberté, St.Denis cedex 02, 93526, France; Jouen, F., EPHE, Complex System Modelling and Cognition, European Joint Research Lab, 41 rue Gay Lussac, Paris, 75005, France; Poitrenaud, S., Laboratoire Cognition and Usages, Université Paris 8, 2 rue de la Liberté, St.Denis cedex 02, 93526, France; Zibetti, E., Laboratoire Cognition and Usages, Université Paris 8, 2 rue de la Liberté, St.Denis cedex 02, 93526, France","The Hierarchical Segmentation Theory is about how Human operators process objects, events and actions through cognitive processes that start from segmentation external inputs to decision making by the use of the mathematical formalism developed as Generalized Galois Lattices Air Control Interfaces are computed on line for determining the attentional and cognitive levels of complexity that affect cognitive load (attention and memory) and decision-making. The complexity and efficiency metrics help making predictions about the increasing of complexity, and help designing more safety interfaces for risky management of Air Control Procedures. Experimental designs for evaluating how much the Hierarchical Segmentation Model predicts cognitive load of Air Traffic controllers [6] includes the measurement of complexity at different states, the making of simpler situation (changing attributes of interfaces or number of events to be processed) and collecting data from change blindness experiments and from eyes movements Such methods are adapted for virtual environments and augmented reality devices for which it is simple to change the distribution of features over categories. For real world objects, and human operators that operate on them, the online computation allows a survey of the complexity level and a ""simplify it first"" planning of operations.","Air traffic control; Attention and decision-making; Cognitive load; Complexity metrics; Generalized galois lattices; Human factors","Advanced traffic management systems; Augmented reality; Controllers; Decision making; Decision theory; Eye movements; Human computer interaction; Human engineering; Image segmentation; Interface states; Lattice theory; Mathematical operators; Social networking (online); Virtual reality; Air traffic controller; Cognitive loads; Complexity metrics; Galois lattices; Hierarchical segmentation; Mathematical formalism; Online computations; Real-world objects; Air traffic control",Conference Paper,"Final","",Scopus,2-s2.0-84926385659
"Lam Y.S., Man D.W.K., Tam S.F., Weiss P.L.","36340111700;7006360144;7202037317;55435137100;","Virtual reality training for stroke rehabilitation",2006,"NeuroRehabilitation","21","3",,"245","253",,58,"10.3233/nre-2006-21308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845485888&doi=10.3233%2fnre-2006-21308&partnerID=40&md5=03282fd7c7f659f4f80af53a6e81f293","Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hong Kong; Department of Occupational Therapy, University of Haifa, Israel","Lam, Y.S., Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hong Kong; Man, D.W.K., Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hong Kong; Tam, S.F., Department of Rehabilitation Sciences, Hong Kong Polytechnic University, Hong Kong; Weiss, P.L., Department of Occupational Therapy, University of Haifa, Israel","Objective: To evaluate the effectiveness of a 2-D virtual reality (2DVR) programme in the training of people with stroke on how to access and use the station facilities of the Mass Transit Railway (MTR). Method: A flat-screen 2DVR based training programme and a corresponding, typical psycho-educational programme with video modelling were developed for comparison through a research design that involved a randomised control group pre-test and post-test. Results: Twenty and sixteen subjects respectively received 10 training sessions using the 2DVR strategy and a video-based psycho-educational programme. An additional 22 subjects formed the control group. They were assessed by using a behavioural checklist of MTR skills and a newly validated MTR self-efficacy scale. The subjects of both training groups showed a significant improvement in their knowledge, skills and self-efficacy in using the MTR (p<0.01), whereas, the MTR skills and self-efficacy of the control group remained stable over a four-week interval. Conclusion: Though both training programmes were effective in training the patients with stroke, they demonstrated differential improvements in MTR skills and related self-efficacy. Additional studies are recommended to identify the most effective training procedures for maintaining these skills and the best transfer ratio in the training of VR-based community living skills of people with stroke. © 2006 - IOS Press and the authors. All rights reserved.","Community; Independence; Orientation; Rehabilitation; Stroke; Virtual reality","adult; aged; analysis of variance; article; behavior; clinical article; clinical trial; controlled clinical trial; controlled study; demography; female; health education; health program; human; male; psychoeducation; railway; randomized controlled trial; self concept; statistical analysis; stroke; training; videorecording; virtual reality",Article,"Final","",Scopus,2-s2.0-33845485888
"Everitt K., Shen C., Ryall K., Forlines C.","13006301800;8594330100;6603589675;8594330700;","MultiSpace: Enabling electronic document micro-mobility in table-centric, multi-device environments",2006,"Proceedings of the First IEEE International Workshop on Horizontal Interactive Human-Computer Systems, TABLETOP'06","2006",, 1579187,"8","15",,33,"10.1109/TABLETOP.2006.23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749407358&doi=10.1109%2fTABLETOP.2006.23&partnerID=40&md5=1105c531aaa3a706f169b17346e17eaf","Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, United States; University of Washington, Seattle, WA, United States","Everitt, K., Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, United States, University of Washington, Seattle, WA, United States; Shen, C., Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, United States; Ryall, K., Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, United States; Forlines, C., Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, United States","Although electronic media has changed how people interact with documents, today's electronic documents and the environments in which they are used are still impoverished relative to traditional paper documents when used by groups of people and across multiple computing devices. Vertical interfaces (e.g., walls and monitors) afford a less democratic style of interaction than generally observed when people are working around a table. In this paper, we introduce MultiSpace, a research effort which explores the role of the table as a central hub to support ad hoc collaboration in a multi-device environment. The table-centric approach offers new interaction techniques to provide egalitarian access and shared transport of data, supporting mobility and micromobility [11] of electronic content between tables and other devices. Our observations show how people use these techniques, and how tabletop technology can support and augment collaborative tasks. © 2006 IEEE.",,"Computer supported cooperative work; Data transfer; Interfaces (computer); Virtual reality; Ad hoc collaboration; Electronic media; Multi-device environment; Tabletop technology; Electronic document identification systems",Conference Paper,"Final","",Scopus,2-s2.0-33749407358
"Pathomaree N., Charoenseang S.","14054483200;6506423045;","Augmented reality for skill transfer in assembly task",2005,"Proceedings - IEEE International Workshop on Robot and Human Interactive Communication","2005",, 1513829,"500","504",,38,"10.1109/ROMAN.2005.1513829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746641755&doi=10.1109%2fROMAN.2005.1513829&partnerID=40&md5=7af7593f05a4b585bda02b505fd41eb2","Institute of FIeld Robotics (FIBO), King Mongkut's University of Technology Thonburi, Suksawasd 48, Bangmod Bangkok, 10140, Thailand","Pathomaree, N., Institute of FIeld Robotics (FIBO), King Mongkut's University of Technology Thonburi, Suksawasd 48, Bangmod Bangkok, 10140, Thailand; Charoenseang, S., Institute of FIeld Robotics (FIBO), King Mongkut's University of Technology Thonburi, Suksawasd 48, Bangmod Bangkok, 10140, Thailand","In this research, an augmented reality is proposed to enhance the skill transfer in the assembly task. In this system, a user can see the additional graphics information superimposed on the real world scenes. Graphical instructions and virtual objects are used for advising the user with the assembly steps and the targeted positions in assembly task. Furthermore, the system's judgment component can guide the user for the sequences of assembly and check whether the user performs actions correctly. The experimental results show that the training system with augmented reality has high percentages of transferability and high transfer effectiveness ratio. The system can also reduce assembly completion times and the number of assembly steps. Moreover, the questionnaire results show that the users are very satisfied with this kind of system. Therefore, the training system embedded with an augmented reality would be a new trend to improve the user's skills. ©2005 IEEE.",,"Embedded systems; Graphic methods; Information dissemination; Real time systems; Robotic assembly; Virtual reality; Graphics information; Skill transfer; Targeted positions; Transfer effectiveness ratio; Technology transfer",Conference Paper,"Final","",Scopus,2-s2.0-33746641755
"Dähne P., Seibert H.","6507191408;7004142744;","Managing data flow in interactive MR environments",2005,"13th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2005, WSCG'2005 - In Co-operation with EUROGRAPHICS, Full Papers",,,,"173","176",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861040059&partnerID=40&md5=5eedd27a43dd6ca2aa4205bbdb8e5fc9","Computer Graphics Center (ZGDV), Fraunhoferstraße 5, D-64283 Darmstadt, Germany","Dähne, P., Computer Graphics Center (ZGDV), Fraunhoferstraße 5, D-64283 Darmstadt, Germany; Seibert, H., Computer Graphics Center (ZGDV), Fraunhoferstraße 5, D-64283 Darmstadt, Germany","In this paper the concept and design of a software framework which provides a transparent data flow for interactive Mixed Reality (MR) applications is discussed. The design was affected by our demands on platform independency, simplicity, network transparency, maximum performance and availability of runtime debugging facilities. Our software framework tries to simplify the development of MR applications by using the concept of a data flow graph. The developer builds such a graph from a library of small software components that communicate via the edges of the graph. Copyright UNION Agency - Science Press.","Augmented reality; Device management; Interaction; Mixed Reality; Tracking; Virtual reality","Data flow; Device management; Interaction; Mixed reality; Runtimes; Software component; Software frameworks; Augmented reality; Computer programming; Data flow analysis; Data flow graphs; Data transfer; Surface discharges; Virtual reality; Visualization; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84861040059
"Del Río A., Fischer J., Köbele M., Bartz D., Straßer W.","55393205800;8345270000;55758712600;56031275800;7004523663;","Augmented reality interaction for semiautomatic volume classification",2005,"9th International Workshop on Immersive Projection Technology - 11th Eurographics Symposium on Virtual Environments, IPT/EGVE 2005",,,,"113","120",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878735362&partnerID=40&md5=07f911d08719b4b9fe156487f81e3046","WSI/GRIS-VCM, University of Tübingen, Germany","Del Río, A., WSI/GRIS-VCM, University of Tübingen, Germany; Fischer, J., WSI/GRIS-VCM, University of Tübingen, Germany; Köbele, M., WSI/GRIS-VCM, University of Tübingen, Germany; Bartz, D., WSI/GRIS-VCM, University of Tübingen, Germany; Straßer, W., WSI/GRIS-VCM, University of Tübingen, Germany","In the visualization of 3D medical data, the appropriateness of the achieved result is highly dependent on the application. Therefore, an intuitive interaction with the user is of utter importance in order to determine the particular aim of the visualization. In this paper, we present a novel approach for the visualization of 3D medical data with volume rendering combined with AR-based user interaction. The utilization of augmented reality (AR), with the assistance of a set of simple tools, allows the direct manipulation in 3D of the rendered data. The proposed method takes into account regions of interest defined by the user and employs this information to automatically generate an adequate transfer function. Machine learning techniques are utilized for the automatic creation of transfer functions, which are to be used during the classification stage of the rendering pipeline. The validity of the proposed approach for medical applications is illustrated. © The Eurographics Association 2005.",,"Automatic creations; Direct manipulation; Intuitive interaction; Machine learning techniques; Regions of interest; Rendering pipelines; User interaction; Volume classifications; Augmented reality; Learning systems; Medical applications; Optical projectors; Transfer functions; Virtual reality; Visualization; Volume rendering; Data visualization",Conference Paper,"Final","",Scopus,2-s2.0-84878735362
"Wierinck E., Puttemans V., Swinnen S., van Steenberghe D.","8243408900;6506533201;7005154622;7101895992;","Effect of augmented visual feedback from a virtual reality simulation system on manual dexterity training",2005,"European Journal of Dental Education","9","1",,"10","16",,50,"10.1111/j.1600-0579.2004.00351.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-14544298450&doi=10.1111%2fj.1600-0579.2004.00351.x&partnerID=40&md5=456dfc2bb2ed0629d188260ba7d4366e","Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium; Motor Control Laboratory, Department of Kinesiology, School of Dentistry, Oral Pathology and Maxillofacial Surgery Katholieke Universiteit Leuven, Leuven, Belgium; Department of Periodontology, School of Dentistry,Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium","Wierinck, E., Selfteaching and Skills Training Centre, School of Dentistry, Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium; Puttemans, V., Motor Control Laboratory, Department of Kinesiology, School of Dentistry, Oral Pathology and Maxillofacial Surgery Katholieke Universiteit Leuven, Leuven, Belgium; Swinnen, S., Motor Control Laboratory, Department of Kinesiology, School of Dentistry, Oral Pathology and Maxillofacial Surgery Katholieke Universiteit Leuven, Leuven, Belgium; van Steenberghe, D., Department of Periodontology, School of Dentistry,Oral Pathology and Maxillofacial Surgery, Katholieke Universiteit Leuven, Leuven, Belgium","Little research has been published about the impact of simulation technology on the learning process of novel motor skills. Especially the role of augmented feedback (FB) on the quality of performance and the transfer of the acquired behaviour to a no-augmented FB condition require further investigation. Therefore, novice dental students were randomly assigned to one of three groups and given the task of drilling a geometrical class 1 cavity. The FB group trained under augmented visual FB conditions, provided by the virtual reality (VR) system (DentSim™). The no-FB group practised under normal vision conditions, in the absence of augmented FB. A control group performed the test sessions without participating in any training programme. All preparations were evaluated by the VR grading system according to four traditional (outline shape, floor depth, floor smoothness and wall inclination), and two critical, criteria (pulp exposure and damage to adjacent teeth). Performance analyses revealed an overall trend towards significant improvement with training for the experimental groups. The FB group obtained the highest scores. It scored better for floor depth (P < 0.001), whilst the no-FB group was best for floor smoothness (P < 0.005). However, at the retention tests, the FB group demonstrated inferior performance in comparison with the no-FB group. The transfer test on a traditional unit revealed no significant differences between the training groups. Consequently, drilling experience on a VR system under the condition of frequently provided FB and lack of any tutorial input was considered to be not beneficial to learning. The present data are discussed in view of the guidance hypothesis of FB, which refers to the apprentice's dependence on FB. © Blackwell Munksgaard, 2005.","Augmented feedback; Guidance; Skill acquisition; Transfer; Virtual reality","adolescent; adult; article; classification; clinical trial; comparative study; computer interface; computer simulation; controlled clinical trial; controlled study; dental surgery; education; endodontics; feedback system; human; learning; long term memory; methodology; motor performance; randomized controlled trial; teaching; tooth injury; vision; Adolescent; Adult; Computer Simulation; Dental Cavity Preparation; Dental Pulp Exposure; Dentistry, Operative; Feedback; Humans; Learning; Motor Skills; Retention (Psychology); Teaching; Tooth Injuries; Transfer (Psychology); User-Computer Interface; Visual Perception",Article,"Final","",Scopus,2-s2.0-14544298450
"Dorfmüller-Ulhaas K., André E.","8925688800;7006887726;","The synthetic character ritchie: First steps towards a virtual companion for mixed reality",2005,"Proceedings - Fourth IEEE and ACM International Symposium on Symposium on Mixed and Augmented Reality, ISMAR 2005","2005",, 1544682,"178","179",,4,"10.1109/ISMAR.2005.61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750938837&doi=10.1109%2fISMAR.2005.61&partnerID=40&md5=3d6c727711d5cfc161a3d8925e151de5","University of Augsburg, Germany","Dorfmüller-Ulhaas, K., University of Augsburg, Germany; André, E., University of Augsburg, Germany","Unlike most existing work on traversable interfaces, we focus on the use of synthetic characters to accompany the user in Mixed Reality (MR) applications. We examine virtual companions as a promising means to design smooth transitions between different worlds and to avoid orientation problems. We propose a taxonomy to describe the spatial relationship between character and user which has an important impact on the style of interaction. To flexibly transfer user and character into different spaces, we have created a platform that supports the design of interfaces derived from the proposed taxonomy as well as transitions between them. © 2005 IEEE.",,"Computer aided design; Computer applications; Problem solving; User interfaces; Mixed reality (MR); Smooth transitions; Spatial relationships; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-33750938837
"Fuhrmann A.L., Splechtna R.C., Mroz L., Hauser H.","7004448962;55481478800;6603388015;7202841889;","Distributed software-based volume visualization in a virtual environment",2005,"9th International Workshop on Immersive Projection Technology - 11th Eurographics Symposium on Virtual Environments, IPT/EGVE 2005",,,,"129","139",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878689138&partnerID=40&md5=8471fd68a6473911e998e41b5587efad","VRVis Research Center, Austria; TIANI MedGraph, Austria","Fuhrmann, A.L., VRVis Research Center, Austria; Splechtna, R.C., VRVis Research Center, Austria; Mroz, L., TIANI MedGraph, Austria; Hauser, H., VRVis Research Center, Austria","In this paper we present our integration of volume rendering into virtual reality, combining a fast and flexible software implementation of direct volume rendering with the intuitive manipulation and navigation techniques of a virtual environment. By distributing the visualization and interaction tasks to two low-end PCs we managed to realize a highly interactive, yet inexpensive set-up. The volume objects are seamlessly integrated into the polygonal virtual environment through image-based rendering. The interaction techniques include scalar parameterization of transfer functions, direct 3D selection, 3D highlighting of volume objects and clipping cubes and cutting planes. These methods combined with the interaction and display devices of virtual reality form a powerful yet intuitive environment for the investigation of volume data sets. As main application areas we propose training and education. © The Eurographics Association 2005.",,"Direct volume rendering; Image-Based Rendering; Interaction techniques; Navigation techniques; Software implementation; Training and education; Volume data sets; Volume visualization; Display devices; Image reconstruction; Optical projectors; Sensory perception; Visualization; Volume rendering; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84878689138
"Melzer A., Hadley L., Winkler T., Herczeg M.","25825230800;22334315600;57197359620;15020598500;","Developing, implementing, and testing mixed reality and high interaction media applications in schools",2005,"IADIS International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2005",,,,"123","130",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880564323&partnerID=40&md5=236aa44edff2d3d798c28f104fb6218f","University of Luebeck, Ratzeburger Allee 160, D-23538 Luebeck, Germany","Melzer, A., University of Luebeck, Ratzeburger Allee 160, D-23538 Luebeck, Germany; Hadley, L., University of Luebeck, Ratzeburger Allee 160, D-23538 Luebeck, Germany; Winkler, T., University of Luebeck, Ratzeburger Allee 160, D-23538 Luebeck, Germany; Herczeg, M., University of Luebeck, Ratzeburger Allee 160, D-23538 Luebeck, Germany","Current pedagogical approaches emphasize the procurement of media literacy in classroom learning. In this respect, mixed reality systems and high interaction media applications provide potentially exciting learning experiences for children by bridging the gap between theoretical and experiential learning in the physical world. We describe the constructive implementation of digitally augmented learning environments in elementary and secondary schools during an ongoing research and knowledge transfer project at the University of Luebeck, Germany. Three class projects are introduced that focus on storytelling within vision-based action scenarios, and mobile and ubiquitous learning, respectively. Evaluation data from different variables (e.g., motivation, achievement of learning goals, problem solving) and different target groups (e.g., students, teachers, team members) show the overall success of the projects. However, the data also indicate the complexity and difficulties of implementing digitally augmented learning in schools.","Constructivist learning; Mixed reality; Pervasive computing","Constructivist learning; Experiential learning; Learning environments; Learning experiences; Mixed reality; Mixed reality systems; Pedagogical approach; Ubiquitous learning; Computer aided instruction; Knowledge management; Societies and institutions; Teaching; Ubiquitous computing; Virtual reality; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84880564323
"Wang X., Dunston P.S.","8945580300;6602079727;","Heavy equipment operator training via virtual modeling technologies",2005,"Construction Research Congress 2005: Broadening Perspectives - Proceedings of the Congress",,,,"1241","1250",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644482011&partnerID=40&md5=9994234da9f351de5eac8b281ba6edf4","Purdue University, School of Civil Engrg., 550 Stadium Mall Dr., West Lafayette, IN 47907-2051, United States","Wang, X., Purdue University, School of Civil Engrg., 550 Stadium Mall Dr., West Lafayette, IN 47907-2051, United States; Dunston, P.S., Purdue University, School of Civil Engrg., 550 Stadium Mall Dr., West Lafayette, IN 47907-2051, United States","Off-site and on-the-job training programs constitute current methods to train construction equipment operators. Being time-intensive, expensive, and potentially hazardous, these methods give novices only limited opportunity to experience real working conditions. Computer modeling technologies - Augmented Reality, Augmented Virtuality, Virtual Reality, teleoperation, and simulator - are envisaged to meet this challenge. To facilitate comparisons between different training schemes, a taxonomy is presented to identify distinctions. Skill transfer from the training program to real task performance is noted to be a critical ergonomics issue, and thus a cognitive-motor spectrum was developed for classifying categories of transferred skills prevalent in current equipment operator training schemas.","Equipment operator training; Virtual modeling; Virtual Reality","Equipment operator modeling; On-the-job training; Training schemes; Virtual modeling; Cognitive systems; Construction equipment; Simulators; Virtual reality; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-27644482011
"De Buck S., Maes F., Ector J., Bogaert J., Dymarkowski S., Heidbüchel H., Suetens P.","55967150700;23005146800;9245896600;7005505812;6701426477;7004984289;23006621700;","An augmented reality system for patient-specific guidance of cardiac catheter ablation procedures",2005,"IEEE Transactions on Medical Imaging","24","11",,"1512","1524",,60,"10.1109/TMI.2005.857661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-27744510557&doi=10.1109%2fTMI.2005.857661&partnerID=40&md5=b291d1eb5766cf2871670cc6632deebc","IEEE; Medical Image Computing, Department of Electrical Engineering, Katholieke Universiteit Leuven, Herestraat 49, 3000 Leuven, Belgium; Medical Image Computing, Department of Electrical Engineering, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Department of Cardiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Department of Radiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium","De Buck, S., Medical Image Computing, Department of Electrical Engineering, Katholieke Universiteit Leuven, Herestraat 49, 3000 Leuven, Belgium; Maes, F., Medical Image Computing, Department of Electrical Engineering, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Ector, J., Department of Cardiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Bogaert, J., Department of Radiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Dymarkowski, S., Department of Radiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Heidbüchel, H., Department of Cardiology, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Suetens, P., IEEE, Medical Image Computing, Department of Electrical Engineering, Katholieke Universiteit Leuven, 3000 Leuven, Belgium","We present a system to assist in the treatment of cardiac arrhythmias by catheter ablation. A patient-specific three-dimensional (3-D) anatomical model, constructed from magnetic resonance images, is merged with fluoroscopic images in an augmented reality environment that enables the transfer of electrocardiography (ECG) measurements and cardiac activation times onto the model. Accurate mapping is realized through the combination of: a new calibration technique, adapted to catheter guided treatments; a visual matching registration technique, allowing the electrophysiologist to align the model with contrast-enhanced images; and the use of virtual catheters, which enable the annotation of multiple ECG measurements on the model. These annotations can be visualized by color coding on the patient model. We provide an accuracy analysis of each of these components independently. Based on simulation and experiments, we determined a segmentation error of 0.6 mm, a calibration error in the order of 1 mm and a target registration error of 1.04 ± 0.45 mm. The system provides a 3-D visualization of the cardiac activation pattern which may facilitate and improve diagnosis and treatment of the arrhytmia. Because of its low cost and similar advantages we believe our approach can compete with existing commercial solutions, which rely on dedicated hardware and costly catheters. We provide qualitative results of the first clinical use of the system in 11 ablation procedures. © 2005 IEEE.","Arrhythmia; Augmented reality; Cardiac ablation","Computer simulation; Error analysis; Magnetic resonance imaging; Mathematical models; Patient treatment; Arrhytmia; Cardiac ablation; Cardiac activation pattern; Fluoroscopic images; Virtual reality; algorithm; article; artificial intelligence; biological model; catheter ablation; clinical trial; computer assisted diagnosis; computer assisted surgery; computer interface; electrocardiography; heart arrhythmia; human; image enhancement; image subtraction; methodology; nuclear magnetic resonance imaging; reproducibility; sensitivity and specificity; three dimensional imaging; Algorithms; Arrhythmia; Artificial Intelligence; Body Surface Potential Mapping; Catheter Ablation; Humans; Image Enhancement; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Models, Cardiovascular; Reproducibility of Results; Sensitivity and Specificity; Subtraction Technique; Surgery, Computer-Assisted; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-27744510557
"Gerling G.J., Thomas G.W.","35271881600;7404575819;","Augmented, pulsating tactile feedback facilitates simulator training of clinical breast examinations",2005,"Human Factors","47","3",,"670","681",,21,"10.1518/001872005774860050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644859569&doi=10.1518%2f001872005774860050&partnerID=40&md5=a6f37fedd30c81d751968cc299619990","University of Virginia, Charlottesville, VA, United States; University of Iowa, Iowa City, IA, United States; University of Virginia, Department of Systems and Information Engineering, 151 Engineer's Way, Charlottesville, VA 22904, United States; University of Virginia, Department of Systems and Information Engineering, United States; Department of Mechanical and Industrial Engineering; GROK (Graphical Representation of Knowledge) Laboratory, University of Iowa, Iowa City, IA, United States","Gerling, G.J., University of Virginia, Charlottesville, VA, United States, University of Virginia, Department of Systems and Information Engineering, 151 Engineer's Way, Charlottesville, VA 22904, United States, University of Virginia, Department of Systems and Information Engineering, United States; Thomas, G.W., University of Iowa, Iowa City, IA, United States, Department of Mechanical and Industrial Engineering, GROK (Graphical Representation of Knowledge) Laboratory, University of Iowa, Iowa City, IA, United States","Haptic training devices can facilitate tactile skill development by providing repeatable exposures to rare stimuli. Extant haptic training simulator research primarily emphasizes realistic stimuli representation; however, the experiments reported herein suggest that providing augmented feedback can improve training effectiveness, even when the feedback is not natural. A novel clinical breast examination training device uses inflated balloons embedded in silicone to simulate breast lumps. Oscillating the balloon water pressure makes the lumps pulsate. The pulsating lumps are easier to detect than the static lumps used in current simulators, and this manipulation seems to effectively introduce trainees to small, deep lumps that are initially difficult to perceive. A study of 48 medical students indicates that training with the dynamic breast model increased the number of lumps detected, F(1, 47) = 9.34, p = .004, decreased the number of false positives, F(1, 47) = 5.78, p = .020, and improved intersimulator skill transfer, F(1, 47) = 26.56, p < .001. The results suggest that at least in this case, augmented, tactile feedback increases training effectiveness, despite the fact that the feedback does not attempt to mimic any physical phenomenon present in the natural stimulus. Applications of this research include training techniques and tools for improved detection of palpable cancers. Copyright © 2005, Human Factors and Ergonomics Society. All rights reserved.",,"Clinical breast examination; Haptic training devices; Simulator research; Tactile skill development; Haptic interfaces; Mathematical models; Personnel training; Silicones; Simulators; Feedback; adult; article; breast examination; cancer diagnosis; clinical article; device; female; human; human experiment; laboratory diagnosis; male; medical student; normal human; simulation; skill retention; tactile discrimination; training; audiovisual equipment; breast tumor; clinical competence; clinical trial; comparative study; controlled clinical trial; controlled study; physical examination; randomized controlled trial; reinforcement; touch; Adult; Breast Neoplasms; Clinical Competence; Female; Humans; Knowledge of Results (Psychology); Male; Models, Anatomic; Physical Examination; Students, Medical; Touch",Article,"Final","",Scopus,2-s2.0-33644859569
"Holden M.K., Dyar T.A., Schwamm L., Bizzi E.","7102163511;8589238100;7004541381;7005489607;","Virtual-environment-based telerehabilitation in patients with stroke",2005,"Presence: Teleoperators and Virtual Environments","14","2",,"214","233",,70,"10.1162/1054746053967058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-20344401310&doi=10.1162%2f1054746053967058&partnerID=40&md5=81c355a5589b1878103416e4beddd35d","Department of Brain and Cognitive Science, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, United States; Department of Brain and Cognitive Science, Clinical Research Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Clinical Research Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, United States","Holden, M.K., Department of Brain and Cognitive Science, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, United States; Dyar, T.A., Department of Brain and Cognitive Science, Clinical Research Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Schwamm, L., Clinical Research Center, Massachusetts Institute of Technology, Cambridge, MA, United States, Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, United States; Bizzi, E., Department of Brain and Cognitive Science, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, United States, Department of Brain and Cognitive Science, Clinical Research Center, Massachusetts Institute of Technology, Cambridge, MA, United States","We describe a telerehabilitation system that has been developed in our laboratory, and initial results following use of the system on 2 patients with stroke. The system allows a therapist in a remote location to conduct treatment sessions, using a virtual-environment-based motor-training system, with a patient who is located at home. The system consists of a patient computer with motion-capture equipment and video camera, a therapist computer with video camera, and virtual-environment software that is synchronized over a high-speed Internet connection. The patient's movements are animated within the context of a virtual scene as she attempts to imitate a prerecorded movement, while the therapist can direct and monitor the activity in real time, as displayed in the animated virtual scene and via videoconference. The design, technical testing, and clinical feasibility testing of the system are reported. Results from the first 2 stroke patients to use the system indicate that patients made significant gains in upper-extremity function as measured by standard clinical tests and by their subjective report. As well, both patients demonstrated gains on quantitative kinematic measures of upper-extremity trajectories performed in the real world, indicating transfer of training from VE to real-world performance. © 2005 by the Massachusetts Institute of Technology.",,"Clinical tests; Medical rehabilitation; Therapists; Therapy; Cameras; Cost effectiveness; Health care; Kinematics; Patient rehabilitation; Patient treatment; Trajectories; Virtual reality",Article,"Final","",Scopus,2-s2.0-20344401310
"Evreinov G., Evreinova T., Raisamo R.","6603119110;56085829900;35610443700;","Manipulating vibro-tactile sequences on mobile PC",2005,"Lecture Notes in Computer Science","3425",,,"245","252",,2,"10.1007/11431879_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344476761&doi=10.1007%2f11431879_16&partnerID=40&md5=f20828bcfcc861422e3aa31f4fd1ff73","TAUCHI Computer-Human Interaction Unit, Department of Computer Sciences, FIN-33014 Tampere, Finland","Evreinov, G., TAUCHI Computer-Human Interaction Unit, Department of Computer Sciences, FIN-33014 Tampere, Finland; Evreinova, T., TAUCHI Computer-Human Interaction Unit, Department of Computer Sciences, FIN-33014 Tampere, Finland; Raisamo, R., TAUCHI Computer-Human Interaction Unit, Department of Computer Sciences, FIN-33014 Tampere, Finland","Tactile memory is the crucial factor in coding and transfer of the semantic information through a single vibrator. While some simulators can produce strong vibro-tactile sensations, discrimination of several tactile patterns can remain quite poor. Currently used actuators, such as shaking motor, have also technological and methodological restrictions. We designed a vibro-tactile pen and software to create tactons and semantic sequences of vibro-tactile patterns on mobile devices (iPAQ pocket PC). We proposed special games and techniques to simplify learning and manipulating vibro-tactile patterns. The technique for manipulating vibro-tactile sequences is based on gesture recognition and spatial-temporal mapping for imaging vibro-tactile signals. After training, the tactons could be used as awareness cues or the system of non-verbal communication signals. © IFIP International Federation for Information Processing 2005.",,"Computer software; Mobile telecommunication systems; Personal computers; Semantics; Signal processing; Mobile devices; Tactile memory; Tactile patterns; Vibro-tactile patterns; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-24344476761
"Streit A., Christie R., Boud A.","8948758200;57215471529;6505890731;","Understanding next-generation VR: Classifying commodity clusters for immersive virtual reality",2004,"Proceedings GRAPHITE 2004 - 2nd International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia",,,,"222","229",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344255082&partnerID=40&md5=36507f2a5412384f9e9974519a3be5f0","QUT, Australia; VR Solutions, Australia","Streit, A., QUT, Australia; Christie, R., QUT, Australia; Boud, A., VR Solutions, Australia","Commodity clusters offer the ability to deliver higher performance computer graphics at lower prices than traditional graphics supercomputers. Immersive virtual reality systems demand notoriously high computational requirements to deliver adequate real-time graphics, leading to the emergence of commodity clusters for immersive virtual reality. Such clusters deliver the graphics power needed by leveraging the combined power of several computers to meet the demands of real-time interactive immersive computer graphics. However, the field of commodity cluster-based virtual reality is still in early stages of development and the field is currently adhoc in nature and lacks order. There is no accepted means for comparing approaches and implementers are left with instinctual or trial-and-error means for selecting an approach. This paper provides a classification system that facilitates understanding not only of the nature of different clustering systems but also the interrelations between them. The system is built from a new model for generalized computer graphics applications, which is based on the flow of data through a sequence of operations over the entire context of the application. Prior models and classification systems have been too focused in context and application whereas the system described here provides a unified means for comparison of works within the field.","Computer Clusters; Real-time Graphics; Virtual and Augmented reality","Classification systems; Computer clusters; Real-time graphics; Scalable capacity; Classification (of information); Computer graphics; Computer hardware; Data acquisition; Interactive computer systems; Mathematical models; Supercomputers; User interfaces; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-12344255082
"Adamovich S.V., Merians A.S., Boian R., Tremaine M., Burdea G.S., Recce M., Poizner H.","6603916707;6603018031;6603659582;6603562993;35612697900;6603906003;7005963417;","A virtual reality based exercise system for hand rehabilitation post-stroke: Transfer to function",2004,"Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","26 VII",,,"4936","4939",,61,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144291509&partnerID=40&md5=cf82af860c1c7d7732e82784d82a9477","Dept. of Biomedical Engineering, New Jersey Institute of Technology, Newark, NJ, United States; College of Computer Science, New Jersey Institute of Technology, Newark, NJ, United States; Dept. of Developmental Science, Univ. Med. and Dent. of New Jersey, Newark, NJ, United States; Ctr. for Molec. and Behav. Neurosci., Rutgers University, Newark, NJ, United States; School of Engineering, Ctr. for Adv. Information Processing, Rutgers University, Piscataway, NJ, United States","Adamovich, S.V., Dept. of Biomedical Engineering, New Jersey Institute of Technology, Newark, NJ, United States, Dept. of Developmental Science, Univ. Med. and Dent. of New Jersey, Newark, NJ, United States, Ctr. for Molec. and Behav. Neurosci., Rutgers University, Newark, NJ, United States; Merians, A.S., Dept. of Developmental Science, Univ. Med. and Dent. of New Jersey, Newark, NJ, United States; Boian, R., School of Engineering, Ctr. for Adv. Information Processing, Rutgers University, Piscataway, NJ, United States; Tremaine, M., College of Computer Science, New Jersey Institute of Technology, Newark, NJ, United States; Burdea, G.S., School of Engineering, Ctr. for Adv. Information Processing, Rutgers University, Piscataway, NJ, United States; Recce, M., College of Computer Science, New Jersey Institute of Technology, Newark, NJ, United States; Poizner, H., Ctr. for Molec. and Behav. Neurosci., Rutgers University, Newark, NJ, United States","We present preliminary results from a virtual reality (VR)-based system for hand rehabilitation that uses a CyberGlove and a Rutgers Master II-ND haptic glove. This system trains finger range of motion, finger flexion speed, independence of finger motion and finger strength. Eight chronic post-stroke subjects participated. In keeping with variability in both the lesion site and in initial upper extremity function, each subject showed improvement on a unique combination of movement parameters in VR training. These improvements transferred to gains on clinical tests, as well as to significant reductions in task completion times for the prehension of real objects. These results are indicative of the potential feasibility of this exercise system for rehabilitation in patients with hand dysfunction resulting from neurological impairment.","CyberGlove; Grasping; Hand function; Rutgers Master II-ND; Stroke; Virtual reality (VR)","CyberGlove; Grasping; Hand function; Rutgers Master II-ND; Stroke; Data reduction; Haptic interfaces; Information retrieval; Joints (anatomy); Object recognition; Virtual reality; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-11144291509
"Yao Y.X., Xia P.J., Liu J.S., Li J.G.","55722539100;13404322900;8205876700;22953365000;","A pragmatic system to support interactive assembly planning and training in immersive virtual environment (I-VAPTS)",2004,"Advances in e-Engineering and Digital Enterprise Technology - I. Proceedings of the Fourth International Conference on e-Engineering and Digital Enterprise Technology",,,,"285","295",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-32444449587&partnerID=40&md5=d47a67ad34e2d2e093dfb4652f87b5ea","School of Mechanical and Electrical Engineering, Harbin Institute of Technology, China","Yao, Y.X., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, China; Xia, P.J., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, China; Liu, J.S., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, China; Li, J.G., School of Mechanical and Electrical Engineering, Harbin Institute of Technology, China","Assembly planning for complex product is a difficult task with which both intensive knowledge and experience are needed. Computer aided assembly planning (CAAP) systems have been the subject of considerable research in recent years without achieving a wide application in manufacture industry. An alternative approach based on the adoption of immersive virtual reality is presented to generate optimal assembly planning scheme in this paper. A product is assembled from CAD models by providing a CAD interface to transfer assembly constraint information from CAD system to virtual environment; In virtual environment an efficient geometry constraint dynamic recognition and management method based on geometry surface is present, and a process-oriented assembly task model is established to support interactive assembly planning and evaluation. The system is implemented using object oriented methodology, and has been successfully applied to train and guide the assembly workers in a pump assembly process.","Assembly planning and training; CAD; Virtual reality","Assembly; Computer aided design; Mathematical models; Object oriented programming; Planning; Virtual reality; Assembly planning; Computer aided assembly planning (CAAP); Process-oriented assembly; Interactive computer systems",Conference Paper,"Final","",Scopus,2-s2.0-32444449587
"Zhou Z., Cheok A.D., Yang X., Qiu Y.","23986510500;7003447496;22236065700;36724983800;","An experimental study on the role of 3D sound in augmented reality environment",2004,"Interacting with Computers","16","6",,"1043","1068",,23,"10.1016/j.intcom.2004.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-9944230931&doi=10.1016%2fj.intcom.2004.06.016&partnerID=40&md5=c593a65389cf3e596874fc1cfe187c82","Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Zhou, Z., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Cheok, A.D., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Yang, X., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore; Qiu, Y., Department of Electrical Engineering, National University of Singapore, Singapore 119260, Singapore","Investigation of augmented reality (AR) environments has become a popular research topic for engineers, computer and cognitive scientists. Although application oriented studies focused on audio AR environments have been published, little work has been done to vigorously study and evaluate the important research questions of the effectiveness of three-dimensional (3D) sound in the AR context, and to what extent the addition of 3D sound would contribute to the AR experience. Thus, we have developed two AR environments and performed vigorous experiments with human subjects to study the effects of 3D sound in the AR context. The study concerns two scenarios. In the first scenario, one participant must use vision only and vision with 3D sound to judge the relative depth of augmented virtual objects. In the second scenario, two participants must cooperate to perform a joint task in a game-based AR environment. Hence, the goals of this study are (1) to access the impact of 3D sound on depth perception in a single-camera AR environment, (2) to study the impact of 3D sound on task performance and the feeling of 'human presence and collaboration', (3) to better understand the role of 3D sound in human-computer and human-human interactions, (4) to investigate if gender can affect the impact of 3D sound in AR environments. The outcomes of this research can have a useful impact on the development of audio AR systems, which provide more immersive, realistic and entertaining experiences by introducing 3D sound. Our results suggest that 3D sound in AR environment significantly improves the accuracy of depth judgment and improves task performance. Our results also suggest that 3D sound contributes significantly to the feeling of human presence and collaboration and helps the subjects to 'identify spatial objects'. © 2004 Elsevier B.V. All rights reserved.","3D sound; Augmented reality; User study","Acoustic waves; Cameras; Cognitive systems; Feedback; Human computer interaction; Multiprocessing systems; Three dimensional; Three dimensional computer graphics; Transfer functions; Head related transfer functions (HRTF); Human-human interactions; Three-dimensional (3D) sound; User study; Virtual reality",Article,"Final","",Scopus,2-s2.0-9944230931
"Gross M., Würmlin S., Naef M., Lamboray E., Spagno C., Kunz A., Koller-Meier E., Svoboda T., Van Gool L., Lang S., Strehlke K., Moere A.V., Staadt O.","7403745074;7801645956;7004639110;56633827500;6507617717;7005939819;7801399931;35085537600;22735702300;23028281500;13008773800;55664476700;6602657927;","Blue-c: A spatially immersive display and 3D video portal for telepresence",2003,"ACM SIGGRAPH 2003 Papers, SIGGRAPH '03",,,,"819","827",,138,"10.1145/1201775.882350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953981472&doi=10.1145%2f1201775.882350&partnerID=40&md5=5ad977f7fc3eb33223c3575ded586eb9","ETH Zürich, Switzerland; University of California, Davis, CA, United States","Gross, M., ETH Zürich, Switzerland; Würmlin, S., ETH Zürich, Switzerland; Naef, M., ETH Zürich, Switzerland; Lamboray, E., ETH Zürich, Switzerland; Spagno, C., ETH Zürich, Switzerland; Kunz, A., ETH Zürich, Switzerland; Koller-Meier, E., ETH Zürich, Switzerland; Svoboda, T., ETH Zürich, Switzerland; Van Gool, L., ETH Zürich, Switzerland; Lang, S., ETH Zürich, Switzerland; Strehlke, K., ETH Zürich, Switzerland; Moere, A.V., ETH Zürich, Switzerland; Staadt, O., ETH Zürich, Switzerland, University of California, Davis, CA, United States","We present blue-c, a new immersive projection and 3D video acquisition environment for virtual design and collaboration. It combines simultaneous acquisition of multiple live video streams with advanced 3D projection technology in a CAVE#8482;-like environment, creating the impression of total immersion. The blue-c portal currently consists of three rectangular projection screens that are built from glass panels containing liquid crystal layers. These screens can be switched from a whitish opaque state (for projection) to a transparent state (for acquisition), which allows the video cameras to ""look through"" the walls. Our projection technology is based on active stereo using two LCD projectors per screen. The projectors are synchronously shuttered along with the screens, the stereo glasses, active illumination devices, and the acquisition hardware. From multiple video streams, we compute a 3D video representation of the user in real time. The resulting video inlays are integrated into a networked virtual environment. Our design is highly scalable, enabling blue-c to connect to portals with less sophisticated hardware. © 2003 ACM.","3D Video; graphics hardware; real-time graphics; spatially immersive displays; virtual environments","3D video; Graphics hardware; Immersive display; Real time graphics; Virtual environments; Glass; Interactive computer graphics; Liquid crystal displays; Liquid crystals; Mergers and acquisitions; Projection systems; Technology transfer; Three dimensional; Video cameras; Video recording; Video streaming; Virtual reality; Visual communication; Projection screens",Conference Paper,"Final","",Scopus,2-s2.0-77953981472
"Raskar R., Van Baar J., Beardsley P., Willwacher T., Rao S., Forlines C.","6602886524;6603929342;7005022858;57190524639;36142023900;8594330700;","iLamps: Geometrically aware and self-configuring projectors",2003,"ACM SIGGRAPH 2003 Papers, SIGGRAPH '03",,,,"809","818",,120,"10.1145/1201775.882349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953997133&doi=10.1145%2f1201775.882349&partnerID=40&md5=ddeec34fea4160ab956debbdf3cf0255","Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States","Raskar, R., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States; Van Baar, J., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States; Beardsley, P., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States; Willwacher, T., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States; Rao, S., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States; Forlines, C., Mitsubishi Electric Research Labs. (MERL), Cambridge, MA, United States","Projectors are currently undergoing a transformation as they evolve from static output devices to portable, environment-aware, communicating systems. An enhanced projector can determine and respond to the geometry of the display surface, and can be used in an ad-hoc cluster to create a self-configuring display. Information display is such a prevailing part of everyday life that new and more flexible ways to present data are likely to have significant impact. This paper examines geometrical issues for enhanced projectors, relating to customized projection for different shapes of display surface, object augmentation, and co-operation between multiple units.We introduce a new technique for adaptive projection on nonplanar surfaces using conformal texture mapping. We describe object augmentation with a hand-held projector, including interaction techniques. We describe the concept of a display created by an ad-hoc cluster of heterogeneous enhanced projectors, with a new global alignment scheme, and new parametric image transfer methods for quadric surfaces, to make a seamless projection. The work is illustrated by several prototypes and applications. © 2003 ACM.","ad-hoc clusters; augmented reality; calibration; projector; quadric transfer; seamless display","ad-hoc clusters; Display surface; Global alignment; Hand-held projectors; Information display; Interaction techniques; Non-planar surfaces; Parametric image; Quadric surfaces; Self-configuring; Significant impacts; Static output; Texture mapping; Augmented reality; Calibration; Interactive computer graphics; Software prototyping; Virtual reality; Flexible displays",Conference Paper,"Final","",Scopus,2-s2.0-77953997133
"Correia N., Romão T., Santos C., Trabuco A., Santos R., Romero L., Danado J., Dias E., Câmara A., Nobre E.","7006236302;6507404367;56983263200;7801337962;7201375307;56870548700;7801472052;57219399648;7005091449;6602500818;","A flexible augmented reality architecture applied to environmental management",2003,"Proceedings of SPIE - The International Society for Optical Engineering","5006",,,"519","528",,,"10.1117/12.477347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042861462&doi=10.1117%2f12.477347&partnerID=40&md5=ccf8b1a84bcdb565f71772da938151bc","Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal; Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal","Correia, N., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Romão, T., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Santos, C., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Trabuco, A., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Santos, R., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Romero, L., Computer Science Department, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Danado, J., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Dias, E., Computer Science Department, University of Évora, Rua Romão Ramalho 59, 7000 Ẽvora, Portugal, Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Câmara, A., Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal; Nobre, E., Environmental Systems Analysis Group, New University of Lisbon, Quinta da Torre, 2825 Caparica, Portugal","Environmental management often requires in loco observation of the area under analysis. Augmented Reality (AR) technologies allow real time superimposition of synthetic objects on real images, providing augmented knowledge about the surrounding world. Users of an AR system can visualize the real surrounding world together with additional data generated in real time in a contextual way. The work reported in this paper was done in the scope of ANTS (Augmented Environments) project. ANTS is an AR project that explores the development of an augmented reality technological infrastructure for environmental management. This paper presents the architecture and the most relevant modules of ANTS. The system's architecture follows the client-server model and is based on several independent, but functionally interdependent modules. It has a flexible design, which allows the transfer of some modules to and from the client side, according to the available processing capacities of the client device and the application's requirements. It combines several techniques to identify the user's position and orientation allowing the system to adapt to the particular characteristics of each environment. The determination of the data associated to a certain location involves the use of both a 3D Model of the location and the multimedia geo-referenced database.","Augmented reality; Contextual information; Geo-referenced information; Mobile information services; Positioning; Software Components; System Architecture","Environmental impact; Multimedia systems; Servers; Environmental management; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0042861462
"Höll D., Leplow B., Schönfeld R., Mehdorn M.","56953607800;56086907900;7003809631;7006783173;","Is it possible to learn and transfer spatial information from virtual to real worlds?",2003,"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)","2685",,,"143","156",,3,"10.1007/3-540-45004-1_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344234240&doi=10.1007%2f3-540-45004-1_9&partnerID=40&md5=5e2be3c4182ed7deb9d3d37f5c4c1e60","Clinic for Neurosurgery, Chrstn.-Albrechts-University of Kiel, Weimarer Str. 8, 24106 Kiel, Germany; Department of Psychology, Martin-Luther-University of Halle, Brandbergweg 23, 06099 Halle (Saale), Germany","Höll, D., Clinic for Neurosurgery, Chrstn.-Albrechts-University of Kiel, Weimarer Str. 8, 24106 Kiel, Germany; Leplow, B., Department of Psychology, Martin-Luther-University of Halle, Brandbergweg 23, 06099 Halle (Saale), Germany; Schönfeld, R., Department of Psychology, Martin-Luther-University of Halle, Brandbergweg 23, 06099 Halle (Saale), Germany; Mehdorn, M., Clinic for Neurosurgery, Chrstn.-Albrechts-University of Kiel, Weimarer Str. 8, 24106 Kiel, Germany","In the present study spatial behavior was assessed by utilization of a desktop virtual environment and a locomotor maze task. In the first phase of the experiment, two groups of healthy middle-aged participants had to learn and remember five out of 20 target locations either in a ""real"" locomotor maze or an equivalent VR-version of this maze. The group with the VR-training was also confronted with the task in the real maze after achieving a learning criterion. Though acquisition rates were widely equivalent in the VR- and locomotor groups, VR participants had more problems learning the maze in the very first learning trials. Good transfer was achieved from the virtual to the real version of the maze by this group and they were significantly better in the acquisition phase of the locomotor task than the group that had not received VR-training. In the second phase of the experiment -the probe trials- when the cue configuration was changed the group with the VR-training seemed to have specific problems. A considerable number of participants of this group were not able to transfer information.","Memory; Orientation; Spatial cognition; Spatial memory; Spatial orientation; VR-environment","Data acquisition; Ecology; Learning systems; Personnel training; Problem solving; Virtual reality; Memory orientation; Spatial cognition; Spatial memory; Spatial orientation; Cognitive systems",Conference Paper,"Final","",Scopus,2-s2.0-8344234240
"Kaufmann H., Schmalstieg D.","7203004662;55101019100;","Mathematics and geometry education with collaborative augmented reality",2003,"Computers and Graphics (Pergamon)","27","3",,"339","345",,298,"10.1016/S0097-8493(03)00028-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037681614&doi=10.1016%2fS0097-8493%2803%2900028-1&partnerID=40&md5=add781c3fa93f1470a43aaaef5034eaf","Interactive Media Systems Group, Vienna University of Technology, Favoritenstrasse 9-11/188, Vienna A-1040, Austria","Kaufmann, H., Interactive Media Systems Group, Vienna University of Technology, Favoritenstrasse 9-11/188, Vienna A-1040, Austria; Schmalstieg, D., Interactive Media Systems Group, Vienna University of Technology, Favoritenstrasse 9-11/188, Vienna A-1040, Austria","Construct3D is a 3D geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system ""Studierstube"". We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a testbed for future evaluations. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills. © 2003 Elsevier Science Ltd. All rights reserved.","Augmented reality; Geometry education; Mathematics education; Spatial intelligence","Education; Learning systems; Societies and institutions; Students; Geometry education; Computer supported cooperative work",Conference Paper,"Final","",Scopus,2-s2.0-0037681614
"Hernández L., Taibo J., Seoane A., López R., López R.","55511555400;6507592553;7003406149;57213074028;55578883100;","The empty museum. Multi-user interaction in an immersive and physically walkable VR space",2003,"Proceedings - 2003 International Conference on Cyberworlds, CW 2003",,, 1253488,"446","452",,5,"10.1109/CYBER.2003.1253488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946202504&doi=10.1109%2fCYBER.2003.1253488&partnerID=40&md5=170c016c3b0fe366e21be66f6dd98f7a","Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain","Hernández, L., Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain; Taibo, J., Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain; Seoane, A., Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain; López, R., Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain; López, R., Architecture and Urban Design Visualisation Group, VideaLAB, Universidade Da Coruña, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Campus de Elviña, Spain","Until quite recently, virtual reality systems consisted of fixed devices which enabled the user to feel immersed in a spot of the virtual space by means of the adequate hardware. The recent emergence of wireless systems for motion capture, together with the increase in graphic power of laptops, and the generalisation of wireless networks has allowed the appearance of the first systems in which at last the user is able to walk physically within a given space framed in the real one, and containing the objects and elements of the virtual space [2][5][6]. Some examples of this hybrid space have already been accomplished worldwide. However, beyond the technical problems with the development of these systems, we must bear in mind the types of contents to be shown, making the most of the possibilities offered by the fact that the user him/herself is the pointer in this kind of virtual reality, while the space itself is the interface. The authors have recently developed a system similar to the ones described. This is a totally immersive, walkable and wireless system called the Empty Museum [7]. This paper outlines its enlargement with the purpose of making it simultaneously usable by several persons. Besides, an example of content is provided which has been specifically designed in order to be experienced in multi-user mode with this equipment: the Virtual Art Gallery. © 2003 IEEE.",,"Virtual reality; Wireless networks; First systems; Generalisation; Motion capture; Multi-user interaction; Virtual art gallery; Virtual reality system; Virtual spaces; Wireless systems; Museums",Conference Paper,"Final","",Scopus,2-s2.0-84946202504
"Borst C.W., Volz R.A.","9736479200;7006417396;","Observations on and modifications to the Rutgers Master to support a mixture of passive haptics and active force feedback",2003,"Proceedings - 11th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, HAPTICS 2003",,, 1191335,"430","437",,7,"10.1109/HAPTIC.2003.1191335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954276679&doi=10.1109%2fHAPTIC.2003.1191335&partnerID=40&md5=4dc4d991d30d98941bb35b782014074f","Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA  70504, United States; Department of Computer Science, Texas A and M University, College Station, TX  77843, United States","Borst, C.W., Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA  70504, United States; Volz, R.A., Department of Computer Science, Texas A and M University, College Station, TX  77843, United States","We are researching the use of haptic feedback during user interactions with a virtual control panel. One approach we consider is a combination of passive haptics and active force feedback. We use a static panel for the passive component and a Rutgers Master system for the active component. This mixed approach requires higher spatial accuracy and haptic quality than has been required by previous applications of force-feedback gloves. We relate the capabilities and limitations of the device to the requirements of our approach and describe a number of developments that result in a successful mix of passive haptics and active force feedback. These are useful for other applications of the device, and some can be generalized to other equipment. © 2003 IEEE.","Application software; Computer science; Control systems; Displays; Force control; Force feedback; Haptic interfaces; Software systems; Virtual environment; Virtual reality","Application programs; Computer control systems; Computer science; Computer software; Control systems; Display devices; Force control; Mice (computer peripherals); Remote control; Virtual reality; Active components; Force feedback; Haptic feedbacks; Mixed approach; Software systems; Spatial accuracy; User interaction; Virtual control; Haptic interfaces",Conference Paper,"Final","",Scopus,2-s2.0-77954276679
"Brewster S., Lumsden J., Bell M., Hall M., Tasker S.","7006514160;35610886600;8598005600;56250522000;7006836962;","Multimodal 'eyes-free' interaction techniques for wearable devices",2003,"Conference on Human Factors in Computing Systems - Proceedings",,,,"473","480",,144,"10.1145/642611.642694","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038713858&doi=10.1145%2f642611.642694&partnerID=40&md5=5678e6f69e54393ab79ba7c616caf9ae","Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; National Research Council, Inst. for Info. Technol. - e-Bus., 46 Dineen Drive, Fredericton, NB E3B 9W4, Canada","Brewster, S., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Lumsden, J., National Research Council, Inst. for Info. Technol. - e-Bus., 46 Dineen Drive, Fredericton, NB E3B 9W4, Canada; Bell, M., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Hall, M., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom; Tasker, S., Glasgow Interactive Systems Group, Department of Computing Science, University of Glasgow, Glasgow G12 8QQ, United Kingdom","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, 'eyes-free' device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users' gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.","Gestural interaction; Wearable computing","Computer keyboards; Gesture recognition; Graphical user interfaces; Mobile computing; Three dimensional; Transfer functions; Gesture recognition; Human computer interaction; Human engineering; Mobile devices; User interfaces; Wearable computers; Eyes free interaction; 3D audio; Gestural interaction; Gesture recognition system; Input/output; Interaction techniques; Interface designs; Multi-modal; Multimodal interaction techniques; Pie menus; Screen space; Task completion time; Visual Attention; Walking speed; Wearable computing; Wearable devices; Wearable computers; Audio acoustics",Conference Paper,"Final","",Scopus,2-s2.0-0038713858
"Szita I., Takács B., Lorincz A.","6603167419;57196853281;26643373200;","ε-MDPs: Learning in varying environments",2003,"Journal of Machine Learning Research","3","1",,"145","174",,25,"10.1162/153244303768966148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042967671&doi=10.1162%2f153244303768966148&partnerID=40&md5=0986ca54878fc09f65b3cc97938ebb3f","Department of Information Systems, Eötvös Loránd Univ., Pazmany Peter setany 1/C, Budapest H-1117, Hungary","Szita, I., Department of Information Systems, Eötvös Loránd Univ., Pazmany Peter setany 1/C, Budapest H-1117, Hungary; Takács, B., Department of Information Systems, Eötvös Loránd Univ., Pazmany Peter setany 1/C, Budapest H-1117, Hungary; Lorincz, A., Department of Information Systems, Eötvös Loránd Univ., Pazmany Peter setany 1/C, Budapest H-1117, Hungary","In this paper ε-MDP-models are introduced and convergence theorems are proven using the generalized MDP framework of Szepesvári and Littman. Using this model family, we show that Q-learning is capable of finding near-optimal policies in varying environments. The potential of this new family of MDP models is illustrated via a reinforcement learning algorithm called event-learning which separates the optimization of decision making from the controller. We show that event-learning augmented by a particular controller, which gives rise to an ε-MDP, enables near optimal performance even if considerable and sudden changes may occur in the environment. Illustrations are provided on the two-segment pendulum problem.","ε-MDP; Convergence; Event-learning; Generalized MDP; MDP; Reinforcement learning; SARSA; SDS controller","Convergence of numerical methods; Decision making; Learning algorithms; Software agents; Theorem proving; Virtual reality; Markovian decision problems (MDP); Learning systems",Article,"Final","",Scopus,2-s2.0-0042967671
"Kaufmann H.","7203004662;","Construct3D: An augmented reality application for mathematics and geometry education",2002,"Proceedings of the ACM International Multimedia Conference and Exhibition",,,,"656","657",,47,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038715045&partnerID=40&md5=6e8d3b1954c187c570468ce835970e44","Inst. Software Technol./I.S., Vienna University of Technology, Favoritenstrasse 9-11/188, 1040 Wien, Austria","Kaufmann, H., Inst. Software Technol./I.S., Vienna University of Technology, Favoritenstrasse 9-11/188, 1040 Wien, Austria","Construct3D is a three dimensional geometry construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system ""Studierstube"". We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. Means of application and integration in mathematics and geometry education at high school as well as university level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions and improves spatial skills.","Geometry education; Mathematics education; Spatial intelligence","Computer aided design; Computer aided instruction; Multimedia systems; Optimization; Virtual reality; Geometry education; Computational geometry",Conference Paper,"Final","",Scopus,2-s2.0-0038715045
"Landin H., Lundgren S., Prison J.","36720216400;25630780100;55944211900;","The iron horse: A sound ride",2002,"ACM International Conference Proceeding Series","31",,,"303","305",,6,"10.1145/572020.572075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953877648&doi=10.1145%2f572020.572075&partnerID=40&md5=06f189a20b7c907a68f04c8d2ef89cdf","HCI/Interaction Design, IT University of Göteborg, Chalmers/Göteborg University, P.O. Box 8718, SE-402 75 Göteborg, Sweden","Landin, H., HCI/Interaction Design, IT University of Göteborg, Chalmers/Göteborg University, P.O. Box 8718, SE-402 75 Göteborg, Sweden; Lundgren, S., HCI/Interaction Design, IT University of Göteborg, Chalmers/Göteborg University, P.O. Box 8718, SE-402 75 Göteborg, Sweden; Prison, J., HCI/Interaction Design, IT University of Göteborg, Chalmers/Göteborg University, P.O. Box 8718, SE-402 75 Göteborg, Sweden","The Iron Horse combines modern technology with a childhood dream. It's a bike - but its sounds like a horse. By biking at different speeds, one can get it to walk, trot or gallop. Sometimes it snorts, and it greets its owner and other iron horses with a neigh. In the project, we explored how to transfer the auditive expressions of horses into the art of cycling using computational technology, to stretch the boundaries of riding, cycling and interaction design. The technology should be an inspiration for the cyclist's fantasy; turning the playground into a jumping track, the way to school into a race, and the cycle path into a piece of the prairie. © 2002 ACM.","aesthetics; audial expression; augmented reality; bicycle; horse; interactive audio; sound","aesthetics; audial expression; Computational technology; Cycle path; Different speed; Interaction design; Modern technologies; Audio acoustics; Augmented reality; Bicycles; Knowledge management; Virtual reality; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-77953877648
"Rose F.D., Brooks B.M., Attree E.A.","7102651672;7201573008;6603053737;","An exploratory investigation into the usability and usefulness of training people with learning disabilities in a virtual environment",2002,"Disability and Rehabilitation","24","11-12",,"627","633",,32,"10.1080/09638280110111405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037142868&doi=10.1080%2f09638280110111405&partnerID=40&md5=0b350e839b02ae69c5fb916a59549196","School of Psychology, University of East London, London E15 4LZ, United Kingdom","Rose, F.D., School of Psychology, University of East London, London E15 4LZ, United Kingdom; Brooks, B.M., School of Psychology, University of East London, London E15 4LZ, United Kingdom; Attree, E.A., School of Psychology, University of East London, London E15 4LZ, United Kingdom","Purpose: Two studies sought to answer the following questions. Are people with learning disabilities capable of using a virtual environment? Are they motivated to learn using this training method? Do they show any benefit from using a virtual environment? Does any benefit transfer to improved real world performance? Method: In the first study, 30 students with learning disabilities were sequentially allocated to an active or a passive experimental group. Active participants explored a virtual bungalow searching for a toy car. Passive participants watched the exploration undertaken by the preceding active participant and searched for the toy car. All participants then performed spatial and object recognition tests of their knowledge of the virtual environment. In the second study, the errors of 45 participants on a real steadiness tester task were noted before they were randomly allocated to three groups-a real training group, a virtual training group and a no training group. After training, the participants performed a second test trial on the real steadiness tester. Results: The students were capable of using a virtual environment and were motivated to use this training method. Active exploration of a virtual environment was found to enhance their memory of the spatial layout of the bungalow but not their memory of the virtual objects. In the second study, virtual training was found to transfer to real task performance. Conclusions: These two laboratory-based studies provide answers to four important questions concerning virtual training of people with learning disabilities. Hopefully, the findings will encourage this training aid to be used more widely.",,"adolescent; adult; article; clinical trial; computer program; computer simulation; controlled clinical trial; controlled study; female; human; learning disorder; major clinical study; male; motivation; priority journal; psychologic test; randomized controlled trial; recognition; spatial memory; student; task performance; virtual reality; vocational education; Achievement; Adolescent; Adult; Cohort Studies; Education, Special; Educational Measurement; Female; Great Britain; Humans; Learning Disorders; Male; Middle Aged; Program Evaluation; Sampling Studies; Task Performance and Analysis; Teaching; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0037142868
"Yang U., Kim G.J.","7006006027;7403061980;","Implementation and evaluation of ""just follow me"": An immersive, VR-based, motion-training system",2002,"Presence: Teleoperators and Virtual Environments","11","3",,"304","323",,91,"10.1162/105474602317473240","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036625537&doi=10.1162%2f105474602317473240&partnerID=40&md5=5a670f3cffae5bb3c1b2a98769e0a833","Virtual Reality Laboratory, Dept. of Comp. Sci. and Engineering, Pohang Univ. of Sci. and Technology, San 31 Hyoja-dong, Pohang, Kyungbuk 790-784, South Korea","Yang, U., Virtual Reality Laboratory, Dept. of Comp. Sci. and Engineering, Pohang Univ. of Sci. and Technology, San 31 Hyoja-dong, Pohang, Kyungbuk 790-784, South Korea; Kim, G.J., Virtual Reality Laboratory, Dept. of Comp. Sci. and Engineering, Pohang Univ. of Sci. and Technology, San 31 Hyoja-dong, Pohang, Kyungbuk 790-784, South Korea","Training is usually regarded as one of the most natural application areas of virtual reality (VR). To date, most VR-based training systems have been situation based, but this paper examines the utility of VR for a different class of training: learning to execute exact motions, which are often required in sports and the arts. In this paper, we propose an interaction method, called Just Follow Me (JFM), that uses an intuitive ""ghost"" metaphor and a first-person viewpoint for effective motion training. Using the ghost metaphor (GM), JFM visualizes the motion of the trainer in real time as a ghost (initially superimposed on the trainee) that emerges from one's own body. The trainee who observes the motion from the first-person viewpoint ""follows"" the ghostly master as closely as possible to learn the motion. Our basic hypothesis is that such a VR system can help a student learn motion effectively and quickly, comparably to the indirect real-world teaching methods. Our evaluation results show that JFM produces training and transfer effects as good as - and, in certain situations, better than - in the real world. We believe that this is clue to the more direct and correct transfer of proprioceptive information from the trainer to the trainee.",,"Ghost metaphor; Human computer interaction; Motion planning; Personnel training; Students; Virtual reality",Article,"Final","",Scopus,2-s2.0-0036625537
"Sakagawa Y., Katayama A., Kotake D., Tamura H.","6603419778;57203314186;8365662100;35306198600;","Data compression and hardware implementation of ray-space rendering for interactive augmented virtuality",2002,"Presence: Teleoperators and Virtual Environments","11","2",,"203","219",,1,"10.1162/1054746021470630","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036555664&doi=10.1162%2f1054746021470630&partnerID=40&md5=37b41237eda525396f3fa6364f6e3eb0","Mixed Reality Systems Lab. Inc., 2-2-1 Nakane, Meguro-ku, Tokyo 152-0031, Japan","Sakagawa, Y., Mixed Reality Systems Lab. Inc., 2-2-1 Nakane, Meguro-ku, Tokyo 152-0031, Japan; Katayama, A., Mixed Reality Systems Lab. Inc., 2-2-1 Nakane, Meguro-ku, Tokyo 152-0031, Japan; Kotake, D., Mixed Reality Systems Lab. Inc., 2-2-1 Nakane, Meguro-ku, Tokyo 152-0031, Japan; Tamura, H., Mixed Reality Systems Lab. Inc., 2-2-1 Nakane, Meguro-ku, Tokyo 152-0031, Japan","This article describes approaches to solve two drawbacks of ray-space representation: the large amount of data necessary to represent an object and the massive use of CPU to render an image. Ray-space representation, an image-based rendering technique, is used in our interactive augmented virtuality system. We developed a compression method optimized for ray-space data and a hardware architecture to render images from ray-space data. The compression method uses a hybrid combination of motion-compensated prediction, discrete cosine transform, and vector quantization. The proposed method compresses the data while assuring random and fast access to the decoded data. The dedicated hardware architecture for consumer PCs to interactively render photorealistic images using ray-space representation is also described. This hardware architecture is used to efficiently transfer the processing load from the CPU to the hardware. These two improvements help to use PCs that do not have much memory and CPU resources in applications such as an interactive virtual museum, in which the scenes are generated from both geometric model data and ray-space data.",,"Ray-space representation; Computer hardware; Data compression; Personal computers; Vector quantization; Virtual reality",Article,"Final","",Scopus,2-s2.0-0036555664
"Kaufmann H., Schmalstieg D.","7203004662;55101019100;","Mathematics and geometry education with collaborative augmented reality",2002,"ACM SIGGRAPH 2002 Conference Abstracts and Applications, SIGGRAPH 2002",,,,"37","41",,79,"10.1145/1242073.1242086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945962878&doi=10.1145%2f1242073.1242086&partnerID=40&md5=cd3ae11246c24f1a573439d80740e178","Vienna University of Technology, Austria","Kaufmann, H., Vienna University of Technology, Austria; Schmalstieg, D., Vienna University of Technology, Austria","Construct3D is a three-dimensional geometric construction tool specifically designed for mathematics and geometry education. It is based on the mobile collaborative augmented reality system ""Studierstube."" We describe our efforts in developing a system for the improvement of spatial abilities and maximization of transfer of learning. In order to support various teacher-student interaction scenarios we implemented flexible methods for context and user dependent rendering of parts of the construction. Together with hybrid hardware setups they allow the use of Construct3D in today's classrooms and provide a test bed for future evaluations. Means of application and integration in mathematics and geometry education at the high school, as well as the university, level are being discussed. Anecdotal evidence supports our claim that Construct3D is easy to learn, encourages experimentation with geometric constructions, and improves spatial skills.","Augmented reality; Geometry education; Mathematics education; Spatial intelligence","Augmented reality; Computer graphics; Geometry; Interactive computer graphics; Teaching; Anecdotal evidences; Collaborative augmented realities; Geometric construction; Mathematics education; Spatial abilities; Spatial intelligence; Student interactions; Transfer of learning; Education",Conference Paper,"Final","",Scopus,2-s2.0-84945962878
"Mason A.H., MacKenzie C.L.","7203073987;7202852924;","The effects of visual information about self-movement on grasp forces when receiving objects in an augmented environment",2002,"Proceedings - 10th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, HAPTICS 2002",,, 998947,"105","112",,6,"10.1109/HAPTIC.2002.998947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949802915&doi=10.1109%2fHAPTIC.2002.998947&partnerID=40&md5=fc9a763ab672810de58720a8db15c7ec","School of Kinesiology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Department of Kinesiology, University of Wisconsin - Madison, 2000 Observatory Drive, Madison, WI  53706-1189, United States","Mason, A.H., School of Kinesiology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada, Department of Kinesiology, University of Wisconsin - Madison, 2000 Observatory Drive, Madison, WI  53706-1189, United States; MacKenzie, C.L., School of Kinesiology, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada","This work explored how the presence of visual information about self-movement affected grasp forces when receiving an object from a partner. Twelve subjects either reached to grasp or grasped without reaching objects that were passed by a partner or rested on a table surface. Visual feedback about self-movement was available for half the trials and was removed for the other half. Results indicated that a graphic representation of self-movement significantly decreased transfer time when objects were passed between subjects. Results also indicated decreased time to peak grip force and peak grip force rate by the receiver with this visual feedback. These results suggest that grip force production on objects acquired from another person benefit from a crude graphical representation of the finger pads. Furthermore, these results suggest that sources of sensory feedback cannot be studied in isolation. Instead we must consider how feedback modalities are integrated for successful interaction. Implications for the design of virtual environments and integrated feedback devices are discussed. © 2002 IEEE.","Computer graphics; Electrical capacitance tomography; Force feedback; Haptic interfaces; Humans; Production; RAKE receivers; Read only memory; Tellurium; Virtual environment","Computer graphics; Electric impedance tomography; Haptic interfaces; Mice (computer peripherals); Production; Remote control; ROM; Sensory feedback; Signal receivers; Tellurium; Tellurium compounds; Virtual reality; Visual communication; Augmented environments; Electrical Capacitance Tomography; Force feedback; Graphic representation; Graphical representations; Humans; RAKE receiver; Visual information; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84949802915
"Stedmon A.W., Stone R.J.","6507120158;7402456435;","Re-viewing reality: Human factors of synthetic training environments",2001,"International Journal of Human Computer Studies","55","4",,"675","698",,40,"10.1006/ijhc.2001.0498","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035497284&doi=10.1006%2fijhc.2001.0498&partnerID=40&md5=c830af567662fa8dc99db5ea3953cd89","HUSAT Research Institute, Loughborough University, Loughborough, LE11 1RG, United Kingdom; Virtual Presence Ltd, Chester House, 79 Dane Road, Sale M33 7BP, United Kingdom; Virutal Reality Appl. Res. Team, University of Nottingham, University Park, Nottingham NG7 2RD, United Kingdom","Stedmon, A.W., HUSAT Research Institute, Loughborough University, Loughborough, LE11 1RG, United Kingdom, Virutal Reality Appl. Res. Team, University of Nottingham, University Park, Nottingham NG7 2RD, United Kingdom; Stone, R.J., Virtual Presence Ltd, Chester House, 79 Dane Road, Sale M33 7BP, United Kingdom","Computer-based training (CBT) has become an important training tool and is used effectively in providing part-task activities. In the military domain virtual environments (VEs) have long been exploited, mainly through virtual reality (VR), to create realistic working environments. More recently, augmented reality (AR) and advanced embedded training (AET) concepts have also emerged and the development of ""AR-AET"" and ""VR-CBT"" concepts promise to become essential tools within military training. Whilst the advantages of both AR and VR are attractive, the challenges for delivering such applications are, generally, technology led. Equally as important, however, is the incorporation of human factors design and implementation techniques and this has been recognized by the development and publication of International Standard ISO 13407, Human-Centred Design Processes for Interactive Systems. Examples described in this paper serve to review Human Factors issues associated With the use of both AR and VR training systems. Whilst there are common issues between AR and VR applications in considering the potential of synthetic training environments, it is also necessary to address particular human-centred design issues within each application domain. © 2001 Academic Press.","Augmented reality; Computer based training; Human factors of synthetic training; Virtual environments; Virtual reality","Computer aided design; Personnel training; Technology transfer; Virtual reality; Computer-based training (CBT); Human engineering",Conference Paper,"Final","",Scopus,2-s2.0-0035497284
"Gourlay D., Lun K.C., Lee Y.N., Tay J.","15739566700;7004523484;15739807900;57196704926;","Virtual reality for relearning daily living skills",2000,"International Journal of Medical Informatics","60","3",,"255","261",,42,"10.1016/S1386-5056(00)00100-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034532146&doi=10.1016%2fS1386-5056%2800%2900100-3&partnerID=40&md5=e662b908faaf96c9ec30e221d781378e","Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore","Gourlay, D., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Lun, K.C., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Lee, Y.N., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore; Tay, J., Medical Informatics Programme, Department of Community, Occupational and Family Medicine, National University of Singapore, Lower Kent Ridge Road, 119260 Singapore, Singapore","The explosive increase in the power of computers has enabled the creation of fast, interactive 3D environments, sometimes called virtual reality (VR). This technology, often associated with arcade games, is increasingly being used for more serious applications. This paper describes research showing transfer of skills from a virtual environment to the real world. We then describe our VR authoring tool and an application to help cognitively impaired individuals relearn important daily living skills. Additionally we describe the development of a prototype networked system to enable a doctor to monitor remotely the rehabilitation of a group of patients. © 2000 Elsevier Science Ireland Ltd.",,"Computer applications; Health care; Patient monitoring; Patient rehabilitation; Telemedicine; Head mounted display; Traumatic brain injury; Virtual reality; cognitive defect; computer; daily life activity; human; learning; patient monitoring; physician attitude; priority journal; rehabilitation medicine; review; technology; virtual reality; Activities of Daily Living; Brain Injuries; Cerebrovascular Accident; Cognition Disorders; Humans; Learning; Therapy, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-0034532146
"Beaudouin-Lafon M.","6602828964;","Instrumental interaction: An interaction model for designing post-WIMP user interfaces",2000,"Conference on Human Factors in Computing Systems - Proceedings",,,,"446","453",,289,"10.1145/332040.332473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033701492&doi=10.1145%2f332040.332473&partnerID=40&md5=4760ac09d71760f3af6f5cb9f2df252c","Dept of Computer Science, University of Aarhus, Aabogade 34, DK-8200 Aarhus N, Denmark","Beaudouin-Lafon, M., Dept of Computer Science, University of Aarhus, Aabogade 34, DK-8200 Aarhus N, Denmark","This article introduces a new interaction model called Instrumental Interaction that extends and generalizes the principles of direct manipulation. It covers existing interaction styles, including traditional WIMP interfaces, as well as new interaction styles such as two-handed input and augmented reality. It defines a design space for new interaction techniques and a set of properties for comparing them. Instrumental Interaction describes graphical user interfaces in terms of domain objects and interaction instruments. Interaction between users and domain objects is mediated by interaction instruments, similar to the tools and instruments we use in the real world to interact with physical objects. The article presents the model, applies it to describe and compare a number of interaction techniques, and shows how it was used to create a new interface for searching and replacing text. Copyright ACM 2000.","Direct manipulation; Instrumental interaction; Interaction model; Post-WIMP interfaces; WIMP interfaces","Design spaces; Direct manipulation; Instrumental interaction; Interaction model; Interaction styles; Interaction techniques; Physical objects; Two-handed input; Augmented reality; Computer systems; Graphical user interfaces; Human computer interaction; Human engineering; Degrees of freedom (mechanics); Mice (computer peripherals); Virtual reality; Instruments; Human computer interaction; Augmented reality; Direct manipulation; Domain objects; Instrumentation Interaction; Interaction instruments; Windows Icons Menus and Pointing interfaces",Conference Paper,"Final","",Scopus,2-s2.0-0033701492
"Pyssysalo T., Repo T., Turunen T., Lankila T., Röning J.","6506266607;7005816838;7004232184;57035285800;6701703474;","CyPhone - Bringing augmented reality to next generation mobile phones",2000,"Proceedings of DARE 2000 on Designing Augmented Reality Environments",,,,"11","21",,12,"10.1145/354666.354668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978868040&doi=10.1145%2f354666.354668&partnerID=40&md5=f959ef8fbb122825843c0a86d16604ba","University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland","Pyssysalo, T., University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland; Repo, T., University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland; Turunen, T., University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland; Lankila, T., University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland; Röning, J., University of Oulu, Department of Electrical Engineering, Computer Engineering Laboratory, University of Oulu, P.O.Box 450090014, Finland","We describe a prototype implementation of a future mobile phone called CyPhone. In addition to voice calls, it has been designed to support context-specific and multi-user multimedia services in an augmented reality manner. Context-awareness has been implemented with GPS-based navigation techniques and a registration algorithm, capable of detecting a predefined 3-D model or a landmark in the environment. A new adaptive transport protocol has been developed to support real-time packet-switched data transfer between concurrent users of mobile augmented reality applications. The prototype itself is based on PC/104+ architecture. As a case example we describe an augmented reality-based personal navigation service. Copyright 2000 ACM.","Mobile communication; Navigation; Networked virtual reality; Realtime data transport protocols; Registration","Cellular telephones; Data transfer; Mobile phones; Mobile telecommunication systems; Multimedia services; Navigation; Packet switching; Telephone sets; Three dimensional computer graphics; Virtual reality; GPS-based navigation; Mobile augmented reality; Mobile communications; Personal navigation; Prototype implementations; Real-time data; Registration; Registration algorithms; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-84978868040
"Brereton M., McGarry B.","12238974100;6603055253;","An observational study of how objects support engineering design thinking and communication: Implications for the design of tangible media",2000,"Conference on Human Factors in Computing Systems - Proceedings",,,,"217","224",,79,"10.1145/332040.332434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033700825&doi=10.1145%2f332040.332434&partnerID=40&md5=9fefa3ddb27912844ae483d11b5cb3fc","Department of Computer Science and Electrical Engineering, University of Queensland, Brisbane, QLD 4072, Australia","Brereton, M., Department of Computer Science and Electrical Engineering, University of Queensland, Brisbane, QLD 4072, Australia; McGarry, B., Department of Computer Science and Electrical Engineering, University of Queensland, Brisbane, QLD 4072, Australia","There has been an increasing interest in objects within the HCI field particularly with a view to designing tangible interfaces. However, little is known about how people make sense of objects and how objects support thinking. This paper presents a study of groups of engineers using physical objects to prototype designs, and articulates the roles that physical objects play in supporting their design thinking and communications. The study finds that design thinking is heavily dependent upon physical objects, that designers are active and opportunistic in seeking out physical props and that the interpretation and use of an object depends heavily on the activity. The paper discusses the trade-offs that designers make between speed and accuracy of models, and specificity and generality in choice of representations. Implications for design of tangible interfaces are discussed. Copyright ACM 2000.","Augmented reality; Cognitive models; Design thinking; Interaction design; Tangible media; User models","Augmented reality; Economic and social effects; Human engineering; Communication; Computer aided design; Response time (computer systems); Social sciences computing; User interfaces; Virtual reality; Cognitive model; Design thinking; Interaction design; Tangible media; User models; Augmented reality; Cognitive models; Design thinking; Interaction design; Tangible media; User models; Human computer interaction; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-0033700825
"Rekimoto J., Saitoh M.","6603848632;7201721667;","Augmented surfaces: A spatially continuous work space for hybrid computing environments",1999,"Conference on Human Factors in Computing Systems - Proceedings",,,,"378","385",,383,"10.1145/302979.303113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032642174&doi=10.1145%2f302979.303113&partnerID=40&md5=945f2727194e9adc96c4f418db896c99","Sony Computer Science Laboratories Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 14 l-0022, Japan; Department of Computer Science, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223, Japan","Rekimoto, J., Sony Computer Science Laboratories Inc., 3-14-13 Higashigotanda, Shinagawa-ku, Tokyo 14 l-0022, Japan; Saitoh, M., Department of Computer Science, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223, Japan","This paper describes our design and implementation of a computer augmented environment that allows users to smoothly interchange digital information among their portable computers, table and wall displays, and other physical objects. Supported by a camera-based object recognition system, users can easily integrate their portable computers with the pre-installed ones in the environment. Users can use displays projected on tables and walls as a spatially continuous extension of their portable computers. Using an interaction technique called hyperdragging, users can transfer information from one computer to another, by only knowing the physical relationship between them. We also provide a mechanism for attaching digital data to physical objects, such as a videotape or a document folder, to link physical and digital spaces. Copyright © 2012 ACM, Inc.","Architectural media; Augmented reality; Multiple device user interfaces; Physical space; Portable computers; Table-sized displays; Ubiquitous computing; Wall-sized displays","Architectural media; Multiple devices; Physical space; Portable computers; Wall-sized displays; Augmented reality; Computer systems; Copyrights; Microcomputers; Object recognition; Portable equipment; Ubiquitous computing; User interfaces; Cameras; Display devices; Graphical user interfaces; Pattern recognition systems; Virtual reality; Display devices; Human computer interaction; Hybrid computing environments; Multiple device user interfaces",Conference Paper,"Final","",Scopus,2-s2.0-0032642174
"Yoneda M., Arai F., Fukuda T., Miyata K., Naito T.","7202342356;7102069340;36037263500;7201750472;36641561600;","VR training system with adaptive operational assistance considering straight-line transfer operation",1999,"Robot and Human Communication - Proceedings of the IEEE International Workshop",,,,"142","147",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033487867&partnerID=40&md5=bfb8dbc69dfb62cccc284f0c6b8cdd44","Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan","Yoneda, M., Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan; Arai, F., Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan; Fukuda, T., Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan; Miyata, K., Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan; Naito, T., Nagoya University, Dept. of Micro System Engineering, Furo-cho 1, Chikusa-ku, Nagoya 464-8603, Japan","This paper deals with an operational assistance system of a rough terrain crane. A proper control rule to operate the payload in the straight-line motion is proposed. The system assists straight-line operation with force display based on the rule. The strength of assistance can be adapted to operator's skill level by changing the gain of force display. We present some experiments on a crane simulator, and show the effectiveness of the proposed operational assistance system.",,"Adaptive systems; Artificial intelligence; Control system analysis; Cranes; Display devices; Motion control; Simulators; Straight-line transfer operation; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0033487867
"Brown Judith R., van Dam Andy, Earnshaw Rae, Encarnacao Jose, Guedj Richard, Preece Jennifer, Shneiderman Ben, Vince John","7409454038;7005933610;7004754547;55996501100;57213366006;7005115923;7006313615;7004934617;","Human-centered computing, online communities and virtual environments",1999,"Insulation Journal",,"AUG.",,"42","62",,15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032599405&partnerID=40&md5=1874c00e9c132e477f104dca4c1fecb3","Univ of Iowa, United States","Brown, Judith R., Univ of Iowa, United States; van Dam, Andy, Univ of Iowa, United States; Earnshaw, Rae, Univ of Iowa, United States; Encarnacao, Jose, Univ of Iowa, United States; Guedj, Richard, Univ of Iowa, United States; Preece, Jennifer, Univ of Iowa, United States; Shneiderman, Ben, Univ of Iowa, United States; Vince, John, Univ of Iowa, United States","Presented is a report on the First Joint European Commission/National Science Foundation Advanced Research Workshop in June 1-4, 1999, Chateau de Bonas, France. The Workshop focused on research frontiers of human-computer interaction and virtual environments, particularly the desires that interaction be more centered around human needs and capabilities, and that the human environment be considered in virtual environments and in other contextual information processing activities.",,"Interfaces (computer); Mobile computing; Online systems; Technical presentations; Technology transfer; Virtual reality; Augmented reality; Component based software; Human centered computing; Information technology",Article,"Final","",Scopus,2-s2.0-0032599405
"Camurri A., Ferrentino P.","7003668526;6508049506;","Interactive environments for music and multimedia",1999,"Multimedia Systems","7","1",,"32","47",,29,"10.1007/s005300050109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032642024&doi=10.1007%2fs005300050109&partnerID=40&md5=c3ef0fb632897e284e959d6db11636c3","Laboratory of Musical Informatics, DIST - Dept. Info., Syst. Telecom., University of Genoa, Viale Causa 13, I-16145 Genoa, Italy","Camurri, A., Laboratory of Musical Informatics, DIST - Dept. Info., Syst. Telecom., University of Genoa, Viale Causa 13, I-16145 Genoa, Italy; Ferrentino, P., Laboratory of Musical Informatics, DIST - Dept. Info., Syst. Telecom., University of Genoa, Viale Causa 13, I-16145 Genoa, Italy","Multimodal Environments (MEs) are systems capable of establishing creative, multimodal user interaction by exhibiting real-time adaptive behaviour. In a typical scenario, one or more users are immersed in an environment allowing them to communicate by means of full-body movement, singing or playing. Users get feedback from the environment in real time in terms of sound, music, visual media, and actuators, i.e. movement of semi-autonomous mobile systems including mobile scenography, on-stage robots behaving as actors or players, possibly equipped with music and multimedia output. MEs are therefore a sort of extension of augmented reality environments. From another viewpoint, an ME can be seen as a sort of prolongation of the human mind and senses. From an artificial intelligence perspective, an ME consists of a population of physical and as software agents capable of changing their reactions and their social interaction over time. For example, a gesture of the user(s) can mean different things in different situations, and can produce changes in the agents populating the ME. The paradigm adopted for movement recognition is that of a human observer of the dance, where the focus of attention changes according to the evolution of the dance itself and of the music produced. MEs are therefore agents able to observe the user, extract `gesture gestalts', and change their state, including artificial emotions, over time. MEs open new niches of application, many still to be discovered, including music, dance, theatre, interactive arts, entertainment, interactive exhibitions and museal installations, information atelier, edutainment, training, industrial applications and cognitive rehabilitation (e.g. for autism). The environment can be a theatre, a museum, a discotheque, a school classroom, a rehabilitation centre for patients with a variety of sensory/motor and cognitive impairments, etc. The ME concept generalizes the bio-feedback methods which already have found widespread applications. The paper introduces MEs, then a flexible ME architecture, with a special focus on the modeling of the emotional component of the agents forming an ME. Description of four applications we recently developed, currently used in several real testbeds, conclude the paper.",,"Acoustic waves; Artificial intelligence; Computer architecture; Interactive computer systems; Mobile robots; Real time systems; Virtual reality; Multimodal environment (ME) systems; Multimedia systems",Article,"Final","",Scopus,2-s2.0-0032642024
"Nurkkala Eero A., Pyssysalo Tino, Roning Juha","55951082600;6506266607;6701703474;","Wireless video monitoring and robot control in security applications",1998,"Proceedings of SPIE - The International Society for Optical Engineering","3522",,,"104","113",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032292824&partnerID=40&md5=f50b1fe038b9e306c684ccbb62b05f45","Dep of Electrical Engineering, Oulu, Finland","Nurkkala, Eero A., Dep of Electrical Engineering, Oulu, Finland; Pyssysalo, Tino, Dep of Electrical Engineering, Oulu, Finland; Roning, Juha, Dep of Electrical Engineering, Oulu, Finland","This research focuses on applications based on wireless monitoring and robot control, utilizing motion image and augmented reality. These applications include remote services and surveillance-related functions such as remote monitoring. A remote service can be, for example, a way to deliver products at a hospital or old people's home. Due to the mobile nature of the system, monitoring at places with privacy concerns is possible. On the other hand, mobility demands wireless communications. Suitable and present technologies for wireless video transfer are weighted. Identification of objects with the help of Radio Frequency Identifying (RFID) technology and facial recognition results in intelligent actions, for example, where the control of a robot does not require extensive workload from the user. In other words, tasks can be partially autonomous. RFID can be also used in augmentation of the video view with virtual objects. As a real-life experiment, a prototype environment is being constructed that consists of a robot equipped with a video camera and wireless links to the network and multimedia computer.",,"Augmented reality; Robot control; Wireless video monitoring; Computer vision; Mobile robots; Object recognition; Radio links; Remote control; Robot applications; Security systems; Video cameras; Virtual reality; Wireless telecommunication systems; Intelligent robots",Conference Paper,"Final","",Scopus,2-s2.0-0032292824
"Nemire K.","6602500249;","Individual combatant simulator for tactics training and mission rehearsal",1998,"Proceedings of SPIE - The International Society for Optical Engineering","3295",,,"435","441",,1,"10.1117/12.307192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032224291&doi=10.1117%2f12.307192&partnerID=40&md5=fb459fea6a57d3716ddbdcbb1bd3e990","Interface Technologies Corporation, 1840 Forty-First Avenue, Suite 102, Capitola, CA 95010, United States","Nemire, K., Interface Technologies Corporation, 1840 Forty-First Avenue, Suite 102, Capitola, CA 95010, United States","This individual combatant simulator (ICS) provides ground force leaders opportunities to practice tactical skills on the simulated battlefield by directing dismounted computer-generated forces in combatant and non-combatant exercises. Integrated hardware and software systems allow leaders to operate on the simulated battlefield as they would on the physical battlefield using combinations of voice commands, arm signals, virtual tools, and virtual weapons. Hardware components include image generator, head-mounted display, 3-D sound, spatial tracking, instrumented glove, synthesized speech, and voice recognition systems. The training simulator can be operated on a network. Four types of evaluations, including performance of authentic tasks and subjective evaluations, were conducted using dismounted infantry soldiers and university students as participants. The results indicated that the ICS was easy to learn and use, could be used to conduct training exercises, supported skillful performance in training exercises, and was engaging and compelling for the users. These initial evaluations indicated ease of learning and use of the simulator, as well as the potential for training effectiveness.","Dismounted infantry; Electronic battlefield; Immersive; Mission rehearsal; Simulator; Training; Virtual","Computer hardware; Data transfer; Decision making; Interoperability; Military applications; Simulators; Speech recognition; User interfaces; Individual combatant simulator; Tactics training; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-0032224291
"Panayi M., Roy D.M.","6506644381;55025762500;","BodyTek: Technology enhanced interactive physical theatre for people with cognitive impairment",1998,"Proceedings of the 6th ACM International Conference on Multimedia: Technologies for Interactive Movies, MULTIMEDIA 1998",,,,"35","38",,1,"10.1145/306774.306785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957621873&doi=10.1145%2f306774.306785&partnerID=40&md5=356a2d3beacfe0cab40bbbfc0d36302e","Natural Interactive Systems Laboratory, Odense Unwersity, Forskeparken, 10, Odense M, 5230, Denmark","Panayi, M., Natural Interactive Systems Laboratory, Odense Unwersity, Forskeparken, 10, Odense M, 5230, Denmark; Roy, D.M., Natural Interactive Systems Laboratory, Odense Unwersity, Forskeparken, 10, Odense M, 5230, Denmark","We show how non-encumbering interactive technology m the form of a ""virtual sound space"" typically used in traditional interactive performance and installation can be adapted to create an exploratory and improvisational environment that facilitates the co-construction of Imaginary scenarios and narrative. During workshop sessions involving students with severe to profound cognitive impairment, we describe the emergence of enactment consistent with physical theatre. Within the study group, there were students who became more engaged and expressive than had previously been observed in normal drama sessions. Applications within educational, creative and therapeutic programmes for people with cognitive impairment are proposed. We discuss how the outcomes of this work can be generalized to form the basis of a theoretical framework for a new interactive digital medium that facilitates improvisation, co-construction and reflection. © 1998 ACM.","Augmented reality; Co-construction of narrative; Cognitive impairment; Creative and Imaginative play; Gestural human-computer interaction; Mime; Physical theatre; Unencumbered interactive media","Augmented reality; Theaters; Co-construction; Cognitive impairment; Creative and Imaginative play; Interactive media; Mime; Physical theatre; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84957621873
"Mouchtaris A., Lim J.-S., Holman T., Kyriakakis C.","6602611014;7403454149;7005598991;7004689290;","Head-related transfer function synthesis for immersive audio",1998,"1998 IEEE 2nd Workshop on Multimedia Signal Processing","1998-December",,,"155","160",,6,"10.1109/MMSP.1998.738928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871622816&doi=10.1109%2fMMSP.1998.738928&partnerID=40&md5=d87d06b8590f112dceb9c97bbee89668","Integrated Media Syst. Center, Univ. of Southern California, 3740 McClintock Ave., Los Angeles, CA  90089-2564, United States","Mouchtaris, A., Integrated Media Syst. Center, Univ. of Southern California, 3740 McClintock Ave., Los Angeles, CA  90089-2564, United States; Lim, J.-S., Integrated Media Syst. Center, Univ. of Southern California, 3740 McClintock Ave., Los Angeles, CA  90089-2564, United States; Holman, T., Integrated Media Syst. Center, Univ. of Southern California, 3740 McClintock Ave., Los Angeles, CA  90089-2564, United States; Kyriakakis, C., Integrated Media Syst. Center, Univ. of Southern California, 3740 McClintock Ave., Los Angeles, CA  90089-2564, United States","Immersive audio systems are being envisioned for applications that include teleconferencing and telepresence; augmented and virtual reality for manufacturing and entertainment; air traffic control, pilot warning, and guidance systems; displays for the visually-or aurally-impaired; home entertainment; distance learning; and professional sound and picture editing for television and film. The principal function of such systems is to synthesize, manipulate, and render sound fields in real time. In this paper we examine the limitations that are inherent in spatial sound delivery over loudspeakers and propose a method that generates virtual sound sources based on synthetic head-related transfer functions with the same spectral characteristics as those of the real source. © 1998 IEEE.",,"Acoustic fields; Air navigation; Air traffic control; Audio acoustics; Audio systems; Distance education; Multimedia signal processing; Real time systems; Teleconferencing; Television applications; Television systems; Virtual reality; Visual communication; Augmented and virtual realities; Guidance system; Head related transfer function; Home entertainment; Immersive audio; Principal functions; Spectral characteristics; Virtual sound sources; Transfer functions",Conference Paper,"Final","",Scopus,2-s2.0-84871622816
"Feng Chen-Chin, Yang Shi-Nine","7402911438;7408515899;","Parallel hierarchical radiosity algorithm for complex scenes",1997,"Proceedings of the IEEE Symposium on Parallel Rendering, PRS",,,,"71","77",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031332258&partnerID=40&md5=41190ba23adb9d1e1ed708fa4c509425","Natl Tsing Hua Univ","Feng, Chen-Chin, Natl Tsing Hua Univ; Yang, Shi-Nine, Natl Tsing Hua Univ","The radiosity method is known for generating high quality images of diffuse environments. However it suffers from extensive computation and requires a great amount of memory, especially when the given scene is rather complex. As parallel computings become widely available, it is natural to exploit parallelism to speed up radiosity computations. This paper presents a general parallel radiosity algorithm for complex scenes based on hierarchical method. To achieve the generality the algorithm is primarily designed for a parallel model machine with distributed memory so that it can be realized easily on any set of homogeneous or heterogeneous machines with message passing capability. To cope with the huge volume of input data and to solve the task allocation problem in parallel computation, the complex scene is first subdivided into cells by axial occluders such as walls, floors and ceilings, and then every cells is augmented to form its cell-visible set according to cell's visibility. Then the radiosity of these cell-visible sets are scheduled for parallel processing. In order to facilitate the parallel construction of visible sets, the notion of visible buffer is introduced. Moreover, by exploiting the principle of spatial coherence, a scheduling scheme for visible sets is incorporated so that both the communication and disk access overhead can be reduced. Finally we implement our algorithm on the PVM (Parallel Virtual Machine) programming environment over 8 DEC Alpha 3000 machines to investigate its performance. The experiment results show that our parallel algorithm achieve good speedups.",,"Computer systems programming; Data communication systems; Data storage equipment; Data structures; Image quality; Parallel algorithms; Three dimensional computer graphics; Virtual reality; Parallel hierarchical radiosity algorithms; Parallel virtual machine (PVM) programming environments; Parallel processing systems",Conference Paper,"Final","",Scopus,2-s2.0-0031332258
"Wang Yue, Xuan Jianhua, Sesterhenn Isabell A., Hayes Wendelin S., Ebert David, Lynch John H., Mun Seong K.","55584806908;7004718083;35392562500;7202648856;35361180100;57189634195;7101645460;","Statistical modeling and visualization of localized prostate cancer",1997,"Proceedings of SPIE - The International Society for Optical Engineering","3031",,,"73","84",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031355207&partnerID=40&md5=f991db99b1883393cad753757d50900e","Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA","Wang, Yue, Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Xuan, Jianhua, Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Sesterhenn, Isabell A., Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Hayes, Wendelin S., Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Ebert, David, Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Lynch, John H., Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA; Mun, Seong K., Catholic Univ. of America and, Georgetown Univ. Medical Cent.,, Catholic Univ, DC, USA","In this paper, a statistically significant master model of localized prostate cancer is developed with pathologically- proven surgical specimens to spatially guide specific points in the biopsy technique for a higher rate of prostate cancer detection and the best possible representation of tumor grade and extension. Based on 200 surgical specimens of the prostates, we have developed a surface reconstruction technique to interactively visualize in the clinically significant objects of interest such as the prostate capsule, urethra, seminal vesicles, ejaculatory ducts and the different carcinomas, for each of these cases. In order to investigate the complex disease pattern including the tumor distribution, volume, and multicentricity, we created a statistically significant master model of localized prostate cancer by fusing these reconstructed computer models together, followed by a quantitative formulation of the 3D finite mixture distribution. Based on the reconstructed prostate capsule and internal structures, we have developed a technique to align all surgical specimens through elastic matching. By labeling the voxels of localized prostate cancer by '1' and the voxels of other internal structures by '0', we can generate a 3D binary image of the prostate that is simply a mutually exclusive random sampling of the underlying distribution f cancer to gram of localized prostate cancer characteristics. In order to quantify the key parameters such as distribution, multicentricity, and volume, we used a finite generalized Gaussian mixture to model the histogram, and estimate the parameter values through information theoretical criteria and a probabilistic self-organizing mixture. Utilizing minimally-immersive and stereoscopic interactive visualization, an augmented reality can be developed to allow the physician to virtually hold the master model in one hand and use the dominant hand to probe data values and perform a simulated needle biopsy. An adaptive self- organizing vector quantization method is developed to determine the optimal locations of selective biopsies where maximum likelihood of cancer detection and the best possible representation of tumor grade and extension can be achieved theoretically, thus allowing a comprehensive analysis of pathological information. The preliminary results show that a statistical pattern of localized prostate cancer exists, and a better understanding of disease patterns associated with tumor volume, distribution, and multicentricity of prostate carcinoma can be obtained from the computerized master model.",,"Computer simulation; Elasticity; Image reconstruction; Oncology; Statistical methods; Surgery; Visualization; Maximum likelihood; Prostate cancer; Tumor detection; Medical imaging",Conference Paper,"Final","",Scopus,2-s2.0-0031355207
"Pavlovic V.I., Sharma R., Huang T.S.","13606159500;57218970684;35513984600;","Visual interpretation of hand gestures for human-computer interaction: A review",1997,"IEEE Transactions on Pattern Analysis and Machine Intelligence","19","7",,"677","695",,1202,"10.1109/34.598226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031193007&doi=10.1109%2f34.598226&partnerID=40&md5=66607934d058cc493402b46824983bc3","Beckman Institute, Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL 61801, United States; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA 16802, United States","Pavlovic, V.I., Beckman Institute, Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL 61801, United States, Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA 16802, United States; Sharma, R., Beckman Institute, Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL 61801, United States; Huang, T.S., Beckman Institute, Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL 61801, United States","The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient ""purposive"" approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction. © 1997 IEEE.","Gesture analysis; Hand tracking; Human-computer interaction; Nonrigid motion analysis; Vision-based gesture recognition","Computational complexity; Computer simulation; Computer vision; Human computer interaction; Image analysis; Image understanding; Real time systems; User interfaces; Hand gesture recognition; Nonrigid motion analysis; Visual interpretation; Pattern recognition",Article,"Final","",Scopus,2-s2.0-0031193007
"Szalavári Z., Gervautz M.","6507879085;6701389426;","The personal interaction panel - A two-handed interface for augmented reality",1997,"Computer Graphics Forum","16","3",,"C335","C346",,124,"10.1111/1467-8659.00171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031552897&doi=10.1111%2f1467-8659.00171&partnerID=40&md5=f9ff4637f6c8a299b478e6d5e0613cc3","Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria","Szalavári, Z., Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria; Gervautz, M., Institute of Computer Graphics, Vienna University of Technology, Karlsplatz 13/186/2, A-1040 Vienna, Austria","This paper describes the introduction of a new interaction paradigm to augmented reality applications. The everyday tool handling experience of working with pen and notebooks is extended to create a three dimensional two-handed interface, that supports easy-to-understand manipulation tasks in augmented and virtual environments. In the design step we take advantage from the freedom, given by our very low demands on hardware and augment form and functionality to this device. On the basis of examples from object manipulation, augmented research environments and scientific visualization we show the generality of applicability. Although being in the first stages implementation, we consider the wide spectrum of suitability for different purposes.","3D user interface; Augmented reality; Two handed interaction","Interactive computer graphics; Personal computers; Three dimensional computer graphics; User interfaces; Virtual reality; Visualization; Augmented reality; Object manipulation; Two handed interaction; Human computer interaction",Article,"Final","",Scopus,2-s2.0-0031552897
"Glantz K., Durlach N.I., Barnett R.C., Aviles W.A.","6603248945;7006481672;55663045100;36954569800;","Virtual reality (VR) and psychotherapy: Opportunities and challenges",1997,"Presence: Teleoperators and Virtual Environments","6","1",,"87","105",,66,"10.1162/pres.1997.6.1.87","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031061711&doi=10.1162%2fpres.1997.6.1.87&partnerID=40&md5=61a9f440114db055c95803c2e80b428f","Virtual Reality Therapies, Inc., MIT (VETREC), Cambridge, MA 02139, United States","Glantz, K., Virtual Reality Therapies, Inc., MIT (VETREC), Cambridge, MA 02139, United States; Durlach, N.I., Virtual Reality Therapies, Inc., MIT (VETREC), Cambridge, MA 02139, United States; Barnett, R.C., Virtual Reality Therapies, Inc., MIT (VETREC), Cambridge, MA 02139, United States; Aviles, W.A., Virtual Reality Therapies, Inc., MIT (VETREC), Cambridge, MA 02139, United States","Virtual reality technology is now being used to provide exposure and desensitization for a number of phobic conditions. In this paper, we first review these current applications and discuss the work needed to refine and expand these applications to phobias. We then comment briefly on some preliminary applications of VR technology to mental-health problems outside the domain of phobias. Finally, we consider ways in which VR might be used to further enhance psychotherapy and assist in the treatment of a wide vanety of disorders. Various possible interventions are discussed, along with the technological developments needed to make them possible.",,"adaptive behavior; article; behavior therapy; classification; cognitive therapy; computer interface; computer simulation; human; imagination; instrumentation; memory; methodology; phobia; psychological aspect; psychotherapy; technology; Adaptation, Psychological; Behavior Therapy; Cognitive Therapy; Computer Simulation; Desensitization, Psychologic; Humans; Imagination; Memory; Phobic Disorders; Psychotherapy; Technology Transfer; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-0031061711
"Todorov E., Shadmehr R., Bizzi E.","6701358567;7004335847;7005489607;","Augmented feedback presented in a virtual environment accelerates learning of a difficult motor task",1997,"Journal of Motor Behavior","29","2",,"147","158",,193,"10.1080/00222899709600829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030764586&doi=10.1080%2f00222899709600829&partnerID=40&md5=44a07632de2de2f291c5a062d0fbdfc1","M.I.T., E25-526, Cambridge, MA 02139, United States","Todorov, E., M.I.T., E25-526, Cambridge, MA 02139, United States; Shadmehr, R.; Bizzi, E.","One can use a number of techniques (e.g., from videotaping to computer enhancement of the environment) to augment the feedback that a subject usually receives during training on a motor task. Although some forms of augmented feedback have been shown to enhance performance on isolated isometric tasks during training, when the feedback has been removed subjects have sometimes not been able to perform as well in the 'realworld' task as controls. Indeed, for realistic, nonisometric motor tasks, improved skill acquisition because of augmented feedback has not been demonstrated. In the present experiments, subjects (Experiment 1, N = 42: Experiment 2, N = 21) performed with a system thai was designed for teaching a difficult multijoint movement in a table tennis environment. The system was a fairly realistic computer animation of the environment and included paddles for the teacher and subject, as well as a virtual ball. Each subject attempted to learn a difficult shot by matching the pattern of movements of the expert teacher. Augmented feedback focused the attention of the subject on a minimum set of movement details that were most relevant to the task; feedback was presented in a form that required the least perceptual processing. Effectiveness of training was determined by measuring their performance in the real task. Subjects who received the virtual environment training performed significantly better than subjects who received a comparable amount of real- task practice or coaching. Kinematic analysis indicated that practice with the expert's trajectory served as a basis for performance on the real-world task and that the movements executed after training were subject-specific modifications of the expert's trajectory. Practice with this trajectory alone was not sufficient for transfer to the real task, however: When a critical component of the virtual environment was removed, subjects showed no transfer to the real task.","Augmented feedback; Motor learning; Table tennis; Virtual environment","article; feedback system; human; human experiment; learning; motor performance; normal human; task performance; training; virtual reality",Article,"Final","",Scopus,2-s2.0-0030764586
